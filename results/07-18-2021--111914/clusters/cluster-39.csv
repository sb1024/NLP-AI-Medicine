text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,9739919,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Base Management', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistics', 'subchondral bone', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,588354,0.003249296768356611
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9713512,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2019,647706,-0.005069713361399266
"Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine Project Summary  The objective of this proposal is to provide a robust course of training for Gilmer Valdes, PhD, DABR, a candidate with an excellent foundation in clinical and machine learning research, to enable him to become an independent investigator. The proposed research aims to address a tradeoff between interpretability and accuracy of modern machine learning algorithms which limits their use in clinical practice. The candidate’s central hypothesis is that the current tradeoff is not a law of nature but rather a limitation of current interpretable machine learning algorithms. Towards proving this hypothesis, the candidate, leading a multidisciplinary team, have developed unique mathematical frameworks (MediBoost and the Conditional Interpretable Super Learner) to build interpretable and accurate models. The proposed research will I) implement and extensively benchmark these frameworks and II) use the algorithms develop to solve three clinical problems where potentially suboptimal models are currently used to make clinical decisions: 1) predicting mortality in the Intensive Care Unit, 2) predicting risk of Hospital Acquired Venous Thromboembolism, 3) predicting which prostate cancer patients benefit the most from adjuvant radiotherapy. The candidate’s training and research plan, multidisciplinary by nature, takes advantage of the proximity of UC San Francisco, Stanford and UC Berkeley and proposes a training plan that cannot be easily replicated elsewhere. Recognizing the multidisciplinary nature of the work proposed, the author will be mentored and work closely with a stellar committee from three institutions and different scientific areas (Machine Learning, Biostatistics, Statistics, Hospital Medicine, Cancer Research and Quality Assurance in Medicine): Jerome H. Friedman PhD (Stanford Statistics Department), Mark Van der Laan PhD (Berkeley Biostatistics and Statistics Department), Mark Segal (UCSF Epidimiology and Biostatistics Deparments), Andrew Auerbach MD (UCSF Medicine Department), Felix Y. Feng MD (UCSF Radiation Oncology),and Timothy D. Solberg PhD (UCSF Radiation Oncology). This committee will be coordinated by Dr Solberg. The candidate also counts with a strong a multidisciplinary team of collaborators. Successful completion of the proposed research will develop the next generation of accurate and interpretable Machine Learning algorithms and solve three important clinical problems where linear models are currently used in clinical settings. This proposal has wide-ranging implications across the healthcare spectrum. The intermediate-term goal is for the candidate to acquire the knowledge, technical skills and expertise necessary to submit a successful R01 proposal. PROJECT NARRATIVE Current state of the art machine learning algorithms have a marked tradeoff between accuracy and interpretability. In medicine, where errors can have a dire consequence and knowledge representation and validation is as relevant as accuracy, the development of accurate and interpretable algorithms is of paramount importance. My research project will address a critical public health need by developing machine learning algorithms that are both accurate and interpretable, and apply them to solve specific clinical problems.",Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine,9743021,K08EB026500,"['Address', 'Adjuvant Radiotherapy', 'Algorithms', 'Area', 'Benchmarking', 'Biometry', 'Cancer Patient', 'Classification', 'Clinical', 'Collection', 'Communities', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Doctor of Philosophy', 'Foundations', 'Goals', 'Healthcare', 'Hospitals', 'Institution', 'Intensive Care Units', 'Knowledge', 'Label', 'Laws', 'Libraries', 'Limb structure', 'Linear Models', 'Machine Learning', 'Malignant neoplasm of prostate', 'Mathematics', 'Mediating', 'Medical', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Modernization', 'Nature', 'Patient Triage', 'Patients', 'Performance', 'Physicians', 'Pneumonia', 'Polynomial Models', 'Public Health', 'Radiation Oncology', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'San Francisco', 'Structure', 'Survival Analysis', 'Technical Expertise', 'Testing', 'Thromboembolism', 'Training', 'Trees', 'Validation', 'Venous', 'Work', 'anticancer research', 'artificial neural network', 'asthmatic patient', 'classification trees', 'clinical decision-making', 'clinical practice', 'design', 'improved', 'information organization', 'machine learning algorithm', 'medical specialties', 'mortality', 'multidisciplinary', 'neural network', 'next generation', 'novel', 'quality assurance', 'random forest', 'regression trees', 'standard care', 'statistics', 'task analysis', 'theories']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2019,191912,-0.02506039993886277
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,9733308,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2019,388750,-0.010263360100326944
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9747977,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,115051,0.001539555166720194
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9771473,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Hazard Models', 'Health system', 'Individual', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning algorithm', 'learning strategy', 'multimodality', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'prognostic', 'repository', 'response', 'risk prediction model', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2019,251983,0.013982413175843787
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9664620,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2019,573396,-0.006252749219229742
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9645433,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2019,92070,-0.03142342850753444
"A novel computing framework to automatically process cardiac valve image data and predict treatment outcomes PROJECT SUMMARY  There is a massive amount of clinical three-dimensional (3D) cardiac image data available today in numerous hospitals, but such data has been considerably underutilized in both clinical and engineering analyses of cardiac function. These 3D data offers unique and valuable information, allowing researchers to develop innovative, personalized approaches to treat diseases. Furthermore, using these 3D datasets as input to computational models can facilitate a population-based analysis that can be used to quantify uncertainty in treatment procedures, and can be utilized for virtual clinical trials for innovative device development. However, there are several critical technical bottlenecks preventing simulation-based clinical evaluation a reality: 1) difficulty in automatic 3D reconstruction of thin complex structures such as heart valve leaflets from clinical images, 2) computational models are constructed without mesh correspondence, which makes it challenging to run batch simulations and conduct large patient population data analyses due to inconsistencies in model setups, and 3) computing time is long, which inhibits prompt feedback for clinical use.  A potential paradigm-changing solution to the challenges is to incorporate machine learning algorithms to expedite the geometry reconstruction and computational analysis procedures. Therefore, the objective of this proposal is to develop a novel computing framework, using advanced tissue modeling and machine learning techniques, to automatically process pre-operative clinical image data and predict post-operative clinical outcomes. Transcatheter aortic valve replacement (TAVR) intervention will serve as a testbed for the modeling methods. In Aim 1, we will develop novel shape dictionary learning (SDL) based methods for automatic reconstruction of TAVR patient aortic valves. Through the modeling process, mesh correspondence will be established across the patient geometric models. The distribution and variation of TAVR patient geometries will be described by statistical shape models (SSMs). In Aim 2, population-based FE analysis of the TAVR procedure will be conducted on thousands of virtual patient models generated by the SSMs (Aim 1). A deep neural network (DNN) will be developed and trained to learn the relationship between the TAVR FE inputs and outputs. Successful completion of this study will result in a ML-FE surrogate for TAVR analysis, combining the automated TAVR patient geometry reconstruction algorithms and the trained DNN, to provide fast TAVR biomechanics analysis without extensive re-computing of the model. Furthermore, the algorithms developed in this study can be generalized for other applications and devices. PROJECT NARRATIVE Current clinical image modalities can be utilized to develop patient-specific computational models to pre-operatively plan transcatheter aortic valve replacement (TAVR) procedures. However, the computational modeling and simulation processes are time-consuming, which limits clinical translatability. Thus, the objective of this proposal is to develop algorithms using machine learning techniques to rapidly process and predict TAVR computational simulation outcomes directly from clinical image data.",A novel computing framework to automatically process cardiac valve image data and predict treatment outcomes,9706921,R01HL142036,"['3-Dimensional', 'Adverse event', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Attention', 'Biomechanics', 'Biomedical Computing', 'Clinical', 'Clinical Engineering', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Consumption', 'Coronary Occlusions', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Device Designs', 'Device or Instrument Development', 'Devices', 'Dictionary', 'Dimensions', 'Disease', 'Elements', 'Evaluation', 'Extravasation', 'Feedback', 'Finite Element Analysis', 'Generations', 'Geometry', 'Goals', 'Guidelines', 'Heart Valves', 'Hospitals', 'Hour', 'Human', 'Image', 'Intervention', 'Laboratories', 'Language', 'Learning', 'Left ventricular structure', 'Machine Learning', 'Manuals', 'Methods', 'Mitral Valve', 'Modeling', 'Outcome', 'Output', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Postoperative Period', 'Problem Sets', 'Procedures', 'Process', 'Property', 'Research Personnel', 'Response Elements', 'Running', 'Rupture', 'Sampling', 'Shapes', 'Statistical Data Interpretation', 'Stents', 'Structure', 'Techniques', 'Testing', 'Thinness', 'Time', 'Tissue Model', 'Training', 'Translations', 'Treatment outcome', 'Uncertainty', 'Variant', 'X-Ray Computed Tomography', 'aortic valve', 'aortic valve replacement', 'ascending aorta', 'base', 'calcification', 'clinical application', 'clinical imaging', 'clinical practice', 'clinically translatable', 'deep learning', 'deep neural network', 'heart function', 'heart imaging', 'imaging modality', 'improved', 'innovation', 'machine learning algorithm', 'models and simulation', 'novel', 'patient population', 'personalized approach', 'population based', 'prevent', 'reconstruction', 'research clinical testing', 'simulation', 'speech recognition', 'time resolved data', 'two-dimensional', 'virtual', 'virtual clinical trial']",NHLBI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2019,381629,-0.013739946571871694
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9756832,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Trauma', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'deep learning', 'design', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neurophysiology', 'novel', 'predictive modeling', 'prevent', 'random forest', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,250346,-0.004090958117407291
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9636581,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2019,489919,0.0027991605880493225
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9803774,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2019,631809,0.016537015761885286
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9724345,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'random forest', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2019,292500,-0.00925066404418964
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9731367,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'machine learning algorithm', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2019,410000,-0.07122624888029289
"Development of a novel photocatalytic system for direct deoxyfunctionalization of alcohols involving machine learning Project Summary Development of general and efficient methods for functionalization of alcohols is highly warranted due to the ubiquity and prominence of this functional group in natural products. Such methods would allow for late-stage diversification of complex molecules and, consequently, could have a broad impact in natural product synthesis and preparation of relevant pharmaceutical materials. However, owing to the chemical inertness of alcohols, most methods typically require installation of activating groups for functionalization, making them unattractive from an atom- and step-economical perspective. Nonetheless, many advances have been made. In particular, the Barton-McCombie reaction has become an indispensable tool for reductive functionalization of alcohols. Unfortunately, this transformation requires pre- functionalization of the alcohol substrate, employs highly toxic tin reagents, and invokes the use high reaction temperatures or harmful UV light for initiation of radical intermediates. Furthermore, the overall transformation is limited to H-atom incorporation or reductive coupling with alkenes. Lastly, only a few deoxygenation methods exist that are amenable for late-stage and site-selective deoxygenation in complex systems. Moreover, physical organic chemistry tools available to facilitate the selection of a set of conditions or parameters to afford site-selectivity are limited. In this proposal, we will develop a mild and practical photocatalytic deoxygenation of alcohols. Our strategy will focus on solving the inherent limitation of the Barton McCombie reaction by 1) avoiding the use of toxic tin reagents, 2) obviating the need for pre-functionalization of the alcohol substrate, and 3) allowing for modular coupling of formed alkyl radicals via Ni-catalysis. Specific aim 1 explores the development of a novel photoredox-catalyzed deoxygenation of alcohols. In addition, we outline a general protocol for deoxyfunctionalization of alcohols via inception of the alkyl radical intermediate, formed via β-scission, with various radical electrophiles. Moreover, we highlight an innovative method for the direct cross-coupling of alcohols via metallophotoredox catalysis in both racemic and enantioselective fashion. Specific aim 2 addresses the design strategy for implementing physical chemistry techniques such as Machine Learning in order to facilitate optimization and prediction of reaction performance in multi-dimensional chemical space. Also, we outline applying this strategy to identify a set of optimal conditions to confer site-selective functionalization in complex polyols. Project Narrative Mild and site-controlled deoxygenation of alcohols could significantly accelerate the late-stage synthesis/diversification of important organic molecules; however, current methods often employ toxic tin reagents, harsh reaction conditions, and require prefunctionalization of the alcohols employed. The strategy proposed would allow for a mild photocatalytic deoxygenation, as well as deoxyfunctionalization, of alcohols that solves the aforementioned limitations of prior art. Moreover, the proposed strategy outlines implementation of physical organic chemistry tools Machine Learning in order to facilitate optimization and prediction of reaction performance in multi-dimensional chemical space.",Development of a novel photocatalytic system for direct deoxyfunctionalization of alcohols involving machine learning,9759306,F32GM129910,"['Address', 'Alcohol consumption', 'Alcohols', 'Alkenes', 'Arts', 'Catalysis', 'Chemicals', 'Complex', 'Coupling', 'Development', 'Employment', 'Intercept', 'Machine Learning', 'Methods', 'Natural Products', 'Organic Chemistry', 'Organic Synthesis', 'Performance', 'Pharmacologic Substance', 'Phosphines', 'Physical Chemistry', 'Preparation', 'Protocols documentation', 'Reaction', 'Reagent', 'Site', 'System', 'Techniques', 'Temperature', 'Tin', 'Ultraviolet Rays', 'Visible Radiation', 'alcohol involvement', 'catalyst', 'design', 'functional group', 'innovation', 'novel', 'polyol', 'predictive tools', 'tool']",NIGMS,PRINCETON UNIVERSITY,F32,2019,61226,-0.027122460303526576
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9753295,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Machine Learning', 'Marines', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'contig', 'dark matter', 'deep learning', 'deep learning algorithm', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,279949,0.007258757341071691
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,-0.01689336472728789
"Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications Abstract Atrial fibrillation (AF) is a pervasive disease which affects over 30 million individuals worldwide, in whom it is associated with morbidity and mortality, yet for which therapeutic outcomes are suboptimal. One major limitation to mechanistic and clinical advances in AF is its taxonomy, which is based on number of days of detected AF rather than increasingly reported functional and personalized mechanisms. I reasoned that a digital and scalable AF taxonomy, based on interactions of anatomic and functional factors and clinical features, may better guide existing therapy and catalyze future mechanistic and therapeutic advances. I set out to create a predictive tool to guide therapy in AF patients using machine learning of rich mechanistic data from a large multicenter registry of patients undergoing ablation. I hypothesized that clinically actionable AF phenotypes can be defined by statistical clustering between electrophysiologic features, anatomic regions and clinical indices, that can be uncovered by physiological and statistical quantification and machine learning. I have two Specific Aims: 1) To construct a multimodal digital atlas of atrial fibrillation which registers functional indices at absolute and relative spatial locations in both atria from a multicenter registry, and make this atlas available as an open-source software resource. This deliverable will uniquely map the probability that specific mechanisms will be relevant to AF in a specific patient of given clinical characteristics. Novel pathophysiological phenotypes will be defined via probabilistic interactions in these individual components. 2) To develop a predictive tool using machine learning to estimate the likelihood that ablation at any site(s) will contribute to success tailored to individual characteristics, by learning clusters of electrophysiologic features, clinical indices, and anatomic regions in a training population and applying it to a validation cohort from a large multicenter registry. This project uses state-of-the-art computational tools and statistical methods that may reconcile divergent AF mechanistic hypotheses to define novel functional AF phenotypes and guide therapy. In the process, I will be mentored by world leading mentors, in an extraordinary training environment to facilitate this development into an independent physician-scientist in bioengineering-heart rhythm medicine. Project Narrative This research provides an avenue to define atrial fibrillation in an actionable classification rooted in pathophysiologic and mechanistic observations. Such a classification scheme would further our understanding and refine our conversation about complex arrhythmia in cardiac tissue. Only an understanding at this level is will provide truly effective and safe treatments of each individual patient’s arrhythmic condition.","Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications",9772892,F32HL144101,"['3-Dimensional', 'Ablation', 'Affect', 'Anatomy', 'Anti-Arrhythmia Agents', 'Applications Grants', 'Arrhythmia', 'Atlases', 'Atrial Fibrillation', 'Biomedical Engineering', 'Cardiac', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Communities', 'Comorbidity', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Electrophysiology (science)', 'Enrollment', 'Environment', 'Faculty', 'Foundations', 'Freedom', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Heart Atrium', 'Individual', 'Injury', 'Language', 'Learning', 'Location', 'Machine Learning', 'Maps', 'Measurable', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Mission', 'Morbidity - disease rate', 'Obstructive Sleep Apnea', 'Patients', 'Pharmacotherapy', 'Phenotype', 'Physicians', 'Physiological', 'Plant Roots', 'Population', 'Probability', 'Procedures', 'Process', 'Pulmonary veins', 'Randomized Clinical Trials', 'Registries', 'Reporting', 'Research', 'Resources', 'Scientist', 'Site', 'Statistical Methods', 'Structure', 'Taxonomy', 'Testing', 'Therapeutic', 'Therapy trial', 'Tissues', 'Training', 'Translations', 'United States National Institutes of Health', 'Validation', 'base', 'clinically actionable', 'cohort', 'computer science', 'computerized tools', 'convolutional neural network', 'deep learning', 'digital', 'disease classification', 'health care service utilization', 'heart rhythm', 'improved outcome', 'indexing', 'individual patient', 'mortality', 'multimodality', 'novel', 'open source', 'patient registry', 'patient response', 'patient stratification', 'predictive tools', 'success', 'supervised learning', 'therapy outcome', 'tool', 'trial design']",NHLBI,STANFORD UNIVERSITY,F32,2019,66778,-0.011587517613108196
"Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations Q-Chem is a state-of-the-art commercial computational quantum chemistry program that has aided about 60,000 users in their modeling of molecular processes in a wide range of disciplines, including biology, chemistry, and materials science. In this proposal, we seek to significantly reduce the computational time (now around 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions. Specifically, we propose to use a multiple time step (MTS) simulation method, where a low-level (and less accurate) quantum chemistry method is used to propagate the system (i.e. move all atoms) at each time step (usually 0.5 or 1 fs), and then a high-level (i.e. more accurate and expensive) quantum chemistry method is used to correct the force on the atoms at longer time intervals. In this way, the simulation can be performed at the high-level energy surface in a fraction of time, compared with simulations performed only using the high-level quantum chemical method. In the Phase I proposal, our goal is to allow the high-level force update only once every 40—50 fs by identifying appropriate lower-level theories (Aim 1) and incorporating machine-learning techniques (Aim 2). This will accelerate accurate free energy simulations by 20—25 fold, reducing the overall computer time to around 25,000 CPU hours. Thus, our new MTS simulation method will make it feasible to routinely perform computational studies on enzymatic reaction mechanism. The addition of these new tools will also further strengthen Q-Chem's position as a global leader in the molecular modeling software market, making our program the most efficient and reliable computational quantum chemistry package for simulating large, complex chemical/biological systems. In this project, we seek to significantly reduce the computational time (ca. 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions to ca. 25,000 CPU Hours. Building upon sophisticated quantum mechanics, this can lead to reliable and quick predictions of enzyme activities.",Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations,9778517,R43GM133270,"['Acceleration', 'Accounting', 'Adopted', 'Back', 'Biochemical', 'Biochemical Reaction', 'Biology', 'Biomedical Research', 'Chemicals', 'Chemistry', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Development', 'Discipline', 'Enzymes', 'Foundations', 'Free Energy', 'Goals', 'Hour', 'Hybrids', 'Lead', 'Machine Learning', 'Maps', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Pathway interactions', 'Performance', 'Phase', 'Positioning Attribute', 'Potential Energy', 'Process', 'Protein Conformation', 'Proteins', 'Quantum Mechanics', 'Reaction', 'Recipe', 'Research', 'Research Personnel', 'Sampling', 'Scheme', 'Solvents', 'Surface', 'System', 'Techniques', 'Time', 'Update', 'biological systems', 'computer studies', 'cost', 'density', 'enzyme activity', 'enzyme model', 'improved', 'innovation', 'learning strategy', 'materials science', 'molecular mechanics', 'molecular modeling', 'programs', 'quantum', 'quantum chemistry', 'quantum computing', 'simulation', 'theories', 'time interval', 'tool']",NIGMS,"Q-CHEM, INC.",R43,2019,132011,0.00672286325493475
"Computational Techniques for Advancing Untargeted Metabolomics Analysis PROJECT SUMMARY/ABSTRACT Detecting and quantifying products of cellular metabolism using mass spectrometry (MS) has already shown great promise in biomarker discovery, nutritional analysis and other biomedical research fields. Despite recent advances in analysis techniques, our ability to interpret MS measurements remains limited. The biggest challenge in metabolomics is annotation, where measured compounds are assigned chemical identities. The annotation rates of current computational tools are low. For several surveyed metabolomics studies, less than 20% of all compounds are annotated. Another contributing factor to low annotation rates is the lack of systematic ways of designing a candidate set, a listing of putative chemical identities that can be used during annotation. Relying on exiting databases is problematic as considering the large combinatorial space of molecular arrangements, there are many biologically relevant compounds not catalogued in databases or documented in the literature. A secondary yet important challenge is interpreting the measurements to understand the metabolic activity of the sample under study. Current techniques are limited in utilizing complex information about the sample to elucidate metabolic activity. The goal of this project is to develop computational techniques to advance the interpretation of large-scale metabolomics measurements. To address current challenges, we propose to pursue three Aims: (1) Engineering candidate sets that enhance biological discovery. (2) Developing new techniques for annotation including using deep learning and incremental build out methods to recommend novel chemical structures that best explain the measurements. (3) Constructing probabilistic models to analyze metabolic activity. Each technique will be rigorously validated computationally and experimentally using chemical standards. Two detailed case studies on the intestinal microbiota will allow us to further validate our tools. Microbiota-derived metabolites have been detected in circulation and shown to engage host cellular pathways in organs and tissues beyond the digestive system. Identifying these metabolites is thus critical for understanding the metabolic function of the microbiota and elucidating their mechanisms. The complex test cases will challenge our techniques, provide feedback during development, and allow us to further disseminate our techniques. We will work closely with early adopters of our tools, as proposed in supporting letters, to further validate our tools and encourage wide adoption. All proposed tools will be open source and made accessible through the web. Our tools promise to change current practices in interpreting metabolomics data beyond what is currently possible with databases, current annotation tools, statistical and overrepresentation analysis, or combinations thereof. The use of machine learning and large data sets as proposed herein defines the most promising research direction in metabolomics analysis. PROJECT NARRATIVE  Untargeted Metabolomics is a recently developed technique that allows the measurement of thousands of molecules in a biological sample. This work proposes several novel computational techniques that address limitations of current metabolomics analysis tools. We anticipate that this work will advance discoveries in biomedical research and have direct benefits to human health.",Computational Techniques for Advancing Untargeted Metabolomics Analysis,9886611,R01GM132391,"['Address', 'Adoption', 'Biological', 'Biomedical Research', 'Blood Circulation', 'Case Study', 'Chemical Structure', 'Chemicals', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Engineering', 'Ensure', 'Feedback', 'Goals', 'Health', 'Human', 'Internet', 'Intestines', 'Label', 'Letters', 'Literature', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Medical', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Structure', 'Nutritional', 'Organ', 'Pathway interactions', 'Performance', 'Play', 'Probability', 'Property', 'PubChem', 'PubMed', 'Public Domains', 'Research', 'Research Personnel', 'Role', 'Running', 'Sampling', 'Statistical Models', 'Structure', 'Subject Headings', 'Surveys', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Uncertainty', 'Validation', 'Work', 'annotation  system', 'base', 'biomarker discovery', 'chemical standard', 'combinatorial', 'computerized tools', 'cost', 'dark matter', 'deep learning', 'design', 'drug development', 'drug discovery', 'experimental study', 'gastrointestinal system', 'gut microbiota', 'interest', 'metabolome', 'metabolomics', 'microbiota', 'microbiota metabolites', 'neural network', 'novel', 'nutrition', 'open source', 'physical property', 'small molecule', 'tool']",NIGMS,TUFTS UNIVERSITY MEDFORD,R01,2019,379614,0.013531956441384631
"Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics The human microbiota plays an important role in health and disease, and its therapeutic manipulation is being actively investigated for a wide range of diseases that span every NIH institute. Our microbiota are inherently dynamic, and analyzing these time-dependent properties is key to robustly linking the microbiota to disease, and predicting the effects of therapies targeting the microbiota; indeed, longitudinal microbiome data is being acquired with increasing frequency, and is a major component of many NIH-funded projects. However, there is currently a dearth of computational tools for analyzing microbiome time-series data, which presents several special challenges including high measurement noise, irregular and sparse temporal sampling, and complex dependencies between variables. The objective of this proposal is to introduce new capabilities, improve on, and provide state-of-the-art implementations of tools for analyzing dynamics, or patterns of change in microbiome time-series data. The tools we develop will use Bayesian machine learning methods, which are well-recognized for their strong conceptual and practical advantages, particularly in biomedical domains. Tools will be rigorously tested and validated on synthetic and real human microbiome data, including publicly available datasets and those from collaborators providing 16S rRNA sequencing, metagenomic, and metabolomics data. We propose three specific aims. For Aim 1, we will develop integrated Bayesian machine learning tools for predicting population dynamics of the microbiome and its responses to perturbations. These tools will include a new model that simultaneously learns groups of microbes with similar interaction structure and predicts their behavior over time, and that incorporates prior phylogenetic information. The model will be further improved by incorporating stochastic microbial dynamics and errors in measurements throughout the model. For Aim 2, we will develop Bayesian machine learning tools to predict host status from microbiome dynamics. The tools will learn easily interpretable, human-readable rules that predict host status from microbiome time-series data, and will be further extended to handle a variety of longitudinal study designs. For Aim 3, we will engineer our microbiome dynamics analysis software tools for optimal performance, ease-of- use, maintainability, extensibility, and dissemination to the community. In total, the proposed work will yield a suite of contemporary software tools for analyzing microbiome dynamics, with expected broad use and major impact. The software will allow investigators to answer important scientific and translational questions about the microbiome, including discovering which microbial taxa or their metagenomes are affected over time by perturbations such as changes in diet or invasion by pathogens; predicting the effects of these perturbations over time, including changes in composition or stability of the gut microbiota; and finding temporal signatures in multi-‘omic microbiome data that predict disease risk in the human host. The human microbiota, or collection of micro-organisms living on and within us, plays an important role in health, and when disrupted or abnormal, may contribute to many types of diseases including infections, kidney diseases, bowel diseases, diabetes, heart diseases, arthritis, allergies, brain diseases, and cancer. Sophisticated computer-based tools are needed to make sense of human microbiota data, particularly time- series data, which can yield important insights into how our microbiomes change over time. This work will develop new and improved computer-based tools for analyzing microbiota time-series data, which will be made freely available and will enable scientists to increase our fundamental knowledge about how our microbiota affect us and ultimately to apply this knowledge to prevent and treat human illnesses.",Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics,9787546,R01GM130777,"['16S ribosomal RNA sequencing', 'Affect', 'Algorithms', 'Antibiotics', 'Arthritis', 'Autoimmunity', 'Bayesian learning', 'Behavior', 'Biological Markers', 'Biological Models', 'Brain Diseases', 'Cardiovascular Diseases', 'Childhood', 'Clostridium difficile', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Dependence', 'Diabetes Mellitus', 'Diet', 'Disease', 'Engineering', 'Environmental Exposure', 'Frequencies', 'Funding', 'Health', 'Heart Diseases', 'Human', 'Human Microbiome', 'Hypersensitivity', 'Infection', 'Institutes', 'Intervention', 'Intestines', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Learning', 'Link', 'Longitudinal Studies', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Metagenomics', 'Microbe', 'Modeling', 'Names', 'Noise', 'Oligosaccharides', 'Outcome', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Play', 'Population Dynamics', 'Property', 'Pythons', 'Readability', 'Recurrence', 'Research Design', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Series', 'Shotguns', 'Software Engineering', 'Software Tools', 'Speed', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Work', 'base', 'computerized tools', 'design', 'disorder risk', 'dynamic system', 'gut microbiota', 'human data', 'human microbiota', 'human subject', 'improved', 'insight', 'learning algorithm', 'learning strategy', 'man', 'metabolomics', 'metagenome', 'microbial', 'microbiome', 'microbiome analysis', 'microbiome sequencing', 'microbiota', 'microorganism', 'nervous system disorder', 'novel', 'open source', 'pathogen', 'predictive tools', 'prevent', 'response', 'software development', 'targeted treatment', 'tool']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R01,2019,312939,0.006591634814488331
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,-0.0033539208013623205
"Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization Project Summary  This application is being submitted in response to NOT-RM-19-009 as a supplement to the parent award U54NS093793.  The Common Fund supports a number of resources that can significantly enhance gene and variant prioritization for study in the Model Organisms Screening Center of the Undiagnosed Diseases Network and beyond. To facilitate the use of these resources, we propose to create a tool that can be easily accessed by clinical geneticists and model organism scientists alike.  MARRVEL (Model organism Aggregated Resources for Rare Variant ExpLoration) was created two years ago because important data that is necessary for rare variant analysis for personalized medicine is spread throughout the internet in tens of different locations. To improve efficiency and streamline access to these data sources, we created a web-tool that allows users to query tens of data sources at once, including GTEx, and links to IMPC, the display portal for KOMP2.  In this proposal, our goal is to develop version 2 of MARRVEL to promote the use of Common Fund resources in the rare disease research community for manual and automated data analysis. This goal will be accomplished by developing MARRVEL 2.0 by integrating KOMP2 (IMPC) and PHAROS data and using the aggregated dataset to develop a machine-assisted gene and variant prioritization for diagnosis and animal model generation.  Our goals align with those of the NIH Common Fund to increase the utility of resources for broader use in the biomedical community. Project Narrative  We aim to promote the use of Common Fund resources and facilitate the diagnosis of rare diseases and the subsequent generation of animal models for the Undiagnosed Diseases Network and beyond. This goal will be accomplished by developing the web resource, MARRVEL 2.0.",Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization,9984757,U54NS093793,"['Affect', 'Animal Model', 'Artificial Intelligence', 'Award', 'Clinical', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Data Sources', 'Development', 'Diagnosis', 'Discipline', 'Disease', 'Disease model', 'Drosophila genus', 'Drug Targeting', 'Expert Systems', 'Family', 'Funding', 'Generations', 'Genes', 'Genetic Diseases', 'Genotype-Tissue Expression Project', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genetics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Manuals', 'Medical', 'Medical Genetics', 'Modeling', 'Mus', 'Parents', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Proteins', 'Rare Diseases', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Suggestion', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Time', 'Training', 'United States National Institutes of Health', 'Variant', 'Visit', 'Yeasts', 'Zebrafish', 'base', 'data wrangling', 'design', 'experimental study', 'feeding', 'fly', 'genetic disorder diagnosis', 'genetic variant', 'human data', 'improved', 'interest', 'learning community', 'machine learning algorithm', 'model organisms databases', 'online resource', 'personalized medicine', 'phenotypic data', 'rare genetic disorder', 'rare variant', 'response', 'screening', 'supervised learning', 'tool', 'web-based tool']",NINDS,BAYLOR COLLEGE OF MEDICINE,U54,2019,320000,-0.01176676052131089
"Accelerating phage evolution and tools via synthetic biology and machine learning Summary Phages, which are the naturally evolved predators of bacteria, may hold the key to combating bacterial pathogens, including the looming threat of multidrug resistant bacteria. Phages are viruses which while harmless to humans and have been successfully engineered as tools to separate, concentrate, and detect their bacterial hosts. Additionally, phages have been used as therapeutic agents to treat patients infected with pathogens resistant to known antibiotics. While the potential benefits of phages are numerous, certain limitations must be addressed in order to fully employ them. The central hypothesis of this proposal is that both top-down and bottom-up approaches can be utilized to design and synthesize novel phages, through a combination of synthetic biology and machine learning. This will result in phage-based tools with increased functionality and customizable host ranges. The rationale for the proposed research is that as the threat of bacterial infections including those with multi-drug resistance continues to grow, phages, which have evolved to efficiently recognize and kill bacteria, will become indispensable tools. Therefore, the ability to rapidly design and engineer new phages for biosensing and therapeutics will be a critical advantage to human health. The proposal contains three specific aims which are supported by preliminary data and cited literature. Aim 1: Site-directed conjugation for advanced phage-based biosensors and therapeutics. Under this aim, phages will be modified with alkyne-containing unnatural amino acids allowing their direct conjugation to 1) azide decorated magnetic nanoparticles, and 2) azide terminated polyethylene glycol. The modifications will allow the development of magnetic phages for bacteria separation and detection, and phages that are more effective therapeutics due to their ability to avoid a patient’s innate immune response, respectively. Aim 2: Decoding phage biorecognition elements using machine learning. In this aim, machine learning will be used to model the binding of phages and their bacterial hosts. The model will enable the prediction of host interactions as well as allow the design and synthesis of novel phage tail fibers which can target specific bacterial isolates. Aim 3: Repurposing phage biorecognition for a broader host ranges. Under the final aim, phage-binding proteins will be replaced with those known to recognize conserved regions of the bacterial LPS, resulting in a phage with a much broader host range. This approach is innovative because it uses top-down characterizations for bottom-up design and synthesis of novel phages. Traditional phage screening methods will be replaced with the rapid synthesis of phages, which are optimized for a particular bacterial isolate. Following the successful completion of the specific aims, the expected outcome is the design and synthesis of phages that can be used to target a selected group of bacteria within Enterobacteriaceae for advanced biosensing and therapeutics. A publically available computer model will allow rapid design of custom phage biorecognition elements which can be added to functionalized phages. These technologies will allow researchers to tip the scales of the co-evolutionary arms race between phage and bacteria. Narrative The project is relevant to public health because it accelerates the development of phage-based tools for the rapid detection of bacterial pathogens in human, food, and environmental samples, and the treatment of diseases from multidrug resistant bacteria by integrating machine learning and synthetic biology. Thus, it is specifically relevant to part of NIH's mission that pertains to the diagnosis, prevention, and cure of human diseases.",Accelerating phage evolution and tools via synthetic biology and machine learning,9714883,R01EB027895,"['Acinetobacter baumannii', 'Address', 'Alkynes', 'Amino Acid Sequence', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Azides', 'Bacteria', 'Bacterial Genome', 'Bacterial Infections', 'Bacteriophage T4', 'Bacteriophages', 'Binding', 'Binding Proteins', 'Biosensing Techniques', 'Biosensor', 'CRISPR/Cas technology', 'Capsid', 'Chemistry', 'Clinical', 'Computer Simulation', 'Consumption', 'Custom', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Drug resistance', 'Elements', 'Engineering', 'Enterobacteriaceae', 'Environment', 'Escherichia coli', 'Evolution', 'Family', 'Fiber', 'Food', 'Future', 'Genes', 'Genome', 'Goals', 'Health', 'Human', 'Infection', 'Innate Immune Response', 'Innate Immune System', 'Intervention', 'Life', 'Literature', 'Machine Learning', 'Magnetic nanoparticles', 'Magnetism', 'Methods', 'Mission', 'Modeling', 'Modification', 'Multi-Drug Resistance', 'Multidrug-resistant Acinetobacter', 'Multiple Bacterial Drug Resistance', 'Natural Immunity', 'Outcome', 'Patients', 'Phenotype', 'Polyethylene Glycols', 'Prevention', 'Process', 'Property', 'Public Health', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Site', 'Specificity', 'Surface', 'System', 'Tail', 'Technology', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Training', 'Treatment Efficacy', 'United States National Institutes of Health', 'Viral', 'Virus', 'arm', 'base', 'combat', 'design', 'human disease', 'innovation', 'next generation', 'novel', 'pathogen', 'pathogenic bacteria', 'rapid detection', 'receptor', 'resistance mechanism', 'screening', 'synthetic biology', 'tool', 'unnatural amino acids']",NIBIB,CORNELL UNIVERSITY,R01,2019,666637,-0.002051296295869582
"Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions PROJECT SUMMARY The research interests of my group are rooted in explorations of new and useful conceptual models to improve the control and prediction of noncovalent interactions. Our research involves the use of a variety of computational quantum chemical tools, applications of density functional theory (DFT), cheminformatics, and machine-learning methods. A premise of our research is that aromaticity may be used to modulate many types of noncovalent interactions (such as hydrogen bonding, π-stacking, anion-π interactions). The reciprocal relationship we find, between “aromaticity” in molecules and the strengths of “noncovalent interactions,” is surprising especially since they are typically considered as largely separate ideas in chemistry. The innovation of this research is that it will enable use of intuitive “back-of-the-envelope” electron-counting rules (such as the 4n+2πe Hückel rule for aromaticity) to make predictions of experimental outcomes regarding the impact of noncovalent interactions. A five-year goal is to realize the use of our conceptual models in real synthetic examples prepared by our experimental collaborators. My research vision is to bridge discoveries of innovative concepts to their practical impacts for biomedical and biomolecular research. PROJECT NARRATIVE This research proposal includes four projects that are jointly motivated by the challenge to control and predict noncovalent interactions in organic and biomolecular systems. The proposed work involves applications of a variety of computational quantum chemical tools and synergistic investigations with experimental collaborators. We seek to identify new and useful concepts to guide experimental designs of novel “non-natural” molecular systems (e.g., receptors, biosensors, and hydrogels) that have potential biomedical applications.",Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions,9798401,R35GM133548,"['Anions', 'Back', 'Biosensor', 'Chemicals', 'Chemistry', 'Electrons', 'Experimental Designs', 'Goals', 'Hydrogels', 'Hydrogen Bonding', 'Intuition', 'Investigation', 'Machine Learning', 'Modeling', 'Molecular', 'Outcome', 'Plant Roots', 'Research', 'Research Project Summaries', 'Research Proposals', 'System', 'Vision', 'Work', 'cheminformatics', 'density', 'improved', 'innovation', 'interest', 'learning strategy', 'novel', 'quantum computing', 'receptor', 'theories', 'tool']",NIGMS,UNIVERSITY OF HOUSTON,R35,2019,377200,-0.004169493309363108
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9791176,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Infrastructure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'feature detection', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2019,1339073,-0.007816809302741054
"Image-guided robot for high-throughput microinjection of Drosophila embryos PROJECT SUMMARY This proposal is submitted in response to the NIH Development of Animal Models and Related Biological Materials for Research (R21) program. The proposal develops an image-guided robotic platform that performs the automated delivery of molecular genetic tools and non-genetically encoded reagents such as chemical libraries, fluorescent dyes to monitor cellular processes, functionalized magnetic beads, or nanoparticles into thousands of Drosophila embryos in a single experimental session. The proposed work builds on recent engineering innovations in our collaborative group which has developed image-guided robotic systems that can precisely interface with single cells in intact tissue. The two Specific Aims provide for a systematic development of the proposed technologies. AIM 1 first engineers a robotic platform (‘Autoinjector’) that can scan and image Drosophila embryos in arrays of egg laying plates. We will utilize machine learning algorithms for automated detection of embryos, followed by thresholding and morphology analysis to detect embryo centroids and annotate injection sites. In AIM 2, we will utilize microprocessor-controlled fluidic circuits for programmatic delivery of femtoliter to nanoliter volumes of reagents into individual embryos. We will quantify the efficacy of the Autoinjector by comparing the survival, fertility, and transformation rates of transposon or PhiC31-mediated transgenesis to manual microinjection datasets. Finally, we will demonstrate the efficient delivery of sgRNAs and mutagenesis in the presence of Cas9. This project fits very well within the goals of the program by engineering a novel tool for producing and improving animal models. The Autoinjector will accelerate Drosophila research and empower scientists to perform novel experiments and genome-scale functional genomics screens that are currently too inefficient or labor intensive to be conducted on a large scale and may additionally enable other novel future applications. PROJECT NARRATIVE This proposal develops a technology platform that will enable automated microinjection of molecular genetic tools and non-genetically encoded tools such as chemical libraries, fluorescent dyes, functionalized magnetic beads, or nanoparticles, into thousands of Drosophila embryos in a single experimental session. The successful development of this technology will empower Drosophila biologists to perform screens and develop new applications that are currently too inefficient or labor intensive to contemplate and will accelerate research into the function of the nervous system and the molecular and genetic underpinnings of numerous diseases in this important animal model.",Image-guided robot for high-throughput microinjection of Drosophila embryos,9806367,R21OD028214,"['Animal Model', 'Biocompatible Materials', 'Biological Assay', 'Caliber', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Collection', 'Computer Vision Systems', 'Cryopreservation', 'Data Set', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Drosophila melanogaster', 'Embryo', 'Engineering', 'Expenditure', 'Exploratory/Developmental Grant', 'Fertility', 'Fluorescent Dyes', 'Future', 'Gene Transfer Techniques', 'Genetic', 'Goals', 'Guide RNA', 'Image', 'Individual', 'Injections', 'Investigation', 'Laboratories', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Mediating', 'Methods', 'Microinjections', 'Microprocessor', 'Microscope', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Morphology', 'Motivation', 'Mutagenesis', 'Needles', 'Nervous System Physiology', 'Performance', 'Process', 'Reagent', 'Research', 'Resources', 'Robot', 'Robotics', 'Scanning', 'Scientist', 'Signaling Molecule', 'Site', 'Space Perception', 'System', 'Technology', 'Tissues', 'Transgenes', 'Transgenic Organisms', 'United States National Institutes of Health', 'Work', 'animal model development', 'base', 'biological research', 'cost', 'egg', 'experience', 'experimental study', 'functional genomics', 'gene product', 'genetic manipulation', 'genome-wide', 'image guided', 'improved', 'innovation', 'machine learning algorithm', 'magnetic beads', 'mutant', 'mutation screening', 'nanolitre', 'nanoparticle', 'novel', 'novel strategies', 'programs', 'response', 'robotic system', 'screening', 'small molecule libraries', 'stem', 'technology development', 'tool']",OD,UNIVERSITY OF MINNESOTA,R21,2019,184118,-0.003386462310788058
"Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways ﻿    DESCRIPTION (provided by applicant): This project aims to develop new statistical machine learning methods for metabolomics data from diverse platforms, including targeted and unbiased/global mass spectrometry (MS), labeled MS experiments for measuring metabolic ﬂux and Nuclear Magnetic Resonance (NMR) platforms. Unbiased MS and NMR proﬁling studies result in identifying a large number of unnamed spectra, which cannot be directly matched to known metabolites and are hence often discarded in downstream analyses. The ﬁrst aim develops a novel kernel penalized regression method for analysis of data from unbiased proﬁling studies. It provides a systematic framework for extracting the relevant information from unnamed spectra through a kernel that highlights the similarities and differences between samples, and in turn boosts the signal from named metabolites. This results in improved power in identiﬁcation of named metabolites associated with the phenotype of interest, as well as improved prediction accuracy. An extension of this kernel-based framework is also proposed to allow for systematic integration of metabolomics data from diverse proﬁling studies, e.g. targeted and unbiased MS proﬁling technologies. The second aim pro- vides a formal inference framework for kernel penalized regression and thus complements the discovery phase of the ﬁrst aim. The third aim focuses on metabolic pathway enrichment analysis that tests both orchestrated changes in activities of steady state metabolites in a given pathway, as well as aberrations in the mechanisms of metabolic reactions. The fourth aim of the project provides a uniﬁed framework for network-based integrative analysis of static (based on mass spectrometry) and dynamic (based on metabolic ﬂux) metabolomics measurements, thus providing an integrated view of the metabolome and the ﬂuxome. Finally, the last aim implements the pro- posed methods in easy-to-use open-source software leveraging the R language, the capabilities of the Cytoscape platform and the Galaxy workﬂow system, thus providing an expandable platform for further developments in the area of metabolomics. The proposed software tool will also provide a plug-in to the Data Repository and Coordination Center (DRCC) data sets, where all regional metabolomics centers supported by the NIH Common Funds Metabolomics Program deposit curated data. PUBLIC HEALTH RELEVANCE: Metabolomics, i.e. the study of small molecules involved in metabolism, provides a dynamic view into processes that reﬂect the actual physiology of the cell, and hence offers vast potential for detection of novel biomarkers and targeted therapies for complex diseases. However, despite this potential, the development of computational methods for analysis of metabolomics data lags the rapid growth of metabolomics proﬁling technologies. The current application addresses this need by developing novel statistical machine learning methods for integrative analysis of static and dynamic metabolomics measurements, as well as easy-to-use open-source software to facilitate the application of these methods.",Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways,9667435,R01GM114029,"['Address', 'Adoption', 'Anabolism', 'Area', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Assay', 'Cardiovascular Diseases', 'Cell physiology', 'Cells', 'Characteristics', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Deposition', 'Detection', 'Development', 'Diabetes Mellitus', 'Disease', 'Environment', 'Environmental Risk Factor', 'Equilibrium', 'Funding', 'Galaxy', 'Homeostasis', 'Imagery', 'Knowledge', 'Label', 'Language', 'Letters', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Names', 'Network-based', 'Nuclear Magnetic Resonance', 'Pathway interactions', 'Phase', 'Phenotype', 'Plug-in', 'Procedures', 'Process', 'Prognostic Marker', 'Proteomics', 'Reaction', 'Sampling', 'Signal Transduction', 'Software Tools', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Work', 'base', 'biological systems', 'biomarker discovery', 'data warehouse', 'diagnostic biomarker', 'experimental study', 'flexibility', 'high dimensionality', 'improved', 'insight', 'interest', 'learning strategy', 'metabolome', 'metabolomics', 'new technology', 'novel', 'novel diagnostics', 'novel marker', 'open source', 'programs', 'public health relevance', 'rapid growth', 'response', 'small molecule', 'targeted treatment', 'tool', 'transcriptomics']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,337658,0.020232428570069322
"Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry PROJECT SUMMARY  Cellular interactions with the environment form the basis of health and disease for all organisms. Exposure to nutrients, toxins, and neighboring cells trigger coordinated molecular responses that impact cell function and metabolism in a beneficial, adaptive, or detrimental manner. Although the benefits of multicellularity for the formation of complex tissue structures or the function of entire organ systems has been long appreciated, it has only recently been understood that microbial inhabitants of vertebrates also have a tremendous impact on host cell function and dysfunction. Despite this, an understanding of these interactions has not moved beyond simple associations, and there are virtually no molecular technologies available that adequately define how a complex microbial ecosystem impacts host cell function, or how the host response to microbial colonization affects the bacterial community. This gap in knowledge is striking when one considers the broad and significant impact that microbes have on human health. In this application, we propose to expressly fill this knowledge gap through development of a novel multimodal imaging pipeline that will provide 3-dimensional information on the molecular heterogeneity of microbial communities and the immune response at the host-pathogen interface.  This proposal combines our expertise in immunology, infection biology, mass spectrometry, small animal imaging, machine learning, and computer vision to develop an integrated multimodal visualization method for studying infectious disease. Our unique approach will computationally combine ultra-high speed (~50px/s) MALDI-TOF images, ultra-high mass resolution (>200,000 resolving power) MALDI FTICR IMS, metal imaging by LA-ICP-IMS, high-spatial resolution optical microscopy, and MR imaging using data-driven image fusion. This strategy will enable 3-D molecular images to be generated for thousands of elements, metabolites, lipids, and proteins with an unprecedented combination of chemical specificity and spatial fidelity more than 50x faster than is currently possible. We will use this next-generation imaging capability to (i) define the heterogeneous microbial subpopulations throughout the 3-D volume of a S. aureus community, (ii) uncover the host molecules that form the abscess and accumulate to restrict microbial growth in murine models, and (iii) elucidate molecular markers that differentiate in vivo biofilms at the host-pathogen interface, between abscesses at various stages of progression, and under distinct degrees of nutrient stress. These studies will uncover new targets for therapeutic intervention and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research. PROJECT NARRATIVE This proposal will enable detailed views of the molecular components of infectious disease with unprecedented resolution through the development of a multimodal, 3-dimensional imaging platform. The proposed technologies will improve throughput and molecular specificity, enable automated high-precision and high-accuracy image alignment, and allow for descriptions of molecular signals in 3-D through the fusion of multi-modal imaging data. These studies will uncover targets for therapeutic intervention and antibiotic development and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research.",Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry,9788239,R01AI138581,"['3-Dimensional', 'Abscess', 'Affect', 'Animal Model', 'Animals', 'Anterior nares', 'Antibiotics', 'Antibodies', 'Architecture', 'Awareness', 'Bacteria', 'Bacterial Infections', 'Bacterial Proteins', 'Behavior', 'Biology', 'Biomedical Research', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Cellular Metabolic Process', 'Chemicals', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Custom', 'Data', 'Development', 'Diagnosis', 'Differentiation Antigens', 'Dimensions', 'Disease', 'Ecosystem', 'Elements', 'Environment', 'Exposure to', 'Fourier transform ion cyclotron resonance', 'Functional disorder', 'Genus staphylococcus', 'Glean', 'Growth', 'Health', 'Health Promotion', 'Heterogeneity', 'Histology', 'Human', 'Image', 'Imagery', 'Imaging technology', 'Immune', 'Immune response', 'Immunology', 'Imprisonment', 'Individual', 'Infection', 'Infectious Diseases Research', 'Integration Host Factors', 'Knowledge', 'Label', 'Lesion', 'Lipids', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mass Spectrum Analysis', 'Metals', 'Methodology', 'Methods', 'Microbe', 'Microbial Biofilms', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nutrient', 'Optics', 'Organism', 'Pathogenesis', 'Physiological', 'Population', 'Process', 'Proteins', 'Reagent', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Spatial Distribution', 'Specificity', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Speed', 'Staphylococcus aureus', 'Stress', 'Structure', 'Techniques', 'Technology', 'Therapeutic Intervention', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Tissues', 'Toxin', 'Vertebrates', 'Work', 'animal imaging', 'bacterial community', 'base', 'body system', 'commensal bacteria', 'experimental study', 'host colonization', 'imaging capabilities', 'imaging detection', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'innovation', 'interest', 'microbial', 'microbial colonization', 'microbial community', 'microscopic imaging', 'molecular imaging', 'molecular marker', 'mouse model', 'multimodality', 'neutrophil', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'protein expression', 'response', 'supervised learning', 'targeted treatment', 'virtual']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2019,562232,-0.027582009515359343
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9642618,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Effectiveness', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Imagery', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Universities', 'Virginia', 'absorption', 'artificial neural network', 'base', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2019,375602,0.010629840215813294
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9731665,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'informatics\xa0tool', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2019,599962,-0.002252379758272073
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9656981,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2019,431185,-0.031016580773474814
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,9859232,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'One-Step dentin bonding system', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2019,345016,0.013500281385887188
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9857305,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'machine learning algorithm', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2019,249000,-8.362175047626606e-05
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9773141,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2019,360843,-0.027423666432595514
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9750722,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Disease model', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Mathematics', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'experimental study', 'improved', 'infancy', 'insight', 'network models', 'novel', 'predictive modeling', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2019,527585,-0.0020303860461086815
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9772886,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics\xa0tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2019,663419,0.008043049687097197
"Application of Advanced Quantitative Methods to Schizophrenia Research in Macedonia PROJECT SUMMARY  Abnormalities of white matter are important in schizophrenia. A preponderance of studies have found decreased levels of transcripts for myelin-related proteins in autopsy brains. Some have found decrease in the proteins themselves, and some have not. Hundreds of diffusion tensor imaging (DTI) studies have found reduced fractional anisotropy (FA) in the brains of many people with schizophrenia (SCH). Decreased FA is interpreted as disruption of normal architecture. However, postmortem examination has failed to identify characteristic abnormalities. This suggests that abnormalities are subtle, and perhaps postmortem examinations have not used the right tools to find them. We have therefore been developing, as part of a collaboration supported by our concluding Fogarty project, two new methods to characterize white matter at high resolution. The first is a machine learning protocol to measure axonal diameters and myelin sheath thickness in electron microscope (EM) images of prefrontal white matter, recognizing and avoiding artifacts in EM of autopsy tissue. This will enable us to measure thousands of fibers in EM images produced as part of our concluding Fogarty project, from individuals with SCH, major depressive disorder (MDD), or no psychiatric illness (NPI). The second method, suggested by the DTI findings, is to analyze the arrangement of the axons themselves. We will use 3-dimensional (3D) reconstructions of high-resolution images of the axons themselves, identified by Bielschowsky silver stain or immunohistochemistry for phosphorylated neurofilament protein. To obtain high-resolution images of Bielschowsy stains, we will take advantage of the recent observation by Dr. Mark Sonders, co-investigator on this project, that these and other heavy metal stains luminesce under 2-photon infrared excitation. This yields clear and measurable images of individual axons. We will perform these procedures on sections from existing paraffin blocks that comprise a complete left prefrontal coronal section from 36 triads containing 1 case each of SCH, MDD, or NPI, matched for sex and age. These brains were included in earlier studies that yielded data on protein composition, mRNA for myelin-related proteins, DNA methylation, microglial activation, and semiquantitative myelin histology. In a third, exploratory aim, we will employ graphical models to combine these various types of data with known properties of CNS white matter and myelin to build a model of what is disturbed in schizophrenia. We expect that novel techniques for data fusion will reveal associations based on multidimensional correlations that could not be detected by modeling the single-domain datasets separately. In the process of completing these scientific aims, we will pursue the pedagogic goals of training the first two professional biostatisticians in Macedonia, and an academic pathologist. We will also hold a seminar course for biological researchers to build awareness and understanding of the power of biostatistical and other computational methods to enrich their research. NARRATIVE Our ongoing Fogarty/NIMH research project in Macedonia (R01 MH060877, “Building Schizophrenia Research in Macedonia”), has demonstrated biochemical abnormalities of white matter in schizophrenia that are not present in major depressive disorder. However, we have not seen anatomical abnormalities of white matter, which MRI studies of schizophrenia tell us should exist, and as the biochemistry also suggests. To explore white matter in novel ways, we are developing new methods of microscopy, image analysis and statistical inference, which we now propose to employ on a large scale.",Application of Advanced Quantitative Methods to Schizophrenia Research in Macedonia,9953486,R56MH117769,"['3-Dimensional', 'Academy', 'Age', 'Anisotropy', 'Architecture', 'Autopsy', 'Awareness', 'Axon', 'Biochemical', 'Biochemistry', 'Biological', 'Biological Assay', 'Biometry', 'Brain', 'Caliber', 'Cerebrum', 'Characteristics', 'Charge', 'Collaborations', 'Complex', 'Computer-Assisted Diagnosis', 'Computers', 'Computing Methodologies', 'Confocal Microscopy', 'DNA Methylation', 'Data', 'Data Set', 'Deformity', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electron Microscope', 'Electrons', 'Fiber', 'Goals', 'Heavy Metals', 'Histologic', 'Histology', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Individual', 'Informatics', 'International', 'Knowledge', 'Learning', 'Left', 'Macedonia', 'Machine Learning', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Measurable', 'Measurement', 'Measures', 'Medical', 'Mental disorders', 'Messenger RNA', 'Methodology', 'Methods', 'Microscopy', 'Modeling', 'Modernization', 'Morphologic artifacts', 'Morphology', 'Multiomic Data', 'Myelin', 'Myelin Sheath', 'National Institute of Mental Health', 'Neurofilament Proteins', 'Paraffin', 'Pathologist', 'Pathology', 'Positioning Attribute', 'Procedures', 'Process', 'Property', 'Proteins', 'Proteomics', 'Protocols documentation', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Schizophrenia', 'Scientist', 'Silver Staining', 'Stains', 'Statistical Data Interpretation', 'Statistical Methods', 'Structural Models', 'Students', 'Techniques', 'Thick', 'Time', 'Tissues', 'Training', 'Transcript', 'Translational Research', 'Triad Acrylic Resin', 'base', 'cognitive function', 'computerized', 'computerized tools', 'data modeling', 'deep neural network', 'diffusion anisotropy', 'high resolution imaging', 'histological image', 'histological studies', 'imaging study', 'innovation', 'interest', 'low and middle-income countries', 'microscopic imaging', 'multidimensional data', 'multimodality', 'network models', 'novel', 'pedagogy', 'reconstruction', 'sex', 'tool', 'two photon microscopy', 'two-photon', 'water diffusion', 'white matter']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R56,2019,10000,-0.02645761848784382
"Big Omics Data Engine 2 Supercomputer Computational and data science has transformed biomedical scientific discovery: its approaches are embedded into a wide range of workflows for diseases such as schizophrenia, depression, Alzheimer's, epilepsy, influenza, autism, drug addiction, pediatric cardiac care, Inflammatory Bowel Disease, prostate cancer and multiple myleloma. Sixty-one basic and translational researchers at Mount Sinai representing over $100 million in NIH funding, along with their collaborators from 75 external institutions, have utilized the Big Omics Data Engine (BODE) supercomputer to elucidate significant scientific findings in over 167 publications, including high impact journals such as Nature and Science, with 2,427 citations in three years. These researchers have also shared the data generated on BODE throughout their consortia and into national data sharing repositories. BODE is nearing the end of its vendor maintainable life, and researchers need increased computational throughput and storage space. To empower researchers to not only continue their inquiries, but to also tackle more complex scientific questions with decreased time to solution, we propose the Big Omics Data Engine 2 Supercomputer (BODE2). BODE2 will contain a total of 3,200 Intel Cascade Lake cores with 15 terabytes of memory and 14 petabytes of raw storage, and will leverage an existing 250 terabytes of SSDs. An instrument of this size is not available elsewhere affordably. With the proposed instrument, researchers will be able to take advantage of three major benefits: (1) the ability to receive results faster for overall greater scientific throughput; (2) the ability to increase the fidelity of their simulations and analyses; and (3) the ability to migrate research applications seamlessly to the software environment for greater scientific productivity. As with data produced on BODE, BODE2 data products will also be shared with the broader scientific community. BODE2 will provide the critical infrastructure needed by the wide range of researchers and clinicians for the genetics and population analysis, gene expression, machine learning and structural and chemical biology approaches used to make advances in these diseases. A specialized Big Omics Data Engine 2 Supercomputer instrument will provide necessary computational and data science infrastructure for 61 research projects with 75 collaborating institutions in diverse areas such as Alzheimer's, autism, schizophrenia, drug addiction, influenza, pediatric cardiac care, depression, epilepsy, prostate cancer and multiple myeloma. Data generated from this instrument will be shared in national databases.",Big Omics Data Engine 2 Supercomputer,9708160,S10OD026880,"['Alzheimer&apos', 's Disease', 'Biology', 'Cardiac', 'Caring', 'Chemicals', 'Childhood', 'Communities', 'Complex', 'Computational Science', 'Computer software', 'Data', 'Data Science', 'Disease', 'Drug Addiction', 'Environment', 'Epilepsy', 'Funding', 'Gene Expression', 'Inflammatory Bowel Diseases', 'Influenza', 'Infrastructure', 'Institution', 'Journals', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Memory', 'Mental Depression', 'Nature', 'Population Analysis', 'Productivity', 'Publications', 'Research', 'Research Personnel', 'Schizophrenia', 'Science', 'Structure', 'Time', 'United States National Institutes of Health', 'Vendor', 'autism spectrum disorder', 'data sharing', 'genetic analysis', 'instrument', 'petabyte', 'repository', 'simulation', 'supercomputer', 'terabyte', 'translational scientist']",OD,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,S10,2019,1998264,-0.024343911490350045
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9827788,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,33190,0.015366060773164082
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9693030,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,36510,0.015366060773164082
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9625823,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,343157,0.015366060773164082
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9855180,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2019,702655,-0.01038185953290107
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9911854,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2019,15000,-0.003915398038826289
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9608754,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2019,356625,0.014723992563666022
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9755666,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,24201,0.005704982489860433
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9668156,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,381513,0.005704982489860433
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9786702,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2019,431816,0.01886946853885739
"Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack Project Summary: Tufts University physics professor Allan Cormack pioneered the field of tomography. His seminal work, from 1963 and 1964, provided both the mathematical foundations of computerized tomography (CT), and tangible proof-of-concept by engineering a rudimentary CT scanner. Taken together, this effort represented the first practical method to ""see into"" an object without physically breaking it open. Along with the engineer Godfrey Hounsfield, he won the 1979 Nobel Prize in Physiology or Medicine for these contributions. Since then, tomography has broadened to include a wide range of modalities and problems. This field is unique for the rich interplay among applications in medicine, security, earth sciences, industry, physics, and the mathematics required to solve these problems. This international conference at Tufts, “Modern Challenges in Imaging: In the Footsteps of Allan Cormack” will honor the achievements of Cormack and reflect this diversity in the field by gathering top international researchers in mathematics, engineering, science, and medicine to communicate the most current research and challenges in the field. This will include work on mathematical models of emerging modalities, tomographic machine learning, dynamic methods, and spectral imaging with applications include medicine and security. The best research from the conference will be disseminated in a special issue of the journal Inverse Problems. Talks will be posted on the conference website. The organizers will recruit a diverse set of experienced participants and trainees, and the conference will be advertised in a range of publications reflecting the scientific and demographic diversity of the field. This conference is unique in that it combines high-level mathematical participants with experts in medical and industrial CT. It is structured to encourage participants from different fields to talk with each other, broaden their horizons, and make connections between problems and methodologies in the various fields. Several of the plenary talks will provide introductions to the areas. Trainees will be integrated into the conference through an informal welcome lunch and a poster session to introduce them to researchers in the field. This supports goals 1, 4, and 5, of the NIBIB: Researchers will present innovative biomedical technologies, engineering solutions, and mathematical methods to better image the body and objects more generally. The synergy between research areas will support the translation of technologies from the academic sphere to medical utility. The training opportunities for graduate students and beginners support the training of the next generation of diverse scientists. Project Narrative This conference will bring together medical, scientific, engineering, and applied mathematical researchers to present their newest research for a range of tomographic problems. Graduate students and beginners will be encouraged to participate and learn by being offered introductory talks, a student poster session, a welcome event, and an informal atmosphere. The conference will be structured so researchers will learn about important challenges in practical tomography as well as new techniques and methods, thereby creating synergies and research connections among the areas.",Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack,9837131,R13EB028700,"['Achievement', 'Advertising', 'Algorithms', 'Area', 'Biomedical Technology', 'Communication', 'Development', 'Earth science', 'Engineering', 'Environment', 'Event', 'Fertilization', 'Foundations', 'Goals', 'Image', 'Individual', 'Industrialization', 'Industry', 'International', 'Journals', 'Lead', 'Learning', 'Lightning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modality', 'Modernization', 'National Institute of Biomedical Imaging and Bioengineering', 'Nobel Prize', 'Outcome', 'Participant', 'Physics', 'Physiology', 'Population', 'Problem Solving', 'Publications', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Security', 'Seminal', 'Societies', 'Structure', 'Students', 'Techniques', 'Technology', 'Time', 'Training Support', 'Translations', 'Underrepresented Groups', 'Universities', 'Work', 'X-Ray Computed Tomography', 'cohort', 'demographics', 'design', 'experience', 'graduate student', 'higher level mathematics', 'informal atmosphere', 'innovation', 'mathematical methods', 'mathematical model', 'meetings', 'member', 'next generation', 'posters', 'professor', 'recruit', 'spectrograph', 'symposium', 'synergism', 'tomography', 'training opportunity', 'web site']",NIBIB,TUFTS UNIVERSITY MEDFORD,R13,2019,10000,0.002274470188014331
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9741121,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'side effect', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,281932,0.0022863702422892913
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9634069,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2019,293003,0.0005675679414603596
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9747967,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2019,514129,-0.004442602507557163
"The Effects of Insurance Benefit Design Innovation on Patient Health Abstract My research in health economics has focused on how information and targeted consumer cost-sharing influences how patients choose providers and the financial savings of incentivizing patients to choose low-price providers. I have also examined the opposite side of the market, how patient use of information and targeted consumer incentives spurs provider price competition. These topics provided the framework for my research as a PhD student in Health Economics at the University of California, Berkeley and I continue to build on these topics while a policy researcher at the RAND Corporation. A natural next step for my career is to expand this line of research but in a more in-depth manner and using more advanced statistical methods. Performing mentored research in these areas will help me successfully make the transition from directed to independent research. The proposed study will help me to (1) contribute to a deeper understanding of patient health effects of an innovative insurance benefit design that is particularly relevant for the aging population; (2) continue to build capabilities working with large medical claims data sets and develop expertise in innovative statistical methods from different disciplines; (3) gain training in aging-related health-services research; (4) expand my exposure to the aging, health economics, and health services research communities; and (5) develop my abilities as an independent health services researcher and build the foundations to successfully compete for R01-level grants.  In this project, I propose to examine whether reference pricing for colonoscopies and pharmaceuticals decreases adherence to recommended colorectal cancer screening and medication therapies among the near- elderly population. I will also examine the impact of reference pricing on patient health outcomes and the aging process. To do so, I intend to apply novel machine-learning statistical methods that have been recently developed in the computer science and statistics fields. As part of this proposal, I have built a formal training plan to develop expertise in these methods. This project will provide me with the flexibility and support to develop a long-term research agenda that focuses on using innovative statistical methods to evaluate the comprehensive effects of consumer cost- sharing programs. Although this study focuses on a single cost- sharing program, reference pricing, the skills I gain through this award will allow me to independently lead evaluations of future benefit designs. The application of machine-learning methods to the setting of reference pricing will provide a framework that I or other researchers can use to evaluate other insurance benefit designs or alternative patient populations. ! Narrative An increasingly popular insurance benefit design, reference pricing, provides targeted financial incentives for consumers to receive care at low-cost providers. While the financial savings from reference pricing programs are well-known, the health impacts have yet to be studied. The proposed career grant will apply machine learning techniques to develop a long-term research agenda focused on understanding the patient health effects of reference pricing for colonoscopies and medication therapies, which are services that are especially relevant for the aging population. !",The Effects of Insurance Benefit Design Innovation on Patient Health,9646811,K01AG061274,"['Accident and Emergency department', 'Adherence', 'Admission activity', 'Adult', 'Advisory Committees', 'Age', 'Aging', 'Area', 'Award', 'Behavior', 'Big Data', 'California', 'Caring', 'Chronic', 'Chronic Disease', 'Colonoscopy', 'Communities', 'Comorbidity', 'Cost Sharing', 'Data Set', 'Deductibles', 'Development', 'Diabetes Mellitus', 'Discipline', 'Elderly', 'Employee', 'Evaluation', 'Exposure to', 'Foundations', 'Future', 'Grant', 'Health', 'Health Benefit', 'Health Care Costs', 'Health Services', 'Health Services Research', 'Healthcare', 'Heart Diseases', 'Heart Rate', 'Heterogeneity', 'Hospitals', 'Incentives', 'Individual', 'Inpatients', 'Insurance', 'Insurance Benefits', 'Insurance Carriers', 'Internal Medicine', 'Journals', 'Lead', 'Link', 'Machine Learning', 'Medical', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'New England', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Policies', 'Population', 'Preventive service', 'Price', 'Process', 'Provider', 'Publications', 'Publishing', 'Quality of life', 'Research', 'Research Methodology', 'Research Personnel', 'Retirement', 'Savings', 'Screening for cancer', 'Services', 'Side', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Training', 'Universities', 'Work', 'aging population', 'asthmatic patient', 'career', 'colorectal cancer screening', 'compliance behavior', 'computer science', 'cost', 'design', 'doctoral student', 'financial incentive', 'flexibility', 'health data', 'health economics', 'health plan', 'improved', 'innovation', 'learning strategy', 'mortality', 'novel', 'patient population', 'programs', 'response', 'semiparametric', 'skills', 'statistics', 'treatment effect']",NIA,RAND CORPORATION,K01,2019,130618,-0.019728192632007542
"Pacific Northwest Advanced Compound Identification Core OVERALL SUMMARY The capability to chemically identify thousands of metabolites and other chemicals in clinical samples will revolutionize the search for environmental, dietary, and metabolic determinants of disease. By comparison to near-comprehensive genetic information, comparatively little is understood of the totality of the human metabolome, largely due to insufficiencies in molecular identification methods. Through innovations in computational chemistry and advanced ion mobility separations coupled with mass spectrometry, we propose to overcome a significant, long standing obstacle in the field of metabolomics: the absence of methods for accurate and comprehensive identification of metabolites without relying on data from analysis of authentic chemical standards. A paradigm shift in metabolomics, we will use gas-phase molecular properties that can be both accurately predicted computationally and consistently measured experimentally, and which can thus be used for comprehensive identification of the metabolome without the need for authentic chemical standards. The outcomes of this proposal directly advance the mission and goals of the NIH Common Fund by: (i) transforming metabolomics science by enabling consideration of the totality of the human metabolome through optimized identification of currently unidentifiable molecules, eventually reaching hundreds of thousands of molecules, and (ii) developing standardized computational tools and analytical methods to increase the national capacity for biomedical researchers to identify metabolites quickly and accurately. This work is significant because it enables comprehensive and confident chemical measurement of the metabolome. This work is innovative because it utilizes an integrated quantum-chemistry and machine learning computational pipeline to accurately predict physical-chemical properties of metabolites coupled to measurements. OVERALL NARRATIVE This project will utilize integrated quantum-chemistry and machine learning computational computational approaches coupled with advanced instrumentation to characterize the human metabolome, and identify currently unidentifiable molecules without the use of authentic chemical standards. Results from these studies will contribute to the goal of understanding diseases, and the tools and resources will be made publically available for biomedical researchers.",Pacific Northwest Advanced Compound Identification Core,10012251,U2CES030170,"['Adoption', 'Algorithms', 'Analytical Chemistry', 'Attributes of Chemicals', 'Biological', 'Biological Markers', 'Biomedical Research', 'Chemical Structure', 'Chemicals', 'Clinical', 'Communities', 'Computer Simulation', 'Computers and Advanced Instrumentation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Databases', 'Dependence', 'Diet', 'Disease', 'Educational workshop', 'Engineering', 'Exposure to', 'Funding', 'Gases', 'Genetic', 'Goals', 'High Performance Computing', 'Human', 'Isotopes', 'Libraries', 'Liquid substance', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Molecular', 'Outcome', 'Pacific Northwest', 'Phase', 'Predictive Analytics', 'Probability', 'Procedures', 'Property', 'Reference Standards', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Science', 'Serum', 'Source', 'Standardization', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Uncertainty', 'United States National Institutes of Health', 'Urine', 'Work', 'analytical method', 'base', 'chemical property', 'chemical standard', 'comparative', 'computational chemistry', 'computerized tools', 'dark matter', 'drug candidate', 'drug discovery', 'experience', 'genetic information', 'human disease', 'improved', 'innovation', 'instrumentation', 'ion mobility', 'metabolome', 'metabolomics', 'non-genetic', 'novel', 'novel therapeutics', 'programs', 'quantum chemistry', 'small molecule libraries', 'stereochemistry', 'tool']",NIEHS,BATTELLE PACIFIC NORTHWEST LABORATORIES,U2C,2019,141763,0.015429367831007406
"Pacific Northwest Advanced Compound Identification Core OVERALL SUMMARY The capability to chemically identify thousands of metabolites and other chemicals in clinical samples will revolutionize the search for environmental, dietary, and metabolic determinants of disease. By comparison to near-comprehensive genetic information, comparatively little is understood of the totality of the human metabolome, largely due to insufficiencies in molecular identification methods. Through innovations in computational chemistry and advanced ion mobility separations coupled with mass spectrometry, we propose to overcome a significant, long standing obstacle in the field of metabolomics: the absence of methods for accurate and comprehensive identification of metabolites without relying on data from analysis of authentic chemical standards. A paradigm shift in metabolomics, we will use gas-phase molecular properties that can be both accurately predicted computationally and consistently measured experimentally, and which can thus be used for comprehensive identification of the metabolome without the need for authentic chemical standards. The outcomes of this proposal directly advance the mission and goals of the NIH Common Fund by: (i) transforming metabolomics science by enabling consideration of the totality of the human metabolome through optimized identification of currently unidentifiable molecules, eventually reaching hundreds of thousands of molecules, and (ii) developing standardized computational tools and analytical methods to increase the national capacity for biomedical researchers to identify metabolites quickly and accurately. This work is significant because it enables comprehensive and confident chemical measurement of the metabolome. This work is innovative because it utilizes an integrated quantum-chemistry and machine learning computational pipeline to accurately predict physical-chemical properties of metabolites coupled to measurements. OVERALL NARRATIVE This project will utilize integrated quantum-chemistry and machine learning computational computational approaches coupled with advanced instrumentation to characterize the human metabolome, and identify currently unidentifiable molecules without the use of authentic chemical standards. Results from these studies will contribute to the goal of understanding diseases, and the tools and resources will be made publically available for biomedical researchers.",Pacific Northwest Advanced Compound Identification Core,9769745,U2CES030170,"['Adoption', 'Algorithms', 'Analytical Chemistry', 'Attributes of Chemicals', 'Biological', 'Biological Markers', 'Biomedical Research', 'Chemical Structure', 'Chemicals', 'Clinical', 'Communities', 'Computer Simulation', 'Computers and Advanced Instrumentation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Databases', 'Dependence', 'Diet', 'Disease', 'Educational workshop', 'Engineering', 'Exposure to', 'Funding', 'Gases', 'Genetic', 'Goals', 'High Performance Computing', 'Human', 'Isotopes', 'Libraries', 'Liquid substance', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Molecular', 'Outcome', 'Pacific Northwest', 'Phase', 'Predictive Analytics', 'Probability', 'Procedures', 'Property', 'Reference Standards', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Science', 'Serum', 'Source', 'Standardization', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Uncertainty', 'United States National Institutes of Health', 'Urine', 'Work', 'analytical method', 'base', 'chemical property', 'chemical standard', 'comparative', 'computational chemistry', 'computerized tools', 'dark matter', 'drug candidate', 'drug discovery', 'experience', 'genetic information', 'human disease', 'improved', 'innovation', 'instrumentation', 'ion mobility', 'metabolome', 'metabolomics', 'non-genetic', 'novel', 'novel therapeutics', 'programs', 'quantum chemistry', 'small molecule libraries', 'stereochemistry', 'tool']",NIEHS,BATTELLE PACIFIC NORTHWEST LABORATORIES,U2C,2019,998631,0.015429367831007406
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9625118,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'multidimensional data', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2019,162123,-0.07212322481595698
"An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography SUMMARY – OVERALL Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function. Although the basic electron cryo-tomography technique has been used for several decades, the technology is being revolutionized by recent advances in sample preparation, electron cryo-microscopy hardware, improved capabilities for automatic data collection, direct electron detection imaging devices, and phase plate technologies. Combined, these advances led to the ability to generate extraordinarily large numbers of cellular cryo-tomograms of exquisite quality. In principle, such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. However, collection of cellular data is now at a far faster rate than can currently be analyzed with existing methods, producing a serious barrier to progress: to match the data production rates of a single laboratory, at least 50 experienced scientists would need to handle the data analysis. The primary goal of this Program Project is to establish quantitative and highly automated tools for the reconstruction and interpretation of highly complex cellular tomographic data. We have assembled a highly synergistic team of PIs with complimentary expertise in cutting-edge computational and experimental electron microscopy techniques to achieve this goal through collaborative efforts. Project 1 (Hanein & Penczek) focuses on development and implementation of tomogram quality assessment and validation techniques and on experimentally guided optimization of data collection strategies. Project 2 focuses on automatic tomographic reconstruction technology, extraction of various features from the tomograms, and the analysis of distribution patterns derived from the extracted features. Project 3 focuses on development of quantitative tools for tomogram annotation through deep learning and sub-tomogram alignment as well as interactive visualization tools. The set of highly automated tools developed in this Program Project will permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study. NARRATIVE Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function and—with recent instrumental advances—it is now possible to image hundreds of cells per months, enabling collection of cellular data at a far faster rate than can currently be analyzed. Such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. This Program Project brings together an accomplished team of investigators to develop new strategies for effectively processing and interpreting this massive influx of data, developing a set of highly automated tools to permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study.",An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography,9769773,P01GM121203,"['Address', 'Algorithms', 'Artificial Intelligence', 'Big Data', 'Biological', 'Biology', 'Cancer Diagnostics', 'Cell physiology', 'Cells', 'Classification', 'Collection', 'Complex', 'Computing Methodologies', 'Cryo-electron tomography', 'Cryoelectron Microscopy', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Disease', 'Electron Microscopy', 'Electrons', 'Environment', 'Floods', 'Goals', 'Health', 'Heart', 'Human', 'Image', 'Imaging Device', 'Individual', 'Knowledge', 'Laboratories', 'Methodology', 'Methods', 'Molecular', 'Molecular Structure', 'Morphology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Preparation', 'Process', 'Production', 'Real-Time Systems', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Stimulus', 'Structure', 'System', 'Techniques', 'Technology', 'Tomogram', 'Validation', 'Variant', 'Visualization software', 'base', 'computer framework', 'convolutional neural network', 'deep learning', 'electron tomography', 'experience', 'imaging detection', 'improved', 'insight', 'knowledge base', 'learning strategy', 'nanomachine', 'novel diagnostics', 'particle', 'programs', 'reconstruction', 'response', 'software development', 'statistics', 'tomography', 'tool', 'virtual']",NIGMS,SANFORD BURNHAM PREBYS MEDICAL DISCOVERY INSTITUTE,P01,2019,928444,-0.028929419664939282
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9658873,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,317858,-0.01181065832004035
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9694279,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer’s disease biomarker', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multidimensional data', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2019,341691,-0.05925774429233081
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9789907,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Infrastructure', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2019,366616,0.009746786964973388
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,9851583,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'learning strategy', 'mHealth', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2019,741688,-0.028204534928390493
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. • ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. • UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types • U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database • St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9853317,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Infrastructure', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'multidimensional data', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2019,855741,-0.030359726364737976
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",9882865,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2019,340827,0.016217469238226753
"Acquisition of a next-generation computing cluster We request funds to purchase our next-generation computing cluster to support computationally intensive NIH-funded research at Washington University in St. Louis. This system will become the foundation of the Center for High Performance Computing (CHPC) to support our active, diverse user community. It has been designed to meet our current and future computing needs. It adds additional capabilities to support emerging fields such as “Deep Learning”. The CHPC currently supports over 775 users from 300 different groups across 33 departments. 58 papers have cited the CHPC. The Center has a proven funding model and is economically sustainable. The Center has partnered with other University organizations to offer training workshops, not only on the use of the cluster, but also on introductory programming for users with no prior programming experience. If this proposal is funded, we will be able to continue to support this ever-growing diverse community of researchers. The proposed system would replace critical components including the management node, the login nodes, the storage, and upgrade the Infiniband networking. We would add substantial upgrades to our computing power with state-of-the-art processors, increased memory capacity for growing jobs, General Purpose Graphical Processing units (GPGPUs), and new capabilities for “Deep Learning”. Nearly all fields of NIH-funded research are faced with increasingly large data sets that require additional computing power to analyze. We propose building a next-generation computing cluster to support this research. Our Center has a proven track record in supporting a large, diverse group of users in all aspects of their computationally demanding research.",Acquisition of a next-generation computing cluster,9707936,S10OD025200,"['Communities', 'Educational workshop', 'Foundations', 'Funding', 'Future', 'High Performance Computing', 'Memory', 'Modeling', 'Occupations', 'Paper', 'Research', 'Research Personnel', 'System', 'Training', 'United States National Institutes of Health', 'Universities', 'Washington', 'cluster computing', 'deep learning', 'design', 'experience', 'next generation']",OD,WASHINGTON UNIVERSITY,S10,2019,597200,0.010604876161599072
"Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria PROJECT SUMMARY Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria. Antibiotic-resistant Gram-negative bacterial infections are increasing in incidence and novel antibiotics are urgently needed to combat this growing threat to public health. A major roadblock to the development of novel antibiotics is our poor understanding of the structural features of small molecules that correlate with bacterial penetration and efflux. As a result, while potent biochemical inhibitors can often be identified for new targets, developing them into compounds with whole-cell antibacterial activity has proven challenging. To address this critical problem, we propose herein a comprehensive, multidisciplinary approach to develop quantitative models to predict small-molecule penetration and efflux in Gram-negative bacteria. We have pioneered a general platform for systematic, quantitative evaluation of small-molecule accumulation in bacteria, using label-free LC-MS/MS detection and multivariate cheminformatic analysis. We have also developed unique isogenic strain sets of wild-type, hyperporinated, efflux-knockout, and doubly-compromised E. coli, P. aeruginosa, and A. baumannii that allow us to dissect the individual contributions of outer/inner membrane penetration and active efflux to net accumulation, using a kinetic model that accurately recapitulates available experimental data. Moreover, we have developed machine learning and neural network approaches to QSAR (quantitative structure–activity relationship) modeling of pharmacological properties that will now be used to develop predictive cheminformatic models for Gram-negative accumulation, penetration, and efflux. This project will be carried out by a multidisciplinary SPEAR-GN Project Team (Small-molecule Penetration & Efflux in Antibiotic-Resistant Gram-Negatives, “speargun”) involving the labs of Derek Tan (MSK, PI), Helen Zgurskaya (OU, PI), Bradley Sherborne (Merck, Lead Collaborator), Valentin Rybenkov (OU, Co-I), Adam Duerfeldt (OU, Co-I), Carl Balibar (Merck, Collaborator), and David McLaren (Merck, Collaborator), comprising extensive combined expertise in organic and diversity-oriented synthesis, biochemistry, microbiology, high- throughput screening, mass spectrometry, biophysical modeling, cheminformatics, and medicinal chemistry. Herein, we will design and synthesize chemical libraries with diverse structural and physicochemical properties; analyze their accumulation in the isogenic strain sets in both high-throughput and high-density assay formats; extract kinetic parameters for penetration and efflux from the resulting experimental datasets; develop and validate robust QSAR models for accumulation, penetration, and efflux; and demonstrate the utility of these models in medicinal chemistry campaigns to develop novel Gram-negative antibiotics against three targets. This project will provide a major advance in the field of antibacterial drug discovery, providing powerful enabling tools to the scientific community to address this major threat to public health. PUBLIC HEALTH RELEVANCE Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria. Antibiotic-resistant Gram-negative bacteria pose a growing threat to public health in the U.S. and globally. A major obstacle to the development of new antibiotics to combat such infections is our poor understanding of the chemical requirements for small molecules to enter Gram-negative cells and to avoid ejection by efflux pumps. The proposed comprehensive, multidisciplinary research program aims to develop predictive computational tools to identify such molecules by carrying out large-scale, quantitative analyses of the accumulation of diverse small molecules in Gram-negative bacteria. These tools will then enable medicinal chemistry campaigns to develop novel antibiotics.",Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria,9761970,R01AI136795,"['Acinetobacter baumannii', 'Address', 'Algorithmic Software', 'Anti-Bacterial Agents', 'Antibiotic Resistance', 'Antibiotics', 'Architecture', 'Bacteria', 'Biochemical', 'Biochemistry', 'Biological Assay', 'Biological Availability', 'Cells', 'Chemicals', 'Communities', 'Data', 'Data Set', 'Detection', 'Development', 'Effectiveness', 'Escherichia coli', 'Gram-Negative Bacteria', 'Gram-Negative Bacterial Infections', 'Human', 'Incidence', 'Individual', 'Infection', 'Interdisciplinary Study', 'Kinetics', 'Knock-out', 'Lead', 'Libraries', 'Machine Learning', 'Mammalian Cell', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Membrane', 'Microbiology', 'Modeling', 'Oral', 'Partner in relationship', 'Penetration', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Pharmacology', 'Property', 'Pseudomonas aeruginosa', 'Public Health', 'Quantitative Evaluations', 'Quantitative Structure-Activity Relationship', 'Role', 'Structure', 'Testing', 'Variant', 'analog', 'base', 'biophysical model', 'cell envelope', 'cheminformatics', 'combat', 'computerized tools', 'density', 'design', 'drug discovery', 'efflux pump', 'high throughput screening', 'improved', 'inhibitor/antagonist', 'interdisciplinary approach', 'lead optimization', 'learning network', 'multidisciplinary', 'neural network', 'novel', 'off-label use', 'predictive modeling', 'programs', 'prospective', 'public health relevance', 'screening', 'small molecule', 'small molecule libraries', 'success', 'tool']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,R01,2019,1212566,-0.004537977075403326
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9638561,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computational platform', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'human microbiota', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2019,364865,-0.006127996653114619
"The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers Project Summary  One of the most glaring yet easily addressable gaps in our current scientific workflow and publication system is improving the way that methods are reported, in particular, the lack of key methodological details necessary for interpreting and reproducing a study. Most authors continue to cite the name of the reagent, like an antibody using the vendor, and the city where the vendor is located, but omit the catalog and lot number making antibodies very difficult to track down, thereby reducing reproducibility of the paper. The Resource Identification, RRID, Initiative has successfully implemented a solution to this lack of identification, by asking authors to include a persistent unique identifier (RRID) for each antibody along with a standard syntax that includes the lot information. This syntax is now required in about 100 journals and accepted in at least 400, was accepted into the EQUATOR network of standards and is under consideration by the JATS committee, the NISO standard for journal article metadata. For antibodies, RRIDs are assigned by the AntibodyRegistry.org, which accepts full catalogs from antibody companies and individual antibody records from authors, who are unable to locate the record for the antibody that they used in a study or one which they created in their lab. This process should be made as easy as possible for authors, and through text analysis we have devised a set of tools that should help authors create better records with less work.  The AntibodyRegistry.org was created as part of an academic project and it has successfully incorporated millions of antibody records, thousands of which have now been cited in scientific papers using the RRID syntax. The use of RRID is growing, and in order to support the longer term sustainability of the AntibodyRegistry.org, a core community authority for RRIDs, this resource needs to be enhanced to align with the available commercial and non-commercial funding sources. We propose the addition of features, valuable to antibody companies and journals, to improve market intelligence and reporting around antibodies. We also propose to auto-generate antibody entries for authors and curators when submitting/curating an antibody to decrease the time it takes to complete the task, thereby reducing the barrier to entry and cost. Project Narrative The AntibodyRegistry.org is a catalog of antibodies used in research and serves as the antibody identifier source for the Research Resource Identifier (RRID) initiative. The use of these identifiers improves rigor and transparency in compliance with both journal and NIH standards. As the RRID initiative grows, the AntibodyRegistry.org is becoming an increasingly well-used and valuable resource; requiring increased attention from curation staff. Improvements to the AntibodyRegistry.org are needed to increase commercial value and thereby its sustainability, decrease curation cost, and provide a higher level of service to the scientific community.",The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers,9680073,R41GM131551,"['Adopted', 'Antibodies', 'Asia', 'Attention', 'Award', 'Businesses', 'California', 'Catalogs', 'Cities', 'Cloud Computing', 'Communities', 'Data', 'Data Analytics', 'Databases', 'Electronic Mail', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Funding', 'Funding Agency', 'Generations', 'Glare', 'Goals', 'Individual', 'Intelligence', 'International', 'Journals', 'Machine Learning', 'Marketing', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Names', 'Neurosciences', 'Paper', 'Performance', 'Phase', 'Process', 'Provider', 'Publications', 'Reagent', 'Records', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Services', 'Small Business Technology Transfer Research', 'Source', 'System', 'Text', 'Time', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Work', 'authority', 'base', 'cost', 'improved', 'information framework', 'journal article', 'syntax', 'text searching', 'tool']",NIGMS,"SCICRUNCH, INC.",R41,2019,149373,-0.016368220719812716
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9700030,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric infection', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2019,26919,-0.012842344176691394
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9744013,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Algorithms', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelium', 'Esophageal', 'Esophageal Adenocarcinoma', 'Esophagitis', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Intelligence', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stem cells', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2019,171720,-0.029829720754620932
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,9864664,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2019,269849,-0.0008196221096183337
"Administrative Supplement to the OAIC Pepper Center Coordinating Center We wish to advantage of 2 new key opportunities that could significantly enhance achievement of the overall goals of the OIAC Coordinating Center (OAIC CC) and 2 key, unexpected administrative needs. Project 1) Develop, test and implement an innovative set of tools to perform Integrative Data Analysis (IDA) for combining and analyzing independent data sets across the OAIC network An over-arching goal of the OAIC CC is to build collaborations between OAICs that unlock synergy. Each of the OAICs has many small/medium-sized completed studies relevant to the OAIC theme, and that have measured key domains of physical function. Combining these studies could provide large, powerful databases for answering critical questions not possible with individual studies. However, this is currently not possible because different measurement instruments are often used across centers and across studies. This project overcomes this critical limitation by taking advantage of 2 newly available technologies and an ongoing study. IDA is a set of strategies in which two or more independent data sets which contain measures addressing similar domains but using different measurement instruments are combined into one and then statistically analyzed. The proposed project is timely because it leverages an ongoing clinical study to validate new procedures for harmonizing measures of physical and cognitive function across 20 Pepper center studies. The resources created by the project will significantly enhance collaboration across the OAIC program network, benefiting researchers at all OAICs, and can be disseminated to other NIA center programs. Project 2) Develop a robust, interactive database of OAIC Program accomplishments that will automatically be updated via an efficient, streamlined, electronic annual reporting process.  It is widely believed that the NIA-funded Pepper Center program has been highly productive. However, there is no means of assessing the overall effectiveness of the Pepper Center, or of ‘cataloging’ its impressive accomplishments. This project will take advantage of new open-source technology to efficiently develop a robust, comprehensive, searchable, interactive database of past accomplishments. It will also develop a streamlined electronic Annual Directory Report template, and link it to the new OAIC database so that it is automatically updated each year. Achieving the goals of this project will reduce administrative burden for sites, facilitate NIA review of performance of centers, and create an annually updated database of OAIC accomplishments, projects, publications, and outcomes, and facilitate collaborations between centers and investigators across NIA programs. This application also requests support for 2 key, unexpected administrative needs that have arisen: 1) Increase in funding amount for the annual OAIC CC Multi-center pilot project. 2) Support for additional Pepper Centers that will soon be added to the OAIC network. Relevance Statement for OAIC Coordinating Center Administrative Supplement The Coordinating Center of the OAIC coordinates the activities of all the individual centers in the NIA- funded, OAIC network; its over-arching goal is to build collaborations between the individual OAICs and thereby unlock synergy and enable projects that could not be undertaken by any single OAIC center. This administrative supplement application proposes 2 developmental projects that will significantly enhance the capabilities of the OAIC to achieve these goals and which takes advantage of newly available methods and technology. This also includes additional support for the possible increase in the number of Pepper Centers and an increase in the pilot award budget.",Administrative Supplement to the OAIC Pepper Center Coordinating Center,9961004,U24AG059624,"['Achievement', 'Address', 'Administrative Supplement', 'Aging', 'Annual Reports', 'Award', 'Budgets', 'Capsicum', 'Cataloging', 'Catalogs', 'Clinical', 'Clinical Research', 'Cognition', 'Collaborations', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Directories', 'Effectiveness', 'Elderly', 'Equipment and supply inventories', 'Evaluation', 'Funding', 'Goals', 'Health', 'Individual', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Online Systems', 'Outcome', 'Participant', 'Performance', 'Physical Function', 'Pilot Projects', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Update', 'Walking', 'analytical method', 'analytical tool', 'base', 'cognitive function', 'cost effective', 'data modeling', 'forest', 'innovation', 'instrument', 'interest', 'lifestyle intervention', 'new technology', 'novel', 'open source', 'programs', 'recruit', 'response', 'synergism', 'theories', 'tool']",NIA,WAKE FOREST UNIVERSITY HEALTH SCIENCES,U24,2019,149775,0.00515793820410272
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9751297,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Consumption', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2019,77750,0.0028941582374467775
"IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP The purpose of this contract is to provide bioinformatic support to researchers in the Divisions of National Toxicology Program (DNTP) and Intramural Research (DIR) at the National Institute of Environmental Health Sciences (NIEHS). NIEHS researchers conduct studies that produce large amounts of data, varying in size and complexity. Fields of scientific study are diverse and include toxicology, genomics, transcriptomics, high throughput screening (HTS) data and data extraction from diverse text resources. The variety and complexity of NIEHS scientific studies dictates the need for innovative analytical techniques and the development of new software tools. Bioinformatic data analyses are required to support accurate and precise interpretation of study results. Specific bioinformatics needs include data analysis, data mining, creating bioinformatics pipelines for gene expression and pathway analysis and computational support for the vast amount of data collected through studies conducted at NIEHS and NIEHS contract laboratories. n/a",IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP,9915697,73201700001C,"['Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'ChIP-seq', 'Chemical Exposure', 'Chemicals', 'Contractor', 'Contracts', 'DNA Methylation', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Epigenetic Process', 'Evaluation', 'Exons', 'Gene Expression', 'Genes', 'Genomics', 'Informatics', 'Intramural Research', 'Knowledge', 'Laboratories', 'Literature', 'Measures', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Output', 'Pathway Analysis', 'Peer Review', 'Privatization', 'Programming Languages', 'Proteomics', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scientific Evaluation', 'Scientist', 'Series', 'Software Tools', 'Specific qualifier value', 'Technology', 'Text', 'Toxicogenomics', 'Toxicology', 'analysis pipeline', 'bioinformatics tool', 'bisulfite sequencing', 'cheminformatics', 'computational intelligence', 'data integration', 'data mining', 'differential expression', 'high throughput screening', 'innovation', 'meetings', 'metabolomics', 'method development', 'next generation sequencing', 'physical property', 'programs', 'screening', 'technique development', 'transcriptomics', 'whole genome']",NIEHS,"SCIOME, LLC",N01,2019,2464037,-0.0069765909121653905
"Accelerating Community-Driven Medical Innovation with VTK Abstract Thousands of medical researchers around the world use VTK —the Visualization Toolkit— an open-source, freely available software development toolkit providing advanced 3D interactive visualization, image processing and data analysis algorithms. They either use VTK directly in their in-house research applications or indirectly via one of the multitude of medical image analysis and bioinformatics applications that is built using VTK: Osirix, 3D Slicer, BioImageXD, MedINRIA, SCIRun, ParaView, and others. Furthermore, VTK also provides 3D visualizations for clinical applications such as BrainLAB’s VectorVision surgical guidance system and Zimmer’s prosthesis design and evaluation platform. VTK has been downloaded many hundreds of thousands of times since its initial release in 1993. Considering its broad distribution and prevalent use, it can be argued that VTK has had a greater impact on medical research, and patient care, than any other open-source visualization package.  This proposal is in response to the multitude of requests we have been receiving from the VTK medical community. The aims are as follows:  1. Aim 1: Adaptive visualization framework: Produce an integrated framework that supports  visualization applications that balance server-side and client-side processing depending on data size,  analysis requirements, and the user platform (e.g., phone, tablet, or GPU-enabled desktop).  2. Aim 2: Integrated, interactive applications: Extend VTK to support a diversity of programming  paradigms ranging from C++ to JavaScript to Python and associated tools such as Jupyter Notebooks,  integrating with emerging technologies such as deep learning technologies.  3. Aim 3: Advanced rendering, including AR/VR: Target shader-based rendering systems and AR/VR  libraries that achieve high frame rates with minimal latency for ubiquitous applications that combine  low-cost, portable devices such as phones, ultrasound transducers, and other biometric sensors for  visually monitoring, guiding, and delivering advanced healthcare.  4. Aim 4: Infrastructure, Outreach, and Validation: Engage the VTK community and the proposed  External Advisory Board during the creation and assessment of the proposed work and corresponding  modern, digital documentation in the form of videos and interactive web-based content. Project Narrative The Visualization Toolkit (VTK) is an open source, freely available software library for the interactive display and processing of medical images. It is being used in most major medical imaging research applications, e.g., 3D Slicer and Osirix, and in several commercial medical applications, e.g., BrainLAB’s VectorVision surgical guidance system. VTK development began in 1993 and since then an extensive community of users and developers has grown around it. However, the rapid advancement of cloud computing, GPU hardware, deep learning algorithms, and VR/AR systems require corresponding advances in VTK so that the research and products that depend on VTK continue to deliver leading edge healthcare technologies. With the proposed updates, not only will existing applications continue to provide advanced healthcare, but new, innovative medical applications will also be inspired.",Accelerating Community-Driven Medical Innovation with VTK,9740493,R01EB014955,"['3-Dimensional', 'Adopted', 'Algorithmic Analysis', 'Algorithms', 'Augmented Reality', 'Bioinformatics', 'Biomechanics', 'Biomedical Technology', 'Biometry', 'Client', 'Cloud Computing', 'Cloud Service', 'Code', 'Communities', 'Computational Geometry', 'Computer software', 'Data', 'Data Analyses', 'Development', 'Devices', 'Documentation', 'Emerging Technologies', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Explosion', 'Foundations', 'Funding', 'Grant', 'Health Technology', 'Healthcare', 'Hybrids', 'Image Analysis', 'Imagery', 'Industry', 'Infrastructure', 'Internet', 'Language', 'Letters', 'Libraries', 'Licensing', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modernization', 'Monitor', 'Online Systems', 'Operative Surgical Procedures', 'Patient Care', 'Prevalence', 'Process', 'Prosthesis Design', 'Publications', 'Pythons', 'Research', 'Research Personnel', 'Resources', 'Side', 'Surveys', 'System', 'Tablets', 'Techniques', 'Technology', 'Telephone', 'TensorFlow', 'Testing', 'Time', 'Training', 'Ultrasonic Transducer', 'Update', 'Validation', 'Visual', 'Work', 'base', 'clinical application', 'cloud based', 'computerized data processing', 'cost', 'deep learning', 'deep learning algorithm', 'design', 'digital', 'health care delivery', 'image processing', 'innovation', 'interest', 'learning strategy', 'meetings', 'new technology', 'open source', 'outreach', 'point of care', 'portability', 'processing speed', 'real world application', 'response', 'sensor', 'software development', 'statistics', 'success', 'supercomputer', 'synergism', 'tool', 'trend', 'virtual reality', 'web services']",NIBIB,"KITWARE, INC.",R01,2019,508446,-0.004732505087275002
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9716628,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2019,763474,-0.01994368769505162
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9817000,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2019,334891,-0.014870697150628625
"COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data",9938885,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'Intelligence', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'commune', 'computational platform', 'computer framework', 'computing resources', 'connectome', 'cost', 'data anonymization', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'preservation', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2019,585151,0.015199723945323254
"Methods for determination of glycoprotein glycosylation similarities among disease states Abstract This application addresses NIGMS PAR-17-045 “Focused Technology Research and Development (R01)”. This initiative supports projects that focus solely on development of technologies with the potential to enable biomedical research. Dysregulation of the cellular microenvironment occurs in cancers, neurodevelopmental and neuropsychiatric diseases. Known as the matrisome, the set of extracellular matrix and cell surface molecules control the availability of growth factors to cellular receptors and the mechanical-physical properties of the cell microenvironment. Currently, the limited understanding of regulation of matrisome glycosylation hinders understanding of the roles of glycosylation-dependent matrisome networks in the basic mechanisms necessary for targeted intervention of many diseases. Matrisome function depends on networks of interaction among glycosylated proteins and glycan-binding lectins. It is not possible using present proteomics and glycoproteomics methods to compare using rigorous statistics similarities of glycoproteins that differ by disease-related changes in site-specific glycosylation. We propose to develop technologies to meet this need. Present proteomics methods quantify proteins using a few representative peptides per gene product; sequence coverage for most proteins is low. Such low sequence coverage does not suffice to reconstruct the predominant glycosylated proteoforms active in a biological context. We propose to develop technologies to compare glycoprotein similarities among biological sample sets. To do this, we will develop MS acquisition and bioinformatics methods for rapid, sensitive and reproducible mapping of glycoprotein glycosylation to enable statistically rigorous comparison of glycoprotein similarities. By making these technologies available, we will enable a new level of understanding of the roles of matrisome networks in human diseases. Project narrative The matrisome consists of glycosylated extracellular matrix and cell surface proteins that surround cells and support normal physiological activity. While it is known that glycosylation changes during disease processes, it has not been possible to quantitatively compare glycoprotein structure among biological samples. We aim to develop technologies to meet this need.",Methods for determination of glycoprotein glycosylation similarities among disease states,9800244,R01GM133963,"['Address', 'Algorithms', 'Atherosclerosis', 'Autoimmune Diseases', 'Binding', 'Bioinformatics', 'Biological', 'Biological Process', 'Biomedical Research', 'Bos taurus structural-GP protein', 'Brain', 'Brain region', 'CSPG3 gene', 'Cell Surface Proteins', 'Cell surface', 'Cells', 'Chondroitin Sulfate Proteoglycan', 'Collagen', 'Complex', 'Core Protein', 'Data', 'Data Set', 'Disease', 'Dissociation', 'Electron Transport', 'Environment', 'Enzymes', 'Extracellular Matrix', 'Family', 'Functional disorder', 'Genes', 'Glycopeptides', 'Glycoproteins', 'Growth Factor', 'Growth Factor Receptors', 'Heart', 'Heparitin Sulfate', 'Intelligence', 'Intervention', 'Ions', 'Knowledge', 'Lectin', 'Liquid Chromatography', 'Machine Learning', 'Malignant Neoplasms', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Neurodegenerative Disorders', 'Pathway interactions', 'Peptides', 'Physiological', 'Polysaccharides', 'Process', 'Protein Glycosylation', 'Proteins', 'Proteoglycan', 'Proteomics', 'Receptor Protein-Tyrosine Kinases', 'Regulation', 'Reproducibility', 'Role', 'Sampling', 'Signal Pathway', 'Site', 'Structure', 'Technology', 'Tissues', 'aggrecan', 'bioinformatics tool', 'brevican', 'cell growth', 'data acquisition', 'data to knowledge', 'extracellular', 'gene product', 'glycoprotein structure', 'glycoproteomics', 'glycosylation', 'human disease', 'hydrophilicity', 'neuropsychiatric disorder', 'pathogen', 'physical property', 'rapid technique', 'receptor', 'statistics', 'technology development', 'technology research and development', 'versican']",NIGMS,BOSTON UNIVERSITY MEDICAL CAMPUS,R01,2019,420750,-0.031209875801652943
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9658524,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,305167,-0.00283188936337735
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary  Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and effect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable to discover local and global alteration of matter without the need to apriori select an anatomical region of interest.  The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. ​A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image data set is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focus on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,9787575,R41MH118845,"['Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Calibration', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Computer software', 'Data Set', 'Databases', 'Dementia', 'Deterioration', 'Development', 'Diffuse', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Foundations', 'Goals', 'Grain', 'Image', 'Image Analysis', 'Imagery', 'Internet', 'Lead', 'Location', 'Machine Learning', 'Medical Imaging', 'Methodology', 'Methods', 'Modality', 'Nature', 'Nerve Degeneration', 'Neurologic', 'Neurologic Effect', 'Online Systems', 'Outcome', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Study', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Structure', 'Technology', 'Temporal Lobe', 'Testing', 'Validation', 'Variant', 'autism spectrum disorder', 'base', 'clinical Diagnosis', 'experience', 'frontal lobe', 'gray matter', 'high throughput screening', 'image processing', 'image registration', 'imaging capabilities', 'improved', 'interest', 'learning strategy', 'morphometry', 'nervous system disorder', 'predict clinical outcome', 'predictive modeling', 'programs', 'research and development', 'shape analysis', 'software development', 'task analysis', 'tool', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R41,2019,291536,-0.01999072410545296
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9690782,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Dysregulation', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'bioinformatics tool', 'circadian', 'circadian regulation', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'informatics\xa0tool', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2019,328155,-0.011356588945683279
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9785367,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2019,247413,0.01542057000872389
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive, PhysioBank, was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. PhysioToolkit, its software collection, supports exploration and quantitative analyses of PhysioBank and similar data with a wide range of well-documented, rigorously tested open-source software that can be run on any platform. PhysioNet's team of researchers leverages results of other funded projects to drive the creation and enrichment of: i) Data collections that provide increasingly comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC III (Medical Information Mart for Intensive Care) Database of critical care patients; ii) Analytic methods that lead to more timely and accurate diagnoses of major public health problems (such as life-threatening cardiac arrhythmias, infant apneas, fall risk in older individuals and those with neurologic disease, and seizures), and iii) Elucidation of dynamical changes associated with a variety of pathophysiologic processes and aging (such as cardiopulmonary interactions during sleep disordered breathing syndromes); User interfaces, reference materials and services that add value and improve accessibility to PhysioNet's data and software (such as PhysioNetWorks, a virtual laboratory for data sharing). Impact: Cited in The White House Fact Sheet on Big Data Across the Federal Government (March 29, 2012), PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are inaccessible otherwise. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world- wide, growing community of researchers, clinicians, educators, students, and medical instrument and software developers, retrieve about 380 GB of data per day. By providing free access to its unique and wide-ranging data and software collections, PhysioNet is invaluable to studies that currently result in an impressive average of nearly 250 new scholarly articles per month by academic, clinical, and industry-affiliated researchers worldwide. Over the next year we aim to sustain and enhance PhysioNet's impact with new technology and data; and complete the 2019 PhysioNet/Computing in Cardiology Challenge on sepsis. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,9993811,R01GM104987,"['Aging', 'Algorithms', 'Apnea', 'Area', 'Arrhythmia', 'Big Data', 'Biomedical Research', 'Boston', 'Bypass', 'Cardiology', 'Cardiopulmonary', 'Categories', 'Clinical', 'Clinical Data', 'Cloud Service', 'Collection', 'Communities', 'Community Outreach', 'Complex', 'Computer software', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Dedications', 'Development', 'Diagnostic radiologic examination', 'Entropy', 'FAIR principles', 'Federal Government', 'Functional disorder', 'Funding', 'Grant', 'Imagery', 'Individual', 'Industry', 'Infant', 'Infrastructure', 'Intensive Care', 'Israel', 'Journals', 'Laboratories', 'Lead', 'Licensing', 'Life', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical center', 'Methods', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase Transition', 'Physiological', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Roentgen Rays', 'Role', 'Running', 'Seizures', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Students', 'Switzerland', 'Syndrome', 'Testing', 'Thoracic Radiography', 'Time', 'United States National Institutes of Health', 'University Hospitals', 'Visit', 'accurate diagnosis', 'analytical method', 'clinical application', 'computerized data processing', 'computing resources', 'data archive', 'data sharing', 'experience', 'fall risk', 'heart rate variability', 'improved', 'innovation', 'instrument', 'instrumentation', 'interest', 'member', 'nervous system disorder', 'new technology', 'open source', 'preservation', 'repository', 'signal processing', 'software repository', 'symposium', 'time interval', 'virtual laboratory']",NIGMS,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2019,409563,0.009438902829344906
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9765194,R44CA206782,"['Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'data warehouse', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'off-patent', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2019,238768,0.014988998928993635
"Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders Project Summary/Abstract  In the field of Alzheimer’s and related disorder, there has been very little work focusing on imaging genomics biomarker approaches, despite considerable promise. In part this is due to the fact that most studies have fo- cused on candidate gene approaches or those that do not capitalize on capturing (and amplifying) small effects spread across many sites. Even for genome wide studies, the vast majority of imaging genomic studies still rely on massive univariate analyses. The use of multivariate approaches provides a powerful tool for analyzing the data in the context of genomic and connectomic networks (i.e. weighted combinations of voxels and genetic variables). It is clear that imaging and genomic data are high dimensional and include complex relationships that are poorly understood. Multivariate data fusion models that have been proposed to date typically suffer from two key limitations: 1) they require the data dimensionality to match (i.e. 4D fMRI data has to be reduced to 1D to match with the 1D genomic data, and 2) models typically assume linear relationships despite evidence of non- linearity in brain imaging and genomic data. New methods are needed that can handle data that has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. Secondly, methods that can handle complex relationships, such as groups of networks that are tightly coupled or nonlinear relationships in the data. To ad- dress these challenges, we introduce a new framework called flexible subspace analysis (FSA) that can auto- matically identify subspaces (groupings of unimodal or multimodal components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus leveraging the power of deep learning. We will apply the developed models to a large longitudinal dataset of individuals at various stages of cognitive impair- ment and dementia. Using follow-up outcomes data we will evaluate the predictive accuracy of a joint analysis compared to a unimodal analysis, as well as its ability to characterize various clinical subtypes including those driven by vascular effects including subcortical ischemic vascular dementia versus those that are more neuro- degenerative. We will evaluate the single subject predictive power of these profiles in independent data to max- imize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data promises to advance our understanding of Alzheimer’s and related disorders in addition to providing new tools that can be widely applied to other studies of complex disease. 3 Project Narrative  It is clear that multimodal data fusion provides benefits over unimodal analysis, however existing approaches typically require the data to have matched dimensionality, leading to a loss of information. In addition, most models assume linear relationships, despite strong evidence of nonlinear relationships in the data. We propose to develop new flexible models to capture multi-scale brain imaging and genomics data which we will use to study a large data set of individuals with Alzheimer’s disease and Alzheimer’s disease related disorders. 2",Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders,9826772,RF1AG063153,"['3-Dimensional', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Behavior', 'Benchmarking', 'Biological', 'Blood Vessels', 'Brain', 'Brain imaging', 'Brain region', 'Candidate Disease Gene', 'Categories', 'Classification', 'Communities', 'Complex', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Dementia', 'Diagnostic', 'Dimensions', 'Disease', 'Evaluation', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Image', 'Impaired cognition', 'Individual', 'Joints', 'Lead', 'Linear Models', 'Link', 'Magnetic Resonance Imaging', 'Meta-Analysis', 'Methods', 'Modality', 'Modeling', 'Motivation', 'Nerve Degeneration', 'Neurobiology', 'Noise', 'Outcome', 'Pathway interactions', 'Pattern', 'Research Personnel', 'Rest', 'Sampling', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Structure', 'Subgroup', 'Time', 'Vascular Dementia', 'Work', 'base', 'blind', 'clinical subtypes', 'connectome', 'data anonymization', 'data warehouse', 'deep learning', 'flexibility', 'follow-up', 'functional genomics', 'genome-wide analysis', 'genomic biomarker', 'genomic data', 'longitudinal dataset', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'patient subsets', 'statistics', 'structural genomics', 'subcortical ischemic vascular disease', 'tool', 'user friendly software', 'white matter damage']",NIA,GEORGIA STATE UNIVERSITY,RF1,2019,3319889,-0.06498701826210457
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine No abstract available PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,10063300,U01LM012675,[' '],NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2019,375751,5.114177645386762e-05
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9752596,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,323659,-0.01592777280271827
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9731544,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'repository', 'research and development', 'software development', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,350620,0.00532311996938296
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,9573854,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2018,388750,-0.010263360100326944
"Machine Learning and Deep Learning Solutions Supplement: Matching Methods for Causal Inference with Complex Data NARRATIVE SUMMARY The landscape of data formats is rapidly expanding, with image, text and other complex formats becoming available for health related outcomes. By considering such data within the context of observational causal inference, they can be leveraged to improve clinical decisions, help evaluate treatment efficacy by estimating individualized treatment effects and help develop intelligent therapeutic systems where individualized treatments can be deployed. In R01EB025021, we concentrate on understanding how nearly exact matching can be achieved in the presence of a large number of categorical covariates. The proposed approach (called FLAME - Fast Large Almost Matching Exactly) is able to quickly learn which categorical covariates are important and to produce high quality matches \citep{wang2017flame,dieng2018collapsing}. The main shortfall in the proposed work for R01EB025021 is that it does not naturally extend to more complex data types, it only works for categorical data in which each feature is meaningful. {\bf This proposal will develop new statistical and computational tools for causal analysis of complex data structures.} Our new approach is called {\emph Matching After Learning to Stretch (MALTS)}. For each unit (e.g. patient), we propose learn a latent representation of their covariate information and a distance metric on the latent space such that units that are matched tend to provide accurate estimates of treatment effect. MALTS can use deep learning to encode the latent representations for the units, or it can learn basis transformations in linear space (stretching and rotation matrices) for simpler continuous data types. We will develop the MALTS algorithm, and apply it in a medical context. Our goal is to construct high quality matches for the following types of data: (i) medical images, such as x-rays and CT scans, (ii) medical record data, (iii) time series data (continuous EEG data), (iv) a combination of any of the first three types of data. We aim to leverage the newly developed tools to continue our evaluation of the efficacy of isolation for flu-like ailments as well as to apply them more broadly to publicly available modern datasets such as the MIMIC III database. Reliable and consistent causal analysis of public health interventions requires the use of massive previously unavailable datastreams. For example, evaluation of the efficacy of isolation interventions on flu-like-illness spread must include information on friendships and interactions between individuals, biometric information, imaging, longitudinal health record data as well as standard demographic data. The proposed research provides machine learning and deep learning tools for properly employing this data for the identification and quantification of causal effects of such treatments that can lead to the development of better public health interventions.",Machine Learning and Deep Learning Solutions Supplement: Matching Methods for Causal Inference with Complex Data,9750434,R01EB025021,"['Algorithms', 'Biometry', 'Categories', 'Clinical', 'Complex', 'Data', 'Data Set', 'Databases', 'Development', 'Electroencephalography', 'Friendships', 'Goals', 'Health', 'Image', 'Individual', 'Intervention', 'Lead', 'Learning', 'Machine Learning', 'Medical', 'Medical Imaging', 'Medical Records', 'Methods', 'Modernization', 'Outcome', 'Patients', 'Research', 'Roentgen Rays', 'Rotation', 'Series', 'Stretching', 'Structure', 'System', 'Text', 'Therapeutic', 'Time', 'Treatment Efficacy', 'Work', 'X-Ray Computed Tomography', 'computerized tools', 'data format', 'deep learning', 'efficacy evaluation', 'flu', 'health record', 'improved', 'individualized medicine', 'novel strategies', 'public health intervention', 'tool', 'treatment effect']",NIBIB,DUKE UNIVERSITY,R01,2018,98714,-0.004215608454586865
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9527181,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,545116,0.001539555166720194
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9532186,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2018,259358,0.013982413175843787
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9520706,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2018,591130,-0.006252749219229742
"Machine Learning for Generalized Multiscale Modeling Project Summary/Abstract  This project develops machine learning approaches that describe statistical systems in biology. By combining analytic results calculated from the exact probabilistic description of the system with machine learning inference, our new methods present exciting opportunities to model previously inaccessible complex dynamics. The resulting Boltzmann machine-like learning algorithms present a new class of modeling techniques based on the powerful in- ference of arti cial neural networks. Further development of this approach will bring the groundbreaking advances from the surge of recent interest in machine learning into the biological modeling eld. The mathematical methods we develop will be used to derive e cient algorithms for multiscale simulation, directly applicable to large scale biological modeling. In particular, the algorithms will be used to study the dynamics of stochastic biochemistry at synapses, with direct relevance to learning and memory formation in the brain. Current studies of these processes are limited by the long timescales involved and the highly spatially organized structures featured. In addition to leveraging the machine learning expertise we are developing, we also employ new electron microscopy datasets to produce 3D reconstructions of neural tissue with unprecedented accuracy. Consequentially, we will be able to study the fundamental mechanisms underlying synaptic plasticity, as well as the biochemical basis of oscillatory behavior in networks of neurons that occurs during sleep. Furthermore, the interactions of these highly stochastic ion channels with electrical in neurons will be explored through groundbreaking hybrid simulation environments. The software that we will develop combines existing popular simulation tools into multiscale approaches, and will be distributed as a powerful tool to the broader biological modeling community. Its usage in further computational experiments can present a key advancement in the development of pharmaceuticals, allowing the direct study of the interactions of biochemistry and whole neuron electrophysiology without making limiting assumptions to sim- plify the simulations. This has promising implications for intervening in age-related learning de cits, as well as in neurological disorders such as Alzheimers. Finally, this proposal will bring together our existing multiscale modeling community, the National Center for Multi-scale Modeling of Biological Systems (MMBioS), with the MSM consortium. The interactions of these organizations and their communities of expert researchers will foster new collaborative work on exciting multiscale problems in biology, including applications of the machine learning frameworks and software we are developing. 1 Project Narrative  A wide variety of biological systems can be described statistically, from molecular biochemistry up to the network level activity of neurons. This work develops machine learning approaches to approximate these systems, enabling new simulation methods that bridge di erent levels of description. The resulting computational studies aim to shed light on the basis of learning and computation in the brain, and will enable the development of pharmaceutical targets for learning de cits associated with aging and neurological disorders such as Alzheimers. 1",Machine Learning for Generalized Multiscale Modeling,9791802,R56AG059602,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Area', 'Behavior', 'Biochemical', 'Biochemistry', 'Biological Models', 'Biological Neural Networks', 'Biology', 'Brain', 'Calcium', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consequentialism', 'Coupling', 'Data Set', 'Development', 'Dimensions', 'Electron Microscopy', 'Electrophysiology (science)', 'Environment', 'Equation', 'Equilibrium', 'Evolution', 'Fostering', 'Hybrids', 'Image', 'Investigation', 'Ion Channel', 'Learning', 'Libraries', 'Light', 'Machine Learning', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Morphology', 'National Institute of General Medical Sciences', 'Neurons', 'Neuropil', 'Neurosciences', 'Pharmacologic Substance', 'Physics', 'Population', 'Potassium Channel', 'Process', 'Pythons', 'Reaction', 'Research Personnel', 'Sleep', 'Structure', 'Synapses', 'Synaptic plasticity', 'System', 'Techniques', 'Time', 'Tissues', 'United States National Institutes of Health', 'Vertebral column', 'Work', 'age related', 'base', 'biological systems', 'calmodulin-dependent protein kinase II', 'computer studies', 'experimental study', 'information processing', 'insight', 'interest', 'mathematical methods', 'men who have sex with men', 'microscopic imaging', 'multi-scale modeling', 'nervous system disorder', 'particle', 'postsynaptic', 'reconstruction', 'relating to nervous system', 'simulation', 'software development', 'success', 'tool', 'working group']",NIA,UNIVERSITY OF CALIFORNIA-IRVINE,R56,2018,619053,-0.006062348982001519
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9523638,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2018,478806,0.0027991605880493225
"Machine Learning for Identifying Adverse Drug Events ﻿    DESCRIPTION (provided by applicant): Because of the profound effect of adverse drug events (ADEs) on patient safety, the FDA, AHRQ and Institute of Medicine have flagged post-marketing pharmacovigilance of emerging medications as a high national research priority. The FDA, Foundation for the NIH and PhARMA formed the Observational Medical Outcomes Partnership (OMOP) to develop and compare methods for identification of ADEs, and the FDA announced its Sentinel Initiative. Congress created the Reagan Udall Foundation (RUF) for the FDA in response to the FDA's own ""FDA Science and Mission at Risk"" report, and two years ago OMOP activities were incorporated into RUF. As the FDA moves forward with its development of Sentinel, including work on Mini-Sentinel, there is a need for researchers around the country to continue to develop better methods, and better evaluation methodologies for those methods. A robust research community working on algorithms for pharmacosurveillance, using electronic health records (EHRs) and claims databases will provide a substrate of ever-improving methods on which the nation's regulatory pharmacovigilance infrastructure can build. Indeed an important motivation of OMOP and Mini-Sentinel was to spur the development of such a community. Machine learning has attracted widespread attention across a range of disciplines for its ability to construct accurate predictive models. Therefore machine learning is especially appropriate for the problems of ADE identification and prediction: identifying ADEs from observational data, and predicting which patients are most at risk of suffering the identified ADE. Our current award has demonstrated the ability of machine learning to address both of these tasks. It has added to the existing evidence that consideration of temporal ordering of events, such as drug exposure and diagnoses, is critical for accuracy in identification and prediction of ADEs. The proposed work seeks to further improve upon these methods by building on recent advances in the field of machine learning, by our group and by others, in graphical model learning and in explicit modeling of irregularly-sampled temporal data. The latter is especially important because observational health databases, such as EHRs and claims databases, are not simple time series. Patients typically do not come into the clinic at regular intervals and have the same labs, vitals, and other measurements in lock step with one another. Building better ADE detection and prediction algorithms cannot be accomplished simply by machine learning research, even if that research is taking account of related work from relevant parts of computer science, statistics, biostatistics, epidemiology, pharmaco-epidemiology, and clinical research. Better methods are needed also for evaluation, that is, for estimating how well a new algorithm, or a new use of an existing algorithm, will perform at identifying ADEs associated with a new drug on the market, or at predicting which patients are most at risk of that ADE. More research and evaluation is also needed at the systems level: how can we best construct end-to-end pharmacovigilance systems that sit atop a large observational database and flag potential ADEs for human experts to further investigate? What kinds of information and statistics should such a system provide to the human experts?        This renewal will address the following aims: (1) improve upon machine learning methods for identification and prediction of ADEs, taking advantage of synergies between these two distinct tasks; (2) improve upon existing methods for evaluating ADE detection, building on advances in machine learning for information extraction from scientific literature; (3) improve upon existing methods for evaluating ADE prediction, building upon advances in machine learning for automated support of phenotyping and also building upon improved methods for efficiently obtaining expert labeling of borderline examples of a phenotype; and (4) use the methods developed in the first three aims to construct and evaluate an end-to-end pharmacosurveillance system integrated with the Marshfield Clinic EHR Data Warehouse. Machine learning plays a central and unifying role throughout all four aims. Our investigator team consists of machine learning researchers with experience in analysis of clinical, genomic, and natural language data (Page, Natarajan), a leading pharmaco-epidemiologist with expertise in building systems to efficiently obtain expert evaluation and labeling of phenotypes (Hansen), a leader in phenotyping from EHR data (Peissig), and an MD/PhD practicing physician with years of experience and leadership in the study of ADEs (Caldwell). In addition to building on results of the prior award, we will build on our experiences with OMOP, the International Warfarin Pharmacogenetics Consortium, the DARPA Machine Reading Program, and interactions with the FDA. PUBLIC HEALTH RELEVANCE: Adverse drug events (ADEs) carry a high cost each year in life, health and money. Congress, the FDA, the NIH and PhARMA have responded with new initiatives for identifying and predicting occurrences of ADEs. It has been widely recognized within initiatives such as Sentinel and the Observational Medical Outcomes Partnership that addressing ADEs requires data, standards and methods for data analysis and mining. This proposal addresses the need for new methods for both identifying previously- unanticipated ADEs and predicting occurrences of a known ADE. It also addresses the needs for improved evaluation and integrated systems approaches.",Machine Learning for Identifying Adverse Drug Events,9522037,R01GM097618,"['Address', 'Adverse drug effect', 'Adverse drug event', 'Algorithms', 'Attention', 'Award', 'Biometry', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Communities', 'Congresses', 'Country', 'Coxibs', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Discipline', 'Doctor of Philosophy', 'Drug Exposure', 'Early Diagnosis', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Evaluation Methodology', 'Event', 'Foundations', 'Genomics', 'Health', 'Human', 'Institute of Medicine (U.S.)', 'International', 'Label', 'Leadership', 'Learning', 'Life', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Markov Chains', 'Measurement', 'Medical', 'Methods', 'Mission', 'Modeling', 'Monitor', 'Motivation', 'Myocardial Infarction', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenetics', 'Phenotype', 'Physicians', 'Play', 'Process', 'Reading', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Priority', 'Risk', 'Role', 'Safety', 'Sampling', 'Science', 'Sentinel', 'Series', 'Serious Adverse Event', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Time', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Validation', 'Warfarin', 'Wisconsin', 'Work', 'base', 'computer science', 'cost', 'data mining', 'data warehouse', 'experience', 'improved', 'interest', 'learning strategy', 'natural language', 'novel', 'novel therapeutics', 'patient safety', 'prediction algorithm', 'predictive modeling', 'programs', 'public health relevance', 'response', 'statistics', 'synergism']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R01,2018,536041,-0.020939688290335996
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9503469,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Work', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial and ethnic', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2018,505649,-0.031016580773474814
"A Modular Automated Platform for Large-scale Drosophila Experiments and Handling PROJECT SUMMARY / ABSTRACT Animal model systems are a powerful tool researchers use to investigate almost all aspects of biology: genetics, development, neuroscience, disease, and more. And fruit flies – Drosophila melanogaster – with their small size, easy care, and remarkable array of available genetic toolkits, occupy a sweet spot on the model organism spectrum. Over 75% of human diseases with a genetic basis have an analogue in the fly, and Drosophila have been a part of the research for six Nobel prizes. Furthermore, the advent of CRISPR/cas9 and other modern genetic tools has opened the door to modeling other diseases and pathways, leading to greater use of Drosophila for drug screens. A great deal of the work (and the majority of the budget) involved in fly experiments is tedious manual labor, and with advances in computer vision, machine learning, and other analytic techniques, the stage is set to automate many phenotypic screens. In this Phase I SBIR, we propose a robotic system – modular automated platform for large-scale experiments (MAPLE) – that can accomplish a wide variety of fly-handling tasks in Drosophila labs. This robot is the fruit fly version of a liquid handling robot, with a large, open workspace that can house a plethora of modules and several manipulators that can move small parts and animals around that workspace. Building on a collaboration between the de Bivort Lab and FlySorter completed in 2017, we will design, fabricate and validate a commercial system that can collect virgin flies, run behavioral assays, conduct drug screens, and adapt to the needs of fly labs through easy-to-code Python scripts. By strategically combining modules and instructions to the robot, MAPLE can perform a wide variety of tasks in a fly lab, saving experimentalists from repetitive chores, cutting labor costs, and increasing scientific output. Just as pipette robots have become standard equipment in wet labs, we envision our fly handling robot will be the engine that powers Drosophila labs in academia and pharma, enabling new kinds of experiments and freeing researchers from the drudgery of fly pushing. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are a powerful model organism used in the study of disease, neuroscience, development, genetics, and recently in drug screens, too, largely through phenotypic screening. This labor-intensive work is time consuming and expensive, and ripe for automation. We propose a fly-handling robot – analogous to a liquid pipetting robot in a wet lab – that can perform a variety of tasks in Drosophila labs, free researchers from the drudgery of fly pushing, and enable a broader spectrum of experiments that will increase scientific knowledge.",A Modular Automated Platform for Large-scale Drosophila Experiments and Handling,9623017,R43MH119092,"['Academia', 'Address', 'Affect', 'Air', 'Anesthesia procedures', 'Animal Model', 'Animals', 'Architecture', 'Automation', 'Basic Science', 'Behavior', 'Behavioral Assay', 'Biological Models', 'Biology', 'Budgets', 'CRISPR/Cas technology', 'Carbon Dioxide', 'Caring', 'Code', 'Collaborations', 'Computer Vision Systems', 'Computer software', 'Computers', 'Custom', 'Data Collection', 'Deposition', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Screening', 'Drug usage', 'Ensure', 'Equipment', 'Feedback', 'Genetic', 'Genetic Screening', 'Genetic study', 'Grant', 'Hand', 'Human', 'Instruction', 'Knowledge', 'Libraries', 'Liquid substance', 'Machine Learning', 'Manuals', 'Modeling', 'Modernization', 'Neurosciences', 'Nobel Prize', 'Organism', 'Output', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Protocols documentation', 'Pythons', 'Reagent', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Running', 'Savings', 'Scanning', 'Small Business Innovation Research Grant', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Transgenic Organisms', 'Travel', 'Universities', 'Update', 'Vacuum', 'Work', 'analog', 'bone', 'cost', 'design', 'drug discovery', 'experimental study', 'flexibility', 'fly', 'graduate student', 'health science research', 'human disease', 'improved', 'operation', 'programs', 'repository', 'robot control', 'screening', 'tool', 'touchscreen']",NIMH,"FLYSORTER, LLC",R43,2018,348007,-0.017935052177145545
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,-0.007666549594065681
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,-0.01689336472728789
"Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics The human microbiota plays an important role in health and disease, and its therapeutic manipulation is being actively investigated for a wide range of diseases that span every NIH institute. Our microbiota are inherently dynamic, and analyzing these time-dependent properties is key to robustly linking the microbiota to disease, and predicting the effects of therapies targeting the microbiota; indeed, longitudinal microbiome data is being acquired with increasing frequency, and is a major component of many NIH-funded projects. However, there is currently a dearth of computational tools for analyzing microbiome time-series data, which presents several special challenges including high measurement noise, irregular and sparse temporal sampling, and complex dependencies between variables. The objective of this proposal is to introduce new capabilities, improve on, and provide state-of-the-art implementations of tools for analyzing dynamics, or patterns of change in microbiome time-series data. The tools we develop will use Bayesian machine learning methods, which are well-recognized for their strong conceptual and practical advantages, particularly in biomedical domains. Tools will be rigorously tested and validated on synthetic and real human microbiome data, including publicly available datasets and those from collaborators providing 16S rRNA sequencing, metagenomic, and metabolomics data. We propose three specific aims. For Aim 1, we will develop integrated Bayesian machine learning tools for predicting population dynamics of the microbiome and its responses to perturbations. These tools will include a new model that simultaneously learns groups of microbes with similar interaction structure and predicts their behavior over time, and that incorporates prior phylogenetic information. The model will be further improved by incorporating stochastic microbial dynamics and errors in measurements throughout the model. For Aim 2, we will develop Bayesian machine learning tools to predict host status from microbiome dynamics. The tools will learn easily interpretable, human-readable rules that predict host status from microbiome time-series data, and will be further extended to handle a variety of longitudinal study designs. For Aim 3, we will engineer our microbiome dynamics analysis software tools for optimal performance, ease-of- use, maintainability, extensibility, and dissemination to the community. In total, the proposed work will yield a suite of contemporary software tools for analyzing microbiome dynamics, with expected broad use and major impact. The software will allow investigators to answer important scientific and translational questions about the microbiome, including discovering which microbial taxa or their metagenomes are affected over time by perturbations such as changes in diet or invasion by pathogens; predicting the effects of these perturbations over time, including changes in composition or stability of the gut microbiota; and finding temporal signatures in multi-‘omic microbiome data that predict disease risk in the human host. The human microbiota, or collection of micro-organisms living on and within us, plays an important role in health, and when disrupted or abnormal, may contribute to many types of diseases including infections, kidney diseases, bowel diseases, diabetes, heart diseases, arthritis, allergies, brain diseases, and cancer. Sophisticated computer-based tools are needed to make sense of human microbiota data, particularly time- series data, which can yield important insights into how our microbiomes change over time. This work will develop new and improved computer-based tools for analyzing microbiota time-series data, which will be made freely available and will enable scientists to increase our fundamental knowledge about how our microbiota affect us and ultimately to apply this knowledge to prevent and treat human illnesses.",Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics,9640012,R01GM130777,"['16S ribosomal RNA sequencing', 'Affect', 'Algorithms', 'Antibiotics', 'Arthritis', 'Autoimmunity', 'Behavior', 'Biological Markers', 'Biological Models', 'Brain Diseases', 'Cardiovascular Diseases', 'Childhood', 'Clostridium difficile', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Dependence', 'Diabetes Mellitus', 'Diet', 'Disease', 'Engineering', 'Environmental Exposure', 'Frequencies', 'Funding', 'Health', 'Heart Diseases', 'Human', 'Human Microbiome', 'Hypersensitivity', 'Infection', 'Institutes', 'Intervention', 'Intestines', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Learning', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Metagenomics', 'Microbe', 'Modeling', 'Names', 'Noise', 'Oligosaccharides', 'Outcome', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Play', 'Population Dynamics', 'Property', 'Pythons', 'Readability', 'Recurrence', 'Research Design', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Series', 'Shotguns', 'Software Engineering', 'Software Tools', 'Speed', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Work', 'base', 'computerized tools', 'design', 'disorder risk', 'dynamic system', 'gut microbiota', 'human data', 'human microbiota', 'human subject', 'improved', 'insight', 'learning strategy', 'man', 'metabolomics', 'metagenome', 'microbial', 'microbiome', 'microbiome analysis', 'microbiome sequencing', 'microbiota', 'microorganism', 'multiple omics', 'nervous system disorder', 'novel', 'open source', 'pathogen', 'predictive tools', 'prevent', 'response', 'software development', 'targeted treatment', 'tool']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R01,2018,327382,0.006591634814488331
"A novel computing framework to automatically process cardiac valve image data and predict treatment outcomes PROJECT SUMMARY  There is a massive amount of clinical three-dimensional (3D) cardiac image data available today in numerous hospitals, but such data has been considerably underutilized in both clinical and engineering analyses of cardiac function. These 3D data offers unique and valuable information, allowing researchers to develop innovative, personalized approaches to treat diseases. Furthermore, using these 3D datasets as input to computational models can facilitate a population-based analysis that can be used to quantify uncertainty in treatment procedures, and can be utilized for virtual clinical trials for innovative device development. However, there are several critical technical bottlenecks preventing simulation-based clinical evaluation a reality: 1) difficulty in automatic 3D reconstruction of thin complex structures such as heart valve leaflets from clinical images, 2) computational models are constructed without mesh correspondence, which makes it challenging to run batch simulations and conduct large patient population data analyses due to inconsistencies in model setups, and 3) computing time is long, which inhibits prompt feedback for clinical use.  A potential paradigm-changing solution to the challenges is to incorporate machine learning algorithms to expedite the geometry reconstruction and computational analysis procedures. Therefore, the objective of this proposal is to develop a novel computing framework, using advanced tissue modeling and machine learning techniques, to automatically process pre-operative clinical image data and predict post-operative clinical outcomes. Transcatheter aortic valve replacement (TAVR) intervention will serve as a testbed for the modeling methods. In Aim 1, we will develop novel shape dictionary learning (SDL) based methods for automatic reconstruction of TAVR patient aortic valves. Through the modeling process, mesh correspondence will be established across the patient geometric models. The distribution and variation of TAVR patient geometries will be described by statistical shape models (SSMs). In Aim 2, population-based FE analysis of the TAVR procedure will be conducted on thousands of virtual patient models generated by the SSMs (Aim 1). A deep neural network (DNN) will be developed and trained to learn the relationship between the TAVR FE inputs and outputs. Successful completion of this study will result in a ML-FE surrogate for TAVR analysis, combining the automated TAVR patient geometry reconstruction algorithms and the trained DNN, to provide fast TAVR biomechanics analysis without extensive re-computing of the model. Furthermore, the algorithms developed in this study can be generalized for other applications and devices. PROJECT NARRATIVE Current clinical image modalities can be utilized to develop patient-specific computational models to pre-operatively plan transcatheter aortic valve replacement (TAVR) procedures. However, the computational modeling and simulation processes are time-consuming, which limits clinical translatability. Thus, the objective of this proposal is to develop algorithms using machine learning techniques to rapidly process and predict TAVR computational simulation outcomes directly from clinical image data.",A novel computing framework to automatically process cardiac valve image data and predict treatment outcomes,9518217,R01HL142036,"['Adverse event', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Attention', 'Biomechanics', 'Biomedical Computing', 'Clinical', 'Clinical Engineering', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Coronary Occlusions', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Device Designs', 'Device or Instrument Development', 'Devices', 'Dictionary', 'Dimensions', 'Disease', 'Elements', 'Evaluation', 'Extravasation', 'Feedback', 'Finite Element Analysis', 'Generations', 'Geometry', 'Goals', 'Guidelines', 'Heart Valves', 'Hospitals', 'Hour', 'Human', 'Image', 'Intervention', 'Laboratories', 'Language', 'Learning', 'Left ventricular structure', 'Machine Learning', 'Manuals', 'Methods', 'Mitral Valve', 'Modeling', 'Outcome', 'Output', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Postoperative Period', 'Problem Sets', 'Procedures', 'Process', 'Property', 'Research Personnel', 'Response Elements', 'Running', 'Rupture', 'Sampling', 'Shapes', 'Statistical Data Interpretation', 'Stents', 'Structure', 'Techniques', 'Testing', 'Thinness', 'Time', 'Tissue Model', 'Training', 'Translations', 'Treatment outcome', 'Uncertainty', 'Variant', 'X-Ray Computed Tomography', 'aortic valve', 'aortic valve replacement', 'ascending aorta', 'base', 'calcification', 'clinical application', 'clinical imaging', 'clinical practice', 'clinically translatable', 'deep learning', 'deep neural network', 'heart function', 'heart imaging', 'imaging modality', 'improved', 'innovation', 'models and simulation', 'novel', 'patient population', 'personalized approach', 'population based', 'prevent', 'reconstruction', 'research clinical testing', 'simulation', 'speech recognition', 'time resolved data', 'two-dimensional', 'virtual', 'virtual clinical trial']",NHLBI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2018,392932,-0.013739946571871694
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9674585,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Machine Learning', 'Marines', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'dark matter', 'deep learning', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,289700,0.007258757341071691
"Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications Abstract Atrial fibrillation (AF) is a pervasive disease which affects over 30 million individuals worldwide, in whom it is associated with morbidity and mortality, yet for which therapeutic outcomes are suboptimal. One major limitation to mechanistic and clinical advances in AF is its taxonomy, which is based on number of days of detected AF rather than increasingly reported functional and personalized mechanisms. I reasoned that a digital and scalable AF taxonomy, based on interactions of anatomic and functional factors and clinical features, may better guide existing therapy and catalyze future mechanistic and therapeutic advances. I set out to create a predictive tool to guide therapy in AF patients using machine learning of rich mechanistic data from a large multicenter registry of patients undergoing ablation. I hypothesized that clinically actionable AF phenotypes can be defined by statistical clustering between electrophysiologic features, anatomic regions and clinical indices, that can be uncovered by physiological and statistical quantification and machine learning. I have two Specific Aims: 1) To construct a multimodal digital atlas of atrial fibrillation which registers functional indices at absolute and relative spatial locations in both atria from a multicenter registry, and make this atlas available as an open-source software resource. This deliverable will uniquely map the probability that specific mechanisms will be relevant to AF in a specific patient of given clinical characteristics. Novel pathophysiological phenotypes will be defined via probabilistic interactions in these individual components. 2) To develop a predictive tool using machine learning to estimate the likelihood that ablation at any site(s) will contribute to success tailored to individual characteristics, by learning clusters of electrophysiologic features, clinical indices, and anatomic regions in a training population and applying it to a validation cohort from a large multicenter registry. This project uses state-of-the-art computational tools and statistical methods that may reconcile divergent AF mechanistic hypotheses to define novel functional AF phenotypes and guide therapy. In the process, I will be mentored by world leading mentors, in an extraordinary training environment to facilitate this development into an independent physician-scientist in bioengineering-heart rhythm medicine. Project Narrative This research provides an avenue to define atrial fibrillation in an actionable classification rooted in pathophysiologic and mechanistic observations. Such a classification scheme would further our understanding and refine our conversation about complex arrhythmia in cardiac tissue. Only an understanding at this level is will provide truly effective and safe treatments of each individual patient’s arrhythmic condition.","Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications",9611012,F32HL144101,"['Ablation', 'Affect', 'Anatomy', 'Anti-Arrhythmia Agents', 'Applications Grants', 'Arrhythmia', 'Atlases', 'Atrial Fibrillation', 'Biological Neural Networks', 'Biomedical Engineering', 'Cardiac', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Communities', 'Comorbidity', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Electrophysiology (science)', 'Enrollment', 'Environment', 'Faculty', 'Foundations', 'Freedom', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Heart Atrium', 'Individual', 'Injury', 'Language', 'Learning', 'Location', 'Machine Learning', 'Maps', 'Measurable', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Mission', 'Morbidity - disease rate', 'Obstructive Sleep Apnea', 'Patients', 'Pharmacotherapy', 'Phenotype', 'Physicians', 'Physiological', 'Plant Roots', 'Population', 'Probability', 'Procedures', 'Process', 'Pulmonary veins', 'Randomized Clinical Trials', 'Registries', 'Reporting', 'Research', 'Resources', 'Scientist', 'Site', 'Statistical Methods', 'Structure', 'Supervision', 'Taxonomy', 'Testing', 'Therapeutic', 'Therapy trial', 'Tissues', 'Training', 'Translations', 'United States National Institutes of Health', 'Validation', 'base', 'clinically actionable', 'cohort', 'computer science', 'computerized tools', 'deep learning', 'digital', 'disease classification', 'health care service utilization', 'heart rhythm', 'improved outcome', 'indexing', 'individual patient', 'mortality', 'multimodality', 'novel', 'open source', 'patient registry', 'patient response', 'patient stratification', 'predictive tools', 'success', 'therapy outcome', 'tool', 'trial design']",NHLBI,STANFORD UNIVERSITY,F32,2018,63034,-0.011587517613108196
"Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways ﻿    DESCRIPTION (provided by applicant): This project aims to develop new statistical machine learning methods for metabolomics data from diverse platforms, including targeted and unbiased/global mass spectrometry (MS), labeled MS experiments for measuring metabolic ﬂux and Nuclear Magnetic Resonance (NMR) platforms. Unbiased MS and NMR proﬁling studies result in identifying a large number of unnamed spectra, which cannot be directly matched to known metabolites and are hence often discarded in downstream analyses. The ﬁrst aim develops a novel kernel penalized regression method for analysis of data from unbiased proﬁling studies. It provides a systematic framework for extracting the relevant information from unnamed spectra through a kernel that highlights the similarities and differences between samples, and in turn boosts the signal from named metabolites. This results in improved power in identiﬁcation of named metabolites associated with the phenotype of interest, as well as improved prediction accuracy. An extension of this kernel-based framework is also proposed to allow for systematic integration of metabolomics data from diverse proﬁling studies, e.g. targeted and unbiased MS proﬁling technologies. The second aim pro- vides a formal inference framework for kernel penalized regression and thus complements the discovery phase of the ﬁrst aim. The third aim focuses on metabolic pathway enrichment analysis that tests both orchestrated changes in activities of steady state metabolites in a given pathway, as well as aberrations in the mechanisms of metabolic reactions. The fourth aim of the project provides a uniﬁed framework for network-based integrative analysis of static (based on mass spectrometry) and dynamic (based on metabolic ﬂux) metabolomics measurements, thus providing an integrated view of the metabolome and the ﬂuxome. Finally, the last aim implements the pro- posed methods in easy-to-use open-source software leveraging the R language, the capabilities of the Cytoscape platform and the Galaxy workﬂow system, thus providing an expandable platform for further developments in the area of metabolomics. The proposed software tool will also provide a plug-in to the Data Repository and Coordination Center (DRCC) data sets, where all regional metabolomics centers supported by the NIH Common Funds Metabolomics Program deposit curated data. PUBLIC HEALTH RELEVANCE: Metabolomics, i.e. the study of small molecules involved in metabolism, provides a dynamic view into processes that reﬂect the actual physiology of the cell, and hence offers vast potential for detection of novel biomarkers and targeted therapies for complex diseases. However, despite this potential, the development of computational methods for analysis of metabolomics data lags the rapid growth of metabolomics proﬁling technologies. The current application addresses this need by developing novel statistical machine learning methods for integrative analysis of static and dynamic metabolomics measurements, as well as easy-to-use open-source software to facilitate the application of these methods.",Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways,9462161,R01GM114029,"['Address', 'Adoption', 'Anabolism', 'Area', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Assay', 'Cardiovascular Diseases', 'Cell physiology', 'Cells', 'Characteristics', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Deposition', 'Detection', 'Development', 'Diabetes Mellitus', 'Disease', 'Environment', 'Environmental Risk Factor', 'Equilibrium', 'Funding', 'Galaxy', 'Homeostasis', 'Imagery', 'Knowledge', 'Label', 'Language', 'Letters', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Names', 'Network-based', 'Nuclear Magnetic Resonance', 'Pathway interactions', 'Phase', 'Phenotype', 'Plug-in', 'Procedures', 'Process', 'Prognostic Marker', 'Proteomics', 'Reaction', 'Sampling', 'Signal Transduction', 'Software Tools', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Work', 'base', 'biological systems', 'biomarker discovery', 'data warehouse', 'diagnostic biomarker', 'experimental study', 'flexibility', 'high dimensionality', 'improved', 'insight', 'interest', 'learning strategy', 'metabolome', 'metabolomics', 'new technology', 'novel', 'novel diagnostics', 'novel marker', 'open source', 'programs', 'public health relevance', 'rapid growth', 'response', 'small molecule', 'targeted treatment', 'tool', 'transcriptomics']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,338393,0.020232428570069322
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9789424,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2018,293560,-0.007816809302741054
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9633463,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2018,1583573,-0.007816809302741054
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9525704,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,599947,-0.002252379758272073
"Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry PROJECT SUMMARY  Cellular interactions with the environment form the basis of health and disease for all organisms. Exposure to nutrients, toxins, and neighboring cells trigger coordinated molecular responses that impact cell function and metabolism in a beneficial, adaptive, or detrimental manner. Although the benefits of multicellularity for the formation of complex tissue structures or the function of entire organ systems has been long appreciated, it has only recently been understood that microbial inhabitants of vertebrates also have a tremendous impact on host cell function and dysfunction. Despite this, an understanding of these interactions has not moved beyond simple associations, and there are virtually no molecular technologies available that adequately define how a complex microbial ecosystem impacts host cell function, or how the host response to microbial colonization affects the bacterial community. This gap in knowledge is striking when one considers the broad and significant impact that microbes have on human health. In this application, we propose to expressly fill this knowledge gap through development of a novel multimodal imaging pipeline that will provide 3-dimensional information on the molecular heterogeneity of microbial communities and the immune response at the host-pathogen interface.  This proposal combines our expertise in immunology, infection biology, mass spectrometry, small animal imaging, machine learning, and computer vision to develop an integrated multimodal visualization method for studying infectious disease. Our unique approach will computationally combine ultra-high speed (~50px/s) MALDI-TOF images, ultra-high mass resolution (>200,000 resolving power) MALDI FTICR IMS, metal imaging by LA-ICP-IMS, high-spatial resolution optical microscopy, and MR imaging using data-driven image fusion. This strategy will enable 3-D molecular images to be generated for thousands of elements, metabolites, lipids, and proteins with an unprecedented combination of chemical specificity and spatial fidelity more than 50x faster than is currently possible. We will use this next-generation imaging capability to (i) define the heterogeneous microbial subpopulations throughout the 3-D volume of a S. aureus community, (ii) uncover the host molecules that form the abscess and accumulate to restrict microbial growth in murine models, and (iii) elucidate molecular markers that differentiate in vivo biofilms at the host-pathogen interface, between abscesses at various stages of progression, and under distinct degrees of nutrient stress. These studies will uncover new targets for therapeutic intervention and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research. PROJECT NARRATIVE This proposal will enable detailed views of the molecular components of infectious disease with unprecedented resolution through the development of a multimodal, 3-dimensional imaging platform. The proposed technologies will improve throughput and molecular specificity, enable automated high-precision and high-accuracy image alignment, and allow for descriptions of molecular signals in 3-D through the fusion of multi-modal imaging data. These studies will uncover targets for therapeutic intervention and antibiotic development and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research.",Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry,9659850,R01AI138581,"['3-Dimensional', 'Abscess', 'Affect', 'Animal Model', 'Animals', 'Anterior nares', 'Antibiotics', 'Antibodies', 'Architecture', 'Awareness', 'Bacteria', 'Bacterial Infections', 'Bacterial Proteins', 'Behavior', 'Biology', 'Biomedical Research', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Cellular Metabolic Process', 'Chemicals', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Custom', 'Data', 'Development', 'Diagnosis', 'Differentiation Antigens', 'Dimensions', 'Disease', 'Ecosystem', 'Elements', 'Environment', 'Exposure to', 'Fourier transform ion cyclotron resonance', 'Functional disorder', 'Glean', 'Growth', 'Health', 'Health Promotion', 'Heterogeneity', 'Histology', 'Human', 'Image', 'Imagery', 'Imaging technology', 'Immune', 'Immune response', 'Immunology', 'Imprisonment', 'Individual', 'Infection', 'Infectious Diseases Research', 'Integration Host Factors', 'Knowledge', 'Label', 'Lesion', 'Lipids', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mass Spectrum Analysis', 'Metals', 'Methodology', 'Methods', 'Microbe', 'Microbial Biofilms', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nutrient', 'Optics', 'Organism', 'Pathogenesis', 'Physiological', 'Population', 'Process', 'Proteins', 'Reagent', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Spatial Distribution', 'Specificity', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Speed', 'Staphylococcus aureus', 'Stress', 'Structure', 'Supervision', 'Techniques', 'Technology', 'Therapeutic Intervention', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Tissues', 'Toxin', 'Vertebrates', 'Work', 'animal imaging', 'bacterial community', 'base', 'body system', 'commensal bacteria', 'experimental study', 'host colonization', 'imaging capabilities', 'imaging detection', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'innovation', 'interest', 'microbial', 'microbial colonization', 'microbial community', 'microscopic imaging', 'molecular imaging', 'molecular marker', 'mouse model', 'multimodality', 'neutrophil', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'protein expression', 'response', 'targeted treatment', 'virtual']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2018,593526,-0.027582009515359343
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9519804,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'mild cognitive impairment', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2018,410000,-0.07122624888029289
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9553816,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2018,368020,-0.027423666432595514
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9530668,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Disease model', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Mathematics', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'experimental study', 'improved', 'infancy', 'insight', 'network models', 'novel', 'predictive modeling', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2018,584485,-0.0020303860461086815
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9749413,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,155743,0.008043049687097197
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9560825,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,674602,0.008043049687097197
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9545836,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'deep learning', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2018,569784,-0.0009304929526914965
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9577591,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Supervision', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'forest', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'tool', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2018,243750,-0.00925066404418964
"A computational approach to early sepsis detection Abstract Significance: In this SBIR project, we propose to improve the performance of InSight, a machine-learning- based sepsis screening system, in situations of limited training data from the target clinical site. The proposed work will make possible prospective clinical deployments to sites which are smaller or lack clinical data repositories, by significantly reducing the amount of training data necessary down to a few weeks of clinical observation. Classically, a machine-learning-based system like InSight requires complete retraining for each new clinical setting, in turn requiring a new and large collection of data from each target deployment site. We will circumvent this requirement via transfer learning techniques, which transfer knowledge acquired previously in a source clinical setting to a new, target setting. Research Questions: Which transfer learning methods and paired classification algorithms are most suitable for use with InSight, requiring minimal target-site training data while maintaining strong performance? Are these methods and algorithms robust across the several common sepsis-spectrum definitions? Prior Work: We have developed InSight using the MIMIC-III retrospective data set, on which it attains an area under the receiver operating characteristic curve (AUROC) of 0.88 for sepsis detection, and 0.74 for 4-hour early sepsis prediction. We have also conducted pilot transfer learning  ≥ experiments in a different clinical task, mortality forecasting, in which transfer learning yields a 10-fold reduction in the amount of target-site training data required to achieve AUROC 0.80. Specific Aims: Aim 1 - to implement and assess side-by-side four diverse transfer learning methods for a retrospective clinical sepsis prediction task, where the source data set is MIMIC-III and the simulated clinical target is a data set drawn from UCSF. Aim 2 - to determine which among the best methods from Aim 1 also provide robust performance when applied to two additional sepsis-spectrum gold standards. Methods: We will prepare implementations of transfer learning methods which use instance transfer, residual learning and/or feature augmentation, kernel length scale transfer, and feature transfer. We will test these methods with applicable classifiers on subsets of the UCSF set, using cross-validation and quantifying discrimination performance in terms of AUROC. The best method/classifier pairs will require no more than 30 examples of septic patients from the target set and attain AUROC superiorities of 0.05 in 0- and 4-hour pre-onset sepsis prediction/detection, relative to the best tested alternative screening systems (Aim 1). The top three pairs will then be tested for robustness to gold standard choice, using septic shock (0- and 4-hour) and SIRS-based sepsis (0-hour) gold standards; in these tests, at least one pair must again attain 0.05 margin of superiority in AUROC versus the alternative screening systems (Aim 2). Future Directions: The results of these experiments will enable InSight to be robustly deployed to diverse clinical sites, yielding high performance without the need for extensive target-site data acquisition. Narrative Clinical decision support (CDS) systems present critical information to medical professionals by examining patient data and providing relevant information. Machine learning is a powerful method for creating CDS tools, but accessing its full strength requires re-training with retrospective data from each target clinical site. We will use transfer learning techniques to dramatically reduce the amount of target-site training data required by InSight, our machine-learning-based CDS tool for sepsis prediction, and empirically evaluate several such methods on a patient data set, using three different sepsis-related gold standards.",A computational approach to early sepsis detection,9557664,R43TR002221,"['Address', 'Age', 'Algorithms', 'Area', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Collection', 'Custom', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Discrimination', 'Drops', 'Early Diagnosis', 'Early Intervention', 'Future', 'Gold', 'Healthcare', 'Healthcare Systems', 'Hour', 'Image', 'Immune response', 'Institution', 'Knowledge', 'Learning', 'Length', 'Machine Learning', 'Medical', 'Methods', 'Multicenter Studies', 'Nature', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Psychological Transfer', 'Receiver Operating Characteristics', 'Research', 'Residual state', 'Risk', 'SCAP2 gene', 'Sensitivity and Specificity', 'Sepsis', 'Septic Shock', 'Severities', 'Side', 'Site', 'Small Business Innovation Research Grant', 'Source', 'Survival Rate', 'System', 'Techniques', 'Testing', 'Training', 'Validation', 'Work', 'base', 'clinical data warehouse', 'clinical decision support', 'clinical research site', 'cost', 'data acquisition', 'experimental study', 'improved', 'insight', 'learning strategy', 'mortality', 'performance site', 'portability', 'prospective', 'screening', 'septic', 'septic patients', 'success', 'support tools']",NCATS,"DASCENA, INC.",R43,2018,310782,-0.0006284269681475616
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9549126,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2018,81977,-8.362175047626606e-05
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9422475,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,356646,0.015366060773164082
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,0.004401996491705753
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9635840,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2018,854309,-0.01038185953290107
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9406318,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cancer Etiology', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2018,356625,0.014723992563666022
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9465505,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,375463,0.005704982489860433
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9704539,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,57657,0.005704982489860433
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9448918,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'big biomedical data', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2018,308503,0.0005675679414603596
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9572992,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,297237,0.0022863702422892913
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9589711,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2018,439996,0.01886946853885739
"Robust Control of the Stem Cell Niche Diana Arguijo has a unique background with a double major in biomedical engineering (BME) and electrical and computer engineering (ECE). Leveraging her strong mathematical background, she will develop computational techniques to identify patterns of epigenetic reprogramming during epithelial development and patterns of real- time electrical recoding of the GI tract reflective of sacral nerve modulation. Her work will provide insights into the robustness and plasticity of underlying biological control schemes. Diana Arguijo will develop machine-learning based computational techniques to analyze epigenetic reprogramming of epithelial development and electrical activities of the enteric nervous system. Her analyses will provide insights into the robustness and plasticity of tissue regulation.",Robust Control of the Stem Cell Niche,9731853,R35GM122465,"['Biological', 'Biomedical Engineering', 'Computational Technique', 'Computers', 'Development', 'Engineering', 'Enteric Nervous System', 'Epigenetic Process', 'Epithelial', 'Gastrointestinal tract structure', 'Machine Learning', 'Mathematics', 'Pattern', 'Regulation', 'Sacral nerve', 'Scheme', 'Time', 'Tissues', 'Work', 'base', 'insight', 'stem cell niche']",NIGMS,DUKE UNIVERSITY,R35,2018,42822,-0.02807235076214104
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9527186,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2018,516810,-0.004442602507557163
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9474101,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'human microbiota', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'multidisciplinary', 'novel', 'open source', 'operational taxonomic units', 'oral behavior', 'oral microbial community', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,350321,-0.0027716792624386762
"Pacific Northwest Advanced Compound Identification Core OVERALL SUMMARY The capability to chemically identify thousands of metabolites and other chemicals in clinical samples will revolutionize the search for environmental, dietary, and metabolic determinants of disease. By comparison to near-comprehensive genetic information, comparatively little is understood of the totality of the human metabolome, largely due to insufficiencies in molecular identification methods. Through innovations in computational chemistry and advanced ion mobility separations coupled with mass spectrometry, we propose to overcome a significant, long standing obstacle in the field of metabolomics: the absence of methods for accurate and comprehensive identification of metabolites without relying on data from analysis of authentic chemical standards. A paradigm shift in metabolomics, we will use gas-phase molecular properties that can be both accurately predicted computationally and consistently measured experimentally, and which can thus be used for comprehensive identification of the metabolome without the need for authentic chemical standards. The outcomes of this proposal directly advance the mission and goals of the NIH Common Fund by: (i) transforming metabolomics science by enabling consideration of the totality of the human metabolome through optimized identification of currently unidentifiable molecules, eventually reaching hundreds of thousands of molecules, and (ii) developing standardized computational tools and analytical methods to increase the national capacity for biomedical researchers to identify metabolites quickly and accurately. This work is significant because it enables comprehensive and confident chemical measurement of the metabolome. This work is innovative because it utilizes an integrated quantum-chemistry and machine learning computational pipeline to accurately predict physical-chemical properties of metabolites coupled to measurements. OVERALL NARRATIVE This project will utilize integrated quantum-chemistry and machine learning computational computational approaches coupled with advanced instrumentation to characterize the human metabolome, and identify currently unidentifiable molecules without the use of authentic chemical standards. Results from these studies will contribute to the goal of understanding diseases, and the tools and resources will be made publically available for biomedical researchers.",Pacific Northwest Advanced Compound Identification Core,9589783,U2CES030170,"['Adoption', 'Algorithms', 'Analytical Chemistry', 'Attributes of Chemicals', 'Biological', 'Biological Markers', 'Biomedical Research', 'Chemical Structure', 'Chemicals', 'Clinical', 'Communities', 'Computer Simulation', 'Computers and Advanced Instrumentation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Databases', 'Dependence', 'Diet', 'Disease', 'Educational workshop', 'Engineering', 'Exposure to', 'Funding', 'Gases', 'Genetic', 'Goals', 'High Performance Computing', 'Human', 'Isotopes', 'Libraries', 'Liquid substance', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Molecular', 'Outcome', 'Pacific Northwest', 'Phase', 'Predictive Analytics', 'Probability', 'Procedures', 'Property', 'Reference Standards', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Science', 'Serum', 'Source', 'Standardization', 'Supercomputing', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Uncertainty', 'United States National Institutes of Health', 'Urine', 'Work', 'analytical method', 'base', 'chemical property', 'chemical standard', 'comparative', 'computational chemistry', 'computerized tools', 'dark matter', 'drug candidate', 'drug discovery', 'experience', 'genetic information', 'human disease', 'improved', 'innovation', 'instrumentation', 'ion mobility', 'metabolome', 'metabolomics', 'non-genetic', 'novel', 'novel therapeutics', 'programs', 'quantum chemistry', 'small molecule libraries', 'stereochemistry', 'tool']",NIEHS,BATTELLE PACIFIC NORTHWEST LABORATORIES,U2C,2018,1076717,0.015429367831007406
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9427988,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2018,162800,-0.07212322481595698
"An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography SUMMARY – OVERALL Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function. Although the basic electron cryo-tomography technique has been used for several decades, the technology is being revolutionized by recent advances in sample preparation, electron cryo-microscopy hardware, improved capabilities for automatic data collection, direct electron detection imaging devices, and phase plate technologies. Combined, these advances led to the ability to generate extraordinarily large numbers of cellular cryo-tomograms of exquisite quality. In principle, such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. However, collection of cellular data is now at a far faster rate than can currently be analyzed with existing methods, producing a serious barrier to progress: to match the data production rates of a single laboratory, at least 50 experienced scientists would need to handle the data analysis. The primary goal of this Program Project is to establish quantitative and highly automated tools for the reconstruction and interpretation of highly complex cellular tomographic data. We have assembled a highly synergistic team of PIs with complimentary expertise in cutting-edge computational and experimental electron microscopy techniques to achieve this goal through collaborative efforts. Project 1 (Hanein & Penczek) focuses on development and implementation of tomogram quality assessment and validation techniques and on experimentally guided optimization of data collection strategies. Project 2 focuses on automatic tomographic reconstruction technology, extraction of various features from the tomograms, and the analysis of distribution patterns derived from the extracted features. Project 3 focuses on development of quantitative tools for tomogram annotation through deep learning and sub-tomogram alignment as well as interactive visualization tools. The set of highly automated tools developed in this Program Project will permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study. NARRATIVE Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function and—with recent instrumental advances—it is now possible to image hundreds of cells per months, enabling collection of cellular data at a far faster rate than can currently be analyzed. Such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. This Program Project brings together an accomplished team of investigators to develop new strategies for effectively processing and interpreting this massive influx of data, developing a set of highly automated tools to permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study.",An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography,9416022,P01GM121203,"['Address', 'Algorithms', 'Artificial Intelligence', 'Big Data', 'Biological', 'Biological Neural Networks', 'Biology', 'Cancer Diagnostics', 'Cell physiology', 'Cells', 'Classification', 'Collection', 'Complex', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Disease', 'Electron Microscopy', 'Electrons', 'Environment', 'Floods', 'Goals', 'Health', 'Heart', 'Human', 'Image', 'Imaging Device', 'Individual', 'Knowledge', 'Laboratories', 'Methodology', 'Methods', 'Molecular', 'Molecular Structure', 'Morphology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Preparation', 'Process', 'Production', 'Real-Time Systems', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Tomogram', 'Validation', 'Variant', 'Visualization software', 'base', 'computer framework', 'deep learning', 'electron tomography', 'experience', 'imaging detection', 'improved', 'insight', 'knowledge base', 'learning strategy', 'nanomachine', 'novel diagnostics', 'particle', 'programs', 'reconstruction', 'response', 'software development', 'statistics', 'tomography', 'tool', 'virtual']",NIGMS,SANFORD BURNHAM PREBYS MEDICAL DISCOVERY INSTITUTE,P01,2018,981617,-0.028929419664939282
"2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Summary /Abstract:  The 2018 OSA Biophotonics Congress: Biomedical Optics, 3-6 April 2018, Hollywood, FL, consists of four topical meetings. Two of these meetings, Optical Tomography and Spectroscopy (OTS) and Microscopy, Histopathology and Analytics (Microscopy) provide broad exposure to a very active multidisciplinary field in biomedical imaging and bioengineering focused on illness treatment and health enhancement. The interdisciplinary nature of the co- located meetings will provide cross-fertilization of concepts and techniques between fields with the resulting synergies obtained from such interactions. This proposal is to provide registration and travel support for students and early career professionals presenting at one of these topical meetings.  OTS will focus on new developments in diffuse optics, spectroscopy and other non- invasive tomographic imaging approaches, including the fields of diffuse optical tomography (DOT), photoacoustic tomography (PAT), optical coherence tomography (OCT), wavefront engineering to overcome scattering, as well as new developments in spectroscopic technologies.  Microscopy will include topics central to the development of optical microscopy and in vitro optical sensing for the clinic. Areas such as novel optical approaches, including computational optics, new image processing and segmentation techniques, development of decision-assistance algorithms via machine-learning and other strategies, testing technologies in pre-clinical models, applications to clinical samples, and validation in the clinic will be discussed. Optically enabled microfluidics are included in this track as well. The goal of these efforts should be towards clinical translation.  The general purpose of these meetings is to create an inclusive, open forum for the presentation of high-quality scientific research through plenary and technical sessions, short courses, panels, networking and special events. This method of face-to-face information sharing allows researchers to learn what others in their field and related disciplines are doing and to efficiently learn about new research, tools, and techniques that might be relevant to their work. It allows conversations with colleagues from different institutions around the world and engenders far reaching scientific collaborations – both domestic and international. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering. Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Narrative The 2018 OSA Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics Topical Meetings will discuss important, highly interdisciplinary areas that focus on technological solutions to medical challenges and medical applications, and will cover a diversity of cutting-edge research and innovative new tools and techniques, especially in biomedical imaging and bioengineering. These Topical Meetings will bring together researchers working in all aspects of this field and will serve as a forum for discussion of existing and emerging techniques as well as future directions.","2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering.",9543795,R13EB026325,"['Academic Training', 'Algorithms', 'Area', 'Biomedical Engineering', 'Biophotonics', 'Birds', 'Career Mobility', 'Clinic', 'Clinical', 'Collaborations', 'Congresses', 'Development', 'Diffuse', 'Digital Libraries', 'Discipline', 'Doctor of Philosophy', 'Engineering', 'Ensure', 'Event', 'Exhibits', 'Exposure to', 'Fertilization', 'Fostering', 'Future', 'Goals', 'Grant', 'Health', 'Hearing', 'Histopathology', 'In Vitro', 'Industry', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Massachusetts', 'Medical', 'Methods', 'Microfluidics', 'Microscopy', 'National Institute of Biomedical Imaging and Bioengineering', 'Nature', 'Optical Coherence Tomography', 'Optical Tomography', 'Optics', 'Outcome', 'Paper', 'Participant', 'Peer Review', 'Physicians', 'Pre-Clinical Model', 'Principal Investigator', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Services', 'Societies', 'Special Event', 'Spectrum Analysis', 'Students', 'Techniques', 'Technology', 'Testing', 'Time', 'Translating', 'Travel', 'Underrepresented Minority', 'United States National Institutes of Health', 'Validation', 'Washington', 'Work', 'academic standard', 'base', 'bioimaging', 'career', 'clinical application', 'clinical translation', 'diffuse optical tomography', 'graduate student', 'image processing', 'imaging Segmentation', 'imaging approach', 'indexing', 'innovation', 'meetings', 'multidisciplinary', 'novel', 'optoacoustic tomography', 'posters', 'programs', 'symposium', 'synergism', 'technique development', 'tomography', 'tool']",NIBIB,OPTICAL SOCIETY OF AMERICA,R13,2018,10000,0.0010871895029819884
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9639469,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Research Infrastructure', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computer infrastructure', 'digital', 'disease transmission', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'predictive modeling', 'social', 'social media', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector', 'vector control']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2018,407175,0.009746786964973388
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9515964,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2018,341899,-0.05925774429233081
"mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment Project Summary NIH is increasing its investment in large mutli-center brain MRI studies via projects such as the recently announced BRAIN initiative. The success of these studies depends on the quality of MRIs and the resulting image measurements, regardless of sample size. Even though quality control of MRIs and corresponding measurements could be outsourced, most neuroscience studies rely on in-house procedures that combine automatically generated scores with manually guided checks, such as visual inspection. Implementing these procedures typically requires combining several open-source software systems. For example, the NIH NIAAA and BD2K funded Data Analysis Resource (DAR) of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) uses XNAT to consolidate the structural, diffusion, and functional MRIs acquired across five sites, and has also developed their own custom software package to comply with study requirements that called for a multi-tier, quality control (QC) workflow. However, these custom, one-off tools lack support for multi-site QC workflows as that would require a unified platform, design that supports collaboration and sharing, and strong cohesion between technologies. To improve the effectiveness of QC efforts specific to multi-center neuroimaging studies, we will develop a widely accessible and broadly compatible software platform that supports simplified creation of custom QC workflows in compliance with study requirements, provides core functionality for performing QC of medical images, and automatically generates documentation compliant with the FAIR principle, i.e., making scientific findings findable, accessible, interoperable, and reusable.  Specifically, our multi-site open-source software platform for Medical Image Quality Assurance (mIQa) will enable efficient and accurate QC processing by leveraging open-source, state-of-the-art web interface technologies, such as a web-based dataset caching system, machine learning to aid in QC process, and an interactive electronic notebook platform. Users will be able to configure workflows that not only reflect the specific requirements of medical imaging studies but also minimize the time spent on labor-intensive operations, such as visually reviewing scans. Issue tracking technology will enhance communication between geographically-distributed team members, as they can easily share image annotations and receive automating notifications of outstanding QC issues. The system will be easy to deploy as it will be able to interface with various imaging storage backends, such as local file systems and XNAT. While parts of this functionality have been developed elsewhere, mIQa is unique as it provides a unified, standard interface for efficient QC setup, maintenance, and review for projects analyzing multiple, independently managed data sources.  The usefulness of this unique QC system will be demonstrated on increasing the efficiency of the diverse QC team of the multi-center NCANDA study. Narrative The goal of this proposal is to develop multi-site, open-source software for Medical Image Quality Assurance (mIQa) to address the QC needs of geographically diverse teams using small and large medical image-based studies alike. mIQa will enable efficient and accurate QC processing by levering open-source, state-of-the-art machine learning, data management, and web interface technologies. Our effort will minimize the time spent on labor-intensive review and analysis operations by supporting team-oriented reviewing that is guided by highly customizable workflows seamlessly interacting with existing data management systems.",mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment,9622218,R43MH119022,"['Active Learning', 'Address', 'Adolescence', 'Alcohols', 'BRAIN initiative', 'Big Data to Knowledge', 'Brain', 'Brain imaging', 'Collaborations', 'Communication', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Data Sources', 'Development', 'Diffusion', 'Documentation', 'Effectiveness', 'Ensure', 'Environment', 'Evaluation', 'FAIR principles', 'Four-dimensional', 'Funding', 'Geography', 'Goals', 'Image', 'Image Analysis', 'Imagery', 'International', 'Internet', 'Investments', 'Label', 'Libraries', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Manuals', 'Measurement', 'Medical', 'Medical Imaging', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences', 'Notification', 'Online Systems', 'Peer Review', 'Phase', 'Procedures', 'Process', 'Publications', 'Quality Control', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Scanning', 'Site', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Visual', 'Work', 'Writing', 'application programming interface', 'base', 'cohesion', 'cost', 'dashboard', 'data access', 'data management', 'design', 'experience', 'flexibility', 'image archival system', 'imaging study', 'improved', 'innovation', 'member', 'neurodevelopment', 'neuroimaging', 'open source', 'operation', 'prototype', 'quality assurance', 'research study', 'software systems', 'success', 'tool', 'web interface', 'web-enabled']",NIMH,"KITWARE, INC.",R43,2018,225001,-0.01044180972910509
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9517061,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'data warehouse', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2018,306284,-0.01922786026254218
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9420621,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'human microbiota', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2018,374683,-0.006127996653114619
"Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria PROJECT SUMMARY Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria. Antibiotic-resistant Gram-negative bacterial infections are increasing in incidence and novel antibiotics are urgently needed to combat this growing threat to public health. A major roadblock to the development of novel antibiotics is our poor understanding of the structural features of small molecules that correlate with bacterial penetration and efflux. As a result, while potent biochemical inhibitors can often be identified for new targets, developing them into compounds with whole-cell antibacterial activity has proven challenging. To address this critical problem, we propose herein a comprehensive, multidisciplinary approach to develop quantitative models to predict small-molecule penetration and efflux in Gram-negative bacteria. We have pioneered a general platform for systematic, quantitative evaluation of small-molecule accumulation in bacteria, using label-free LC-MS/MS detection and multivariate cheminformatic analysis. We have also developed unique isogenic strain sets of wild-type, hyperporinated, efflux-knockout, and doubly-compromised E. coli, P. aeruginosa, and A. baumannii that allow us to dissect the individual contributions of outer/inner membrane penetration and active efflux to net accumulation, using a kinetic model that accurately recapitulates available experimental data. Moreover, we have developed machine learning and neural network approaches to QSAR (quantitative structure–activity relationship) modeling of pharmacological properties that will now be used to develop predictive cheminformatic models for Gram-negative accumulation, penetration, and efflux. This project will be carried out by a multidisciplinary SPEAR-GN Project Team (Small-molecule Penetration & Efflux in Antibiotic-Resistant Gram-Negatives, “speargun”) involving the labs of Derek Tan (MSK, PI), Helen Zgurskaya (OU, PI), Bradley Sherborne (Merck, Lead Collaborator), Valentin Rybenkov (OU, Co-I), Adam Duerfeldt (OU, Co-I), Carl Balibar (Merck, Collaborator), and David McLaren (Merck, Collaborator), comprising extensive combined expertise in organic and diversity-oriented synthesis, biochemistry, microbiology, high- throughput screening, mass spectrometry, biophysical modeling, cheminformatics, and medicinal chemistry. Herein, we will design and synthesize chemical libraries with diverse structural and physicochemical properties; analyze their accumulation in the isogenic strain sets in both high-throughput and high-density assay formats; extract kinetic parameters for penetration and efflux from the resulting experimental datasets; develop and validate robust QSAR models for accumulation, penetration, and efflux; and demonstrate the utility of these models in medicinal chemistry campaigns to develop novel Gram-negative antibiotics against three targets. This project will provide a major advance in the field of antibacterial drug discovery, providing powerful enabling tools to the scientific community to address this major threat to public health. PUBLIC HEALTH RELEVANCE Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria. Antibiotic-resistant Gram-negative bacteria pose a growing threat to public health in the U.S. and globally. A major obstacle to the development of new antibiotics to combat such infections is our poor understanding of the chemical requirements for small molecules to enter Gram-negative cells and to avoid ejection by efflux pumps. The proposed comprehensive, multidisciplinary research program aims to develop predictive computational tools to identify such molecules by carrying out large-scale, quantitative analyses of the accumulation of diverse small molecules in Gram-negative bacteria. These tools will then enable medicinal chemistry campaigns to develop novel antibiotics.",Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria,9486312,R01AI136795,"['Acinetobacter baumannii', 'Address', 'Algorithmic Software', 'Anti-Bacterial Agents', 'Antibiotic Resistance', 'Antibiotics', 'Architecture', 'Bacteria', 'Biochemical', 'Biochemistry', 'Biological Assay', 'Biological Availability', 'Biological Neural Networks', 'Cells', 'Chemicals', 'Communities', 'Data', 'Data Set', 'Detection', 'Development', 'Effectiveness', 'Escherichia coli', 'Gram-Negative Bacteria', 'Gram-Negative Bacterial Infections', 'Human', 'Incidence', 'Individual', 'Infection', 'Interdisciplinary Study', 'Kinetics', 'Knock-out', 'Label', 'Lead', 'Libraries', 'Machine Learning', 'Mammalian Cell', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Membrane', 'Microbiology', 'Modeling', 'Oral', 'Partner in relationship', 'Penetration', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Pharmacology', 'Property', 'Pseudomonas aeruginosa', 'Public Health', 'Quantitative Evaluations', 'Quantitative Structure-Activity Relationship', 'Role', 'Structure', 'Testing', 'Variant', 'analog', 'base', 'biophysical model', 'cell envelope', 'cheminformatics', 'combat', 'computerized tools', 'density', 'design', 'drug discovery', 'efflux pump', 'high throughput screening', 'improved', 'inhibitor/antagonist', 'interdisciplinary approach', 'lead optimization', 'learning network', 'multidisciplinary', 'novel', 'predictive modeling', 'programs', 'prospective', 'public health relevance', 'screening', 'small molecule', 'small molecule libraries', 'success', 'tool']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,R01,2018,1527746,-0.004537977075403326
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9672008,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,347221,0.020660792190511656
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9559873,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,300000,0.020660792190511656
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9487840,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2018,141048,-0.012842344176691394
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9495704,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2018,396544,0.004258276640737789
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9583854,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2018,74515,0.0028941582374467775
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9531327,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2018,773018,-0.01994368769505162
"COINSTAC: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community be- comes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2). The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates a dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informat- ics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an inde- pendent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented stor- age vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and pri- vacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix fac- torization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. 4 Project Narrative  Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don’t have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this ‘missing data’ and allow for pooling of both open and ‘closed’ repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed compu- tational solution for a large toolkit of widely used algorithms. 3","COINSTAC: decentralized, scalable analysis of loosely coupled data",9717051,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,282320,0.01273830290967585
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9473021,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,649098,0.01589656301222226
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9445086,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,305167,-0.00283188936337735
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9559432,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2018,325325,-0.005180337205143855
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary  Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and effect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable to discover local and global alteration of matter without the need to apriori select an anatomical region of interest.  The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. ​A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image data set is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focus on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,9679722,R41MH118845,"['Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Autistic Disorder', 'Brain', 'Calibration', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Computer software', 'Data Set', 'Databases', 'Dementia', 'Deterioration', 'Development', 'Diffuse', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Foundations', 'Goals', 'Grain', 'Image', 'Image Analysis', 'Imagery', 'Internet', 'Lead', 'Location', 'Machine Learning', 'Medical Imaging', 'Methodology', 'Methods', 'Modality', 'Nature', 'Nerve Degeneration', 'Neurologic', 'Neurologic Effect', 'Online Systems', 'Outcome', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Study', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Technology', 'Temporal Lobe', 'Testing', 'Validation', 'Variant', 'base', 'clinical Diagnosis', 'experience', 'frontal lobe', 'gray matter', 'high throughput screening', 'image processing', 'image registration', 'imaging capabilities', 'improved', 'interest', 'learning strategy', 'morphometry', 'nervous system disorder', 'predict clinical outcome', 'predictive modeling', 'programs', 'research and development', 'shape analysis', 'software development', 'task analysis', 'tool', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R41,2018,303226,-0.01999072410545296
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9699855,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,75000,-0.011356588945683279
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9537614,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,329257,-0.011356588945683279
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9661636,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2018,263913,0.01542057000872389
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9407024,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2018,395628,-0.03755089549967802
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9535429,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,315184,-0.01592777280271827
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9403171,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Learning', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,548068,0.001539555166720194
"Machine Learning for Identifying Adverse Drug Events ﻿    DESCRIPTION (provided by applicant): Because of the profound effect of adverse drug events (ADEs) on patient safety, the FDA, AHRQ and Institute of Medicine have flagged post-marketing pharmacovigilance of emerging medications as a high national research priority. The FDA, Foundation for the NIH and PhARMA formed the Observational Medical Outcomes Partnership (OMOP) to develop and compare methods for identification of ADEs, and the FDA announced its Sentinel Initiative. Congress created the Reagan Udall Foundation (RUF) for the FDA in response to the FDA's own ""FDA Science and Mission at Risk"" report, and two years ago OMOP activities were incorporated into RUF. As the FDA moves forward with its development of Sentinel, including work on Mini-Sentinel, there is a need for researchers around the country to continue to develop better methods, and better evaluation methodologies for those methods. A robust research community working on algorithms for pharmacosurveillance, using electronic health records (EHRs) and claims databases will provide a substrate of ever-improving methods on which the nation's regulatory pharmacovigilance infrastructure can build. Indeed an important motivation of OMOP and Mini-Sentinel was to spur the development of such a community. Machine learning has attracted widespread attention across a range of disciplines for its ability to construct accurate predictive models. Therefore machine learning is especially appropriate for the problems of ADE identification and prediction: identifying ADEs from observational data, and predicting which patients are most at risk of suffering the identified ADE. Our current award has demonstrated the ability of machine learning to address both of these tasks. It has added to the existing evidence that consideration of temporal ordering of events, such as drug exposure and diagnoses, is critical for accuracy in identification and prediction of ADEs. The proposed work seeks to further improve upon these methods by building on recent advances in the field of machine learning, by our group and by others, in graphical model learning and in explicit modeling of irregularly-sampled temporal data. The latter is especially important because observational health databases, such as EHRs and claims databases, are not simple time series. Patients typically do not come into the clinic at regular intervals and have the same labs, vitals, and other measurements in lock step with one another. Building better ADE detection and prediction algorithms cannot be accomplished simply by machine learning research, even if that research is taking account of related work from relevant parts of computer science, statistics, biostatistics, epidemiology, pharmaco-epidemiology, and clinical research. Better methods are needed also for evaluation, that is, for estimating how well a new algorithm, or a new use of an existing algorithm, will perform at identifying ADEs associated with a new drug on the market, or at predicting which patients are most at risk of that ADE. More research and evaluation is also needed at the systems level: how can we best construct end-to-end pharmacovigilance systems that sit atop a large observational database and flag potential ADEs for human experts to further investigate? What kinds of information and statistics should such a system provide to the human experts?        This renewal will address the following aims: (1) improve upon machine learning methods for identification and prediction of ADEs, taking advantage of synergies between these two distinct tasks; (2) improve upon existing methods for evaluating ADE detection, building on advances in machine learning for information extraction from scientific literature; (3) improve upon existing methods for evaluating ADE prediction, building upon advances in machine learning for automated support of phenotyping and also building upon improved methods for efficiently obtaining expert labeling of borderline examples of a phenotype; and (4) use the methods developed in the first three aims to construct and evaluate an end-to-end pharmacosurveillance system integrated with the Marshfield Clinic EHR Data Warehouse. Machine learning plays a central and unifying role throughout all four aims. Our investigator team consists of machine learning researchers with experience in analysis of clinical, genomic, and natural language data (Page, Natarajan), a leading pharmaco-epidemiologist with expertise in building systems to efficiently obtain expert evaluation and labeling of phenotypes (Hansen), a leader in phenotyping from EHR data (Peissig), and an MD/PhD practicing physician with years of experience and leadership in the study of ADEs (Caldwell). In addition to building on results of the prior award, we will build on our experiences with OMOP, the International Warfarin Pharmacogenetics Consortium, the DARPA Machine Reading Program, and interactions with the FDA. PUBLIC HEALTH RELEVANCE: Adverse drug events (ADEs) carry a high cost each year in life, health and money. Congress, the FDA, the NIH and PhARMA have responded with new initiatives for identifying and predicting occurrences of ADEs. It has been widely recognized within initiatives such as Sentinel and the Observational Medical Outcomes Partnership that addressing ADEs requires data, standards and methods for data analysis and mining. This proposal addresses the need for new methods for both identifying previously- unanticipated ADEs and predicting occurrences of a known ADE. It also addresses the needs for improved evaluation and integrated systems approaches.",Machine Learning for Identifying Adverse Drug Events,9323511,R01GM097618,"['Address', 'Adverse drug effect', 'Adverse drug event', 'Algorithms', 'Attention', 'Award', 'Biometry', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Communities', 'Congresses', 'Country', 'Coxibs', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Discipline', 'Doctor of Philosophy', 'Drug Exposure', 'Early Diagnosis', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Evaluation Methodology', 'Event', 'Foundations', 'Genomics', 'Health', 'Human', 'Institute of Medicine (U.S.)', 'International', 'Label', 'Leadership', 'Learning', 'Life', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Markov Chains', 'Measurement', 'Medical', 'Methods', 'Mission', 'Modeling', 'Monitor', 'Motivation', 'Myocardial Infarction', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenetics', 'Phenotype', 'Physicians', 'Play', 'Process', 'Reading', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Priority', 'Risk', 'Role', 'Safety', 'Sampling', 'Science', 'Sentinel', 'Series', 'Serious Adverse Event', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Time', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Validation', 'Warfarin', 'Wisconsin', 'Work', 'base', 'computer science', 'cost', 'data mining', 'experience', 'improved', 'interest', 'learning strategy', 'natural language', 'novel', 'novel therapeutics', 'patient safety', 'prediction algorithm', 'predictive modeling', 'programs', 'public health relevance', 'response', 'statistics', 'synergism']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R01,2017,536041,-0.020939688290335996
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9392642,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2017,262150,0.013982413175843787
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,-0.007666549594065681
"IGF::OT::IGF Semantic Bibliometric System for Improving Healthcare Research (Topic 162) (Period of Performance: September 15, 2017 - March 14, 2018). BASE AWARD N43DA-17-1217 BCL will expand the current bibliometric methods by developing a Semantic Bibliometric System using machine learning that will examine research publications, rank the publications by quality, and identify research-productive scientific teams. Used in conjunction with current methods this Semantic Bibliometric System will have the dual use of improving the impact of Government Research and improving semantic search on the web and in ecommerce. n/a","IGF::OT::IGF Semantic Bibliometric System for Improving Healthcare Research (Topic 162) (Period of Performance: September 15, 2017 - March 14, 2018). BASE AWARD N43DA-17-1217",9576638,71201700054C,"['Award', 'BCL1 Oncogene', 'Bibliometrics', 'Data', 'Effectiveness', 'Evaluation', 'Feasibility Studies', 'Government', 'Health Care Research', 'Internet', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Performance', 'Procedures', 'Publications', 'Research', 'Semantics', 'System', 'Text', 'improved']",NIDA,"BCL TECHNOLOGIES, INC.",N43,2017,225000,-0.0035100365781214698
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,-0.01689336472728789
"A high-throughput imaging and classification system for fruit flies PROJECT SUMMARY / ABSTRACT In this Phase I SBIR application, FlySorter proposes to development a high throughput imaging and classification system to aid research with fruit flies, a widely-used model organism relevant to both basic science as well as studies in human health. The use of animal model systems is essential for research in almost all aspects of biology: genetics, development, neuroscience, disease, physiology, and beyond. The fruit fly – Drosophila melanogaster – is small and easy to care for, but is complex enough an organism to provide a wealth of information that directly relates to human biology and health. Over 75% of human diseases with a genetic basis (including depression, alcoholism, certain forms of cancer, and many more) are either present or have an analog in Drosophila. Modern genetic tools, such as CRISPR/cas9, allow the creation of transgenic flies that provide the opportunity to study diseases, pathways and systems that don’t exist naturally in Drosophila. With these advances, fruit flies are becoming more frequently subjects for drugs screens. For all the advances in the biological tools and techniques applicable to flies, however, the limiting factor in many experiments is the manual labor involved in a few common tasks: moving flies from vial to vial or other lab equipment; classifying and sorting flies by sex, eye color and other phenotypes; and collecting virgin female flies before they mate so that they can be used in controlled crosses, etc. FlySorter’s patent-pending fly dispensing mechanism can reliably deliver a single organism from a vial containing hundreds of awake flies, and our novel FlyPlate system allows storage of individual flies in custom 96 well plates. FlySorter’s robotic fly handling system, co-developed with the de Bivort Lab at Harvard, is capable of manipulating and transporting those individual flies between vial, 96 well plate, and experimental apparatus. The next piece of the automation puzzle to solve is high throughput imaging and classification. To accomplish this goal, FlySorter will: 1) complete a prototype automated image capture hardware system; 2) adapt state-of-the-art computer vision and machine learning algorithms for use on Drosophila; and 3) build a module that can physically sort the classified flies into different vials. Once integrated into the existing FlySorter product ecosystem, this imaging and classification module will greatly expand the kinds of experiments and screens that can be automated, allowing for the study of larger populations or a wider variety of flies, reducing the impact of human error, and freeing up valuable time for researchers. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are one of the most widely used model organisms in biology, for research in genetics, development, neuroscience, disease, and much more. One of the most common tasks in Drosophila labs is sorting flies by various markers and phenotypes using a microscope and paintbrush. FlySorter aims to build an automated system for sorting flies using high resolution digital cameras and modern computer vision algorithms, which will obviate the need for such tedious manual labor.",A high-throughput imaging and classification system for fruit flies,9408980,R43OD023302,"['Air', 'Alcoholism', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Animal Model', 'Animals', 'Appearance', 'Automation', 'Basic Science', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Neural Networks', 'Biology', 'CRISPR/Cas technology', 'Caring', 'Classification', 'Code', 'Complex', 'Computer Vision Systems', 'Custom', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Pathway', 'Dorsal', 'Drosophila genus', 'Drosophila melanogaster', 'Ecosystem', 'Ensure', 'Eye', 'Eye Color', 'Female', 'Floor', 'Genes', 'Genetic', 'Genetic Screening', 'Genetic study', 'Genotype', 'Goals', 'Grant', 'Head', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Legal patent', 'Lighting', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mechanics', 'Mental Depression', 'Methodology', 'Microscope', 'Modernization', 'Motor', 'Mutation', 'Names', 'Neurosciences', 'Obesity', 'Optics', 'Organism', 'Partner in relationship', 'Phase', 'Phenotype', 'Physiology', 'Population', 'Preclinical Drug Evaluation', 'Pump', 'Research', 'Research Personnel', 'Resolution', 'Robot', 'Robotics', 'Sampling', 'Sclera', 'Shapes', 'Small Business Innovation Research Grant', 'Sorting - Cell Movement', 'Standardization', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transgenic Organisms', 'Universities', 'Vial device', 'Walking', 'Work', 'analog', 'awake', 'base', 'depression model', 'digital', 'digital imaging', 'experimental study', 'fly', 'genetic strain', 'human disease', 'improved', 'interest', 'laboratory equipment', 'male', 'meter', 'novel', 'phenotypic biomarker', 'prevent', 'prototype', 'sex', 'tool', 'virtual']",OD,"FLYSORTER, LLC",R43,2017,225000,-0.015613136835516074
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9310440,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2017,599995,-0.002252379758272073
"Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways ﻿    DESCRIPTION (provided by applicant): This project aims to develop new statistical machine learning methods for metabolomics data from diverse platforms, including targeted and unbiased/global mass spectrometry (MS), labeled MS experiments for measuring metabolic ﬂux and Nuclear Magnetic Resonance (NMR) platforms. Unbiased MS and NMR proﬁling studies result in identifying a large number of unnamed spectra, which cannot be directly matched to known metabolites and are hence often discarded in downstream analyses. The ﬁrst aim develops a novel kernel penalized regression method for analysis of data from unbiased proﬁling studies. It provides a systematic framework for extracting the relevant information from unnamed spectra through a kernel that highlights the similarities and differences between samples, and in turn boosts the signal from named metabolites. This results in improved power in identiﬁcation of named metabolites associated with the phenotype of interest, as well as improved prediction accuracy. An extension of this kernel-based framework is also proposed to allow for systematic integration of metabolomics data from diverse proﬁling studies, e.g. targeted and unbiased MS proﬁling technologies. The second aim pro- vides a formal inference framework for kernel penalized regression and thus complements the discovery phase of the ﬁrst aim. The third aim focuses on metabolic pathway enrichment analysis that tests both orchestrated changes in activities of steady state metabolites in a given pathway, as well as aberrations in the mechanisms of metabolic reactions. The fourth aim of the project provides a uniﬁed framework for network-based integrative analysis of static (based on mass spectrometry) and dynamic (based on metabolic ﬂux) metabolomics measurements, thus providing an integrated view of the metabolome and the ﬂuxome. Finally, the last aim implements the pro- posed methods in easy-to-use open-source software leveraging the R language, the capabilities of the Cytoscape platform and the Galaxy workﬂow system, thus providing an expandable platform for further developments in the area of metabolomics. The proposed software tool will also provide a plug-in to the Data Repository and Coordination Center (DRCC) data sets, where all regional metabolomics centers supported by the NIH Common Funds Metabolomics Program deposit curated data. PUBLIC HEALTH RELEVANCE: Metabolomics, i.e. the study of small molecules involved in metabolism, provides a dynamic view into processes that reﬂect the actual physiology of the cell, and hence offers vast potential for detection of novel biomarkers and targeted therapies for complex diseases. However, despite this potential, the development of computational methods for analysis of metabolomics data lags the rapid growth of metabolomics proﬁling technologies. The current application addresses this need by developing novel statistical machine learning methods for integrative analysis of static and dynamic metabolomics measurements, as well as easy-to-use open-source software to facilitate the application of these methods.",Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways,9250169,R01GM114029,"['Address', 'Adoption', 'Anabolism', 'Area', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Assay', 'Cardiovascular Diseases', 'Cell physiology', 'Cells', 'Characteristics', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Diabetes Mellitus', 'Disease', 'Environment', 'Environmental Risk Factor', 'Equilibrium', 'Funding', 'Galaxy', 'Homeostasis', 'Imagery', 'Knowledge', 'Label', 'Language', 'Letters', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Names', 'Network-based', 'Nuclear Magnetic Resonance', 'Pathway interactions', 'Phase', 'Phenotype', 'Plug-in', 'Procedures', 'Process', 'Prognostic Marker', 'Proteomics', 'Reaction', 'Sampling', 'Signal Transduction', 'Software Tools', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Work', 'base', 'biological systems', 'biomarker discovery', 'diagnostic biomarker', 'experimental study', 'flexibility', 'high dimensionality', 'improved', 'insight', 'interest', 'learning strategy', 'metabolome', 'metabolomics', 'new technology', 'novel', 'novel diagnostics', 'novel marker', 'open source', 'programs', 'public health relevance', 'rapid growth', 'response', 'small molecule', 'targeted treatment', 'tool', 'transcriptomics']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2017,339051,0.020232428570069322
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9355693,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Disease model', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Mathematics', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'experimental study', 'improved', 'infancy', 'insight', 'network models', 'novel', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2017,406785,-0.0020303860461086815
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9307096,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'mild cognitive impairment', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2017,407500,-0.07122624888029289
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility. PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9270528,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'Supervision', 'System', 'Time', 'Training', 'clinical predictors', 'cohort', 'computer science', 'crowdsourcing', 'data sharing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2017,316000,-0.004010512046908976
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9335405,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modeling', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2017,372122,-0.027423666432595514
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9384200,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2017,685783,0.008043049687097197
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,0.008986482862922356
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9371707,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Databases', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'permissiveness', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2017,93824,-8.362175047626606e-05
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,229691,0.004401996491705753
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9312083,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'design', 'experimental study', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,395858,0.005704982489860433
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9260548,R01GM120033,"['Address', 'Algorithms', 'Alpha Cell', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cancer Etiology', 'Cardiovascular system', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical practice', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2017,356625,0.014723992563666022
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9270498,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Taxonomy', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,311153,-0.0027716792624386762
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9398728,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Development', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2017,546372,-0.004442602507557163
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9394127,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Behavior Therapy', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Health Care Research', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health care quality', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'symptomatic improvement', 'tool']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2017,338637,0.0022863702422892913
"ISPW8 Conference: Designing the next generation of closed loop seizure control Project Summary The overall objective of this effort is to convene “ISPW8: Designing the next generation of closed loop seizure control”, that will take place August 20-23, 2017 at the University of Minnesota in Minneapolis. This conference is the next in a series of seizure prediction workshops that started in 2002, and has become the premier international venue for quantitative epilepsy research. The goal of this upcoming meeting is to focus on the development of all stages of a next-generation seizure control device. The first day will have two didactic sessions, one focusing on the clinical aspects of epilepsy, the other on Big Data analytic techniques. The body of the conference will be organized around themes motivated by the three stages of closed loop intervention: 1) input: sensing and biomarkers, 2) processing: system analysis, 3) output: intervention. The theme of the second day of will be on multimodal sensors and biomarkers of epilepsy, focused especially on developing new technology. The theme of the third day will be on utilizing advanced machine learning, statistics, and computational models, to understand and characterize the large datasets from high resolution technology. The final day will be on novel interventions and control theory: new strategies for implantable anti-seizure interventions and methods for optimizing them, as well as implementation into closed loop control devices. “Provocative sessions” at the end of each day will focus on unpublished research and hypotheses followed by discussion and debate. Through this conference we aim to build bridges between the many disciplines that comprise this unique field: theoretical, computational, experimental, clinical, and industry, while also preparing the next wave of young researchers. The overall goal is to develop improved quantitative methods to predict, quantify, characterize, and control seizures. NIH funding is sought for travel support to encourage US participation. Project Relevance/Project Narrative We plan to convene an international meeting to address questions at the intersection of clinical neurophysiology, engineering, computational and epilepsy neuroscience using emerging technologies and quantitative methods. The objectives of this interdisciplinary group – and the themes of the meeting – are to take our growing understanding of the mechanisms and dynamics that lead to seizures at all scales of brain from neuron to organism, and develop successful technologies and therapies for observing, predicting and treating pharmacoresistant epilepsy syndromes.",ISPW8 Conference: Designing the next generation of closed loop seizure control,9398788,R13NS101927,"['Address', 'Behavior', 'Big Data', 'Biological Markers', 'Brain', 'Canis familiaris', 'Clinical', 'Clinical Trials', 'Collaborations', 'Collection', 'Computer Simulation', 'Data Analytics', 'Data Set', 'Detection', 'Development', 'Devices', 'Discipline', 'Educational workshop', 'Electroencephalography', 'Emerging Technologies', 'Engineering', 'Epilepsy', 'Evaluation', 'Fostering', 'Foundations', 'Funding', 'Goals', 'High Frequency Oscillation', 'Industry', 'International', 'Intervention', 'Lead', 'Machine Learning', 'Mentors', 'Methods', 'Minnesota', 'Modeling', 'Neurologist', 'Neurons', 'Neurosciences', 'Organism', 'Output', 'Paper', 'Participant', 'Pattern', 'Physiology', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Seizures', 'Seminal', 'Series', 'Students', 'Surgeon', 'Syndrome', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'career', 'computerized data processing', 'computerized tools', 'control theory', 'cost', 'data modeling', 'data sharing', 'design', 'graduate student', 'improved', 'interest', 'international partnership', 'meetings', 'member', 'multimodality', 'neurophysiology', 'new technology', 'next generation', 'novel', 'peer', 'physiologic model', 'sensor', 'statistics', 'success', 'symposium', 'tool']",NINDS,UNIVERSITY OF MINNESOTA,R13,2017,15000,-0.007367447420820178
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9224405,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2017,163452,-0.07212322481595698
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9251828,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Modernization', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Standardization', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'interoperability', 'knowledge translation', 'learning progression', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool', 'translational impact']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,428512,0.017621233482044054
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9287487,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multimodality', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2017,367055,-0.05925774429233081
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9486059,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2017,1408263,-0.030359726364737976
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9221662,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2017,377226,-0.006127996653114619
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9306122,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2017,306527,-0.01922786026254218
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9275314,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2017,142177,-0.012842344176691394
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9357656,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2017,572778,-0.0009304929526914965
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9279143,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronaviridae', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2017,202814,0.004258276640737789
"Sci-Score, a tool to support rigor and transparency guidelines Project Summary While  standards  in  reporting  of  scientific  methods  are  absolutely  critical  to  producing  reproducible  science,  meeting  such  standards  is  difficult.  Checklists  and  instructions  are  tough  to  follow  often  resulting  in  low  and inconsistent  compliance.  Scientific  journals  and  societies  as  well  as  the  National  Institutes  of  Health  are  now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods  (e.g.,  http://www.cell.com/star-­methods),  but  the  trickier  part  will  be  to  train  the  biomedical  community  to  use these standards to effectively improve how scientific methods are communicated. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency of  Key Biological Resources, we propose to build Sci-­Score a text mining based tool suite to help authors meet the  standard.  Sci-­Score  will  provide  an  automated  check  on  compliance  with  the  RRID  standard  already  implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. The innovation behind Sci-­ score is the provision of a score, which can be obtained by individual investigators, which reflects a numerical  validation of the quality of their methods reporting. We posit that the score will serve as a tool that investigators  and journals can use to compete with themselves and each other, or in the very least allow them to see how  close they are to the average in meeting quality requirements.   Recently, our group has developed a text mining algorithm that has now been successfully been used to detect software tools and databases from the SciCrunch Registry in published papers. Digital tools are one of four resource types that the RRID standard identifies. We propose to extend this approach to the other types of entities: antibodies, cell lines and model organisms. Resource identification along with other quality metrics twill be used to train an algorithm to score the overall quality of the methods document. If successful, the tool could be used by editors, reviewers, and investigators to improve the number of RRIDs, therefore the quality of descriptors of key biological resources in published papers. This SBIR project will build a set of algorithms similar to the resource finding pipeline and develop it into an industrial robust and reconfigurable software system. Our Phase I specific aims include to 1) creating gold sets of data for each resource type and training a set of algorithms for each resource type; 2) designing and evaluating the scoring system; 3) designing and evaluating a report generating system based on the previous aims. In Phase II, we will develop a scalable backend infrastructure to serve the needs of scientific publishers and research community. Standards for scientific methods reporting are absolutely critical to producing reproducible science, but meeting  such standards is difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent  compliance. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency, we propose to build Sci-Score text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. Sci-Score will provide a score rating the quality of  methods reporting in submitted articles, which provides feedback to authors, reviewers and editors on how to improvecompliance with RRIDs and other standards. ","Sci-Score, a tool to support rigor and transparency guidelines",9345707,R43OD024432,"['Address', 'Agreement', 'Algorithms', 'Animal Model', 'Antibodies', 'Area', 'Big Data', 'Biological', 'California', 'Cell Line', 'Cell model', 'Cells', 'Communities', 'Data Set', 'Databases', 'Descriptor', 'Elements', 'Ensure', 'Evaluation', 'Feedback', 'Funding', 'Glare', 'Gold', 'Guidelines', 'Habits', 'Human', 'Individual', 'Industrialization', 'Instruction', 'Journals', 'Learning', 'Literature', 'Machine Learning', 'Manuscripts', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neurosciences', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reading', 'Reagent', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sales', 'Science', 'Scoring Method', 'Services', 'Small Business Innovation Research Grant', 'Societies', 'Software Tools', 'System', 'Technology', 'Text', 'To specify', 'Training', 'United States National Institutes of Health', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'biological systems', 'computerized tools', 'design', 'digital', 'improved', 'innovation', 'interest', 'meetings', 'prototype', 'software systems', 'sound', 'text searching', 'tool', 'vigilance', 'web site']",OD,"SCICRUNCH, INC.",R43,2017,221865,-0.010071020160086425
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9250803,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'experimental study', 'genomic data', 'hazard', 'high dimensionality', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'predictive modeling', 'prevent', 'public health relevance', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2017,66026,-0.006182542511389335
"Predicting Resilience in the Human Microbiome DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience. PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.",Predicting Resilience in the Human Microbiome,9325416,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Modernization', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Validation', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiome', 'gut microbiota', 'human subject', 'innovation', 'learning strategy', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'public health relevance', 'resilience', 'response', 'theories', 'tool', 'transcriptome', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2017,413065,-0.0011115455536214603
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9268713,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Data Sources', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2017,655080,0.01589656301222226
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9349367,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2017,326784,-0.005180337205143855
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9320865,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multimodal Imaging', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'analytical tool', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'high dimensionality', 'imaging Segmentation', 'imaging modality', 'imaging study', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'spatiotemporal', 'study population', 'terabyte', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2017,347156,-0.006054626817592703
"Development of integrative models for early liver toxicity assessment ﻿    DESCRIPTION (provided by applicant): Computational toxicology has become a critical area of research due to the burgeoning need to evaluate thousands of pharmaceutical and environmental chemicals with unknown toxicity profiles, the high demand in time and resources by current experimental toxicity testing, and the growing ethical concerns over animal use in toxicity studies. Despite tremendous efforts, little success has been attained thus far in the development of predictive computational models for toxicity, primarily due to the complexity of toxicity mechanisms as well as the lack of high-quality experimental data for model development.  A critical challenge in toxicity testing of chemicals is that toxicity effects are doe-dependent: the true toxic hits may show no toxicity at all at low dose level. Therefore, traditiona high-throughput screening (HTS) that test chemicals only at a single concentration is not suitable for toxicity screening. On the contrary, the recently developed quantitative high-throughput screening (qHTS) platforms can evaluate each chemical across a broad range of concentrations, and is gaining ever-increasing popularity as a tool for in vitro toxicity profiling The concentration-response information generated by qHTS are expected to provide more accurate and comprehensive information of the toxicity effects of chemicals, offering promising data that can be mined to estimate in vivo toxicities of chemicals. However, our previous studies showed that if processed inappropriately, such concentration-response information contribute little to improve the toxicity prediction. This is especially true when multiple types of qHTS data are used together. Therefore, in this study, we will extend our previous approaches to develop novel statistical and computational tools that can curate, preprocess, and normalize the concentration-response information from multiple different qHTS databases.  Traditionally, toxicity models are based on either the chemical data (such as the quantitative structure- activity relationship analysis), or the in vitro toxicity profiling data (such as the in vitro-in vivo extrapolations). Our previous experiences suggested that integrating biological descriptors such as the in vitro cytotoxicity profiles or the short-term toxigenomic data, with chemical structural features is able to predict rodent acute liver toxicity with reasonable accuracy. Therefore, the second part of this proposal will be devoted to develop novel computational models for hepatotoxicity prediction by integrating qHTS toxicity profiles and chemical structural information In Aim 1, we will curate, preprocess, and normalize collected public liver toxicity datasets. In ths study, we will model toxicity effects using multiple large public datasets such as HTS and qHTS bioassay data (Tox21[1] and ToxCast[2]), hepatotoxicity side effect reports on marketed failed drugs[3], the Liver Toxicity Knowledge Base Benchmark Dataset (LTKB-BD[4]), etc. Statistical methods for cross-study validation and quality control will be applied to the collected datasets to ensure computational compatibility and to select the appropriate datasets for analysis. In Aim 2, we will develop predictive models for chemicals' liver toxicity based on an integrative modeling workflow that will make use of both structural and in vitro toxicity profiles of a chemical. Our previous studies [5] showed that models using both in vitro toxicity profiles and chemical structural data have better accuracy for rodent acute liver toxicity than models using either data type alone. Here, we will develop a novel modeling workflow that start with defining the functional clusters of chemicals via curated qHTS toxicity profiles, and is followed by developing computational models to correlate chemical and biological data with overall toxicity risks in humans. The predictive models will be validated using independent datasets with over 800 compounds. In Aim 3, we propose to prioritize the qHTS profiling assays used in the model for future toxicity testing. We will evaluate all the in vitro assays as biological descriptors from thee perspectives, including descriptor importance in the integrative toxicity model, correlation with i vivo DILI outcomes, and level of information content estimated by a novel approach based on network analysis. PUBLIC HEALTH RELEVANCE: In this study we aim to develop computational models that can identify potential liver toxicants. Liver toxicity is a significant contributor to the high attition rate in drug development. Moreover, toxic chemicals in food, water, and consumer products all pose serious risks for liver toxicity. As a result, there is great interest in developing high-throughput, high-content experimental and computational tools to evaluate the liver toxicity of thousands of pharmaceutical and environmental chemicals. This study focuses on developing novel informatics tools that enable the extraction and integration of chemical concentration-response information from multiple quantitative high-throughput screening databases for model development, and developing statistical models that are able to integrate this concentration-response information with chemical structural features to predict their risk of liver toxicity.  ",Development of integrative models for early liver toxicity assessment,9333370,R03ES026397,"['Acute', 'Address', 'Adverse effects', 'Algorithms', 'Animals', 'Area', 'Benchmarking', 'Biological', 'Biological Assay', 'Chemical Models', 'Chemicals', 'Communities', 'Complex', 'Computer Simulation', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Dose', 'Dreams', 'Ensure', 'Ethics', 'Food', 'Future', 'Gene Expression', 'Genomics', 'Goals', 'Gold', 'Hepatotoxicity', 'Human', 'In Vitro', 'Informatics', 'International', 'Liver', 'Machine Learning', 'Marketing', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Environmental Health Sciences', 'Nature', 'Network-based', 'North Carolina', 'Outcome', 'Pathway Analysis', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Poison', 'Process', 'Productivity', 'Quality Control', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research', 'Resources', 'Risk', 'Rodent', 'Scientist', 'Shapes', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicogenetics', 'Toxicology', 'Translational Research', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'Universities', 'Variant', 'Water', 'base', 'computational toxicology', 'computerized tools', 'consumer product', 'cost', 'cost effective', 'cytotoxicity', 'data modeling', 'drug development', 'drug market', 'drug withdrawal', 'environmental chemical', 'experience', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'interest', 'knowledge base', 'liver injury', 'model development', 'novel', 'novel strategies', 'preclinical study', 'predictive modeling', 'programs', 'public health relevance', 'response', 'screening', 'success', 'tool', 'toxicant', 'validation studies']",NIEHS,UT SOUTHWESTERN MEDICAL CENTER,R03,2017,81000,0.001003631963893052
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9443736,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,50000,0.014988998928993635
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9536289,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,589741,0.014988998928993635
An Integrated Biomedical Dataset Discovery System for Immunological Research Databases  The The Contractor will develop a prototype of an integrated biomedical dataset discovery system for Immunological Research Databases (BIRD) to facilitate integrated search for data/knowledge/tools of interest from DAIT bioinformatics resources. n/a,An Integrated Biomedical Dataset Discovery System for Immunological Research Databases ,9574416,72201700019C,"['Bioinformatics', 'Contractor', 'Data', 'Data Discovery', 'Data Reporting', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Immune system', 'Immunology', 'Ingestion', 'Knowledge', 'Maps', 'Metadata', 'Modeling', 'Natural Language Processing', 'Ontology', 'Research', 'Resources', 'System', 'Techniques', 'Technology', 'Terminology', 'base', 'data integration', 'indexing', 'information organization', 'interest', 'prototype', 'tool', 'user-friendly']",NIAID,"TECHWAVE INTERNATIONAL, INC.",N43,2017,224960,-0.029559977145290284
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9360448,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biological Assay', 'Biology', 'Biometry', 'Cells', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2017,906404,-0.01994368769505162
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9193091,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2017,396248,-0.03755089549967802
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9325275,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Plasticizers', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2017,324508,-0.011356588945683279
"Machine Learning for Identifying Adverse Drug Events ﻿    DESCRIPTION (provided by applicant): Because of the profound effect of adverse drug events (ADEs) on patient safety, the FDA, AHRQ and Institute of Medicine have flagged post-marketing pharmacovigilance of emerging medications as a high national research priority. The FDA, Foundation for the NIH and PhARMA formed the Observational Medical Outcomes Partnership (OMOP) to develop and compare methods for identification of ADEs, and the FDA announced its Sentinel Initiative. Congress created the Reagan Udall Foundation (RUF) for the FDA in response to the FDA's own ""FDA Science and Mission at Risk"" report, and two years ago OMOP activities were incorporated into RUF. As the FDA moves forward with its development of Sentinel, including work on Mini-Sentinel, there is a need for researchers around the country to continue to develop better methods, and better evaluation methodologies for those methods. A robust research community working on algorithms for pharmacosurveillance, using electronic health records (EHRs) and claims databases will provide a substrate of ever-improving methods on which the nation's regulatory pharmacovigilance infrastructure can build. Indeed an important motivation of OMOP and Mini-Sentinel was to spur the development of such a community. Machine learning has attracted widespread attention across a range of disciplines for its ability to construct accurate predictive models. Therefore machine learning is especially appropriate for the problems of ADE identification and prediction: identifying ADEs from observational data, and predicting which patients are most at risk of suffering the identified ADE. Our current award has demonstrated the ability of machine learning to address both of these tasks. It has added to the existing evidence that consideration of temporal ordering of events, such as drug exposure and diagnoses, is critical for accuracy in identification and prediction of ADEs. The proposed work seeks to further improve upon these methods by building on recent advances in the field of machine learning, by our group and by others, in graphical model learning and in explicit modeling of irregularly-sampled temporal data. The latter is especially important because observational health databases, such as EHRs and claims databases, are not simple time series. Patients typically do not come into the clinic at regular intervals and have the same labs, vitals, and other measurements in lock step with one another. Building better ADE detection and prediction algorithms cannot be accomplished simply by machine learning research, even if that research is taking account of related work from relevant parts of computer science, statistics, biostatistics, epidemiology, pharmaco-epidemiology, and clinical research. Better methods are needed also for evaluation, that is, for estimating how well a new algorithm, or a new use of an existing algorithm, will perform at identifying ADEs associated with a new drug on the market, or at predicting which patients are most at risk of that ADE. More research and evaluation is also needed at the systems level: how can we best construct end-to-end pharmacovigilance systems that sit atop a large observational database and flag potential ADEs for human experts to further investigate? What kinds of information and statistics should such a system provide to the human experts?        This renewal will address the following aims: (1) improve upon machine learning methods for identification and prediction of ADEs, taking advantage of synergies between these two distinct tasks; (2) improve upon existing methods for evaluating ADE detection, building on advances in machine learning for information extraction from scientific literature; (3) improve upon existing methods for evaluating ADE prediction, building upon advances in machine learning for automated support of phenotyping and also building upon improved methods for efficiently obtaining expert labeling of borderline examples of a phenotype; and (4) use the methods developed in the first three aims to construct and evaluate an end-to-end pharmacosurveillance system integrated with the Marshfield Clinic EHR Data Warehouse. Machine learning plays a central and unifying role throughout all four aims. Our investigator team consists of machine learning researchers with experience in analysis of clinical, genomic, and natural language data (Page, Natarajan), a leading pharmaco-epidemiologist with expertise in building systems to efficiently obtain expert evaluation and labeling of phenotypes (Hansen), a leader in phenotyping from EHR data (Peissig), and an MD/PhD practicing physician with years of experience and leadership in the study of ADEs (Caldwell). In addition to building on results of the prior award, we will build on our experiences with OMOP, the International Warfarin Pharmacogenetics Consortium, the DARPA Machine Reading Program, and interactions with the FDA. PUBLIC HEALTH RELEVANCE: Adverse drug events (ADEs) carry a high cost each year in life, health and money. Congress, the FDA, the NIH and PhARMA have responded with new initiatives for identifying and predicting occurrences of ADEs. It has been widely recognized within initiatives such as Sentinel and the Observational Medical Outcomes Partnership that addressing ADEs requires data, standards and methods for data analysis and mining. This proposal addresses the need for new methods for both identifying previously- unanticipated ADEs and predicting occurrences of a known ADE. It also addresses the needs for improved evaluation and integrated systems approaches.",Machine Learning for Identifying Adverse Drug Events,9145227,R01GM097618,"['Accounting', 'Address', 'Adverse drug event', 'Algorithms', 'Attention', 'Award', 'Biometry', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Communities', 'Congresses', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Discipline', 'Doctor of Philosophy', 'Drug Exposure', 'Early Diagnosis', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Evaluation Methodology', 'Evaluation Research', 'Event', 'Foundations', 'Genomics', 'Health', 'Human', 'Institute of Medicine (U.S.)', 'Label', 'Leadership', 'Learning', 'Life', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Markov Chains', 'Measurement', 'Medical', 'Methods', 'Mission', 'Modeling', 'Monitor', 'Motivation', 'Myocardial Infarction', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Phenotype', 'Physicians', 'Play', 'Process', 'Reading', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Priority', 'Risk', 'Role', 'Safety', 'Sampling', 'Science', 'Sentinel', 'Series', 'Serious Adverse Event', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Time', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Validation', 'Warfarin', 'Wisconsin', 'Work', 'base', 'computer science', 'cost', 'data mining', 'experience', 'improved', 'inhibitor/antagonist', 'interest', 'international partnership', 'learning strategy', 'natural language', 'novel', 'novel therapeutics', 'patient safety', 'post-market', 'prediction algorithm', 'predictive modeling', 'programs', 'response', 'statistics']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R01,2016,539889,-0.020939688290335996
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9176354,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Staging', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2016,599995,-0.002252379758272073
"Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways ﻿    DESCRIPTION (provided by applicant): This project aims to develop new statistical machine learning methods for metabolomics data from diverse platforms, including targeted and unbiased/global mass spectrometry (MS), labeled MS experiments for measuring metabolic ﬂux and Nuclear Magnetic Resonance (NMR) platforms. Unbiased MS and NMR proﬁling studies result in identifying a large number of unnamed spectra, which cannot be directly matched to known metabolites and are hence often discarded in downstream analyses. The ﬁrst aim develops a novel kernel penalized regression method for analysis of data from unbiased proﬁling studies. It provides a systematic framework for extracting the relevant information from unnamed spectra through a kernel that highlights the similarities and differences between samples, and in turn boosts the signal from named metabolites. This results in improved power in identiﬁcation of named metabolites associated with the phenotype of interest, as well as improved prediction accuracy. An extension of this kernel-based framework is also proposed to allow for systematic integration of metabolomics data from diverse proﬁling studies, e.g. targeted and unbiased MS proﬁling technologies. The second aim pro- vides a formal inference framework for kernel penalized regression and thus complements the discovery phase of the ﬁrst aim. The third aim focuses on metabolic pathway enrichment analysis that tests both orchestrated changes in activities of steady state metabolites in a given pathway, as well as aberrations in the mechanisms of metabolic reactions. The fourth aim of the project provides a uniﬁed framework for network-based integrative analysis of static (based on mass spectrometry) and dynamic (based on metabolic ﬂux) metabolomics measurements, thus providing an integrated view of the metabolome and the ﬂuxome. Finally, the last aim implements the pro- posed methods in easy-to-use open-source software leveraging the R language, the capabilities of the Cytoscape platform and the Galaxy workﬂow system, thus providing an expandable platform for further developments in the area of metabolomics. The proposed software tool will also provide a plug-in to the Data Repository and Coordination Center (DRCC) data sets, where all regional metabolomics centers supported by the NIH Common Funds Metabolomics Program deposit curated data.         PUBLIC HEALTH RELEVANCE: Metabolomics, i.e. the study of small molecules involved in metabolism, provides a dynamic view into processes that reﬂect the actual physiology of the cell, and hence offers vast potential for detection of novel biomarkers and targeted therapies for complex diseases. However, despite this potential, the development of computational methods for analysis of metabolomics data lags the rapid growth of metabolomics proﬁling technologies. The current application addresses this need by developing novel statistical machine learning methods for integrative analysis of static and dynamic metabolomics measurements, as well as easy-to-use open-source software to facilitate the application of these methods.            ",Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways,9104589,R01GM114029,"['Accounting', 'Address', 'Adoption', 'Anabolism', 'Area', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Assay', 'Cardiovascular system', 'Cell physiology', 'Cells', 'Characteristics', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Environment', 'Environmental Risk Factor', 'Equilibrium', 'Funding', 'Galaxy', 'Homeostasis', 'Imagery', 'Knowledge', 'Label', 'Language', 'Letters', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Names', 'Network-based', 'Nuclear Magnetic Resonance', 'Pathway interactions', 'Phase', 'Phenotype', 'Plug-in', 'Procedures', 'Process', 'Prognostic Marker', 'Proteomics', 'Reaction', 'Sampling', 'Signal Transduction', 'Software Tools', 'Staging', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Work', 'base', 'biological systems', 'biomarker discovery', 'design', 'diagnostic biomarker', 'improved', 'insight', 'interest', 'learning strategy', 'metabolome', 'metabolomics', 'new technology', 'novel', 'novel diagnostics', 'novel marker', 'open source', 'programs', 'public health relevance', 'rapid growth', 'research study', 'response', 'small molecule', 'targeted treatment', 'tool', 'transcriptomics']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2016,359776,0.020232428570069322
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,-0.007666549594065681
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9241579,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'improved', 'infancy', 'insight', 'model building', 'network models', 'novel', 'research study', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2016,409580,-0.0020303860461086815
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility.         PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.        ",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9076555,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'System', 'Time', 'Training', 'cohort', 'computer science', 'crowdsourcing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2016,316000,-0.004010512046908976
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9142240,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modeling', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2016,380459,-0.027423666432595514
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,0.004401996491705753
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,0.004401996491705753
"A Machine-Learning Based Software Widget for Resolving Metabolite Identities Owing to recent technological advances in measurement platforms, it is now possible to simultaneously detect and characterize a very large number of metabolites covering a substantial fraction of the small molecules present in a biological sample. This presents an exciting opportunity to develop potentially transformative approaches to study cells and organisms. One major challenge in realizing this potential lies in processing and analyzing the data. A typical dataset from an untargeted experiment contains many of thousands of “features,” each of which could correspond to a unique metabolite. Analyzing such datasets to obtain meaningful biological information depends on reliably and efficiently resolving the chemical identities of the detected features. Currently, in silico fragmentation methods predict candidate metabolites that are scored and ranked based on how well the fragmentation explains the observed MS/MS spectrum, and on other factors influencing fragmentation such as bond dissociation energies and ionization conditions. Deciding which candidate metabolites is the best match for a particular feature in the context of the biological sample, however, is a daunting task. Extensive testing of candidate metabolites against chemical standards library may be prohibitive in terms of cost and efforts. We seek to develop software-enabled workflows centered on resolving metabolite identities. Our approach is to exploit knowledge of the biological context of a sample to identify the metabolites. Recognizing that the metabolites present in a sample result from enzyme-catalyzed biochemical reactions active in the corresponding biological system, we employ topological analysis and inference to best map the metabolites implied by the detected features to metabolic pathways that are feasible based on the genome(s) of cells in the biological system. Aim 1 develops a computational method based on Bayesian-inference to enhance candidate metabolite rankings that are obtained via in silico fragmentation analysis. Our method utilizes all available information (database lookups, in silico fragmentation analysis, and network/pathway context) to maximally inform and adjust the rankings. Aim 2 will build software widgets to implement the metabolite identification workflow within a data-analytics framework. As the analytics framework, we will use Orange, which allows the user to create interactive data analysis pipelines through a plug-and-play graphical user interface (GUI). Aim 3 will validate the computational method and software widget implementation. Experimental validation will utilize high-purity standards to confirm (or reject) the computationally assigned metabolite identities. Widget implementation will be evaluated through a focus group discussion with the widget users in the labs directed by the PIs. As project outcomes, we anticipate both a methodological advance in analyzing mass signature data as well as a suite of easily accessible software in the form of widgets. Metabolomics is concerned with the comprehensive characterization of the small molecule metabolites in biological systems. Owing to recent technological advances in measurement platforms, it is now possible to simultaneously detect and characterize a very large number of metabolites. Prospectively, advanced computational tools and software for metabolomics data analysis can aid discovery efforts aimed at identifying novel bioactive metabolites that could be developed into diagnostic indicators or therapeutic agents. ",A Machine-Learning Based Software Widget for Resolving Metabolite Identities,9223450,R03CA211839,"['Address', 'Algorithms', 'Attention', 'Automatic Data Processing', 'Bayesian Analysis', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Cells', 'Chemicals', 'Classification', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Databases', 'Diagnostic', 'Dissociation', 'Environment', 'Enzymes', 'Feedback', 'Focus Groups', 'Genes', 'Genome', 'Goals', 'Human', 'Knowledge', 'Libraries', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Nuclear Magnetic Resonance', 'Oranges', 'Organism', 'Outcome', 'Pathway Analysis', 'Pathway interactions', 'Pattern', 'Play', 'Process', 'Reproducibility', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Surveys', 'Testing', 'Therapeutic Agents', 'Time', 'Uncertainty', 'Validation', 'Visual', 'base', 'biological systems', 'chemical standard', 'computerized tools', 'cost', 'database query', 'flexibility', 'functional outcomes', 'graphical user interface', 'heuristics', 'inhibitor/antagonist', 'instrument', 'ionization', 'mass spectrometer', 'member', 'metabolomics', 'novel', 'programs', 'protein expression', 'research study', 'small molecule', 'software development']",NCI,TUFTS UNIVERSITY MEDFORD,R03,2016,147569,0.0030118241933262007
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9158909,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Process', 'Reading', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'Technology', 'Testing', 'Time', 'Work', 'abstracting', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'research study', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,311803,-0.0027716792624386762
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies. PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,9117645,R00LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'learning strategy', 'memory process', 'model building', 'mouse model', 'neuronal cell body', 'neurovascular', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'terabyte', 'tool', 'tumor growth']",NLM,UNIVERSITY OF HOUSTON,R00,2016,244245,-0.018309471831753354
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338982,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Basic Science', 'Biological', 'Biomedical Research', 'Cell physiology', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2016,1183132,-0.01038185953290107
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9049511,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Health', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'knowledge translation', 'learning progression', 'learning strategy', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428512,0.017621233482044054
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338977,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'improved', 'innovation', 'inter-individual variation', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2016,1071976,-0.030359726364737976
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9099858,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2016,306754,-0.01922786026254218
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9094553,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Effectiveness of Interventions', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Qualifying', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Staging', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'research study', 'response', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2016,141588,-0.012842344176691394
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9161233,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'Work', 'base', 'dashboard', 'empowered', 'genome sequencing', 'improved', 'insight', 'meetings', 'microbial community', 'peer', 'repository', 'social', 'tool', 'transcriptomics', 'web portal']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2016,575532,-0.0009304929526914965
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9099895,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2016,418572,0.004258276640737789
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,0.007035075504548191
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases. PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,9123353,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency response', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'climate data', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'model building', 'news', 'novel', 'pandemic influenza', 'skills', 'social media', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,UNIVERSITY OF WASHINGTON,K01,2016,107469,-0.0007543091999417943
"Semi-supervised learning with electronic medical records Project Summary/Abstract The implementation of electronic medical record (EMR) systems in routine healthcare has resulted in a rich and inexpensive source of data for translational research. When linked with specimen biobanks, these extensive databases offer a unique opportunity to accelerate the goals of disease genomics as they contain large amounts of detailed clinical and genetic data collected for the purposes of medical care [4; 6; 7; 8; 9]. However, the statistical methods to analyze EMR data are limited and thus the focus of this proposal.  In particular, extracting accurate disease phenotype information is a major challenge impeding EMR-based research [10]. Currently, ICD9 codes are used to conﬁrm presence or absence of a disease in cohorts derived from EMRs. These codes are extremely variable and therefore have a signiﬁcant impact on the statistical power of genetic studies [11; 12]. An alternative approach is to develop a highly accurate algorithm to classify disease status. But due to the laborious medical record review required to obtain validated phenotype information for classiﬁer estimation, phenotypes are only available for a small training set nested in a large cohort. In contrast, predictors of phenotype are available for all observations. To improve accuracy and efﬁciency in model estimation and evaluation, it is therefore of interest to develop semi-supervised learning (SSL) methods that utilize the so- called unlabeled data or observations without conﬁrmed phenotype status in addition to the labeled training set.  Although a great body of literature on SSL exists, nearly all methods concern estimation of classiﬁers or prediction rules when the labeled training set is a simple random sample from the large unlabeled data set [13; 14; 15; 16; 17; 18; 19; 20; 21]. Despite the practical importance of evaluating the prediction performance of an estimated model, no SSL procedures currently exist to improve the estimation of model performance parame- ters. Additionally, the simple random sampling assumption is restrictive and the development of semi-supervised (SS) methods that accommodate more ﬂexible sampling schemes in the context of both model estimation and evaluation is needed.  In this proposal, these limitations are addressed through formulation of an efﬁcient method to estimate various prediction performance measures including the ROC curve within the traditional SS framework of simple random sampling. The stratiﬁed random sampling design in the SS setting is also considered and methods to estimate a classiﬁer and its accuracy are developed. These procedures will be applied to EMR-based studies of bipolar disorder and depression. The success of this work will thus improve efﬁciency in analyzing EMR data and expedite the use of EMRs in clinical and genetic research in neuropsychiatry. Project Narrative  The use of electronic medical records (EMRs) in routine healthcare has generated a rich source of data for in-depth study of disease risk factors. However, EMR data typically consists of a very small number of expensive observations with information on disease status and a large amount of automatically extracted data concerning risk factors such as laboratory results and previous health history. Statistical methods that accommodate this data structure are limited and thus the focus of this proposal.",Semi-supervised learning with electronic medical records,9192096,F31GM119263,"['Address', 'Algorithms', 'Bipolar Depression', 'Bipolar Disorder', 'Caring', 'Clinical', 'Clinical Research', 'Code', 'Computerized Medical Record', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Evaluation', 'Formulation', 'Genes', 'Genetic Research', 'Genetic study', 'Genomics', 'Goals', 'Health', 'Healthcare', 'International', 'Label', 'Laboratories', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Genetics', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Patients', 'Performance', 'Phenotype', 'Procedures', 'ROC Curve', 'Recording of previous events', 'Research', 'Rest', 'Risk Factors', 'Sampling', 'Sampling Biases', 'Scheme', 'Specimen', 'Statistical Methods', 'System', 'Time', 'Training', 'Translational Research', 'Work', 'abstracting', 'base', 'biobank', 'clinical care', 'clinical practice', 'clinically relevant', 'cohort', 'cost effective', 'data structure', 'design', 'disease phenotype', 'disorder risk', 'experience', 'improved', 'interest', 'learning strategy', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'patient population', 'phenome', 'repository', 'success']",NIGMS,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2016,34098,-0.02120631218754514
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9041640,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'genomic data', 'hazard', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2016,255295,-0.006182542511389335
"An Open Source Precision Medicine Platform for Cloud Operating Systems ﻿    DESCRIPTION (provided by applicant):  Rapid improvements in DNA sequencing and synthesis have the potential to usher in a new era of precision medicine. To realize this vision, however, we must re-imagine the computational and storage infrastructure used to manage and extract actionable results from the massive data sets made possible by widely available advances in DNA sequencing and synthetic biology. In conjunction with the Global Alliance for Genomics and Health (GA4GH), we propose to build the Arvados platform so that a new ecosystem of clinical decision support applications will be able to navigate petabytes of global biomedical data and search millions of genomes in real-time (seconds). Our team has a proven track record of commercial success and high impact scientific research. Commercialization of this free and open-source software (FOSS) platform, which will be greatly accelerated by this grant, will permit organizations to seamlessly span on-premise & hosted cloud- operating systems and vastly simplify data-management & computation, all while facilitating compliance with institutional policies and regulatory requirements.         PUBLIC HEALTH RELEVANCE:  The delivery of healthcare based on molecular data specific to an individual patient (i.e. precision medicine) will require the creation of a new ecosystem of Clinical Decision Support (CDS) applications. This work will provide a platform that will make the development of such applications faster, easier, and less expensive.        ",An Open Source Precision Medicine Platform for Cloud Operating Systems,9140741,R44GM109737,"['Address', 'Adopted', 'Big Data', 'Bioinformatics', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Collaborations', 'Communities', 'Computer software', 'Contractor', 'DNA Sequence', 'DNA biosynthesis', 'Data', 'Data Set', 'Databases', 'Development', 'Distributed Systems', 'Ecosystem', 'Feedback', 'Fostering', 'Funding', 'Galaxy', 'Genome', 'Genomics', 'Grant', 'Health', 'Healthcare', 'Human', 'Industry', 'Information Technology', 'Institutional Policy', 'International', 'Internet', 'Language', 'Length', 'Letters', 'Machine Learning', 'Maintenance', 'Manuscripts', 'Measures', 'Medicine', 'Memory', 'Molecular', 'Operating System', 'Phase', 'Policies', 'Production', 'Publications', 'Reproducibility', 'Research', 'Research Infrastructure', 'Resources', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Source Code', 'System', 'Technology', 'Time', 'Training Support', 'Vision', 'Work', 'base', 'big biomedical data', 'cloud platform', 'commercialization', 'data management', 'genome sequencing', 'genomic data', 'health care delivery', 'individual patient', 'meetings', 'new technology', 'next generation sequencing', 'open source', 'operation', 'petabyte', 'portability', 'precision medicine', 'public health relevance', 'repository', 'screening', 'success', 'symposium', 'synthetic biology', 'terabyte', 'web services', 'whole genome']",NIGMS,"CUROVERSE, INC.",R44,2016,985339,0.0008155679621167528
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9100683,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2016,680020,0.01589656301222226
"Predicting Resilience in the Human Microbiome DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience. PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.",Predicting Resilience in the Human Microbiome,9125723,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiome', 'gut microbiota', 'human subject', 'innovation', 'learning strategy', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'resilience', 'response', 'theories', 'tool', 'transcriptome', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2016,413065,-0.0011115455536214603
"Modeling the Dynamics of Early Communication and Development DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts. PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.",Modeling the Dynamics of Early Communication and Development,9124921,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Health', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Untranslated RNA', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'dynamic system', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2016,504474,-0.00687254680207272
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9115248,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'study population', 'terabyte', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2016,347156,-0.006054626817592703
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9135552,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2016,324169,-0.005180337205143855
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9200905,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Decision Modeling', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Immigration', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Legal', 'Legal patent', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2016,221175,0.014988998928993635
"Development of integrative models for early liver toxicity assessment ﻿    DESCRIPTION (provided by applicant): Computational toxicology has become a critical area of research due to the burgeoning need to evaluate thousands of pharmaceutical and environmental chemicals with unknown toxicity profiles, the high demand in time and resources by current experimental toxicity testing, and the growing ethical concerns over animal use in toxicity studies. Despite tremendous efforts, little success has been attained thus far in the development of predictive computational models for toxicity, primarily due to the complexity of toxicity mechanisms as well as the lack of high-quality experimental data for model development.  A critical challenge in toxicity testing of chemicals is that toxicity effects are doe-dependent: the true toxic hits may show no toxicity at all at low dose level. Therefore, traditiona high-throughput screening (HTS) that test chemicals only at a single concentration is not suitable for toxicity screening. On the contrary, the recently developed quantitative high-throughput screening (qHTS) platforms can evaluate each chemical across a broad range of concentrations, and is gaining ever-increasing popularity as a tool for in vitro toxicity profiling The concentration-response information generated by qHTS are expected to provide more accurate and comprehensive information of the toxicity effects of chemicals, offering promising data that can be mined to estimate in vivo toxicities of chemicals. However, our previous studies showed that if processed inappropriately, such concentration-response information contribute little to improve the toxicity prediction. This is especially true when multiple types of qHTS data are used together. Therefore, in this study, we will extend our previous approaches to develop novel statistical and computational tools that can curate, preprocess, and normalize the concentration-response information from multiple different qHTS databases.  Traditionally, toxicity models are based on either the chemical data (such as the quantitative structure- activity relationship analysis), or the in vitro toxicity profiling data (such as the in vitro-in vivo extrapolations). Our previous experiences suggested that integrating biological descriptors such as the in vitro cytotoxicity profiles or the short-term toxigenomic data, with chemical structural features is able to predict rodent acute liver toxicity with reasonable accuracy. Therefore, the second part of this proposal will be devoted to develop novel computational models for hepatotoxicity prediction by integrating qHTS toxicity profiles and chemical structural information In Aim 1, we will curate, preprocess, and normalize collected public liver toxicity datasets. In ths study, we will model toxicity effects using multiple large public datasets such as HTS and qHTS bioassay data (Tox21[1] and ToxCast[2]), hepatotoxicity side effect reports on marketed failed drugs[3], the Liver Toxicity Knowledge Base Benchmark Dataset (LTKB-BD[4]), etc. Statistical methods for cross-study validation and quality control will be applied to the collected datasets to ensure computational compatibility and to select the appropriate datasets for analysis. In Aim 2, we will develop predictive models for chemicals' liver toxicity based on an integrative modeling workflow that will make use of both structural and in vitro toxicity profiles of a chemical. Our previous studies [5] showed that models using both in vitro toxicity profiles and chemical structural data have better accuracy for rodent acute liver toxicity than models using either data type alone. Here, we will develop a novel modeling workflow that start with defining the functional clusters of chemicals via curated qHTS toxicity profiles, and is followed by developing computational models to correlate chemical and biological data with overall toxicity risks in humans. The predictive models will be validated using independent datasets with over 800 compounds. In Aim 3, we propose to prioritize the qHTS profiling assays used in the model for future toxicity testing. We will evaluate all the in vitro assays as biological descriptors from thee perspectives, including descriptor importance in the integrative toxicity model, correlation with i vivo DILI outcomes, and level of information content estimated by a novel approach based on network analysis.    PUBLIC HEALTH RELEVANCE: In this study we aim to develop computational models that can identify potential liver toxicants. Liver toxicity is a significant contributor to the high attition rate in drug development. Moreover, toxic chemicals in food, water, and consumer products all pose serious risks for liver toxicity. As a result, there is great interest in developing high-throughput, high-content experimental and computational tools to evaluate the liver toxicity of thousands of pharmaceutical and environmental chemicals. This study focuses on developing novel informatics tools that enable the extraction and integration of chemical concentration-response information from multiple quantitative high-throughput screening databases for model development, and developing statistical models that are able to integrate this concentration-response information with chemical structural features to predict their risk of liver toxicity.  ",Development of integrative models for early liver toxicity assessment,9017336,R03ES026397,"['Acute', 'Address', 'Adverse effects', 'Algorithms', 'Animals', 'Area', 'Benchmarking', 'Biological', 'Biological Assay', 'Chemicals', 'Communities', 'Complex', 'Computer Simulation', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Dose', 'Dreams', 'Ensure', 'Ethics', 'Food', 'Future', 'Gene Expression', 'Genomics', 'Goals', 'Gold', 'Health', 'Hepatotoxicity', 'Human', 'In Vitro', 'Informatics', 'International', 'Liver', 'Machine Learning', 'Marketing', 'Mining', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Environmental Health Sciences', 'Nature', 'North Carolina', 'Outcome', 'Pathway Analysis', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Poison', 'Process', 'Productivity', 'Quality Control', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research', 'Resources', 'Risk', 'Rodent', 'Scientist', 'Shapes', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Toxic effect', 'Toxicity Tests', 'Toxicogenetics', 'Toxicology', 'Translational Research', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'Universities', 'Variant', 'Water', 'base', 'computerized tools', 'consumer product', 'cost', 'cost effective', 'cytotoxicity', 'data modeling', 'drug development', 'drug market', 'drug withdrawal', 'environmental chemical', 'experience', 'high throughput screening', 'improved', 'in vitro Assay', 'in vivo', 'interest', 'knowledge base', 'liver injury', 'model building', 'model development', 'novel', 'novel strategies', 'post-market', 'preclinical study', 'predictive modeling', 'programs', 'response', 'screening', 'success', 'tool', 'toxicant', 'validation studies']",NIEHS,UT SOUTHWESTERN MEDICAL CENTER,R03,2016,81000,0.001003631963893052
"Computer-aided Design and In vitro Testing of Novel Cannabinoid Receptor Modulators ﻿    DESCRIPTION (provided by applicant): Despite major advances in the subject of marijuana and cannabinoid (CB) research, there are only a few CB ligands currently available as FDA-approved drugs, partly because of the absence of experimental structures of CB receptors and a lack of knowledge of the conformational dynamics of the receptors, as well as a limited knowledge of the ligand-binding (orthosteric or allosteric) sites within the CB receptors. Very few allosteric modulators have been identified for CB receptors. There is great potential for discovery of novel CB ligands targeting either orthosteric or allosteric CB1 or CB2 receptor sites. Activators or inhibitors of these receptors are being investigated as therapeutic agents for many important human ailments including neuropathic pain, neuroinflammation, ischemic/reperfusion injury, anxiety, multiple sclerosis, and epilepsy. In this project, computational modeling of human CB1 and CB2 ligands, receptors, and receptor-ligand interactions will be carried out. We are interested in discovering orthosteric agonists selective for CB2 over CB1, peripherally- restricted antagonists, as well as allosteric CB modulators, all of which may be able to provide efficacy without causing negative psychotropic effects. Systematic, computationally-intensive research will yield novel active hit CB receptor modulators, which can later be developed into lead drug candidates. This will be accomplished through the following specific aims: Aim 1. To model, train and validate various ligand-based mathematical models to predict and classify CB ligands as agonists or antagonists. Literature- reported CB ligands and activity data as well as experimental data obtained for proprietary molecules being tested at the University of Mississippi, School of Pharmacy will be curated. Ligand-based models will be constructed, trained and validated. Aim 2. To construct, validate and simulate computational models of the CB1 and CB2 receptors in their active and inactive states. Receptor models will be developed, some that are tailored for agonists (active state) and some for antagonists or inverse agonists (basal or inactive states), utilizing structural information from multiple homologous GPCR templates. Aim 3. To apply integrated ligand-based and receptor-based virtual screening (VS) protocols to discover orthosteric and allosteric CB modulators. Validated models from Aims 1 and 2 will be used strategically in VS protocols to discover novel orthosteric and allosteric CB ligand chemotypes. We will screen repositories of small molecules including MarinLit, ZINC, and that of the National Center for Natural Products Research (NCNPR) and virtual libraries generated using computational tools to access extended chemical space. Top- ranked VS hits will be procured, purified if necessary, and characterized to confirm their structures. Aim 4. To conduct in vitro testing in CB1 and CB2 receptor binding and functional assays. Potential hit compounds from Aim 3 will be tested in CB1 and CB2 receptor binding and functional assays in the University of Mississippi School of Pharmacy NIH COBRE-supported In Vitro Research Core.         PUBLIC HEALTH RELEVANCE: Through computational modeling of the requirements for modulating the cannabinoid (CB) receptor subtypes, CB1 and CB2, which are class-A G-protein coupled receptors (GPCRs), we intend to discover molecules which act through the CB receptors and that could later be developed into new drugs to treat many important human ailments including neuropathic pain, neuroinflammation, ischemic/reperfusion injury, anxiety, multiple sclerosis, and epilepsy.            ",Computer-aided Design and In vitro Testing of Novel Cannabinoid Receptor Modulators,9099196,R15GM119061,"['Academia', 'Adverse effects', 'Affinity', 'Agonist', 'Allosteric Site', 'Antiepileptic Agents', 'Anxiety', 'Area', 'Binding', 'Biological Assay', 'CNR1 gene', 'CNR2 gene', 'Cannabidiol', 'Cannabinoids', 'Cannabis', 'Cannabis sativa plant', 'Centers of Research Excellence', 'Chemicals', 'Classification', 'Complex', 'Computer Simulation', 'Computer-Aided Design', 'Data', 'Eligibility Determination', 'Epilepsy', 'Evaluation', 'FDA approved', 'G-Protein-Coupled Receptors', 'Government', 'Homology Modeling', 'Human', 'In Vitro', 'Industry', 'Knowledge', 'Lead', 'Ligand Binding', 'Ligands', 'Lipid Bilayers', 'Literature', 'Machine Learning', 'Marijuana', 'Methods', 'Mississippi', 'Modeling', 'Multiple Sclerosis', 'Mutagenesis', 'Natural Products', 'Neurosciences Research', 'Organism', 'Persons', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacy Schools', 'Plants', 'Publishing', 'Reperfusion Injury', 'Reporting', 'Research', 'Roentgen Rays', 'Shapes', 'Site', 'Structure', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Virtual Library', 'base', 'cannabinoid receptor', 'computerized tools', 'design', 'drug candidate', 'improved', 'in vitro testing', 'inhibitor/antagonist', 'interest', 'knowledge base', 'mathematical model', 'nanomolar', 'neuroinflammation', 'novel', 'novel therapeutics', 'painful neuropathy', 'pharmacophore', 'protein structure', 'protein structure prediction', 'public health relevance', 'receptor', 'receptor binding', 'repository', 'screening', 'simulation', 'small molecule', 'virtual']",NIGMS,UNIVERSITY OF MISSISSIPPI,R15,2016,427379,-0.03378897606700881
"Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr) EMPIRICAL TESTING AND ENHANCEMENT OF WEB-BASED ABSTRACT SCREENING TOOL (ABSTRACKR)  In this year-long project, we aim to empirically assess the performance and efficiency of state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine and stakeholder-driven comparative effectiveness reviews. We have developed AbstrackrTM (hereon, Abstrackr), a human-guided computerized abstract screening tool that aims to reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. Abstrackr makes use of machine learning techniques, and is offered as a free web-based tool that enables management of the screening process.  We also aim to revise the web-interface of Abstrackr to make it more intuitive, user friendly, and add documentation and functionalities requested by users; and to revise Abstrackr’s back-end, which includes the way the software parses and analyses citations, fits machine learning models, and makes computations, to make it more efficient. These revisions will ensure that the tool becomes more robust, and that it remains usable for larger projects and for many teams.  The proposed work will be carried out by the developers of Abstrackr, comprising a highly experienced team of systematic review investigators and computer scientists at Brown University and the University of Texas at Austin, who have been working together for at least seven years. We will pursue dissemination of the findings of this assessment and of the revised tool through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its wider adoption by the Agency for Healthcare Research and Quality Evidence-based Practice Center Program, Cochrane Collaboration, and other groups conducting systematic reviews. We will also continue to make all code available online. Our aims are to: Aim 1. Empirically measure the efficiency and accuracy of the prediction algorithms in Abstrackr in the computer-assisted semi-automated screening of citations for eligibility in systematic reviews. Aim 2. Improve and add to the functionality of the Web-based Abstrackr software, based in part on enhancements suggested by a panel of identified heavy users. Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making and systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to assess the performance and efficiency of a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care, and to augment the functionality of its public implementation.",Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr),9168247,R03HS024812,[' '],AHRQ,BROWN UNIVERSITY,R03,2016,99999,-0.00346916664534599
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8997510,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2016,510376,-0.01449431448089798
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8985684,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2016,405708,-0.03755089549967802
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,0.005138042259783222
"Machine Learning for Identifying Adverse Drug Events ﻿    DESCRIPTION (provided by applicant): Because of the profound effect of adverse drug events (ADEs) on patient safety, the FDA, AHRQ and Institute of Medicine have flagged post-marketing pharmacovigilance of emerging medications as a high national research priority. The FDA, Foundation for the NIH and PhARMA formed the Observational Medical Outcomes Partnership (OMOP) to develop and compare methods for identification of ADEs, and the FDA announced its Sentinel Initiative. Congress created the Reagan Udall Foundation (RUF) for the FDA in response to the FDA's own ""FDA Science and Mission at Risk"" report, and two years ago OMOP activities were incorporated into RUF. As the FDA moves forward with its development of Sentinel, including work on Mini-Sentinel, there is a need for researchers around the country to continue to develop better methods, and better evaluation methodologies for those methods. A robust research community working on algorithms for pharmacosurveillance, using electronic health records (EHRs) and claims databases will provide a substrate of ever-improving methods on which the nation's regulatory pharmacovigilance infrastructure can build. Indeed an important motivation of OMOP and Mini-Sentinel was to spur the development of such a community. Machine learning has attracted widespread attention across a range of disciplines for its ability to construct accurate predictive models. Therefore machine learning is especially appropriate for the problems of ADE identification and prediction: identifying ADEs from observational data, and predicting which patients are most at risk of suffering the identified ADE. Our current award has demonstrated the ability of machine learning to address both of these tasks. It has added to the existing evidence that consideration of temporal ordering of events, such as drug exposure and diagnoses, is critical for accuracy in identification and prediction of ADEs. The proposed work seeks to further improve upon these methods by building on recent advances in the field of machine learning, by our group and by others, in graphical model learning and in explicit modeling of irregularly-sampled temporal data. The latter is especially important because observational health databases, such as EHRs and claims databases, are not simple time series. Patients typically do not come into the clinic at regular intervals and have the same labs, vitals, and other measurements in lock step with one another. Building better ADE detection and prediction algorithms cannot be accomplished simply by machine learning research, even if that research is taking account of related work from relevant parts of computer science, statistics, biostatistics, epidemiology, pharmaco-epidemiology, and clinical research. Better methods are needed also for evaluation, that is, for estimating how well a new algorithm, or a new use of an existing algorithm, will perform at identifying ADEs associated with a new drug on the market, or at predicting which patients are most at risk of that ADE. More research and evaluation is also needed at the systems level: how can we best construct end-to-end pharmacovigilance systems that sit atop a large observational database and flag potential ADEs for human experts to further investigate? What kinds of information and statistics should such a system provide to the human experts?        This renewal will address the following aims: (1) improve upon machine learning methods for identification and prediction of ADEs, taking advantage of synergies between these two distinct tasks; (2) improve upon existing methods for evaluating ADE detection, building on advances in machine learning for information extraction from scientific literature; (3) improve upon existing methods for evaluating ADE prediction, building upon advances in machine learning for automated support of phenotyping and also building upon improved methods for efficiently obtaining expert labeling of borderline examples of a phenotype; and (4) use the methods developed in the first three aims to construct and evaluate an end-to-end pharmacosurveillance system integrated with the Marshfield Clinic EHR Data Warehouse. Machine learning plays a central and unifying role throughout all four aims. Our investigator team consists of machine learning researchers with experience in analysis of clinical, genomic, and natural language data (Page, Natarajan), a leading pharmaco-epidemiologist with expertise in building systems to efficiently obtain expert evaluation and labeling of phenotypes (Hansen), a leader in phenotyping from EHR data (Peissig), and an MD/PhD practicing physician with years of experience and leadership in the study of ADEs (Caldwell). In addition to building on results of the prior award, we will build on our experiences with OMOP, the International Warfarin Pharmacogenetics Consortium, the DARPA Machine Reading Program, and interactions with the FDA.         PUBLIC HEALTH RELEVANCE: Adverse drug events (ADEs) carry a high cost each year in life, health and money. Congress, the FDA, the NIH and PhARMA have responded with new initiatives for identifying and predicting occurrences of ADEs. It has been widely recognized within initiatives such as Sentinel and the Observational Medical Outcomes Partnership that addressing ADEs requires data, standards and methods for data analysis and mining. This proposal addresses the need for new methods for both identifying previously- unanticipated ADEs and predicting occurrences of a known ADE. It also addresses the needs for improved evaluation and integrated systems approaches.            ",Machine Learning for Identifying Adverse Drug Events,8964138,R01GM097618,"['Accounting', 'Address', 'Adverse drug event', 'Algorithms', 'Attention', 'Award', 'Biometry', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Communities', 'Congresses', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Discipline', 'Doctor of Philosophy', 'Drug Exposure', 'Early Diagnosis', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Evaluation Methodology', 'Evaluation Research', 'Event', 'Foundations', 'Genomics', 'Health', 'Human', 'Institute of Medicine (U.S.)', 'International', 'Label', 'Leadership', 'Learning', 'Life', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Markov Chains', 'Measurement', 'Medical', 'Methods', 'Mission', 'Modeling', 'Monitor', 'Motivation', 'Myocardial Infarction', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Phenotype', 'Physicians', 'Play', 'Process', 'Reading', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Priority', 'Risk', 'Role', 'Safety', 'Sampling', 'Science', 'Sentinel', 'Series', 'Serious Adverse Event', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Time', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Validation', 'Warfarin', 'Wisconsin', 'Work', 'base', 'computer science', 'cost', 'data mining', 'experience', 'improved', 'inhibitor/antagonist', 'interest', 'natural language', 'novel', 'patient safety', 'post-market', 'predictive modeling', 'programs', 'public health relevance', 'response', 'statistics']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R01,2015,608132,-0.020939688290335996
"Image analysis for high-throughput C. elegans infection and metabolism assays DESCRIPTION (provided by applicant): High-throughput screening (HTS) is a technique for searching large libraries of chemical or genetic perturbants, to find new treatments for a disease or to better understand disease pathways. As automated image analysis for cultured cells has improved, microscopy has emerged as one of the most powerful and informative ways to analyze screening samples. However, many diseases and biological pathways can be better studied in whole animals-particularly diseases that involve organ systems and multicellular interactions, such as metabolism and infection. The worm Caenorhabditis elegans is a well-established and effective model organism, used by thousands of researchers worldwide to study complex biological processes. Samples of C. elegans can be robotically prepared and imaged by high-throughput microscopy, but existing image-analysis methods are insuf- ficient for most assays. In this project, image-analysis algorithms that are capable of scoring high-throughput assays of C. elegans will be developed.  The algorithms will be tested and refined in three high-throughput screens, which will uncover chemical and genetic regulators of fat metabolism and infection: (1) A C. elegans viability assay to identify modulators of infection. The proposed algorithms use a probabilistic shape model of C. elegans in order to identify and mea- sure individual worms even when the animals touch or cross. These methods are the basis for quantifying many other phenotypes, including body morphology and subtle variations in reporter signal levels. (2) A C. elegans lipid assay to identify genes that regulate fat metabolism. The algorithms proposed for illumination correction, level-set-based foreground segmentation, well-edge detection, and artifact removal will result in improved or- business in high-throughput experiments. (3) A fluorescence gene expression assay to identify regulators of the response of the C. elegans host to Staphylococcus aureus infection. The proposed techniques for constructing anatomical maps of C. elegans will make it possible to quantify a variety of changes in fluorescent localization patterns in a biologically relevant way.  In addition to discovering new metabolism- and infection-related drugs and genetic regulators through these specific screens, this work will provide the C. elegans community with (a) a new framework for extracting mor- phological features from C. elegans for quantitative analysis of this organism, and (b) a versatile, modular, open-source toolbox of algorithms enabling the discovery of genetic pathways, chemical probes, and drug can- didates in whole organism high-throughput screens relevant to a variety of diseases.  This work is a close collaboration with C. elegans experts Fred Ausubel and Gary Ruvkun at Massachusetts General Hospital/Harvard Medical School, with Polina Golland and Tammy Riklin-Raviv, experts in model-based segmentation and statistical image analysis at MIT's Computer Science and Artificial Intelligence Laboratory, and with Anne Carpenter, developer of open-source image analysis software at the Broad Institute. PUBLIC HEALTH RELEVANCE: Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.",Image analysis for high-throughput C. elegans infection and metabolism assays,8786567,R01GM095672,"['Address', 'Algorithms', 'Animal Model', 'Animals', 'Anti-Infective Agents', 'Artificial Intelligence', 'Atlases', 'Bacteria', 'Biological', 'Biological Assay', 'Biological Process', 'Businesses', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Cultured Cells', 'Data Quality', 'Descriptor', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drug Targeting', 'Excision', 'Fluorescence', 'Gene Expression Profile', 'Gene Expression Profiling', 'General Hospitals', 'Genes', 'Genetic', 'Goals', 'Health', 'Human', 'Image', 'Image Analysis', 'Immune response', 'Individual', 'Infection', 'Institutes', 'Laboratories', 'Learning', 'Life', 'Lighting', 'Lipids', 'Machine Learning', 'Maps', 'Massachusetts', 'Measurement', 'Measures', 'Metabolism', 'Methods', 'Microscopy', 'Microsporidia', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Organism', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Preparation', 'Process', 'Reporter', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Shapes', 'Signal Transduction', 'Software Engineering', 'Staining method', 'Stains', 'Staphylococcus aureus', 'Techniques', 'Testing', 'Touch sensation', 'Variant', 'Whole Organism', 'Work', 'base', 'biomedical scientist', 'body system', 'chemical genetics', 'computer science', 'computerized tools', 'design', 'drug candidate', 'follow-up', 'high throughput analysis', 'high throughput screening', 'human disease', 'image processing', 'imaging Segmentation', 'imaging platform', 'improved', 'interest', 'lipid metabolism', 'medical schools', 'novel', 'open source', 'pathogen', 'research study', 'response', 'screening', 'small molecule libraries', 'two-dimensional']",NIGMS,"BROAD INSTITUTE, INC.",R01,2015,309263,-0.005790620556844357
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,-0.007666549594065681
"Reproducibility Assessment for Multivariate Assays DESCRIPTION (provided by applicant): This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of features, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combination of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penalization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools availble for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.",Reproducibility Assessment for Multivariate Assays,8828718,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'Health', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'research study']",NIGMS,INSILICOS,R43,2015,64005,-0.00364562876531444
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,0.004401996491705753
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies. PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,8920669,R00LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'memory process', 'model building', 'mouse model', 'neuronal cell body', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'tool', 'tumor growth']",NLM,UNIVERSITY OF HOUSTON,R00,2015,239130,-0.018309471831753354
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"".         PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.                 ",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,8864679,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Solutions', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'design', 'innovation', 'interest', 'knowledge translation', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2015,445887,0.017621233482044054
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages.         PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.            ",Multi-Resolution Docking Methods for Electron Microscopy,8964685,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Image', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Relative (related person)', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2015,307928,-0.01922786026254218
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress. Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8913771,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'imaging system', 'improved', 'interest', 'mild cognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2015,330386,-0.02210762726747168
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs.         PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.            ",New Serological Measures of Infectious Disease Transmission Intensity,8947064,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Effectiveness of Interventions', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Low income', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Qualifying', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Staging', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk', 'improved', 'intervention effect', 'intervention program', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2015,142069,-0.012842344176691394
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",8927659,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2015,434391,0.004258276640737789
"BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci DESCRIPTION (provided by applicant): Ideally, as neuroscientists collect terabytes of image stacks, the data are automatically processed for open access and analysis. Yet, while several labs around the world are collecting data at unprecedented rates- up to terabytes per day-the computational technologies that facilitate streaming data-intensive computing remain absent. Also deploying data-intensive compute clusters is beyond the means and abilities of most experimental labs. This project will extend, develop, and deploy such technologies. To demonstrate these tools, we will utilize them in support of the ongoing mouse brain architecture (MBA) project, which already has amassed over 0.5 petabytes (PBs) of image data. The main computational challenges posed by these datasets are ones of scale. The tasks that follow remain relatively stereotyped across acquisition modalities. Until now, labs collecting data on this scale have been almost entirely isolated, left to ""reinvent the wheel"" for each of these problems. Moreover, the extant solutions are insufficient for a number of reasons: they often include numerous excel spreadsheets that rely on manual data entry, they lack scalable scientific database backends, and they run on ad hoc clusters not specifically designed for the computational tasks at hand. We aim to augment the current state of the art by implementing the following technological advancements into the MBA project pipeline: (1) Data Management will consist of a unified system that automatically captures metadata, launches processing pipelines, and provides quality control feedback in minutes instead of hours. (2) Data Processing tasks will run algorithms ""out-of-core"", appropriate for their computational requirements, including registration, alignment, and semantic segmentation of cell bodies and processes. (3) Data Storage will automatically build databases for storing multimodal image data and extracted annotations learned from the machine vision algorithms. These databases will be spatially co-registered and stored on an optimized heterogeneous compute cluster. (4) Data Access will be automatically available to everyone-including all the image data and data derived products-via Web-services, including 3D viewing, downloading, and further processing. (5) Data Analytics will extend random graph models suitable for multiscale circuit graphs. RELEVANCE (See instructions): Nervous system disorders are responsible for approximately 30% of the total burden of illness in the United States. Whole brain neuroanatomy-available from massive neuroscientific image stacks-is widely believed to be a key missing link in our ability to prevent and treat such illnesses. Thus, this project aims to close this gap via the development and application of BIGDATA tools for management, storage, access, and analytics. n/a",BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci,8792208,R01DA036400,"['Algorithms', 'Architecture', 'Brain', 'Cell physiology', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Feedback', 'Graph', 'Hand', 'Hour', 'Image', 'Instruction', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Metadata', 'Modality', 'Modeling', 'Multimodal Imaging', 'Mus', 'Neuroanatomy', 'Process', 'Quality Control', 'Running', 'Semantics', 'Solutions', 'Stereotyping', 'Stream', 'System', 'Technology', 'United States', 'Vision', 'burden of illness', 'cluster computing', 'computer infrastructure', 'computerized data processing', 'data management', 'design', 'nervous system disorder', 'neuronal cell body', 'prevent', 'tool', 'web services']",NIDA,COLD SPRING HARBOR LABORATORY,R01,2015,248295,-0.010046327945664851
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases     DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases.         PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.            ",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,8935819,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency response', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Report (account)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Science', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'climate data', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'model building', 'news', 'novel', 'pandemic influenza', 'public health relevance', 'skills', 'social', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,UNIVERSITY OF WASHINGTON,K01,2015,107469,-0.0007543091999417943
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,8858662,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2015,248912,-0.006182542511389335
"Predicting Resilience in the Human Microbiome DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience. PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.",Predicting Resilience in the Human Microbiome,8904596,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiota', 'human subject', 'innovation', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'resilience', 'response', 'theories', 'tool', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2015,413065,-0.0011115455536214603
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities.         PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.                ","COINSTAC: decentralized, scalable analysis of loosely coupled data",8975906,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Solutions', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2015,727692,0.01589656301222226
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,8929328,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'genetic makeup', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'personalized medicine', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2015,329422,-0.005180337205143855
"Modeling the Dynamics of Early Communication and Development DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts. PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.",Modeling the Dynamics of Early Communication and Development,8897409,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Health', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Untranslated RNA', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2015,521457,-0.00687254680207272
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,8890255,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2015,347156,-0.006054626817592703
"Informatic tools for predicting an ordinal response for high-dimensional data DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale. Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8900334,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Breast Cancer Patient', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2015,121902,-0.010187887519868209
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8803385,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2015,523965,-0.01449431448089798
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS     DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans.         PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.            ",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8817212,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Books', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2015,406247,-0.03755089549967802
"Image analysis for high-throughput C. elegans infection and metabolism assays    DESCRIPTION (provided by applicant): High-throughput screening (HTS) is a technique for searching large libraries of chemical or genetic perturbants, to find new treatments for a disease or to better understand disease pathways. As automated image analysis for cultured cells has improved, microscopy has emerged as one of the most powerful and informative ways to analyze screening samples. However, many diseases and biological pathways can be better studied in whole animals-particularly diseases that involve organ systems and multicellular interactions, such as metabolism and infection. The worm Caenorhabditis elegans is a well-established and effective model organism, used by thousands of researchers worldwide to study complex biological processes. Samples of C. elegans can be robotically prepared and imaged by high-throughput microscopy, but existing image-analysis methods are insuf- ficient for most assays. In this project, image-analysis algorithms that are capable of scoring high-throughput assays of C. elegans will be developed.  The algorithms will be tested and refined in three high-throughput screens, which will uncover chemical and genetic regulators of fat metabolism and infection: (1) A C. elegans viability assay to identify modulators of infection. The proposed algorithms use a probabilistic shape model of C. elegans in order to identify and mea- sure individual worms even when the animals touch or cross. These methods are the basis for quantifying many other phenotypes, including body morphology and subtle variations in reporter signal levels. (2) A C. elegans lipid assay to identify genes that regulate fat metabolism. The algorithms proposed for illumination correction, level-set-based foreground segmentation, well-edge detection, and artifact removal will result in improved or- business in high-throughput experiments. (3) A fluorescence gene expression assay to identify regulators of the response of the C. elegans host to Staphylococcus aureus infection. The proposed techniques for constructing anatomical maps of C. elegans will make it possible to quantify a variety of changes in fluorescent localization patterns in a biologically relevant way.  In addition to discovering new metabolism- and infection-related drugs and genetic regulators through these specific screens, this work will provide the C. elegans community with (a) a new framework for extracting mor- phological features from C. elegans for quantitative analysis of this organism, and (b) a versatile, modular, open-source toolbox of algorithms enabling the discovery of genetic pathways, chemical probes, and drug can- didates in whole organism high-throughput screens relevant to a variety of diseases.  This work is a close collaboration with C. elegans experts Fred Ausubel and Gary Ruvkun at Massachusetts General Hospital/Harvard Medical School, with Polina Golland and Tammy Riklin-Raviv, experts in model-based segmentation and statistical image analysis at MIT's Computer Science and Artificial Intelligence Laboratory, and with Anne Carpenter, developer of open-source image analysis software at the Broad Institute.       PUBLIC HEALTH RELEVANCE: Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.         ",Image analysis for high-throughput C. elegans infection and metabolism assays,8600293,R01GM095672,"['Address', 'Algorithms', 'Animal Model', 'Animals', 'Anti-Infective Agents', 'Artificial Intelligence', 'Atlases', 'Bacteria', 'Biological', 'Biological Assay', 'Biological Process', 'Businesses', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Cultured Cells', 'Data Quality', 'Descriptor', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drug Targeting', 'Excision', 'Fluorescence', 'Gene Expression', 'Gene Expression Profile', 'General Hospitals', 'Genes', 'Genetic', 'Goals', 'Human', 'Image', 'Image Analysis', 'Immune response', 'Individual', 'Infection', 'Institutes', 'Laboratories', 'Learning', 'Life', 'Lighting', 'Lipids', 'Machine Learning', 'Maps', 'Massachusetts', 'Measurement', 'Measures', 'Metabolism', 'Methods', 'Microscopy', 'Microsporidia', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Organism', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Preparation', 'Process', 'Reporter', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Shapes', 'Signal Transduction', 'Software Engineering', 'Staining method', 'Stains', 'Staphylococcus aureus', 'Techniques', 'Testing', 'Touch sensation', 'Variant', 'Whole Organism', 'Work', 'base', 'biomedical scientist', 'body system', 'chemical genetics', 'computer science', 'computerized tools', 'design', 'drug candidate', 'follow-up', 'high throughput analysis', 'high throughput screening', 'human disease', 'image processing', 'imaging Segmentation', 'improved', 'interest', 'lipid metabolism', 'medical schools', 'novel', 'open source', 'pathogen', 'public health relevance', 'research study', 'response', 'screening', 'small molecule libraries', 'two-dimensional']",NIGMS,"BROAD INSTITUTE, INC.",R01,2014,310129,-0.005790620556844357
"GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY     DESCRIPTION:  Many complex diseases such as cancer, cardiovascular disorders, and schizophrenia may be understood as failures in the functioning of nested hierarchies of biomolecular and cellular networks. These nested hierarchies control a range of processes including the differentiation and migration of cells, remodeling of extracellular matrices and tissues, and information encoding in neuronal subsystems. Washington University has established expertise in cutting edge imaging, molecular biology and genomic technologies synergistic with computational approaches such as machine learning and unraveling the principles of hierarchical organization and dynamics of complex systems. This collective expertise is being leveraged to develop new drugs, improve our ability to interpret sophisticated imaging data, understand how populations of neurons act collectively to accomplish complex tasks, and model the onset and progression of complex diseases as dynamical rewiring of hierarchical, multi-scale networks. Biological network analyses provide a rich set of tools for organizing and interpreting the vast quantities of data produced by state-of-the-art experimental protocols. The rapid advancement of computationally intensive research in these areas is outstripping the capabilities of CPU-based high performance computing (HPC) systems. This application would support the acquisition and integration of a large-scale IBM high performance cluster of Graphics Processor Units (GPUs) to be added as an upgrade to the existing IBM-designed Heterogeneous High Performance Computing environment to form a state-of-the-art hybrid computing capability. Such a resource is essential to match the growing need for high performance computing at Washington University and to support state of the art research software applications that are optimized for GPU computing. The acquisition and integration of a high performance GPU cluster will solve critical computing challenges that exist within Washington University's growing NIH research portfolio. The proposed state-of-the-art hybrid GPU/CPU computing capabilities will be deployed within the framework of a stable, productive and rapidly growing resource center. The addition of high-capacity GPU computing capabilities will allow critical calculation to be performed in hours instead of days and enable substantial increases in productivity for existing projects covering a broad range of application areas as well as enabling new research directions.             n/a",GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY,8640341,S10OD018091,"['Area', 'Biological', 'Biology', 'Cardiovascular Diseases', 'Complex', 'Computer Systems', 'Computer software', 'Data', 'Disease', 'Environment', 'Extracellular Matrix', 'Failure', 'Genomics', 'High Performance Computing', 'Hour', 'Hybrids', 'Image', 'Machine Learning', 'Malignant Neoplasms', 'Modeling', 'Molecular Biology', 'Neurons', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Productivity', 'Protocols documentation', 'Research', 'Resources', 'Schizophrenia', 'System', 'Technology', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'cell motility', 'computing resources', 'design', 'improved', 'innovation', 'tool']",OD,WASHINGTON UNIVERSITY,S10,2014,597700,0.0031288397329062895
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8657051,R01GM053163,"['Address', 'Algorithms', 'Biochemical', 'Budgets', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Development', 'Drug Design', 'Evaluation', 'Funding', 'Geometry', 'Goals', 'Image', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Output', 'Pattern', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Relative (related person)', 'Research', 'Shapes', 'Signal Transduction', 'Solutions', 'Specimen', 'Spottings', 'Structure', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2014,320096,-0.006605887067670223
"Reproducibility Assessment for Multivariate Assays  Project Summary. This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of fea- tures, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combina- tion of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penal- ization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools available for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.            ",Reproducibility Assessment for Multivariate Assays,8647816,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Simulate', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'public health relevance', 'research study']",NIGMS,INSILICOS,R43,2014,131071,-0.003862928648927994
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,0.004401996491705753
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies. PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,8916335,R00LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Commit', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Simulate', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'memory process', 'mouse model', 'neuronal cell body', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'tool', 'tumor growth']",NLM,UNIVERSITY OF HOUSTON,R00,2014,246867,-0.018309471831753354
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8653848,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,436094,-0.005432616707504914
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics     DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress.             Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8714056,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Simulate', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'interest', 'mild cognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2014,330386,-0.02210762726747168
"Models for synthesising molecular, clinical and epidemiological data, and transla     DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)?         PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.            ","Models for synthesising molecular, clinical and epidemiological data, and transla",8703195,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2014,427668,0.004258276640737789
"Building an open-source cloud-based computational platform to improve data access   We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members.  One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. This proposal aims to fill this gap by developing a robust platform that integrates state-of-the-art open-source technologies for data storage, data access, data mining and analysis, annotation, visualization and reporting.  We previously developed a cloud-based BioDatomics platform for Next Generation Sequencing (NGS), BioDX, which has been successful and has been used commercially by several clients. This proposal aims to develop a new platform leveraging our experience with the BioDX platform that integrates: data storage and real-time data querying using Cloudera Impala; powerful and customizable analytics tools using R and its derivative Bioconductor suite of programs for bioinformatics; annotation integration and reporting which is an existing feature of BioDX; and a visual programming interface that will simplify and enhance the development and maintenance of reproducible analytics workflows. We believe this powerful integrated data platform, if successful, will enable real-time collaboration, dramatically reduce data repository costs, and increase the efficiency and efficacy of data analyses for translating experimental data into actionable research products.  We are committed to analyzing stakeholder needs and optimizing hardware, software and information technology systems to meet their demands. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools.  The initial phase of work will involve development of the platform, optimizing performance on the cloud and testing the integration of new technology. BioDatomics is committed to funding the next phase of work which will include usability testing and finalizing a commercial product, following which full commercialization will proceed. Preliminary commercialization plans have demonstrated that the project has the capacity to generate a million dollars in revenue during the first full year after commercial release.  The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This open source platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders. PUBLIC HEALTH RELEVANCE: One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools. The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders.            ",Building an open-source cloud-based computational platform to improve data access,8647860,R43DA036970,"['Academia', 'Apache Indians', 'Bioconductor', 'Bioinformatics', 'Biological', 'Businesses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Clinical', 'Clinical Research', 'Collaborations', 'Commit', 'Complex', 'Computer software', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Distributed Databases', 'Drug Industry', 'Environment', 'Expenditure', 'Funding', 'Genomics', 'Government Agencies', 'Growth', 'Imagery', 'Industry', 'Information Dissemination', 'Information Systems', 'Java', 'Language', 'Licensing', 'Link', 'Machine Learning', 'Maintenance', 'Marketing', 'Messenger RNA', 'Modeling', 'Molecular', 'Molecular Target', 'Mutation', 'Online Systems', 'Patients', 'Performance', 'Pharmacologic Substance', 'Phase', 'Programming Languages', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk Behaviors', 'Savings', 'Services', 'Software Engineering', 'Software Tools', 'Solutions', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Substance Addiction', 'Substance Use Disorder', 'Substance abuse problem', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Visual', 'Work', 'base', 'beneficiary', 'cloud based', 'commercialization', 'computer infrastructure', 'cost', 'cost effective', 'data mining', 'drug discovery', 'experience', 'improved', 'mathematical model', 'meetings', 'member', 'models and simulation', 'new technology', 'next generation sequencing', 'novel', 'open source', 'programs', 'public health relevance', 'substance abuse treatment', 'success', 'tool', 'usability', 'user-friendly']",NIDA,"BIODATOMICS, LLC",R43,2014,195584,-0.05376446167989047
"BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci     DESCRIPTION (provided by applicant): Ideally, as neuroscientists collect terabytes of image stacks, the data are automatically processed for open access and analysis. Yet, while several labs around the world are collecting data at unprecedented rates- up to terabytes per day-the computational technologies that facilitate streaming data-intensive computing remain absent. Also deploying data-intensive compute clusters is beyond the means and abilities of most experimental labs. This project will extend, develop, and deploy such technologies. To demonstrate these tools, we will utilize them in support of the ongoing mouse brain architecture (MBA) project, which already has amassed over 0.5 petabytes (PBs) of image data. The main computational challenges posed by these datasets are ones of scale. The tasks that follow remain relatively stereotyped across acquisition modalities. Until now, labs collecting data on this scale have been almost entirely isolated, left to ""reinvent the wheel"" for each of these problems. Moreover, the extant solutions are insufficient for a number of reasons: they often include numerous excel spreadsheets that rely on manual data entry, they lack scalable scientific database backends, and they run on ad hoc clusters not specifically designed for the computational tasks at hand. We aim to augment the current state of the art by implementing the following technological advancements into the MBA project pipeline: (1) Data Management will consist of a unified system that automatically captures metadata, launches processing pipelines, and provides quality control feedback in minutes instead of hours. (2) Data Processing tasks will run algorithms ""out-of-core"", appropriate for their computational requirements, including registration, alignment, and semantic segmentation of cell bodies and processes. (3) Data Storage will automatically build databases for storing multimodal image data and extracted annotations learned from the machine vision algorithms. These databases will be spatially co-registered and stored on an optimized heterogeneous compute cluster. (4) Data Access will be automatically available to everyone-including all the image data and data derived products-via Web-services, including 3D viewing, downloading, and further processing. (5) Data Analytics will extend random graph models suitable for multiscale circuit graphs. RELEVANCE (See instructions): Nervous system disorders are responsible for approximately 30% of the total burden of illness in the United States. Whole brain neuroanatomy-available from massive neuroscientific image stacks-is widely believed to be a key missing link in our ability to prevent and treat such illnesses. Thus, this project aims to close this gap via the development and application of BIGDATA tools for management, storage, access, and analytics.              n/a",BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci,8631080,R01DA036400,"['Algorithms', 'Architecture', 'Brain', 'Cell physiology', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Feedback', 'Graph', 'Hand', 'Hour', 'Image', 'Instruction', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Metadata', 'Modality', 'Modeling', 'Multimodal Imaging', 'Mus', 'Neuroanatomy', 'Process', 'Quality Control', 'Running', 'Semantics', 'Solutions', 'Stereotyping', 'Stream', 'System', 'Technology', 'United States', 'Vision', 'burden of illness', 'cluster computing', 'computer infrastructure', 'computerized data processing', 'data management', 'design', 'nervous system disorder', 'neuronal cell body', 'prevent', 'tool', 'web services']",NIDA,COLD SPRING HARBOR LABORATORY,R01,2014,250003,-0.010046327945664851
"Heterogeneous and Robust Survival Analysis in Genomic Studies     DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed.         PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.            ",Heterogeneous and Robust Survival Analysis in Genomic Studies,8696520,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,255295,-0.006182542511389335
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases     DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases.         PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.            ",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,8829434,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Coronavirus', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency Situation', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Report (account)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Science', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syndrome', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'news', 'novel', 'pandemic influenza', 'public health relevance', 'respiratory', 'response', 'skills', 'social', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,BOSTON CHILDREN'S HOSPITAL,K01,2014,42469,-0.0007543091999417943
"Predicting Resilience in the Human Microbiome     DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience.         PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.                            ",Predicting Resilience in the Human Microbiome,8741929,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiota', 'human subject', 'innovation', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'public health relevance', 'resilience', 'response', 'theories', 'tool', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2014,413065,-0.0011115455536214603
"Data-Driven Statistical Learning with Applications to Genomics     DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software.         PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.            ",Data-Driven Statistical Learning with Applications to Genomics,8796068,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Simulate', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'high throughput analysis', 'interest', 'novel', 'patient population', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2014,361063,-0.005180337205143855
"Modeling the Dynamics of Early Communication and Development     DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts.         PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.",Modeling the Dynamics of Early Communication and Development,8711519,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Functional RNA', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'public health relevance', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2014,480422,-0.00687254680207272
"Statistical methods for large and complex databases of ultra-high-dimensional     DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences.         PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8738735,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2014,343683,-0.006054626817592703
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8714054,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2014,227026,-0.010187887519868209
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8628132,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2014,525880,-0.01449431448089798
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8737919,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2014,2941548,-0.019607911461657215
"Image analysis for high-throughput C. elegans infection and metabolism assays    DESCRIPTION (provided by applicant): High-throughput screening (HTS) is a technique for searching large libraries of chemical or genetic perturbants, to find new treatments for a disease or to better understand disease pathways. As automated image analysis for cultured cells has improved, microscopy has emerged as one of the most powerful and informative ways to analyze screening samples. However, many diseases and biological pathways can be better studied in whole animals-particularly diseases that involve organ systems and multicellular interactions, such as metabolism and infection. The worm Caenorhabditis elegans is a well-established and effective model organism, used by thousands of researchers worldwide to study complex biological processes. Samples of C. elegans can be robotically prepared and imaged by high-throughput microscopy, but existing image-analysis methods are insuf- ficient for most assays. In this project, image-analysis algorithms that are capable of scoring high-throughput assays of C. elegans will be developed.  The algorithms will be tested and refined in three high-throughput screens, which will uncover chemical and genetic regulators of fat metabolism and infection: (1) A C. elegans viability assay to identify modulators of infection. The proposed algorithms use a probabilistic shape model of C. elegans in order to identify and mea- sure individual worms even when the animals touch or cross. These methods are the basis for quantifying many other phenotypes, including body morphology and subtle variations in reporter signal levels. (2) A C. elegans lipid assay to identify genes that regulate fat metabolism. The algorithms proposed for illumination correction, level-set-based foreground segmentation, well-edge detection, and artifact removal will result in improved or- business in high-throughput experiments. (3) A fluorescence gene expression assay to identify regulators of the response of the C. elegans host to Staphylococcus aureus infection. The proposed techniques for constructing anatomical maps of C. elegans will make it possible to quantify a variety of changes in fluorescent localization patterns in a biologically relevant way.  In addition to discovering new metabolism- and infection-related drugs and genetic regulators through these specific screens, this work will provide the C. elegans community with (a) a new framework for extracting mor- phological features from C. elegans for quantitative analysis of this organism, and (b) a versatile, modular, open-source toolbox of algorithms enabling the discovery of genetic pathways, chemical probes, and drug can- didates in whole organism high-throughput screens relevant to a variety of diseases.  This work is a close collaboration with C. elegans experts Fred Ausubel and Gary Ruvkun at Massachusetts General Hospital/Harvard Medical School, with Polina Golland and Tammy Riklin-Raviv, experts in model-based segmentation and statistical image analysis at MIT's Computer Science and Artificial Intelligence Laboratory, and with Anne Carpenter, developer of open-source image analysis software at the Broad Institute.       PUBLIC HEALTH RELEVANCE: Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.         ",Image analysis for high-throughput C. elegans infection and metabolism assays,8402395,R01GM095672,"['Address', 'Algorithms', 'Animal Model', 'Animals', 'Anti-Infective Agents', 'Artificial Intelligence', 'Atlases', 'Bacteria', 'Biological', 'Biological Assay', 'Biological Process', 'Businesses', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Cultured Cells', 'Data Quality', 'Descriptor', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drug Targeting', 'Excision', 'Fluorescence', 'Gene Expression', 'Gene Expression Profile', 'General Hospitals', 'Genes', 'Genetic', 'Goals', 'Human', 'Image', 'Image Analysis', 'Immune response', 'Individual', 'Infection', 'Institutes', 'Laboratories', 'Learning', 'Life', 'Lighting', 'Lipids', 'Machine Learning', 'Maps', 'Massachusetts', 'Measurement', 'Measures', 'Metabolism', 'Methods', 'Microscopy', 'Microsporidia', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Organism', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Preparation', 'Process', 'Reporter', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Shapes', 'Signal Transduction', 'Software Engineering', 'Staining method', 'Stains', 'Staphylococcus aureus', 'Techniques', 'Testing', 'Touch sensation', 'Variant', 'Whole Organism', 'Work', 'base', 'biomedical scientist', 'body system', 'chemical genetics', 'computer science', 'computerized tools', 'design', 'drug candidate', 'follow-up', 'high throughput analysis', 'high throughput screening', 'human disease', 'image processing', 'imaging Segmentation', 'improved', 'interest', 'lipid metabolism', 'medical schools', 'novel', 'open source', 'pathogen', 'public health relevance', 'research study', 'response', 'screening', 'small molecule libraries', 'two-dimensional']",NIGMS,"BROAD INSTITUTE, INC.",R01,2013,301051,-0.005790620556844357
"Brain Science Computer Cluster     DESCRIPTION (provided by applicant): Computational requirements of contemporary brain science research often exceed financial and resource management limits of individual investigator laboratories. Many contemporary neuroscience research projects require analysis of large data sets with advanced statistical methods and anatomical reconstruction techniques. These methods require high speed computational and graphics engines operating in a multiple processor environments equipped with large capacity, high-speed storage devices. A limitation in the Brown brain science effort at understanding neural processing is the lack of a readily accessible high-speed computational resource. A central computational resource based on a unified cluster of contemporary Linux CPUs and GPUs will serve the computational needs of a core group of brain science investigators at Brown without compromising individual access common to stand-alone workstations. The requested computer cluster has system software that automatically balances CPU and GPU usage, thereby ensuring maximum access to the computational resource for all users. Intensive 3D graphics are off-loaded either to GPUs or to client workstations, thereby further reducing the central computational load. Commercial or open-source software with an open operating environment will be used for analysis using standard and novel statistical and machine learning approaches to assess significance of large data sets. This proposal details the architecture and benefits of a contemporary computational resource for the major and minor users, and more generally the Brown brain science community. The resource was designed to fill immediate and near-term computational and storage needs of a core group of Brown brain scientists. The system can be readily expansion as needs, either computational, storage, or new users, arise. Expansion of the existing core investigators group can occur easily since the computational power or storage capacity of the system can be readily enhanced at relatively low cost. The flexible nature of the system will serve a variety of research needs of the Brown brain science community. The computational resource is expected to bring together researchers at Brown working on the common problem of neural processing.             n/a",Brain Science Computer Cluster,8447697,S10OD016366,"['Architecture', 'Brain', 'Client', 'Communities', 'Computer software', 'Data Set', 'Devices', 'Ensure', 'Environment', 'Equilibrium', 'Individual', 'Laboratories', 'Linux', 'Machine Learning', 'Methods', 'Minor', 'Nature', 'Neurosciences Research', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Speed', 'Statistical Methods', 'System', 'Techniques', 'Work', 'base', 'computer cluster', 'computing resources', 'cost', 'design', 'flexibility', 'novel', 'open source', 'reconstruction', 'relating to nervous system', 'software systems']",OD,BROWN UNIVERSITY,S10,2013,599598,-0.01845560279141277
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8470172,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2013,309127,-0.006605887067670223
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular     DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies.                  PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,8508596,K99LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Commit', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Simulate', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'memory process', 'mouse model', 'neuronal cell body', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'tool', 'tumor growth']",NLM,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,K99,2013,87300,-0.018309471831753354
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8461069,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,425454,-0.005432616707504914
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics     DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress.              Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8538499,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Simulate', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'interest', 'mild cognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2013,313357,-0.02210762726747168
"BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci     DESCRIPTION (provided by applicant): Ideally, as neuroscientists collect terabytes of image stacks, the data are automatically processed for open access and analysis. Yet, while several labs around the world are collecting data at unprecedented rates- up to terabytes per day-the computational technologies that facilitate streaming data-intensive computing remain absent. Also deploying data-intensive compute clusters is beyond the means and abilities of most experimental labs. This project will extend, develop, and deploy such technologies. To demonstrate these tools, we will utilize them in support of the ongoing mouse brain architecture (MBA) project, which already has amassed over 0.5 petabytes (PBs) of image data. The main computational challenges posed by these datasets are ones of scale. The tasks that follow remain relatively stereotyped across acquisition modalities. Until now, labs collecting data on this scale have been almost entirely isolated, left to ""reinvent the wheel"" for each of these problems. Moreover, the extant solutions are insufficient for a number of reasons: they often include numerous excel spreadsheets that rely on manual data entry, they lack scalable scientific database backends, and they run on ad hoc clusters not specifically designed for the computational tasks at hand. We aim to augment the current state of the art by implementing the following technological advancements into the MBA project pipeline: (1) Data Management will consist of a unified system that automatically captures metadata, launches processing pipelines, and provides quality control feedback in minutes instead of hours. (2) Data Processing tasks will run algorithms ""out-of-core"", appropriate for their computational requirements, including registration, alignment, and semantic segmentation of cell bodies and processes. (3) Data Storage will automatically build databases for storing multimodal image data and extracted annotations learned from the machine vision algorithms. These databases will be spatially co-registered and stored on an optimized heterogeneous compute cluster. (4) Data Access will be automatically available to everyone-including all the image data and data derived products-via Web-services, including 3D viewing, downloading, and further processing. (5) Data Analytics will extend random graph models suitable for multiscale circuit graphs. RELEVANCE (See instructions): Nervous system disorders are responsible for approximately 30% of the total burden of illness in the United States. Whole brain neuroanatomy-available from massive neuroscientific image stacks-is widely believed to be a key missing link in our ability to prevent and treat such illnesses. Thus, this project aims to close this gap via the development and application of BIGDATA tools for management, storage, access, and analytics.              n/a",BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci,8599834,R01DA036400,"['Algorithms', 'Architecture', 'Brain', 'Cell physiology', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Feedback', 'Graph', 'Hand', 'Hour', 'Image', 'Instruction', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Metadata', 'Modality', 'Modeling', 'Multimodal Imaging', 'Mus', 'Neuroanatomy', 'Process', 'Quality Control', 'Running', 'Semantics', 'Solutions', 'Stereotyping', 'Stream', 'System', 'Technology', 'United States', 'Vision', 'burden of illness', 'cluster computing', 'computer infrastructure', 'computerized data processing', 'data management', 'design', 'nervous system disorder', 'neuronal cell body', 'prevent', 'tool', 'web services']",NIDA,COLD SPRING HARBOR LABORATORY,R01,2013,249999,-0.010046327945664851
"Predicting Resilience in the Human Microbiome     DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience.         PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.                            ",Predicting Resilience in the Human Microbiome,8549818,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiota', 'human subject', 'innovation', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'public health relevance', 'resilience', 'response', 'theories', 'tool', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2013,1235799,-0.0011115455536214603
"Modeling the Dynamics of Early Communication and Development     DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts.         PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.                ",Modeling the Dynamics of Early Communication and Development,8452565,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Functional RNA', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'public health relevance', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2013,571955,-0.00687254680207272
"Statistical methods for large and complex databases of ultra-high-dimensional  Abstract Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8614974,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2013,373406,-0.006593133632162723
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8538496,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2013,234332,-0.010187887519868209
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8504843,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2013,527736,-0.01449431448089798
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data.  PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.          ",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,8520329,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Health', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'NMR Spectroscopy', 'Negative Staining', 'Neighborhoods', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2013,301072,-0.023407323297262108
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541872,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2013,872488,-0.019607911461657215
"ISMB 2012 Conference Support for Students & Young Scientists     DESCRIPTION (provided by applicant): The 2012 Intelligent Systems for Molecular Biology (ISMB) conference in will be held in Long Beach, California, with 1,500-1,700 attendees, including 33-38% students/post doctoral researchers. ISMB brings together graduate students, post doctoral researchers, faculty, research staff and senior scientists of many different nationalities, all of whom are studying or working in computer science, molecular biology, mathematics or statistics. The conference brings biologists and computational scientists together to focus on research centered on actual biological problems rather than simply theoretical calculations. The combined focus on ""intelligent systems"" and actual biological data makes ISMB a highly relevant meeting, and many years of producing the event has resulted in a professionally organized and respected annual conference. The ISMB conference presents the latest research methods and results developed through the application of computer programming to the study of biological sciences, including advances in sequencing genomes that may lead to a better understanding of how, for instance, cells interact for the treatment of diseases such as cancer. Presentations may describe methods and advances associated with the analysis of existing biological literature, including benchmarking experiments, to create a better public understanding of scientific research reports. Overall, ISMB serves to educate attendees on the latest developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to appy what they have learned as they advance their own research efforts or begin investigating new areas they were exposed to as a result of attending ISMB. The scientific program for each ISMB meeting includes parallel presentation tracks of original research papers, highlights of recently published papers, special sessions focused on emerging topics, technology demos, late breaking research and poster presentations, an art in science exhibition, tutorial workshops, special interest group meetings and a student symposium organized by and for students. For ISMB 2011, 258 original research papers were submitted and 48 selected for the Proceedings Track, while 88 previously published papers were submitted and 38 selected for the Highlights Track. In all, over 225 talks were presented during the course of the 2011 conference, and similar numbers are anticipated for 2012. In all cases, submissions are rigorously reviewed, typically by three members of each track's committee before approval by the track chair, insuring the highest possible quality of work is presented. The specific areas represented in the conference vary each year depending on the areas that researchers find most interesting and innovative, and therefore submit as papers and proposals. This proposal seeks funding to assist students and junior researchers in attending the conference, thus exposing them to the latest research of their own areas as well as areas that may be new to them.        PUBLIC HEALTH RELEVANCE: Bioinformatics is well established as an essential tool for understanding biological systems, largely driven by genomic sequence efforts due to the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field and exposing what's on the horizon of future discoveries, but is distinguished from many other events in computational biology or artificial intelligence by an insistence that the researchers work with real molecular biology data, not theoretical or toy examples. Although the cultures of computer science and biology are so disparate, ISMB bridges this cultural gap by providing a forum among biological conferences that features technical advances as they occur, which otherwise may be shunned until a firm experimental result is published.              Bioinformatics is well established as an essential tool for understanding biological systems, largely driven by genomic sequence efforts due to the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field and exposing what's on the horizon of future discoveries, but is distinguished from many other events in computational biology or artificial intelligence by an insistence that the researchers work with real molecular biology data, not theoretical or toy examples. Although the cultures of computer science and biology are so disparate, ISMB bridges this cultural gap by providing a forum among biological conferences that features technical advances as they occur, which otherwise may be shunned until a firm experimental result is published.            ",ISMB 2012 Conference Support for Students & Young Scientists,8317817,R13GM101868,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Arts', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biology', 'California', 'Cells', 'Computational Biology', 'Computational Technique', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Educational workshop', 'Elements', 'Event', 'Evolution', 'Expert Systems', 'Faculty', 'Feedback', 'Financial Support', 'Funding', 'Future', 'Genomics', 'Graph', 'Group Meetings', 'Home environment', 'Human', 'International', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Limited Stage', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Metabolic Pathway', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Structure', 'Nationalities', 'Oral', 'Paper', 'Participant', 'Pattern Recognition', 'Peer Review', 'Phylogenetic Analysis', 'Postdoctoral Fellow', 'Published Comment', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Robotics', 'Science', 'Scientist', 'Senior Scientist', 'Sequence Analysis', 'Series', 'Specialist', 'Speed', 'Students', 'System', 'Technology', 'Time', 'Toy', 'Training', 'Validation', 'Work', 'biological systems', 'career', 'computer program', 'computer science', 'disorder prevention', 'exhibitions', 'experience', 'genome sequencing', 'graduate student', 'improved', 'information organization', 'innovation', 'interest', 'lectures', 'medical specialties', 'meetings', 'member', 'multidisciplinary', 'next generation', 'novel', 'parallel computing', 'posters', 'practical application', 'programs', 'research study', 'role model', 'satisfaction', 'skills', 'special interest group', 'statistics', 'symposium', 'tool']",NIGMS,INTERNATIONAL SOCIETY/COMP BIOLOGY,R13,2012,25000,-0.006397785107041158
"Image analysis for high-throughput C. elegans infection and metabolism assays    DESCRIPTION (provided by applicant): High-throughput screening (HTS) is a technique for searching large libraries of chemical or genetic perturbants, to find new treatments for a disease or to better understand disease pathways. As automated image analysis for cultured cells has improved, microscopy has emerged as one of the most powerful and informative ways to analyze screening samples. However, many diseases and biological pathways can be better studied in whole animals-particularly diseases that involve organ systems and multicellular interactions, such as metabolism and infection. The worm Caenorhabditis elegans is a well-established and effective model organism, used by thousands of researchers worldwide to study complex biological processes. Samples of C. elegans can be robotically prepared and imaged by high-throughput microscopy, but existing image-analysis methods are insuf- ficient for most assays. In this project, image-analysis algorithms that are capable of scoring high-throughput assays of C. elegans will be developed.  The algorithms will be tested and refined in three high-throughput screens, which will uncover chemical and genetic regulators of fat metabolism and infection: (1) A C. elegans viability assay to identify modulators of infection. The proposed algorithms use a probabilistic shape model of C. elegans in order to identify and mea- sure individual worms even when the animals touch or cross. These methods are the basis for quantifying many other phenotypes, including body morphology and subtle variations in reporter signal levels. (2) A C. elegans lipid assay to identify genes that regulate fat metabolism. The algorithms proposed for illumination correction, level-set-based foreground segmentation, well-edge detection, and artifact removal will result in improved or- business in high-throughput experiments. (3) A fluorescence gene expression assay to identify regulators of the response of the C. elegans host to Staphylococcus aureus infection. The proposed techniques for constructing anatomical maps of C. elegans will make it possible to quantify a variety of changes in fluorescent localization patterns in a biologically relevant way.  In addition to discovering new metabolism- and infection-related drugs and genetic regulators through these specific screens, this work will provide the C. elegans community with (a) a new framework for extracting mor- phological features from C. elegans for quantitative analysis of this organism, and (b) a versatile, modular, open-source toolbox of algorithms enabling the discovery of genetic pathways, chemical probes, and drug can- didates in whole organism high-throughput screens relevant to a variety of diseases.  This work is a close collaboration with C. elegans experts Fred Ausubel and Gary Ruvkun at Massachusetts General Hospital/Harvard Medical School, with Polina Golland and Tammy Riklin-Raviv, experts in model-based segmentation and statistical image analysis at MIT's Computer Science and Artificial Intelligence Laboratory, and with Anne Carpenter, developer of open-source image analysis software at the Broad Institute.      PUBLIC HEALTH RELEVANCE: Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.           Public Health Relevance/Narrative Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.",Image analysis for high-throughput C. elegans infection and metabolism assays,8208036,R01GM095672,"['Address', 'Algorithms', 'Animal Model', 'Animals', 'Anti-Infective Agents', 'Artificial Intelligence', 'Atlases', 'Bacteria', 'Biological', 'Biological Assay', 'Biological Process', 'Businesses', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Cultured Cells', 'Data Quality', 'Descriptor', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drug Delivery Systems', 'Excision', 'Fluorescence', 'Gene Expression', 'Gene Expression Profile', 'General Hospitals', 'Genes', 'Genetic', 'Goals', 'Human', 'Image', 'Image Analysis', 'Immune response', 'Individual', 'Infection', 'Institutes', 'Laboratories', 'Learning', 'Life', 'Lighting', 'Lipids', 'Machine Learning', 'Maps', 'Massachusetts', 'Measurement', 'Measures', 'Metabolism', 'Methods', 'Microscopy', 'Microsporidia', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Organism', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Preparation', 'Process', 'Reporter', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Screening procedure', 'Shapes', 'Signal Transduction', 'Software Engineering', 'Staining method', 'Stains', 'Staphylococcus aureus', 'Techniques', 'Testing', 'Touch sensation', 'Variant', 'Whole Organism', 'Work', 'base', 'biomedical scientist', 'body system', 'chemical genetics', 'computer science', 'computerized tools', 'design', 'drug candidate', 'follow-up', 'high throughput analysis', 'high throughput screening', 'human disease', 'image processing', 'imaging Segmentation', 'improved', 'interest', 'lipid metabolism', 'medical schools', 'novel', 'open source', 'pathogen', 'public health relevance', 'research study', 'response', 'small molecule libraries', 'two-dimensional']",NIGMS,"BROAD INSTITUTE, INC.",R01,2012,311786,-0.00680505630017394
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8269876,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2012,323305,-0.006605887067670223
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy  PROJECT SUMMARY/ABSTRACT Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy.  PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,8271284,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2012,297329,-0.026066224554642553
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8259135,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'public health relevance', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,445075,-0.005432616707504914
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics     DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress.              Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8342777,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Simulate', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'interest', 'mild neurocognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2012,405500,-0.02210762726747168
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8216289,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2012,255679,-0.010187887519868209
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8242742,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2012,392767,0.003735429617885316
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,8281471,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Health', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'NMR Spectroscopy', 'Negative Staining', 'Neighborhoods', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2012,272755,-0.023407323297262108
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8330927,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,1821611,-0.019607911461657215
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541935,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,100000,-0.019607911461657215
"CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING    DESCRIPTION (provided by applicant): Case Studies in Bayesian Statistics and Machine Learning I continues in the tradition of the Case Studies in Bayesian Statistics series. The original series of workshops were held in odd years at Carnegie Mellon University in the early fall. The first edition of the new workshop will be held at Carnegie Mellon University on October 14-15, 2011. The highest level goal of the workshop series is to generate and present successful solutions to difficult substantive problems in a wide variety of areas. The specific objectives of the workshop are to 1. Present and discuss solutions to challenging scientific problems that illustrate the potential for statistical machine learning approaches in substantive research; 2. Present an opportunity for statisticians and computer scientists to present applications-oriented research  that changes the way that data are analyzed in scientific fields; 3. Stimulate discussion of the challenges of the analysis of high-dimensional and complex datasets in a scientifically useful manner; 4. Encourage young researchers, including graduate students, to present their applied work; 5. Provide a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; 6. Expose young researchers to important challenges and opportunities in collaborative research; 7. Include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment; 8. Encourage dissemination of the findings presented at the workshop via well-documented and peer- reviewed journal articles.      PUBLIC HEALTH RELEVANCE: Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.           Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.         ",CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING,8203089,R13CA144626,"['Area', 'Bioinformatics', 'Case Study', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Data Analyses', 'Data Set', 'Disabled Persons', 'Discipline', 'Disease', 'Educational workshop', 'Environment', 'Fostering', 'Goals', 'Hand', 'Health Sciences', 'Image', 'Institutes', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Human Genome Research Institute', 'Participant', 'Peer Review', 'Research', 'Research Personnel', 'Scientist', 'Series', 'Solutions', 'Underrepresented Minority', 'Universities', 'Woman', 'Work', 'anticancer research', 'computer science', 'data modeling', 'falls', 'graduate student', 'interest', 'journal article', 'meetings', 'peer', 'planetary Atmosphere', 'statistics', 'symposium']",NCI,CARNEGIE-MELLON UNIVERSITY,R13,2011,7500,-0.015774417295762653
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.           Project Narrative The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.",Scalable Learning with Ensemble Techniques and Parallel Computing,8045486,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Biological Sciences', 'Biomedical Research', 'Classification', 'Communication', 'Communities', 'Community Financing', 'Companions', 'Complex', 'Computer software', 'Consult', 'Crowding', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Health', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'new technology', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2011,374673,0.02903732401147765
"Image analysis for high-throughput C. elegans infection and metabolism assays    DESCRIPTION (provided by applicant): High-throughput screening (HTS) is a technique for searching large libraries of chemical or genetic perturbants, to find new treatments for a disease or to better understand disease pathways. As automated image analysis for cultured cells has improved, microscopy has emerged as one of the most powerful and informative ways to analyze screening samples. However, many diseases and biological pathways can be better studied in whole animals-particularly diseases that involve organ systems and multicellular interactions, such as metabolism and infection. The worm Caenorhabditis elegans is a well-established and effective model organism, used by thousands of researchers worldwide to study complex biological processes. Samples of C. elegans can be robotically prepared and imaged by high-throughput microscopy, but existing image-analysis methods are insuf- ficient for most assays. In this project, image-analysis algorithms that are capable of scoring high-throughput assays of C. elegans will be developed.  The algorithms will be tested and refined in three high-throughput screens, which will uncover chemical and genetic regulators of fat metabolism and infection: (1) A C. elegans viability assay to identify modulators of infection. The proposed algorithms use a probabilistic shape model of C. elegans in order to identify and mea- sure individual worms even when the animals touch or cross. These methods are the basis for quantifying many other phenotypes, including body morphology and subtle variations in reporter signal levels. (2) A C. elegans lipid assay to identify genes that regulate fat metabolism. The algorithms proposed for illumination correction, level-set-based foreground segmentation, well-edge detection, and artifact removal will result in improved or- business in high-throughput experiments. (3) A fluorescence gene expression assay to identify regulators of the response of the C. elegans host to Staphylococcus aureus infection. The proposed techniques for constructing anatomical maps of C. elegans will make it possible to quantify a variety of changes in fluorescent localization patterns in a biologically relevant way.  In addition to discovering new metabolism- and infection-related drugs and genetic regulators through these specific screens, this work will provide the C. elegans community with (a) a new framework for extracting mor- phological features from C. elegans for quantitative analysis of this organism, and (b) a versatile, modular, open-source toolbox of algorithms enabling the discovery of genetic pathways, chemical probes, and drug can- didates in whole organism high-throughput screens relevant to a variety of diseases.  This work is a close collaboration with C. elegans experts Fred Ausubel and Gary Ruvkun at Massachusetts General Hospital/Harvard Medical School, with Polina Golland and Tammy Riklin-Raviv, experts in model-based segmentation and statistical image analysis at MIT's Computer Science and Artificial Intelligence Laboratory, and with Anne Carpenter, developer of open-source image analysis software at the Broad Institute.      PUBLIC HEALTH RELEVANCE: Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.           Large-scale screening experiments that test the effects of thousands of chemicals or genetic perturbants by microscopy and image analysis can discover new treatments and help biomedical scientists understand dis- ease mechanisms. Microscopy screens of cultured cells are routine, but researchers wish to study complex processes like metabolism and infection in a whole animal like the tiny worm Caenorhabditis elegans, for which existing image analysis methods are insufficient. The goal of this research is to develop open-source software to automatically identify and measure C. elegans in microscopy images, thereby making it possible for researchers worldwide to screen a wide variety of complex biological processes related to human disease.         ",Image analysis for high-throughput C. elegans infection and metabolism assays,8022635,R01GM095672,"['Address', 'Algorithms', 'Animal Model', 'Animals', 'Anti-Infective Agents', 'Artificial Intelligence', 'Atlases', 'Bacteria', 'Biological', 'Biological Assay', 'Biological Process', 'Businesses', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Cultured Cells', 'Data Quality', 'Descriptor', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drug Delivery Systems', 'Excision', 'Fluorescence', 'Gene Expression', 'General Hospitals', 'Genes', 'Genetic', 'Goals', 'Human', 'Image', 'Image Analysis', 'Immune response', 'Individual', 'Infection', 'Institutes', 'Laboratories', 'Learning', 'Life', 'Lighting', 'Lipids', 'Machine Learning', 'Maps', 'Massachusetts', 'Measurement', 'Measures', 'Metabolism', 'Methods', 'Microscopy', 'Microsporidia', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Organism', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Preparation', 'Process', 'Reporter', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Screening procedure', 'Shapes', 'Signal Transduction', 'Software Engineering', 'Staining method', 'Stains', 'Staphylococcus aureus', 'Techniques', 'Testing', 'Touch sensation', 'Variant', 'Whole Organism', 'Work', 'base', 'biomedical scientist', 'body system', 'chemical genetics', 'computer science', 'computerized tools', 'design', 'drug candidate', 'follow-up', 'high throughput analysis', 'high throughput screening', 'human disease', 'image processing', 'imaging Segmentation', 'improved', 'interest', 'lipid metabolism', 'medical schools', 'novel', 'open source', 'pathogen', 'research study', 'response', 'small molecule libraries', 'two-dimensional']",NIGMS,"BROAD INSTITUTE, INC.",R01,2011,304318,-0.006891150329156116
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8062031,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Health', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2011,359403,0.0009055106214009123
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.      PUBLIC HEALTH RELEVANCE: The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.             The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8108523,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2011,341852,-0.0027730228215270694
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,8133946,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computational algorithm', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2011,297497,0.005300232599760823
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy    DESCRIPTION (provided by applicant):    Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy. PUBLIC HEALTH RELEVANCE: The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.           PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,8077861,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Health', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2011,297451,-0.02958102518096397
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8163481,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,459174,-0.005432616707504914
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,8055907,R01LM009731,"['Address', 'Algorithms', 'Area', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'International', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Population', 'Prevention', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'disorder prevention', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2011,325956,-0.0023144844195711416
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,8138486,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2011,266112,-0.006206725121507793
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,8139155,R44RR024094,"['AIDS/HIV problem', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cell physiology', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2011,449663,-0.02344932272009178
"ISMB 2011 Conference Support for Students & Young Scientists    DESCRIPTION (provided by applicant): ISMB 2011 Conference Travel Support for Students and Young Scientists.  The Intelligent Systems for Molecular Biology (ISMB) conference in 2011 will be held in Vienna, Austria, as a conference of approximately 1,500-1,700 attendees, including 33-38% students/post doctoral researchers. ISMB brings together graduate students, post doctoral researchers, faculty, research staff and senior scientists of many different nationalities, all of whom are studying or working in computer science, molecular biology, mathematics and/or statistics. The conference brings biologists and computational scientists together to focus on research centered on actual biological problems rather than simply theoretical calculations. The combined focus on ""intelligent systems"" and actual biological data makes ISMB a highly relevant meeting, and many years of producing the event has resulted in a well organized, well attended, and respected annual conference. The ISMB conference presents the latest research methods and results developed through the application of computer programming to the study of biological sciences, including advances in sequencing genomes that may lead to a better understanding of how, for instance, cells interact for the treatment of diseases such as cancer. Additionally, presentations may describe methods and advances associated with the analysis of existing biological literature, including benchmarking experiments, to create a better public understanding of scientific research reports. Overall, ISMB serves to educate attendees on the latest developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to apply what they have learned as they advance their own research efforts. The scientific program for each ISMB meeting comprises parallel presentation tracks of original research papers, highlights of recently published research, topically focused special sessions on emerging topics, technology demos, tutorial workshops, special interest group meetings and a student symposium organized by and for students. As an example, for ISMB 2010, 234 original research papers were submitted and 48 selected for the Proceedings Track; 126 published papers were submitted and 42 selected for the Highlights Track; nine proposals were submitted and four selected for presentation along with two invited for the Special Sessions Track. In all, well over 200 talks were presented during the course of the 2010 conference, and similar numbers are anticipated for 2011. In all cases, submissions are rigorously reviewed, typically by three members of each track's committee before approval by the track chair, insuring the highest possible quality of work is presented. The specific areas represented in the conference vary each year depending on the areas that researchers find most interesting and innovative, and therefore submit as papers and proposals. This proposal seeks funding to assist students and junior researchers in attending the conference, thus exposing them to the latest research of their own areas as well as areas that may be new to them.      PUBLIC HEALTH RELEVANCE: Relevance Bioinformatics is well established as an essential tool for understanding biological systems. The widespread recognition of bioinformatics has been largely driven by genomic sequence efforts, because laboratory scientists recognize that the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. Biologists are routinely integrating computational tools into their research programs and creating large predictive models based on information found in databases and other electronic resources. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field, as well as exposing what's on the horizon of future discoveries.           Relevance Bioinformatics is well established as an essential tool for understanding biological systems. The widespread recognition of bioinformatics has been largely driven by genomic sequence efforts, because laboratory scientists recognize that the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. Biologists are routinely integrating computational tools into their research programs and creating large predictive models based on information found in databases and other electronic resources. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field, as well as exposing what's on the horizon of future discoveries.         ",ISMB 2011 Conference Support for Students & Young Scientists,8121309,R13GM097938,"['Address', 'Algorithms', 'Area', 'Austria', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Cells', 'Computational Biology', 'Computational Technique', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Educational workshop', 'Electronics', 'Elements', 'Event', 'Evolution', 'Expert Systems', 'Faculty', 'Feedback', 'Financial Support', 'Funding', 'Future', 'Genomics', 'Graph', 'Group Meetings', 'Human', 'Industry', 'International', 'Knowledge', 'Laboratory Scientists', 'Lead', 'Learning', 'Limited Stage', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Metabolic Pathway', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Structure', 'Nationalities', 'Oral', 'Paper', 'Participant', 'Pattern Recognition', 'Peer Review', 'Phylogenetic Analysis', 'Postdoctoral Fellow', 'Published Comment', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Robotics', 'Role', 'Science', 'Scientist', 'Senior Scientist', 'Sequence Analysis', 'Series', 'Speed', 'Students', 'System', 'Technology', 'Time', 'Training', 'Travel', 'Validation', 'Vendor', 'Work', 'base', 'biological systems', 'career', 'computer program', 'computer science', 'computerized tools', 'cost', 'disorder prevention', 'exhibitions', 'experience', 'genome sequencing', 'graduate student', 'improved', 'information organization', 'innovation', 'interest', 'lectures', 'meetings', 'member', 'multidisciplinary', 'next generation', 'novel', 'parallel computing', 'posters', 'practical application', 'predictive modeling', 'programs', 'research study', 'role model', 'satisfaction', 'skills', 'special interest group', 'statistics', 'symposium', 'tool']",NIGMS,INTERNATIONAL SOCIETY/COMP BIOLOGY,R13,2011,20000,-0.00432574940759917
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,8079474,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,250488,-0.04780354542701874
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,0.006843786524569536
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8039246,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2011,526649,0.003735429617885316
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,8098196,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Health', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'NMR Spectroscopy', 'Negative Staining', 'Neighborhoods', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2011,311172,-0.023407323297262108
"COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems.  To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati,8173654,76201000029C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Health Care Costs', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data mining', 'flexibility', 'innovation', 'novel', 'novel strategies', 'rapid growth']",NLM,NORTHEASTERN UNIVERSITY,N03,2010,377968,-0.012617656895183994
"COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que,8173663,76201000032C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Communities', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'rapid growth', 'social']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,N03,2010,301251,-0.01461370214170162
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,8049892,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2010,5742,0.004623630786466938
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173956,76201000033C,"['Biology', 'Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computer software', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Molecular Biology', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data modeling', 'flexibility', 'innovation', 'novel strategies', 'open source', 'prototype', 'rapid growth']",NLM,UNIVERSITY OF COLORADO DENVER,N03,2010,377982,-0.010843116004722594
"Machine learning analysis of tandem mass spectra    DESCRIPTION (provided by applicant): Project summary: Mass spectrometry, the core technology in the field of proteomics, promises to enable scientists to identify and quantify the entire complement of molecules that comprise a complex biological sample. In the biological and health sciences, mass spectrometry is commonly used in a nigh-throughput fashion to identify proteins in a mixture. Currently, the primary bottleneck in this type of experiment is computational. Existing algorithms for interpreting mass spectra are slow and fail to identify a large proportion of the given spectra. We propose to apply techniques and tools from the field of machine learning to the analysis of mass spectrometry data. We will build computational models of peptide fragmentation within the mass spectrometer, as well as larger-scale models of the entire mass spectrometry process. Using these models, we will design and validate algorithms for identifying the set of proteins that best explain an observed set of spectra. Software implementations for all of the methods will be made publicly available in a user-friendly form. In practical terms, this software will enable scientists to more easily, efficiently and accurately analyze and understand their mass spectrometry data. Relevance: The applications of mass spectrometry and its promises for improvements of human health are numerous, including an increased understanding of disease phenotypes and the molecular mechanisms that underlie them, and vastly more sensitive and specific diagnostic and prognostic screens.           n/a",Machine learning analysis of tandem mass spectra,7797540,R01EB007057,"['Abbreviations', 'Algorithms', 'Area', 'Authorship', 'Biochemical', 'Biological', 'Blast Cell', 'Calibration', 'Carbonyl Cyanide m-Chlorophenyl Hydrazone', 'Collection', 'Complement', 'Complex', 'Complex Mixtures', 'Computer Simulation', 'Computer software', 'Data', 'Databases', 'Devices', 'Diagnostic', 'Dissociation', 'FOLH1 gene', 'Genomics', 'Hand', 'Health', 'Health Sciences', 'Hour', 'Human', 'Knowledge', 'Learning', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Molecular', 'Peptide Fragments', 'Peptides', 'Performance', 'Post-Translational Protein Processing', 'Preparation', 'Principal Investigator', 'Procedures', 'Process', 'Protein Biochemistry', 'Proteins', 'Proteomics', 'Receiver Operating Characteristics', 'Research Personnel', 'Rest', 'Running', 'Sampling', 'Scientist', 'Set protein', 'Silicon Dioxide', 'Source Code', 'Staging', 'Statistical Models', 'Techniques', 'Technology', 'Time', 'Training', 'Work', 'computer based statistical methods', 'computer cluster', 'design', 'disease phenotype', 'expectation', 'improved', 'interest', 'markov model', 'mass spectrometer', 'model design', 'prognostic', 'programs', 'research study', 'small molecule', 'tandem mass spectrometry', 'task analysis', 'tool', 'user-friendly']",NIBIB,UNIVERSITY OF WASHINGTON,R01,2010,593600,0.0025235159133013213
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,8013208,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2010,376899,0.030176493112182994
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7881671,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,264640,0.00024677003350943947
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8068069,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,51400,0.0009055106214009123
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7828142,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,338802,0.0009055106214009123
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),8110238,U54CA121852,"['Address', 'Algorithms', 'Area', 'Binding', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Databases', 'Development', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Genes', 'Genetic Engineering', 'Genomics', 'Internet', 'Investigation', 'Literature', 'Machine Learning', 'Maps', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Research Personnel', 'Resources', 'Signal Pathway', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'base', 'biomedical ontology', 'computer framework', 'data mining', 'design', 'genome-wide', 'graphical user interface', 'knowledge base', 'multidisciplinary', 'natural language', 'novel', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2010,29814,0.007988257779740219
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy    DESCRIPTION (provided by applicant):    Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy. PUBLIC HEALTH RELEVANCE: The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.           PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,7845650,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'public health relevance', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2010,306826,-0.02958102518096397
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7912919,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,304151,0.005300232599760823
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7805478,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2010,339537,-0.0023144844195711416
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173647,76201000027C,[' '],NLM,STANFORD UNIVERSITY,N03,2010,378000,-0.010843116004722594
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7754089,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2010,2037327,0.0032732020723107457
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,8115481,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'operation', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,99989,0.011749439133876195
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7825424,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2010,185505,-0.007648835266113752
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7798186,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'allograft rejection', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2010,289814,-0.0016334755911815702
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7929664,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2010,277200,-0.006206725121507793
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7858165,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,253269,-0.04780354542701874
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,7999420,R44RR024094,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'HIV', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2010,449663,-0.02344932272009178
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,8112998,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein complex', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,R01,2010,252819,-0.025310464235478555
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,0.006843786524569536
"Enhancing 3dsvm to improve its interoperability and dissemination    DESCRIPTION (provided by applicant): This research plan outlines crucial software enhancements to a program called 3dsvm, which is a command line program and graphical user interface (gui) plugin for AFNI (Cox, 1996). 3dsvm performs support vector machine (SVM) analysis on fMRI data, which constitutes one important approach to performing multivariate supervised learning of neuroimaging data. 3dsvm originally provided the ability to analyze fMRI data as described in (LaConte et al., 2005). Since its first distribution as a part of AFNI, it has been steadily extended to provide new functionality including regression and non-linear kernels, as well as multiclass classification capabilities. In addition to its integration into AFNI, features that make 3dsvm particularly well suited for fMRI analysis are that it is easy to spatially mask voxels (to include/exclude them in the SVM analysis) as well as to flexibly select subsets of a dataset to use as training or testing samples. It has been used to generate results for our own work and for collaborative efforts and has been cited as a resource by others (Mur et al. 2009; Hanke et al. 2009). Despite many positive aspects of 3dsvm, the priorities of PAR-07-417 address a genuine need that this software project has - the ability to focus on improvements that will increase its dissemination and interoperability. A major motivation for PAR-07-417 is to facilitate the improved interface, characterization, and documentation to enhance the extent of sharing and to provide the groundwork for future extensions. Our aims are well aligned with this program announcement. Further, there is a growing need in the neuroimaging community for tools such as 3dsvm. Since 3dsvm is not a new project, is tightly integrated into the software environment of AFNI, and can be further integrated to enable better functionality to support needs as diverse as NIfTI format capabilities to rtFMRI, this proposed project will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.      PUBLIC HEALTH RELEVANCE: This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.           NARRATIVE This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.",Enhancing 3dsvm to improve its interoperability and dissemination,8278135,R03EB012464,[' '],NIBIB,VIRGINIA POLYTECHNIC INST AND ST UNIV,R03,2010,156500,-0.017893273994071635
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,8076789,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2010,956625,-0.01988432089519382
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7774343,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2010,525262,0.003735429617885316
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7945368,R01HG004836,"['Anatomy', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Communities', 'Competence', 'Complex', 'Computer software', 'Consultations', 'Controlled Vocabulary', 'Dana-Farber Cancer Institute', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Fostering', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'In Vitro', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Molecular', 'National Cancer Institute', 'Nature', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'Validation', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'design', 'empowered', 'gene function', 'graphical user interface', 'information organization', 'instrument', 'interoperability', 'novel', 'open source', 'repository', 'research study', 'response', 'scale up', 'sound', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2010,428079,-0.035060693897963256
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7851333,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Belief', 'Bioinformatics', 'Biological Assay', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Phenotype', 'Physiological', 'Protocols documentation', 'Public Health', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly']",NHLBI,WASHINGTON UNIVERSITY,R01,2010,474912,-0.010053441272605984
"National Center: Multiscale Analysis of Genomic and Cellular Networks (MAGNet) A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.   n/a",National Center: Multiscale Analysis of Genomic and Cellular Networks (MAGNet),8012947,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2010,3757192,0.006835972034610457
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,-0.012721510272034192
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,7901378,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'Muscle Rigidity', 'NMR Spectroscopy', 'Negative Staining', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'public health relevance', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2010,273363,-0.023407323297262108
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7670456,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,74750,0.004623630786466938
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7560409,R01NS051826,"['Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Area', 'Back', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Disease', 'Disease Progression', 'Evaluation', 'Fetal Growth Retardation', 'Goals', 'Gold', 'Human', 'Image', 'Imagery', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Modeling', 'Neonatal', 'Normal Range', 'Operative Surgical Procedures', 'Patients', 'Population', 'Population Study', 'Process', 'Research', 'Schizophrenia', 'Shapes', 'Statistical Distributions', 'Structure', 'System', 'Techniques', 'Testing', 'Training', 'Variant', 'base', 'computer studies', 'disease classification', 'feeding', 'improved', 'neonate', 'nervous system disorder', 'normal aging', 'novel', 'shape analysis', 'tool']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,280500,-0.013773160977459259
"Machine learning analysis of tandem mass spectra    DESCRIPTION (provided by applicant): Project summary: Mass spectrometry, the core technology in the field of proteomics, promises to enable scientists to identify and quantify the entire complement of molecules that comprise a complex biological sample. In the biological and health sciences, mass spectrometry is commonly used in a nigh-throughput fashion to identify proteins in a mixture. Currently, the primary bottleneck in this type of experiment is computational. Existing algorithms for interpreting mass spectra are slow and fail to identify a large proportion of the given spectra. We propose to apply techniques and tools from the field of machine learning to the analysis of mass spectrometry data. We will build computational models of peptide fragmentation within the mass spectrometer, as well as larger-scale models of the entire mass spectrometry process. Using these models, we will design and validate algorithms for identifying the set of proteins that best explain an observed set of spectra. Software implementations for all of the methods will be made publicly available in a user-friendly form. In practical terms, this software will enable scientists to more easily, efficiently and accurately analyze and understand their mass spectrometry data. Relevance: The applications of mass spectrometry and its promises for improvements of human health are numerous, including an increased understanding of disease phenotypes and the molecular mechanisms that underlie them, and vastly more sensitive and specific diagnostic and prognostic screens.           n/a",Machine learning analysis of tandem mass spectra,7581004,R01EB007057,"['Abbreviations', 'Algorithms', 'Area', 'Authorship', 'Biochemical', 'Biological', 'Blast Cell', 'Calibration', 'Carbonyl Cyanide m-Chlorophenyl Hydrazone', 'Collection', 'Complement', 'Complex', 'Complex Mixtures', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Databases', 'Devices', 'Diagnostic', 'Dissociation', 'FOLH1 gene', 'Genomics', 'Hand', 'Health', 'Health Sciences', 'Hour', 'Human', 'Knowledge', 'Learning', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Molecular', 'Peptide Fragments', 'Peptides', 'Performance', 'Post-Translational Protein Processing', 'Preparation', 'Principal Investigator', 'Procedures', 'Process', 'Protein Biochemistry', 'Proteins', 'Proteomics', 'Receiver Operating Characteristics', 'Research Personnel', 'Rest', 'Running', 'Sampling', 'Scientist', 'Set protein', 'Silicon Dioxide', 'Source Code', 'Staging', 'Statistical Models', 'Techniques', 'Technology', 'Time', 'Training', 'Work', 'computer based statistical methods', 'design', 'disease phenotype', 'expectation', 'improved', 'interest', 'markov model', 'mass spectrometer', 'model design', 'prognostic', 'programs', 'research study', 'small molecule', 'tandem mass spectrometry', 'task analysis', 'tool', 'user-friendly']",NIBIB,UNIVERSITY OF WASHINGTON,R01,2009,607717,0.0025235159133013213
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7664538,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'systematic review', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,318898,-0.01323945143361349
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant): This proposal is submitted in response to NOT-OD-09-058 NIH Announces the Availability of Recovery Act Funds for Competitive Revision Applications. Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Our currently funded R03 grant, ""Recursive partitioning and ensemble methods for classifying an ordinal response,"" consists of the following three specific aims (SA.1) extend the recursive partitioning and random forest classification methodologies for predicting an ordinal response by developing computational tools for the R programming environment including implementing our ordinal impurity criteria in rpart and implementing the ordinal impurity criteria in randomForest; (SA.2) evaluate the proposed ordinal classification methods in comparison to existing nominal and continuous response methods using simulated, benchmark, and gene expression datasets; and (SA.3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been described. Herein we propose to extend the L1 penalized method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing a model-based ordinal classification methodology applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies considered in the parent grant. The specific aims of this competitive revision application are to: Aim 1) Extend the L1 penalized methodology to enable predicting an ordinal response by developing computational tools for the R programming environment; Aim 2) Using simulated, benchmark, and gene expression datasets, evaluate L1 penalized ordinal response models by comparing error rates from our L1 fitting algorithm to those obtained when using a forward variable selection modeling strategy and our ordinal random forest approach; and Aim 3) Evaluate methods for assessing important covariates from L1 penalized ordinal response models.           This project will develop L1 penalized ordinal response models and implement them in the R programming environment. By conducting extensive comparisons of various ordinal response modeling methods using simulated, benchmark, and gene expression datasets, we will be able to make a recommendation regarding ordinal response modeling to the scientific community. This research is significant since the ordinal response modeling methods developed during the project period will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.",Recursive partitioning and ensemble methods for classifying an ordinal response,7805045,R03LM009347,"['Advocate', 'Algorithms', 'Applications Grants', 'Area', 'Behavioral Research', 'Benchmarking', 'Bioconductor', 'Biopsy Specimen', 'Cancer Patient', 'Chronic Hepatitis', 'Classification', 'Clinical', 'Communities', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Drug toxicity', 'Economics', 'Education', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Gene Expression', 'Genomics', 'Grant', 'Health', 'Health Status', 'Health Surveys', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Literature', 'Location', 'Logistics', 'Machine Learning', 'Mathematics', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Neoplasm Metastasis', 'Occupations', 'Outcome', 'Patients', 'Performance', 'Positioning Attribute', 'Progressive Disease', 'Recommendation', 'Recovery', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Sample Size', 'Sampling', 'Science', 'Scoring Method', 'Simulate', 'Solid Neoplasm', 'Stable Disease', 'Staging', 'Technology', 'Translational Research', 'Travel', 'Trees', 'United States National Institutes of Health', 'base', 'computerized tools', 'cost', 'forest', 'heuristics', 'improved', 'indexing', 'interest', 'liver biopsy', 'meetings', 'neglect', 'novel', 'parent grant', 'partial response', 'preference', 'programs', 'research study', 'response', 'simulation', 'social', 'software development', 'symposium', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,75000,-0.01204864720582654
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7651387,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,267801,0.00024677003350943947
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7577491,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2009,342223,0.0009055106214009123
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7914681,U54CA121852,"['Address', 'Algorithms', 'Area', 'Binding', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Databases', 'Development', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Genes', 'Genetic Engineering', 'Genomics', 'Internet', 'Investigation', 'Literature', 'Machine Learning', 'Maps', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Research Personnel', 'Resources', 'Signal Pathway', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'base', 'biomedical ontology', 'computer framework', 'data mining', 'design', 'genome-wide', 'graphical user interface', 'knowledge base', 'multidisciplinary', 'natural language', 'novel', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2009,116802,0.007988257779740219
"High-throughput annotation of glycan mass spectra     DESCRIPTION (provided by applicant): The correct functioning of many proteins depends on glycosylation, the addition of sugar molecules (glycans) to selected amino acids in the protein. For example, cancer cells have different glycosylation patterns than ordinary cells, and there is strong evidence that glycoproteins on the surface of egg cells play an essential role in sperm binding. Despite the importance of glycosylation, there are as yet no reliable, high-throughput methods for determining the identity and location of glycans. Glycan identification is currently a manual procedure for experts, involving a combination of chemical assays and mass spectrometry. The automation of the process would have a significant impact on our understanding of this important biological process. The proposed project aims to invent chemical procedures, algorithms, and software for high-throughput analysis of glycan mass spectrometry data. The goal is to bring glycan analysis up to the level of peptide analysis within 3 years. In contrast to peptide analysis, which can leverage genomics data, glycan analysis requires the incorporation of expert knowledge of synthetic pathways, in order to limit the huge number of theoretical combinations of monosaccharides to the much smaller number that are actually synthesized in nature. The project will have to develop novel representations for the evolving expert knowledge, because an exhaustive list- analogous to the human genome- is not currently known. Along with expert knowledge, the project will develop and validate machine learning and statistical techniques for glycan identification. In particular, the project will develop methods for internally calibrating spectra, and will learn fragmentation patterns that can statistically distinguish different types of glycosidic linkages.         n/a",High-throughput annotation of glycan mass spectra,7628516,R01GM074128,"['Address', 'Age', 'Algorithms', 'Amino Acids', 'Area', 'Automation', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Carbon', 'Cartoons', 'Cells', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Development', 'Disclosure', 'Disease', 'Expert Systems', 'Genomics', 'Glycoproteins', 'Goals', 'Graft Rejection', 'Human Genome', 'Isomerism', 'Knowledge', 'Learning', 'Libraries', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Manuals', 'Mass Spectrum Analysis', 'Methods', 'Modification', 'Monosaccharides', 'Nature', 'Occupations', 'Organism', 'Pathway interactions', 'Pattern', 'Peptides', 'Play', 'Polymers', 'Polysaccharides', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Signal Transduction', 'Site', 'Specialist', 'Specific qualifier value', 'Spectrum Analysis', 'Speed', 'Surface', 'System', 'Techniques', 'Technology', 'Training', 'Work', 'cancer cell', 'egg', 'enzyme activity', 'experience', 'glycosylation', 'glycosyltransferase', 'high throughput analysis', 'immune function', 'improved', 'novel', 'programs', 'prototype', 'sperm cell', 'sugar', 'tool']",NIGMS,PALO ALTO RESEARCH CENTER,R01,2009,329306,-0.013763925022365654
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy    DESCRIPTION (provided by applicant):    Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy. PUBLIC HEALTH RELEVANCE: The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.           PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,7736445,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'public health relevance', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2009,317367,-0.02958102518096397
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7670408,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,311541,0.005300232599760823
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7848604,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170861,-0.0023144844195711416
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7901729,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170789,-0.0023144844195711416
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7612766,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,342967,-0.0023144844195711416
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7582301,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2009,2057843,0.0032732020723107457
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7633119,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2009,18437,-0.008365685154324993
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7652508,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,363929,0.011749439133876195
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7620994,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2009,263148,-0.007648835266113752
"Novel Analytic Techniques to Assess Physical Activity Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship. n/a",Novel Analytic Techniques to Assess Physical Activity,7809191,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2009,140804,-0.008227135144644546
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,-0.012721510272034192
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,-0.012721510272034192
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7663288,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Computer Systems Development', 'Computer software', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Source', 'Structure', 'System', 'Systems Integration', 'Technology', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'meetings', 'models and simulation', 'open source', 'outreach', 'protein complex', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2009,437938,-0.0015325504032320998
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7599555,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2009,290671,-0.0016334755911815702
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7790821,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Support', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'flexibility', 'innovation', 'interest', 'meetings', 'prevent', 'product development', 'public health relevance', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2009,4047,-0.009766302958485275
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7693803,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2009,280000,-0.006206725121507793
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of  computer scientists, software engineers, and medical investigators who develop computational tools for the  analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure  and environment for the development of computational algorithms and open source technologies, and then  oversee the training and dissemination of these tools to the medical research community. This world-class  software and development environment serves as a foundation for accelerating the development and  deployment of computational tools that are readily accessible to the medical research community. The team  combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state  of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed,  open-source environment) to enable computational examination of both basic neurosience and neurologicat  disorders. In developing this infrastructure resource, the team will significantly expand upon proven open  systems technology and platforms. The driving biological projects will come initially from the study of  schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open  systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures  and connectivity patterns in the brain, derangements of which have long been thought to play a role in the  etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range  of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially  including microscopic, genomic, and other image data. It will apply to image data from individual  )atients,and to studies executed across large poplulations. The data will be taken from subjects across a  Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7915998,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Genomics', 'Goals', 'Healthcare', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2009,3720664,0.004464990488119505
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7666186,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,256073,-0.04780354542701874
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7570061,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein complex', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,R01,2009,280910,-0.025310464235478555
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,0.006843786524569536
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7660538,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2009,688362,-0.01988432089519382
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7558468,R01HG004836,"['Anatomy', 'Architecture', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Competence', 'Complex', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Side', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'computerized data processing', 'design', 'empowered', 'graphical user interface', 'insight', 'instrument', 'open source', 'programs', 'repository', 'research study', 'response', 'scale up', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2009,428078,-0.035060693897963256
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7565504,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Conflict (Psychology)', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2009,529858,0.003735429617885316
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7676864,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2009,3464579,0.007988257779740219
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7558424,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Area', 'Belief', 'Bioinformatics', 'Biological Assay', 'Budgets', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Research', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Electrocardiogram', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Grant', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Length', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Peer Review', 'Phenotype', 'Physiological', 'Preparation', 'Protocols documentation', 'Public Health', 'Published Comment', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'base', 'bench to bedside', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'graphical user interface', 'interest', 'meetings', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly', 'working group']",NHLBI,WASHINGTON UNIVERSITY,R01,2009,488000,-0.010053441272605984
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,7787325,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'Muscle Rigidity', 'NMR Spectroscopy', 'Negative Staining', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'public health relevance', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2009,293039,-0.023407323297262108
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7470967,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Class', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Numbers', 'Outcome', 'Performance', 'Polymerase Chain Reaction', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Standards of Weights and Measures', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2008,74521,0.004623630786466938
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7498449,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2008,153203,-0.012287586167500357
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7351765,R01NS051826,"['Affect', 'Age', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Anatomy', 'Area', 'Back', 'Class', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diffuse Pattern', 'Disease', 'Disease Progression', 'Evaluation', 'Fetal Growth Retardation', 'Goals', 'Gold', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Intuition', 'Knowledge', 'Learning', 'Localized', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Research', 'Methods', 'Modeling', 'Morphology', 'Neonatal', 'Neuroanatomy', 'Normal Range', 'Operative Surgical Procedures', 'Pathology', 'Patients', 'Population', 'Population Study', 'Process', 'Range', 'Research', 'Schizophrenia', 'Shapes', 'Standards of Weights and Measures', 'Statistical Distributions', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Variant', 'base', 'computer studies', 'desire', 'disease classification', 'feeding', 'imaging Segmentation', 'improved', 'neonate', 'nervous system disorder', 'neurosurgery', 'normal aging', 'novel', 'shape analysis', 'tool']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2008,281588,-0.013773160977459259
"Machine learning analysis of tandem mass spectra    DESCRIPTION (provided by applicant): Project summary: Mass spectrometry, the core technology in the field of proteomics, promises to enable scientists to identify and quantify the entire complement of molecules that comprise a complex biological sample. In the biological and health sciences, mass spectrometry is commonly used in a nigh-throughput fashion to identify proteins in a mixture. Currently, the primary bottleneck in this type of experiment is computational. Existing algorithms for interpreting mass spectra are slow and fail to identify a large proportion of the given spectra. We propose to apply techniques and tools from the field of machine learning to the analysis of mass spectrometry data. We will build computational models of peptide fragmentation within the mass spectrometer, as well as larger-scale models of the entire mass spectrometry process. Using these models, we will design and validate algorithms for identifying the set of proteins that best explain an observed set of spectra. Software implementations for all of the methods will be made publicly available in a user-friendly form. In practical terms, this software will enable scientists to more easily, efficiently and accurately analyze and understand their mass spectrometry data. Relevance: The applications of mass spectrometry and its promises for improvements of human health are numerous, including an increased understanding of disease phenotypes and the molecular mechanisms that underlie them, and vastly more sensitive and specific diagnostic and prognostic screens.           n/a",Machine learning analysis of tandem mass spectra,7365198,R01EB007057,"['Abbreviations', 'Algorithms', 'Altretamine', 'Area', 'Authorship', 'Biochemical', 'Biological', 'Blast Cell', 'Calibration', 'Carbonyl Cyanide m-Chlorophenyl Hydrazone', 'Collection', 'Complement', 'Complex', 'Complex Mixtures', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Databases', 'Devices', 'Diagnostic', 'Dissociation', 'FOLH1 gene', 'Genomics', 'Hand', 'Health', 'Health Sciences', 'Hour', 'Human', 'Knowledge', 'Learning', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Molecular', 'Peptide Fragments', 'Peptides', 'Performance', 'Post-Translational Protein Processing', 'Preparation', 'Principal Investigator', 'Procedures', 'Process', 'Protein Biochemistry', 'Proteins', 'Proteomics', 'Rate', 'Receiver Operating Characteristics', 'Research Personnel', 'Rest', 'Running', 'Sampling', 'Scientist', 'Score', 'Set protein', 'Silicon Dioxide', 'Source Code', 'Staging', 'Statistical Models', 'Techniques', 'Technology', 'Time', 'Today', 'Training', 'Work', 'computer based statistical methods', 'day', 'design', 'disease phenotype', 'expectation', 'improved', 'interest', 'markov model', 'mass spectrometer', 'model design', 'prognostic', 'programs', 'research study', 'small molecule', 'tandem mass spectrometry', 'task analysis', 'tool', 'user-friendly']",NIBIB,UNIVERSITY OF WASHINGTON,R01,2008,602497,0.0025235159133013213
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7363692,K22LM008389,"['Adverse event', 'Area', 'Arts', 'Attention', 'Biological Neural Networks', 'Caregivers', 'Chicago', 'Childhood', 'Classification', 'Clinical', 'Computer software', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Element', 'Data Set', 'Databases', 'Decision Trees', 'Detection', 'Disease', 'Early Intervention', 'Ensure', 'Etiology', 'Event', 'Excision', 'Foundations', 'Genomics', 'Heart Arrest', 'Hour', 'Laboratories', 'Length', 'Life', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Noise', 'Numbers', 'Patients', 'Pediatric Intensive Care Units', 'Performance', 'Physiologic Monitoring', 'Physiological', 'Population', 'Predictive Value', 'Procedures', 'Process', 'Purpose', 'Range', 'Receiver Operating Characteristics', 'Regression Analysis', 'Research Personnel', 'Sample Size', 'Sensitivity and Specificity', 'Series', 'Severity of illness', 'Social Sciences', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Validation', 'Work', 'abstracting', 'base', 'computer based statistical methods', 'data mining', 'data modeling', 'inclusion criteria', 'mortality', 'predictive modeling', 'programs', 'prospective', 'size', 'tool', 'trend', 'vector']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2008,135000,-0.025588133277304814
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7468470,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2008,286582,-0.01323945143361349
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7433144,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Class', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Computers', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Numbers', 'Performance', 'Personal Satisfaction', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Public Health', 'Randomized', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Today', 'Training', 'Voting', 'Work', 'base', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R44,2008,25548,0.030176493112182994
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7468497,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,268274,0.00024677003350943947
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7431959,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Condition', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Numbers', 'Patients', 'Play', 'Population', 'Process', 'Public Health', 'Rate', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'particle', 'size', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2008,376423,0.0009055106214009123
"High-throughput annotation of glycan mass spectra     DESCRIPTION (provided by applicant): The correct functioning of many proteins depends on glycosylation, the addition of sugar molecules (glycans) to selected amino acids in the protein. For example, cancer cells have different glycosylation patterns than ordinary cells, and there is strong evidence that glycoproteins on the surface of egg cells play an essential role in sperm binding. Despite the importance of glycosylation, there are as yet no reliable, high-throughput methods for determining the identity and location of glycans. Glycan identification is currently a manual procedure for experts, involving a combination of chemical assays and mass spectrometry. The automation of the process would have a significant impact on our understanding of this important biological process. The proposed project aims to invent chemical procedures, algorithms, and software for high-throughput analysis of glycan mass spectrometry data. The goal is to bring glycan analysis up to the level of peptide analysis within 3 years. In contrast to peptide analysis, which can leverage genomics data, glycan analysis requires the incorporation of expert knowledge of synthetic pathways, in order to limit the huge number of theoretical combinations of monosaccharides to the much smaller number that are actually synthesized in nature. The project will have to develop novel representations for the evolving expert knowledge, because an exhaustive list- analogous to the human genome- is not currently known. Along with expert knowledge, the project will develop and validate machine learning and statistical techniques for glycan identification. In particular, the project will develop methods for internally calibrating spectra, and will learn fragmentation patterns that can statistically distinguish different types of glycosidic linkages.         n/a",High-throughput annotation of glycan mass spectra,7431760,R01GM074128,"['Address', 'Age', 'Algorithms', 'Amino Acids', 'Area', 'Automation', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Carbon', 'Cartoons', 'Cells', 'Chemicals', 'Class', 'Communities', 'Computer software', 'Data', 'Development', 'Disclosure', 'Disease', 'Expert Systems', 'Facility Construction Funding Category', 'Genomics', 'Glycoproteins', 'Goals', 'Graft Rejection', 'Human Genome', 'Isomerism', 'Knowledge', 'Learning', 'Libraries', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Manuals', 'Mass Spectrum Analysis', 'Methods', 'Modification', 'Monosaccharides', 'Nature', 'Numbers', 'Occupations', 'Organism', 'Pathway interactions', 'Pattern', 'Peptides', 'Play', 'Polymers', 'Polysaccharides', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Range', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Score', 'Signal Transduction', 'Site', 'Specialist', 'Specific qualifier value', 'Spectrum Analysis', 'Speed', 'Surface', 'System', 'Techniques', 'Technology', 'Title', 'Training', 'Work', 'cancer cell', 'egg', 'enzyme activity', 'experience', 'glycosylation', 'glycosyltransferase', 'high throughput analysis', 'immune function', 'improved', 'novel', 'programs', 'prototype', 'sperm cell', 'sugar', 'tool']",NIGMS,PALO ALTO RESEARCH CENTER,R01,2008,329296,-0.013763925022365654
"A Simulation Tool to Enable Identification of Critical Network Interactions Using    DESCRIPTION (provided by applicant): One of the main challenges in the discovery of intracellular biomarkers and identification of therapeutic targets is the lack of a mechanistic understanding of the complex underlying pathways. The tremendous increase in both the quantity and diversity of cellular data represents a significant challenge to researchers seeking to construct biologically relevant interaction maps, and objectively extract specific actionable information. Machine learning based clustering algorithms serve as a preliminary statistical data analysis metric, but they fail to capture the data in the proper biological context. While chemical kinetics based models have proved to be effective in elucidating the pathway mechanisms, accurate estimates for the model parameters are severely lacking and are often impossible to obtain owing to the inherent difficulties involved in making dynamic measurements of specific intracellular phenomena. Additionally, methods for rational prioritization and selection of critical intracellular interactions (in the absence of kinetic information) are sorely lacking. Therefore, there is a clear need for innovative software tools that enable quantitative analysis of available microarray data in a biological pathway context, ultimately leading to the objective identification of critical biological interactions, providing a direction for more focused future efforts. We propose to address this challenge by developing an automated software platform that utilizes microarray data to select and merge relevant canonical biological pathway models thereby placing significantly expressed genes in their biological context. The analysis software will utilize a microarray expression-weighted metric to objectively rank the most critical interactions within the network model using a novel chemical kinetics-free Boolean dynamics algorithm. In the Phase I effort, we will develop a software tool composed of an R library that enables the automated generation of a pathway model from a given microarray dataset. Additionally, a methodology, and associated R library will be developed to objectively rank critical interactions in the pathway model, using a microarray data expression-weighted metric. Demonstration and validation of proposed algorithm will be carried out using a well characterized lipopolysaccharide (LPS) stimulated RAW 264.7 macrophage system. In Phase II, we will extend the scope of the algorithmic framework to include proteomic and metabolomic weighting in the objective ranking of critical interactions, and add workflow improvements through the addition of a graphical user interface (GUI). Experimental verification and validation of critical interactions identified in Phase I will be carried out using gene-silencing techniques. We also intend to establish collaborative partnerships with commercial entities. The proposing team has extensive experience in the areas of systems biology and bioinformatics (CFDRC) and microarray data analysis (Shawn Levy, University of Vanderbilt). CFDRC has a strong track record in the commercialization of software and hardware. PUBLIC HEALTH RELEVANCE:  Recently, there has been a tremendous increase in both the amount and diversity of cellular data available to researchers, representing a clear need for the development of advanced computational analysis software to enable the discovery of biomarkers of disease states, and identification of new therapeutic targets. However, currently available analysis tools do not consider the data in a proper biological context. This research proposes to develop an automated software platform that utilizes available data to develop and analyze mathematical models of complex processes in an automated fashion, resulting in the identification of critical intracellular processes.             n/a",A Simulation Tool to Enable Identification of Critical Network Interactions Using,7482734,R43GM084890,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Bioinformatics', 'Biological', 'Biological Markers', 'Complex', 'Computer Analysis', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Future', 'Gene Silencing', 'Generations', 'Genes', 'Genomics', 'Kinetics', 'Lead', 'Libraries', 'Lipopolysaccharides', 'Machine Learning', 'Maps', 'Measurement', 'Methodology', 'Methods', 'Metric', 'Microarray Analysis', 'Modeling', 'Nature', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Software Tools', 'Statistical Data Interpretation', 'System', 'Systems Biology', 'Techniques', 'Title', 'Universities', 'Urination', 'Validation', 'Weight', 'base', 'chemical kinetics', 'commercialization', 'editorial', 'experience', 'graphical user interface', 'innovation', 'macrophage', 'mathematical model', 'metabolomics', 'network models', 'novel', 'novel therapeutics', 'simulation', 'therapeutic target', 'tool']",NIGMS,CFD RESEARCH CORPORATION,R43,2008,99571,-0.006941135014975598
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7596501,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Chromosome Pairing', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Numbers', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Purpose', 'Rate', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'concept', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,319129,0.005300232599760823
"Nation Center: Multi-Scale Study- Cellular Networks(RMI) A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genomewide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 1 investigators to combine molecular interaction clues from Core 2 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions. n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7674889,U54CA121852,"['Address', 'Algorithms', 'Area', 'Automobile Driving', 'Binding', 'Bioinformatics', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA-Protein Interaction', 'Databases', 'Development', 'Dissection', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Genes', 'Genetic Engineering', 'Genomics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Language', 'Literature', 'Machine Learning', 'Maps', 'Medical', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Published Comment', 'Range', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Science', 'Signal Pathway', 'Skeleton', 'Source', 'Structure', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'Work', 'base', 'computer framework', 'data mining', 'design', 'graphical user interface', 'improved', 'innovation', 'knowledge base', 'multidisciplinary', 'novel', 'response', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2008,113826,0.0067032230299302045
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7354450,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Class', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Epidemiology, Other', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Infectious Disease Epidemiology', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'success', 'theories', 'tool', 'transmission process', 'transposon/insertion element', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2008,342967,-0.0023144844195711416
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7367958,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2008,2037396,0.0032732020723107457
"Classification Algorithms for Chemical Compounds Computational techniques that build models to correctly assign chemical compounds to various classes of interests have extensive applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or non-toxic, and filtering out drug-like compounds from large compound libraries. The overall goal of this proposal is to develop substructure-based classification algorithms for chemical compound datasets. The key elements of these algorithms are that they (i) utilize highly efficient substructure discovery algorithms to mine the chemical compounds and discover all substructures that can be critical for the classification task, (ii) use multiple criteria to generate a set of substructure-based features that simultaneously simplify the compounds' representation while retaining and exposing the features that are responsible for the specific classification problem, and (iii) build predictive models by employing kernel-based methods that take into account the relationships between these substructures at different levels of granularity and complexity, as well as information provided by traditional descriptors. n/a",Classification Algorithms for Chemical Compounds,7495003,R01LM008713,"['Accounting', 'Algorithms', 'Biological', 'Chemicals', 'Class', 'Classification', 'Computational Technique', 'Consensus', 'Data Set', 'Dependency', 'Descriptor', 'Effectiveness', 'Elements', 'Facility Construction Funding Category', 'Figs - dietary', 'Frequencies', 'Generations', 'Goals', 'Graph', 'Hybrids', 'Lead', 'Learning', 'Libraries', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular Conformation', 'Numbers', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Structure', 'Techniques', 'Technology', 'base', 'design', 'desire', 'drug development', 'interest', 'predictive modeling', 'programs', 'vector']",NLM,UNIVERSITY OF MINNESOTA,R01,2008,270892,-0.015001363327228037
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7489320,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2008,52898,-0.008365685154324993
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7495734,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,353327,0.011749439133876195
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7417618,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2008,263507,-0.007648835266113752
"Novel Analytic Techniques to Assess Physical Activity Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship. n/a",Novel Analytic Techniques to Assess Physical Activity,7611584,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2008,142424,-0.008227135144644546
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7407451,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2008,291451,-0.0016334755911815702
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7457647,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2008,437938,-0.0015325504032320998
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7535414,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Analysis', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Purpose', 'Records', 'Research', 'Research Methodology', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'Support of Research', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'desire', 'innovation', 'interest', 'prevent', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2008,99843,-0.009766302958485275
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7467204,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Personal Satisfaction', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Today', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'concept', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2008,280000,-0.006206725121507793
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7386333,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'None or Not Applicable', 'Numbers', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Purpose', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'cancer microarray', 'cancer type', 'design', 'desire', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,255036,-0.04780354542701874
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neurosience and neurologicat disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual )atients,and to studies executed across large poplulations. The data will be taken from subjects across a Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7688808,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Class', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Future', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Link', 'Localized', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Property', 'Purpose', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Services', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Textiles', 'Thinking', 'Time', 'Tissues', 'Today', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,100000,0.004464990488119505
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neurosience and neurologicat disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual )atients,and to studies executed across large poplulations. The data will be taken from subjects across a Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7688368,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Class', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Future', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Link', 'Localized', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Purpose', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Services', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Textiles', 'Thinking', 'Time', 'Tissues', 'Today', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,50000,0.004464990488119505
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7748401,R44GM083965,"['Learning', 'Techniques', 'parallel computing']",NIGMS,INSILICOS,R44,2008,143361,0.030176493112182994
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7433931,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2008,322087,0.009969484048626057
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7346975,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Endopeptidases', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Numbers', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein structure', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,R01,2008,280910,-0.025310464235478555
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.       n/a",Bioconductor: an open computing resource for genomics,7495201,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2008,805222,0.013045732580678544
"Causal Discovery Algorithms for Translational Research with High-Throughput Data Project Summary Causal Discovery Algorithms for Translational Research with High-Throughput Data The long-term goal of this project is to provide to the biomedical community next-generation causal algorithms to facilitate discovery of disease molecular pathways and causative as well as predictive biomarkers and molecular signatures from high-throughput data. Such knowledge and methods are necessary toward earlier and more accurate diagnosis and prognosis, personalized medicine, and rational drug design. If successful, the proposed research will have significant and wide methodological and practical implications spanning several areas of biomedicine with a primary focus and immediate benefits in high-throughput diagnostics and personalized medicine. It will provide significantly improved computational methods and deeper theoretical understanding related to producing molecular signatures and understanding mechanisms of disease and concomitant leads for new drugs. It will provide evidence about applicability of novel causal methods in other types of data. It will generate insights in specific pathways of lung cancer in humans. It will deepen our understanding and solutions to the Rashomon effect in ¿omics¿ data. The proposed research will also shed light on the operational value of the stability heuristic. Finally the research will engage the international research community to address open computational causal discovery problems relevant to high-throughput and other biomedical data. ¿ Aim 1. Evaluate and characterize several novel causal algorithms for biomarker selection, molecular signature creation and reverse network engineering using real, simulated, resimulated, and experimental datasets. Study generality of the methods by means of applicability to non-¿omics¿ datasets. ¿ Aim 2. Evaluate and characterize, novel and state of the art causal algorithms against state-of-the-art non-causal and quasi-causal algorithms. ¿ Aim 3. Systematically investigate the Rashomon effect as it applies to biomarker and signature multiplicity. ¿ Aim 4. Systematically investigate the utility of applying the stability heuristic for causal discovery. ¿ Aim 5. Derive novel biomarkers, pathways and hypotheses for lung cancer. ¿ Aim 6. Induce novel solutions through an international causal discovery competition. ¿ Aim 7. Disseminate findings. n/a",Causal Discovery Algorithms for Translational Research with High-Throughput Data,7643514,R56LM007948,"['AKT1 gene', 'AKT2 gene', 'AKT3 gene', 'Address', 'Affect', 'Algorithms', 'Area', 'Arts', 'Benchmarking', 'Bioinformatics', 'Biologic Characteristic', 'Biological Markers', 'Biology', 'Biometry', 'Book Chapters', 'Books', 'Cancer cell line', 'Causations', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Communities', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Set', 'Depth', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discipline', 'Disease', 'Drug Design', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Ensure', 'Epidermal Growth Factor Receptor', 'European', 'Evaluation', 'Event', 'Excision', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Hereditary Disease', 'Home environment', 'Human', 'Human Cell Line', 'Inferior', 'Information Retrieval', 'Institution', 'International', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Malignant neoplasm of lung', 'Marker Discovery', 'Medicine', 'Methods', 'Modality', 'Molecular', 'Molecular Profiling', 'Neighborhoods', 'Noise', 'Numbers', 'Online Systems', 'Outcome', 'Output', 'Paper', 'Pathway interactions', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteomics', 'Protocols documentation', 'Public Domains', 'Publishing', 'Quality Control', 'Random Allocation', 'Randomized', 'Rate', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Sample Size', 'Sampling', 'Schedule', 'Score', 'Services', 'Simulate', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Text', 'Thinking', 'Tissues', 'Translational Research', 'Variant', 'Work', 'base', 'c-erbB-1 Proto-Oncogenes', 'clinically relevant', 'computer based statistical methods', 'computer science', 'contextual factors', 'coping', 'data mining', 'design', 'drug development', 'heuristics', 'human data', 'human tissue', 'improved', 'innovation', 'insight', 'journal article', 'member', 'new technology', 'next generation', 'novel', 'novel diagnostics', 'outcome forecast', 'reconstruction', 'research study', 'software systems', 'symposium', 'theories', 'tool']",NLM,VANDERBILT UNIVERSITY,R56,2008,4434,-0.04195201117256762
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7479786,U54EB005149,[' '],NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,3534631,0.0054898891084355595
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7502135,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2008,3570137,0.007988257779740219
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7458835,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,503603,0.01218240778193845
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7688793,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,58299,0.01218240778193845
"Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi    DESCRIPTION (provided by applicant): The study of complex biological systems increasingly depends on vast amounts of dynamic information from diverse sources. The scientific analysis of the parasite Trypanosoma cruzi (T.cruzi), the principal causative agent of human Chagas disease, is the driving biological application of this proposal. Approximately 18 million people, predominantly in Latin America, are infected with the T.cruzi parasite. As many as 40 percent of these are predicted eventually to suffer from Chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. Research on T. cruzi is therefore an important human disease related effort. It has reached a critical juncture with the quantities of experimental data being generated by labs around the world, due in large part to the publication of the T.cruzi genome in 2005. Although this research has the potential to improve human health significantly, the data being generated exist in independent heterogeneous databases with poor integration and accessibility. The scientific objectives of this research proposal are to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for T.cruzi. This is in collaboration with the National Center for Biomedical Ontologies (NCBO) and will leverage its resources to achieve the objectives of this proposal as well as effectively to disseminate results to the broader life science community, including researchers in human pathogens. The PSE allows the dynamic integration of local and public data to answer biological questions at multiple levels of granularity. The PSE will utilize state-of- the-art semantic technologies for effective querying of multiple databases and, just as important, feature an intuitive and comprehensive set of interfaces for usability and easy adoption by biologists. Included in the multimodal datasets will be the genomic data and the associated bioinformatics predictions, functional information from metabolic pathways, experimental data from mass spectrometry and microarray experiments, and textual information from Pubmed. Researchers will be able to use and contribute to a rigorously curated T.cruzi knowledge base that will make it reusable and extensible. The resources developed as part of this proposal will be also useful to researchers in T.cruzi related kinetoplastids, Trypanosoma brucei and Leishmania major (among other pathogenic organisms), which use similar research protocols and face similar informatics challenges. PUBLIC HEALTH RELEVANCE: The scientific objective of this proposal is to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for Trypanosoma cruzi, a parasite that infects approximately 18 million people, predominantly in Latin America. As many as 40 percent of those infected are predicted to eventually suffer from Chagas disease, the leading cause of heart disease and sudden death in middle-aged adults in the region. Facilitating T.cruzi research through the PSE, with the aim of identifying vaccine, diagnostic, and therapeutic targets, is an important human disease related endeavor.          n/a",Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi,7428761,R01HL087795,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Animal Model', 'Anti-Retroviral Agents', 'Architecture', 'Archives', 'Area', 'Arts', 'Automobile Driving', 'Beds', 'Behavior', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Body of uterus', 'Buffaloes', 'California', 'Caring', 'Chagas Disease', 'Childhood', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Computers', 'Controlled Vocabulary', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Doctor of Public Health', 'Drops', 'Drosophila genus', 'Educational Activities', 'Educational workshop', 'Electronics', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evolution', 'Face', 'Feedback', 'Foundations', 'Future', 'Gene Mutation', 'Generations', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Geographic Locations', 'Goals', 'HIV', 'HIV Infections', 'Health', 'Heart Diseases', 'Homologous Gene', 'Human', 'Human Resources', 'Imagery', 'Immunologic Deficiency Syndromes', 'Immunology', 'Individual', 'Infection', 'Informatics', 'Information Management', 'Information Resources', 'Information Services', 'Information Technology', 'International', 'Internet', 'Interruption', 'Knowledge', 'Laboratories', 'Laboratory Organism', 'Language', 'Latin America', 'Lead', 'Learning', 'Leishmania major', 'Libraries', 'Link', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Medical Informatics', 'Medicine', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Mind', 'Mining', 'Modeling', 'Mutation', 'Natural Language Processing', 'Nature', 'Online Mendelian Inheritance In Man', 'Online Systems', 'Ontology', 'Operative Surgical Procedures', 'Oregon', 'Organism', 'Orthologous Gene', 'Outcome', 'Parasites', 'Pathogenesis', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Philosophy', 'Physiology', 'Prevention strategy', 'Principal Investigator', 'Problem Solving', 'Process', 'Proteomics', 'Protocols documentation', 'PubMed', 'Public Health', 'Publications', 'Publishing', 'Purpose', 'Randomized Clinical Trials', 'Range', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Resources', 'San Francisco', 'Science', 'Scientist', 'Semantics', 'Services', 'Site', 'Software Tools', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Structure', 'Study models', 'Sudden Death', 'Sum', 'System', 'TAF8 gene', 'Talents', 'Techniques', 'Technology', 'Terminology', 'Testing', 'Thinking', 'Training', 'Treatment Protocols', 'Trypanosoma brucei brucei', 'Trypanosoma cruzi', 'USA Georgia', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Vaccines', 'Vertical Disease Transmission', 'Victoria Austrailia', 'Virtual Library', 'Virus', 'Western Asia Georgia', 'Work', 'Zebrafish', 'abstracting', 'base', 'biocomputing', 'biomedical scientist', 'college', 'computer based Semantic Analysis', 'computer science', 'concept', 'data integration', 'design', 'desire', 'fundamental research', 'human disease', 'improved', 'indexing', 'innovative technologies', 'knowledge base', 'member', 'metabolomics', 'middle age', 'novel', 'novel strategies', 'open source', 'outreach', 'pandemic disease', 'pathogen', 'prevent', 'programs', 'protein protein interaction', 'repository', 'research and development', 'research study', 'syntax', 'theories', 'therapeutic target', 'tool', 'usability']",NHLBI,WRIGHT STATE UNIVERSITY,R01,2008,393930,-0.019578182038272662
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7299922,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2007,181125,-0.012287586167500357
"Machine learning analysis of tandem mass spectra    DESCRIPTION (provided by applicant): Project summary: Mass spectrometry, the core technology in the field of proteomics, promises to enable scientists to identify and quantify the entire complement of molecules that comprise a complex biological sample. In the biological and health sciences, mass spectrometry is commonly used in a nigh-throughput fashion to identify proteins in a mixture. Currently, the primary bottleneck in this type of experiment is computational. Existing algorithms for interpreting mass spectra are slow and fail to identify a large proportion of the given spectra. We propose to apply techniques and tools from the field of machine learning to the analysis of mass spectrometry data. We will build computational models of peptide fragmentation within the mass spectrometer, as well as larger-scale models of the entire mass spectrometry process. Using these models, we will design and validate algorithms for identifying the set of proteins that best explain an observed set of spectra. Software implementations for all of the methods will be made publicly available in a user-friendly form. In practical terms, this software will enable scientists to more easily, efficiently and accurately analyze and understand their mass spectrometry data. Relevance: The applications of mass spectrometry and its promises for improvements of human health are numerous, including an increased understanding of disease phenotypes and the molecular mechanisms that underlie them, and vastly more sensitive and specific diagnostic and prognostic screens.           n/a",Machine learning analysis of tandem mass spectra,7194479,R01EB007057,"['Abbreviations', 'Algorithms', 'Altretamine', 'Area', 'Authorship', 'Biochemical', 'Biological', 'Blast Cell', 'Calibration', 'Carbonyl Cyanide m-Chlorophenyl Hydrazone', 'Collection', 'Complement', 'Complex', 'Complex Mixtures', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Databases', 'Devices', 'Diagnostic', 'Dissociation', 'FOLH1 gene', 'Genomics', 'Hand', 'Health', 'Health Sciences', 'Hour', 'Human', 'Knowledge', 'Learning', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Molecular', 'Peptide Fragments', 'Peptides', 'Performance', 'Post-Translational Protein Processing', 'Preparation', 'Principal Investigator', 'Procedures', 'Process', 'Protein Biochemistry', 'Proteins', 'Proteomics', 'Rate', 'Receiver Operating Characteristics', 'Research Personnel', 'Rest', 'Running', 'Sampling', 'Scientist', 'Score', 'Set protein', 'Silicon Dioxide', 'Source Code', 'Spectrometry', 'Staging', 'Statistical Models', 'Techniques', 'Technology', 'Time', 'Today', 'Training', 'Work', 'computer based statistical methods', 'day', 'design', 'disease phenotype', 'expectation', 'improved', 'interest', 'markov model', 'mass spectrometer', 'model design', 'prognostic', 'programs', 'research study', 'small molecule', 'tandem mass spectrometry', 'task analysis', 'tool', 'user-friendly']",NIBIB,UNIVERSITY OF WASHINGTON,R01,2007,623873,0.0025235159133013213
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7222736,K22LM008389,"['Adverse event', 'Area', 'Arts', 'Attention', 'Biological Neural Networks', 'Caregivers', 'Chicago', 'Childhood', 'Classification', 'Clinical', 'Computer software', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Element', 'Data Set', 'Databases', 'Decision Trees', 'Detection', 'Disease', 'Early Intervention', 'Ensure', 'Etiology', 'Event', 'Excision', 'Foundations', 'Genomics', 'Heart Arrest', 'Hour', 'Laboratories', 'Length', 'Life', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Noise', 'Numbers', 'Patients', 'Pediatric Intensive Care Units', 'Performance', 'Physiologic Monitoring', 'Physiological', 'Population', 'Predictive Value', 'Procedures', 'Process', 'Purpose', 'Range', 'Receiver Operating Characteristics', 'Regression Analysis', 'Research Personnel', 'Sample Size', 'Sensitivity and Specificity', 'Series', 'Severity of illness', 'Social Sciences', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Validation', 'Work', 'abstracting', 'base', 'computer based statistical methods', 'data mining', 'data modeling', 'inclusion criteria', 'mortality', 'predictive modeling', 'programs', 'prospective', 'size', 'tool', 'trend', 'vector']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2007,135000,-0.025588133277304814
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7242352,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH AND SCI UNIVERSITY,R01,2007,292133,-0.01323945143361349
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7186695,R01NS051826,"['Accounting', 'Adult', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Anatomic structures', 'Anatomy', 'Area', 'Atlases', 'Back', 'Biomechanics', 'Boston', 'Brain', 'Caring', 'Class', 'Classification', 'Clinical assessments', 'Clutterings', 'Collaborations', 'Competence', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Corpus Callosum', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diffuse Pattern', 'Discipline of obstetrics', 'Disease', 'Disease Progression', 'Effectiveness', 'Effectiveness of Interventions', 'Electroencephalography', 'Elements', 'Ensure', 'Evaluation', 'Evolution', 'Fetal Growth Retardation', 'General Hospitals', 'Genetic Markers', 'Goals', 'Gold', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Hospitals', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Incidence', 'Individual', 'Infant', 'Intervention', 'Intuition', 'Invasive', 'Knowledge', 'Label', 'Learning', 'Learning Disabilities', 'Link', 'Localized', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Massachusetts', 'Measurement', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Morphologic artifacts', 'Morphology', 'Motivation', 'Neonatal', 'Neuroanatomy', 'Neurosciences', 'Neurosurgeon', 'Noise', 'Normal Range', 'Operative Surgical Procedures', 'Outcome', 'Pathology', 'Patients', 'Pediatric Hospitals', 'Population', 'Population Characteristics', 'Population Study', 'Positioning Attribute', 'Premature Infant', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Property', 'Psyche structure', 'Range', 'Rate', 'Relative (related person)', 'Research', 'Research Personnel', 'Residual state', 'Resolution', 'Rest', 'Role', 'Scanning', 'Schizophrenia', 'Shapes', 'Site', 'Specificity', 'Staging', 'Standards of Weights and Measures', 'Statistical Distributions', 'Statistical Models', 'Statistical Study', 'Statistically Significant', 'Structure', 'Surface', 'Surgeon', 'Surgical Instruments', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Tweens', 'Universities', 'Validation', 'Variant', 'Washington', 'Woman', 'base', 'cohort', 'computer studies', 'computerized tools', 'desire', 'deviant', 'disease classification', 'expectation', 'feeding', 'healthy aging', 'imaging Segmentation', 'improved', 'instrument', 'interest', 'mortality', 'neonate', 'nervous system disorder', 'neuroimaging', 'neurosurgery', 'normal aging', 'novel', 'programs', 'radiologist', 'reconstruction', 'relating to nervous system', 'research clinical testing', 'response', 'shape analysis', 'statistics', 'tool', 'tumor']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2007,282619,-0.013773160977459259
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7226297,R01HL065462,"['Accounting', 'Algorithms', 'Arts', 'Biological', 'Class', 'Classification', 'Communities', 'Computer software', 'Condition', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease regression', 'Documentation', 'Effectiveness', 'Employee Strikes', 'Environment', 'Etiology', 'Gene Expression', 'Gene Expression Profiling', 'Genes', 'Genomics', 'Goals', 'Heart failure', 'Knowledge', 'Least-Squares Analysis', 'Machine Learning', 'Mass Spectrum Analysis', 'Mechanics', 'Medical', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Profiling', 'Motivation', 'Pan Genus', 'Patients', 'Performance', 'Principal Investigator', 'Procedures', 'Property', 'Proteomics', 'Public Domains', 'Research Personnel', 'Sample Size', 'Sampling', 'Scheme', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Tissue-Specific Gene Expression', 'Weight', 'Work', 'base', 'forest', 'gene function', 'improved', 'novel', 'protein protein interaction', 'response', 'statistics', 'tool']",NHLBI,UNIVERSITY OF MINNESOTA,R01,2007,141753,-0.018092880234941107
"A RuleFit Product for Classification and Regression Prediction and data exploration are important aspects of modern commercial and scientific life. Regression methods predict dependent variables (e.g., tumor growth, severity of disease), while classification methods predict class membership (e.g., tumor or disease type). Both use a vector of independent variables to make the predictions. Because they are often superior predictors, can handle large numbers observations and large numbers of variables, can often yield insight into the data not provided by other methods, and because they can adapt to arbitrarily complex relationships, modern machine learning methods based on tree ensembles such as RANDOM FORESTS and MART have become leading modern analytical methods. Here we propose to commercially implement RULEFIT, a recent innovative method extending the RANDOM FORESTS and MART approaches, that shows strong evidence of being consistently more accurate than either ensemble. RULEFIT also includes groundbreaking new methods for variable selection in the face of huge numbers of predictors, and for identifying interactions, and ranking their importance. Optionally, RULEFIT extracts ""rules"" of special interest: succinct statements of conditions under which an outcome is especially likely or unlikely, or especially large or small. The primary output of RULEFIT is a numeric value reecting a prediction of the value of the dependent variable or the probability of a class membership. RULEFIT is likely to become a leading technique in the machine learning and statistics. It builds on RANDOM FORESTS and MART and includes all their useful benefits such as variable selection, data exploration, data reduction, outlier detection, and missing value imputation, while enhancing and extending these benefits.  COMMERCIAL POTENTIAL The market for advanced analytical tools has been growing strongly over the last decade and the growth shows no signs of diminishing. Modelers and data analysts in both university- based and commercial settings are increasingly aware of the power and value of new analytical tools derived from modern statistics and machine learning research. The increased accuracy of the new methods and the acceleration they provide to the analysis of complex data are fueling demand for this new technology. The advances embedded in the proposed product represent substantial improvements to existing technology and include methods to solve vexing problems in contemporary data analysis, and thus should find a welcoming market.  There are further reasons to forecast robust commercial potential for this product. The applicant organization has a strong track record in the industry and is widely recognized as a developer of high quality software. We have been working with consultant Friedman since 1990 and have gained exclusive rights to the proprietary sourcecode for a number of his innovations. These include CART, MARS, MART and PRIM. With the addition of RULEFIT and its associated sub-components, these products represent a unique collection of pedigreed tools. We have also forged a similar relationship with the (late) Leo Breiman and have the exclusive rights to commercialization of Breiman's Random Forests sourcecode. Our proposed package thus occupies a distinctive position in machine learning software which cannot be replicated by other vendors. Keywords: machine learning; classi?cation; prediction; supervised learning; variable importance; inter- action detection; Justi?cation Dr. Steinberg has extensive experience in software development for advanced statistical and machine learning methods, particularly in the area of classi?cation and regression trees, sur- vival analysis, adaptive modeling, RANDOM FORESTS and MART. He will oversee all aspects of the project. He will will work with Dr. Cardell, Professor Friedman, Mr. Colla, and with the Salford Systems software development engineer in creating and studying the software and methods used in this proposal. He will also be responsible for the architecture of the Phase I software. Professor Friedman and Dr. Cardell will provide technical support as follows: Dr. Fried- man is an expert on machine learning methods and is one of the developers of the RULEFIT technique. Regular consultation with him will be in this area. Dr. Cardell is an expert in asymptotic theory, and in the design of Monte Carlo and other tests for the evaluation of ma- chine learning algorithms. He also has extensive experience in machine learning, including adaptive modeling, neural networks, logistic regression, and classi?cation methods. He will review core algorithms of RULEFIT for possible improvement and extension and design the Monte Carlo tests. Mr. Colla has extensive experience in software development and with machine learning methods, including work on the commercial implementations of CART, MARS, RANDOM FORESTS, and MART. Working with Dr. Cardell, he will be responsible for much of the new software coding. 5 Project Description Page 7 Principal Investigator/Program Director (Last, first, middle): Steinberg, Dan Prediction models based upon classification and regression tree ensembles have become important in medical and other research. There are currently no commercial products available that implement the proposed RuleFit methodology. These methods have significant advantages over existing techniques, and will aid researchers in obtaining the best possible predictions.   n/a",A RuleFit Product for Classification and Regression,7268612,R43CA124294,"['Acceleration', 'Agreement', 'Algorithms', 'Architecture', 'Area', 'Beds', 'Build-it', 'Cations', 'Class', 'Classification', 'Code', 'Collection', 'Comparative Study', 'Complex', 'Computer software', 'Condition', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Detection', 'Disease', 'Disease regression', 'Engineering', 'Evaluation', 'Face', 'Generations', 'Growth', 'Industry', 'Information Systems', 'Investigation', 'Learning', 'Left', 'Life', 'Linear Models', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Neural Network Simulation', 'Numbers', 'Outcome', 'Output', 'Painless', 'Pattern', 'Performance', 'Phase', 'Plant Leaves', 'Play', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Rate', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Rights', 'Role', 'Sampling', 'Severity of illness', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Trees', 'Universities', 'Variant', 'Vendor', 'Work', 'analytical method', 'analytical tool', 'base', 'commercialization', 'data mining', 'data structure', 'design', 'evaluation/testing', 'experience', 'forest', 'forging', 'graphical user interface', 'innovation', 'insight', 'interest', 'loss of function', 'man', 'new technology', 'novel', 'professor', 'programs', 'prototype', 'relating to nervous system', 'research study', 'software development', 'statistics', 'theories', 'tool', 'tumor', 'tumor growth', 'vector']",NCI,SALFORD SYSTEMS,R43,2007,91700,-0.030588531321735557
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7299383,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2007,266852,0.00024677003350943947
"High-throughput annotation of glycan mass spectra     DESCRIPTION (provided by applicant): The correct functioning of many proteins depends on glycosylation, the addition of sugar molecules (glycans) to selected amino acids in the protein. For example, cancer cells have different glycosylation patterns than ordinary cells, and there is strong evidence that glycoproteins on the surface of egg cells play an essential role in sperm binding. Despite the importance of glycosylation, there are as yet no reliable, high-throughput methods for determining the identity and location of glycans. Glycan identification is currently a manual procedure for experts, involving a combination of chemical assays and mass spectrometry. The automation of the process would have a significant impact on our understanding of this important biological process. The proposed project aims to invent chemical procedures, algorithms, and software for high-throughput analysis of glycan mass spectrometry data. The goal is to bring glycan analysis up to the level of peptide analysis within 3 years. In contrast to peptide analysis, which can leverage genomics data, glycan analysis requires the incorporation of expert knowledge of synthetic pathways, in order to limit the huge number of theoretical combinations of monosaccharides to the much smaller number that are actually synthesized in nature. The project will have to develop novel representations for the evolving expert knowledge, because an exhaustive list- analogous to the human genome- is not currently known. Along with expert knowledge, the project will develop and validate machine learning and statistical techniques for glycan identification. In particular, the project will develop methods for internally calibrating spectra, and will learn fragmentation patterns that can statistically distinguish different types of glycosidic linkages.         n/a",High-throughput annotation of glycan mass spectra,7239477,R01GM074128,"['Address', 'Age', 'Algorithms', 'Amino Acids', 'Area', 'Automation', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Carbon', 'Cartoons', 'Cells', 'Chemicals', 'Class', 'Communities', 'Computer software', 'Data', 'Development', 'Disclosure', 'Disease', 'Expert Systems', 'Facility Construction Funding Category', 'Genomics', 'Glycoproteins', 'Goals', 'Graft Rejection', 'Human Genome', 'Isomerism', 'Knowledge', 'Learning', 'Libraries', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Manuals', 'Mass Spectrum Analysis', 'Methods', 'Modification', 'Monosaccharides', 'Nature', 'Numbers', 'Occupations', 'Organism', 'Pathway interactions', 'Pattern', 'Peptides', 'Play', 'Polymers', 'Polysaccharides', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Range', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Score', 'Signal Transduction', 'Site', 'Specialist', 'Specific qualifier value', 'Spectrum Analysis', 'Speed', 'Surface', 'System', 'Techniques', 'Technology', 'Title', 'Training', 'Work', 'cancer cell', 'egg', 'enzyme activity', 'experience', 'glycosylation', 'glycosyltransferase', 'high throughput analysis', 'immune function', 'improved', 'novel', 'programs', 'prototype', 'sperm cell', 'sugar', 'tool']",NIGMS,PALO ALTO RESEARCH CENTER,R01,2007,318904,-0.013763925022365654
"The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments    DESCRIPTION (provided by applicant): The goal of this research is to create software that uses mathematical algorithms to detect medical billing coding errors prior to payment. The well-publicized failure of current healthcare cost containment technologies to prevent improper payments in both the commercial healthcare market and the federal Medicare program highlights the urgent need for a new approach to the growing problem of out of control medical costs. A recent federal study by the GAO estimated that improper payments by Medicare alone were in excess of 21 billion dollars, a truly staggering 48.1 percent of all improper payments by federal programs. Like SPAM, whose dynamic nature makes static or post hoc remedies ineffective, effective cost containment in one area often merely leads to the creation of new areas of abuse. Clearly, the ideal solution is a system that can evaluate the fairness of payments before they are made, and that can respond to dynamic patterns of abuse. The first step in creating such a system is the creation of robust method for sorting bills for appropriate rule-based analysis on the basis of the type of bill. Currently neither Medicare nor major insurers are capable of making this classification reliably except through the use of inefficient, static rules and the use of manual sorting--a costly and inefficient approach to assuring timely payment to hospitals and medical providers. We propose a novel method for using mathematical algorithms that utilize machine-learning (ML) methods to address the problem of medical bill categorization, the first step in coding error detection. Specifically, we propose the evaluation of a variety of genetic algorithms that are well adapted to the problems of large, dynamic datasets and can be ""trained"" using real world correctly coded datasets in healthcare claims. This work is particularly timely due to recent Medicare contracting reform. Using more than 50 contractors and carriers, bill classification is largely determined by the carrier's contract. Centralizing this process to only four payment centers will require the classification system we propose. [This research is directed toward the development of software applications that will detect billing errors and perform proper edits to payment of medical bills. Current anticipated changes and reforms in the Medicare system will require these systems, which do not currently exist in the public or private sector.]             n/a",The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments,7316071,R43LM009190,"['Address', 'Age', 'Algorithms', 'Area', 'Arts', 'Classification', 'Code', 'Collaborations', 'Computer Simulation', 'Computer software', 'Contractor', 'Contracts', 'Cost Control', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Development', 'Elements', 'Environment', 'Evaluation', 'Failure', 'Genetic Programming', 'Goals', 'Health Care Costs', 'Health Care Fraud', 'Health Personnel', 'Healthcare', 'Healthcare Market', 'Healthcare Systems', 'Hospitals', 'Industry', 'Inpatients', 'Insurance Carriers', 'Learning', 'Machine Learning', 'Manuals', 'Mathematics', 'Medical', 'Medicare', 'Methods', 'Mining', 'Modeling', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outpatients', 'Pattern', 'Phase', 'Policies', 'Population', 'Prevention', 'Private Sector', 'Process', 'Provider', 'Rate', 'Reporting', 'Research', 'Running', 'Small Business Technology Transfer Research', 'Solutions', 'Sorting - Cell Movement', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Training', 'Work', 'base', 'college', 'computerized', 'cost', 'design', 'experience', 'improved', 'mathematical algorithm', 'novel', 'novel strategies', 'payment', 'prevent', 'programs', 'size', 'software development', 'stem', 'success']",NLM,"QMEDTRIX SYSTEMS, INC.",R43,2007,92482,-0.006868715045362199
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7246847,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2007,2183988,0.0032732020723107457
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7275769,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2007,51278,-0.008365685154324993
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7318595,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA IRVINE,R01,2007,372000,0.011749439133876195
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7262592,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2007,263847,-0.007648835266113752
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,-0.012721510272034192
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7269383,R01RR014477,[' '],NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2007,307022,-0.02015633356701105
"Comparative Visualization and Analysis for GCxGC    DESCRIPTION (provided by applicant): Project Summary. This project will investigate and develop effective information technologies for comparative analysis and visualization of complex data generated by comprehensive two-dimensional gas chromatography (GCxGC). GCxGC is an emerging technology that provides an order-of-magnitude greater separation capacity, significantly better signal-to-noise ratio, and higher dimensional retention-structure relations than traditional GC. The principal challenge for utilization of GCxGC, in a wide range of public-health and other applications, is the difficulty of analyzing and interpreting the large, complex data it generates. The quantity and complexity of GCxGC data necessitates the investigation and development of new information technologies. This project will develop and demonstrate innovative methods and tools for comparative analysis of GCxGC datasets. The expected results of this research and development include a PCA-based method for chemical fingerprinting, decision trees with chemical constraints for sample classification, genetic programming for template and constraint-based matching and classification, and visualization methods for comparative GCxGC analyses. These methods will be implemented in commercial software that will support researchers and laboratory analysts in a wide range of commercial applications, including health care, environmental monitoring, and chemical processing. The power of GCxGC, supported by effective information technologies, will enable better understanding of chemical compositions and processes, a foundation for future scientific advances and discoveries. Relevance to Public Health. Today, a few advanced laboratories are pioneering GCxGC for a variety of applications such as environmental monitoring of exposure profiles in air, soil, food, and water; identification and quantification of toxic products in blood, urine, milk, and breath samples; and qualitative and quantitative metabolomics to provide a holistic view of the biochemical status or biochemical phenotype of an organism. Many analyses in these applications require detailed chemical comparisons of samples, e.g..monitoring changes, comparison to reference standards, chemical matching or ""fingerprinting"", and classification. GCxGC is a powerful new technology for such comparative analyses. This proposal will provide innovative information technologies to support users in these applications.           n/a",Comparative Visualization and Analysis for GCxGC,7270029,R44RR020256,"['Air', 'Archives', 'Biochemical', 'Blood', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Emerging Technologies', 'Environmental Monitoring', 'Fingerprint', 'Food', 'Foundations', 'Future', 'Gas Chromatography', 'Genetic Programming', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Information Technology', 'Investigation', 'Laboratories', 'Language', 'Machine Learning', 'Marketing', 'Methods', 'Milk', 'Monitor', 'Noise', 'Organism', 'Pattern', 'Phase', 'Phenotype', 'Principal Component Analysis', 'Process', 'Public Health', 'Range', 'Reference Standards', 'Reporting', 'Research Personnel', 'Sales', 'Sampling', 'Schedule', 'Scientific Advances and Accomplishments', 'Signal Transduction', 'Software Tools', 'Soil', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Today', 'Trademark', 'Urine', 'Water', 'base', 'chemical fingerprinting', 'commercial application', 'comparative', 'innovation', 'innovative technologies', 'instrument', 'metabolomics', 'new technology', 'research and development', 'tool', 'two-dimensional']",NCRR,"GC IMAGE, LLC",R44,2007,239373,0.008569002574201049
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7247404,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2007,292160,-0.0016334755911815702
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7287965,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2007,446875,-0.0015325504032320998
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7244058,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2007,328325,0.009969484048626057
"Bayesian Methods and Experimental Design for Molecular Biology Experiments    DESCRIPTION (provided by applicant): The goal of this proposal is to provide a suite of software tools for bioinformatics and systems biology researchers who are using molecular biology (Omics) data to identify the best experimental design and to analyze the resulting experimental data using Bayesian tools. A common problem for most bioinformatics experiments is low power due to low replication. This problem can be alleviated economically when an increase in adoption and use of a specific platform leads to a decrease in associated costs, thereby enabling an increase in samples allocated per treatment. Yet, many bioinformatics experiments remain underpowered as researchers use the offsets of decreased costs to explore more complex questions. When designing an experiment, the allocation of samples to treatment regimens, and the choice of treatments to test, are traditionally the only variables to manipulate. Bayesian experimental design provides a framework to find the optimal design out of n possible designs subject to a utility function that can include such items as time and material costs.      Bayesian statistical methods have been gaining substantial favor in bioinformatics and systems biology as they provide a highly flexible framework for fitting and exploring complex models. Bayesian models also provides to domain experts such as biologists and physicians easily interpretable models through posterior probabilities which are more naturally understood than the traditional p-value. While a number of open source tools based on Bayesian models are available, most are applied best in the context of a specific research data analysis problem or model and are not integrated into a single, complete system for data analysis.      We propose to research and develop a statistical analysis software package S+OBAYES (for S-PLUS and R) with generalized tools for Bayesian design of experiments, empirical and fully Bayesian analysis, and modeling and simulation using modern commercial software development practices. These tools will provide functionality for finding the optimal choice and layout of experimental treatments for molecular biology experiments and for fitting Bayesian linear and non-linear models to a variety of data types including time series. We propose to validate the software in molecular biology research problems such as the detection of differential gene, protein, and metabolite abundance. The benefits of this work will be a commercial-quality software package with validated statistical methodology and interactive visualization tools that will appeal to molecular biologists and systems biology investigators. The results of the proposed work will expedite discoveries in basic science, early disease detection, and drug discovery and development.          n/a",Bayesian Methods and Experimental Design for Molecular Biology Experiments,7325828,R43GM083023,"['Address', 'Adoption', 'Algorithms', 'Animal Genetics', 'Arizona', 'Basic Science', 'Bayesian Analysis', 'Bayesian Method', 'Bioconductor', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biometry', 'Biotechnology', 'Cations', 'Chromosome Mapping', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Department of Defense', 'Depth', 'Detection', 'Development', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Employment', 'Ensure', 'Experimental Designs', 'Exposure to', 'Factor Analysis', 'Foundations', 'Funding', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Government', 'Government Agencies', 'Health', 'Imagery', 'Industry', 'Information Systems', 'Institution', 'Iowa', 'Libraries', 'Linear Models', 'Machine Learning', 'Manuals', 'Manuscripts', 'Maps', 'Marketing', 'Mass Spectrum Analysis', 'Measures', 'Medical Informatics', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Biology', 'Non-linear Models', 'Numbers', 'Pathway interactions', 'Phase', 'Physicians', 'Population Study', 'Principal Investigator', 'Probability', 'Property', 'Proteome', 'Proteomics', 'Proxy', 'Quantitative Trait Loci', 'Research', 'Research Personnel', 'Rice', 'Risk Factors', 'SNP genotyping', 'Sampling', 'Science', 'Scientist', 'Series', 'Services', 'Simulate', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Software Tools', 'Software Validation', 'Solutions', 'Speed', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Systems Biology', 'Techniques', 'Telecommunications', 'Testing', 'Time', 'Time Series Analysis', 'Training', 'Treatment Protocols', 'Universities', 'Validation', 'Washington', 'Wisconsin', 'Work', 'animal breeding', 'base', 'cost', 'design', 'drug discovery', 'experience', 'human subject', 'improved', 'interest', 'lecturer', 'models and simulation', 'open source', 'professor', 'programs', 'protein metabolite', 'research and development', 'research study', 'skills', 'software development', 'statistics', 'success', 'theories', 'tool', 'treatment effect']",NIGMS,INSIGHTFUL CORPORATION,R43,2007,103995,0.005574345831994747
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7176188,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Endopeptidases', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Numbers', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein structure', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,2007,286350,-0.025310464235478555
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7214118,R42ES013321,"['Accounting', 'Animals', 'Architecture', 'Biological Assay', 'Biological Neural Networks', 'Chemicals', 'Clinical', 'Clinical Trials', 'Computer Simulation', 'Computer software', 'Contracts', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Drug Formulations', 'Drug Industry', 'Drug toxicity', 'End Point', 'Expert Systems', 'Funding', 'Future', 'Fuzzy Logic', 'Gene Expression', 'Guidelines', 'Health Care Costs', 'Hepatotoxicity', 'Investments', 'Learning', 'Liver', 'Marketing', 'Methods', 'Network-based', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Proteomics', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Rate', 'Relative (related person)', 'Reliance', 'Research', 'Research Personnel', 'Screening procedure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Training', 'Validation', 'base', 'computational chemistry', 'cost', 'data acquisition', 'design', 'highly advanced system', 'improved', 'innovation', 'knowledge base', 'metabolomics', 'quantum', 'serial analysis of gene expression', 'subtraction hybridization', 'tool']",NIEHS,"YAHSGS, LLC",R42,2007,257269,-0.006220257833643909
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7475421,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,160000,-0.01988432089519382
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7248464,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,693808,-0.01988432089519382
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7494181,U54EB005149,"['Address', 'Affect', 'Alcohols', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Appearance', 'Area', 'Atlases', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Cells', 'Characteristics', 'Clinical', 'Clinical Data', 'Cognitive', 'Collaborations', 'Collection', 'Commit', 'Complex', 'Computational algorithm', 'Computer software', 'Computer-Assisted Image Analysis', 'Computing Methodologies', 'Coupling', 'Data', 'Data Correlations', 'Data Sources', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Elements', 'Ensure', 'Epilepsy', 'Feedback', 'Fiber', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Image-Guided Surgery', 'Imagery', 'Imaging Techniques', 'Individual', 'Knowledge', 'Life', 'Link', 'Localized', 'Location', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Medical', 'Medical Imaging', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modification', 'Morphology', 'Multiple Sclerosis', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Organ', 'Other Imaging Modalities', 'Output', 'Participant', 'Patients', 'Pattern', 'Performance', 'Physiological', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Range', 'Recording of previous events', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Schizophrenia', 'Science', 'Services', 'Shapes', 'Software Engineering', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Distributions', 'Structure', 'Syndrome', 'System', 'Systems Analysis', 'Techniques', 'Testing', 'Textiles', 'Time', 'Tissues', 'Today', 'USA Georgia', 'Utah', 'Visible Radiation', 'Vision', 'Western Asia Georgia', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'design', 'disability', 'experience', 'image registration', 'insight', 'interest', 'mathematical model', 'neuroimaging', 'novel', 'prenatal', 'research study', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion', 'white matter']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2007,87500,0.0054898891084355595
"National Alliance for Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance for Medical Imaging Computing (NAMIC)(RMI),7271955,U54EB005149,[' '],NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2007,3888915,0.0054898891084355595
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7286779,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2007,3638557,0.007988257779740219
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7284239,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2007,588968,0.01218240778193845
"Systems Biology of Cell Decision Processes    DESCRIPTION (provided by applicant):  The remarkable complexity of biological systems demands a systematic approach to their analysis.  The goal of this proposal is to establish an MIT Center of Excellence in Cell Decision Processes (CDP) around an interdisciplinary team of cell biologists, computer scientists and microsystems engineers tackling the systems biology of protein networks and signal transduction in mammalian cells. The basic hypothesis of the CDP Center is that understanding cell decision processes requires the development of network models that combine quantitative rigor with molecular detail. These models will be hybrids containing highly specific representations of critical reactions and abstract representations of the system as a whole. Effective models will endure and capture the accumulation of knowledge over time in a rigorous and portable format.  The CDP Center will follow a research paradigm that links systematic experiments and numerical modeling in a four-step feedback loop of manipulation-measurement-mining and modeling. The biological focus of the Center will be the signaling events that control apoptosis. The measurement of protein concentrations, modification state and activity will be undertaken for signaling molecules at many points in the apoptotic network. The measurements will then be analyzed using several modeling approaches ranging from highly specified to highly abstract.  To collect sufficient data for systematic modeling, the CDP Center will automate and standardize biochemical methods, develop compact array-based assays and design novel microfabricated devices with integrated microfluidics and label-free sensors. To organize and systematize the data, informatic methods will be developed that support rigorous approaches to inference. Finally, physico-chemical, structure-systems and Bayesian models will be used to generate biological hypotheses for experimental verification.  The research activities of the CDP Center will be complemented by graduate and undergraduate education and by an outreach program targeted at the scientific community in general and minority-serving institutions in particular.         n/a",Systems Biology of Cell Decision Processes,7285298,P50GM068762,"['Accountability', 'Accounting', 'Address', 'Adopted', 'Algorithms', 'Amino Acid Sequence', 'Animals', 'Annual Reports', 'Apoptosis', 'Apoptotic', 'Appendix', 'Area', 'Artificial Intelligence', 'Attention', 'Authorship', 'Automation', 'Award', 'BCL2L11 gene', 'Belief', 'Binding', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Phenomena', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Engineering', 'Boxing', 'Budgets', 'Cancer Center', 'Cell Extracts', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Chemical Engineering', 'Chemical Structure', 'Chemicals', 'Child', 'Classification', 'Code', 'Collaborations', 'Color', 'Commit', 'Communities', 'Complement', 'Complex', 'Computational Biology', 'Computer software', 'Computers', 'Computing Methodologies', 'Core Facility', 'Data', 'Data Analyses', 'Databases', 'Detection', 'Development', 'Devices', 'Discipline', 'Disputes', 'Doctor of Philosophy', 'Drug Formulations', 'Drug Industry', 'Education', 'Education and Outreach', 'Educational Curriculum', 'Educational process of instructing', 'Electrical Engineering', 'Electronics', 'Engineering', 'Ensure', 'Equilibrium', 'Equipment', 'Event', 'Evolution', 'Experimental Models', 'Facility Construction Funding Category', 'Faculty', 'Feedback', 'Figs - dietary', 'Foundations', 'Funding', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome Components', 'Goals', 'Grant', 'Gray unit of radiation dose', 'Head', 'High Performance Computing', 'High Pressure Liquid Chromatography', 'Historically Black Colleges and Universities', 'Home environment', 'Human', 'Human Resources', 'Hybrids', 'Image', 'Individual', 'Informatics', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internet', 'Interruption', 'Investments', 'Joints', 'Knowledge', 'Label', 'Lead', 'Leadership', 'Learning', 'Letters', 'Life', 'Link', 'Mammalian Cell', 'Manuals', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Metabolic', 'Methods', 'Microfluidics', 'Microscopy', 'Mining', 'Minority-Serving Institution', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Profiling', 'Molecular and Cellular Biology', 'Monitor', 'Mus', 'NCI Center for Cancer Research', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paper', 'Pathway Analysis', 'Pathway interactions', 'Peptide Sequence Determination', 'Performance', 'Phase', 'Phosphorylation', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Productivity', 'Property', 'Protein Analysis', 'Proteins', 'Proteomics', 'Protons', 'Publications', 'Purpose', 'Range', 'Reaction', 'Reliance', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resource Allocation', 'Resources', 'Risk', 'Role', 'Route', 'Running', 'Schools', 'Science', 'Scientist', 'Seeds', 'Semiconductors', 'Series', 'Services', 'Signal Transduction', 'Signaling Molecule', 'Soaps', 'Source', 'Specific qualifier value', 'Students', 'Supervision', 'Support of Research', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'To specify', 'Training', 'Training Programs', 'Transgenic Organisms', 'Underrepresented Minority', 'United States National Institutes of Health', 'Visit', 'Work', 'abstracting', 'anticancer research', 'base', 'chemical reaction', 'college', 'computer based statistical methods', 'computer science', 'computerized data processing', 'concept', 'data mining', 'data modeling', 'design', 'desire', 'drug discovery', 'experience', 'fluid flow', 'forging', 'fundamental research', 'instrument', 'instrumentation', 'interest', 'intracellular protein transport', 'member', 'microsystems', 'molecular modeling', 'network models', 'novel', 'novel strategies', 'open source', 'outreach', 'outreach program', 'programs', 'protein transport', 'research facility', 'research study', 'response', 'sensor', 'size', 'success', 'systems research', 'technology development', 'tool', 'virtual']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,P50,2007,2952859,-0.029830543169383484
"Systems Biology of Cell Decision Processes The remarkable complexity of biological systems demands a systematic approach to their analysis. The goal of this proposal is to establish an MIT Center of Excellence in Cell Decision Processes (CDP) around an interdisciplinary team of cell biologists, computer scientists and microsystems engineers tackling the systems biology of protein networks and signal transduction in mammalian cells. The basic hypothesis of the CDP Center is that understanding cell decision processes requires the development of network models that combine quantitative rigor with molecular detail. These models will be hybrids containing highly specific representations of critical reactions and abstract representations of the system as a whole. Effective models will endure and capture the accumulation of knowledge over time in a rigorous and portable format.  The CDP Center will follow a research paradigm that links systematic experiments and numerical modeling in a four-step feedback loop of manipulation-measurement-mining and modeling. The biological focus of the Center will be the signaling events that control apoptosis. The measurement of protein concentrations, modification state and activity will be undertaken for signaling molecules at many points in the apoptotic network. The measurements will then be analyzed using several modeling approaches ranging from highly specified to highly abstract.  To collect sufficient data for systematic modeling, the CDP Center will automate and standardize biochemical methods, develop compact array-based assays and design novel microfabricated devices with integrated microfluidics and label-free sensors. To organize and systematize the data, inforrnatic methods will be developed that support rigorous approaches to inference. Finally, physico-chemical, structure-systems and Bayesian models will be used to generate biological hypothesis for experimental verification.  The research activities of the CDP Center will be complemented by graduate and undergraduate education and by an outreach program targeted at the scientific community in general and minority-serving institutions in particular n/a",Systems Biology of Cell Decision Processes,7479966,P50GM068762,"['Accountability', 'Accounting', 'Address', 'Adopted', 'Algorithms', 'Amino Acid Sequence', 'Animals', 'Annual Reports', 'Apoptosis', 'Apoptotic', 'Appendix', 'Area', 'Artificial Intelligence', 'Attention', 'Authorship', 'Automation', 'Award', 'BCL2L11 gene', 'Belief', 'Binding', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Phenomena', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Engineering', 'Boxing', 'Budgets', 'Cancer Center', 'Cell Extracts', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Chemical Engineering', 'Chemical Structure', 'Chemicals', 'Child', 'Classification', 'Code', 'Collaborations', 'Color', 'Commit', 'Communities', 'Complement', 'Complex', 'Computational Biology', 'Computer software', 'Computers', 'Computing Methodologies', 'Core Facility', 'Data', 'Data Analyses', 'Databases', 'Detection', 'Development', 'Devices', 'Discipline', 'Disputes', 'Doctor of Philosophy', 'Drug Formulations', 'Drug Industry', 'Education', 'Education and Outreach', 'Educational Curriculum', 'Educational process of instructing', 'Electrical Engineering', 'Electronics', 'Engineering', 'Ensure', 'Equilibrium', 'Equipment', 'Event', 'Evolution', 'Experimental Models', 'Facility Construction Funding Category', 'Faculty', 'Feedback', 'Figs - dietary', 'Foundations', 'Funding', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome Components', 'Goals', 'Grant', 'Gray unit of radiation dose', 'Head', 'High Performance Computing', 'High Pressure Liquid Chromatography', 'Historically Black Colleges and Universities', 'Home environment', 'Human', 'Human Resources', 'Hybrids', 'Image', 'Individual', 'Informatics', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internet', 'Interruption', 'Investments', 'Joints', 'Knowledge', 'Label', 'Lead', 'Leadership', 'Learning', 'Letters', 'Life', 'Link', 'Mammalian Cell', 'Manuals', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Metabolic', 'Methods', 'Microfluidics', 'Microscopy', 'Mining', 'Minority-Serving Institution', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Profiling', 'Molecular and Cellular Biology', 'Monitor', 'Mus', 'NCI Center for Cancer Research', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paper', 'Pathway Analysis', 'Pathway interactions', 'Peptide Sequence Determination', 'Performance', 'Phase', 'Phosphorylation', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Productivity', 'Property', 'Protein Analysis', 'Proteins', 'Proteomics', 'Protons', 'Publications', 'Purpose', 'Range', 'Reaction', 'Reliance', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resource Allocation', 'Resources', 'Risk', 'Role', 'Route', 'Running', 'Schools', 'Science', 'Scientist', 'Seeds', 'Semiconductors', 'Series', 'Services', 'Signal Transduction', 'Signaling Molecule', 'Soaps', 'Source', 'Specific qualifier value', 'Students', 'Supervision', 'Support of Research', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'To specify', 'Training', 'Training Programs', 'Transgenic Organisms', 'Underrepresented Minority', 'United States National Institutes of Health', 'Visit', 'Work', 'abstracting', 'anticancer research', 'base', 'chemical reaction', 'college', 'computer based statistical methods', 'computer science', 'computerized data processing', 'concept', 'data mining', 'data modeling', 'design', 'desire', 'drug discovery', 'experience', 'fluid flow', 'forging', 'fundamental research', 'instrument', 'instrumentation', 'interest', 'intracellular protein transport', 'member', 'microsystems', 'molecular modeling', 'network models', 'novel', 'novel strategies', 'open source', 'outreach', 'outreach program', 'programs', 'protein transport', 'research facility', 'research study', 'response', 'sensor', 'size', 'success', 'systems research', 'technology development', 'tool', 'virtual']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,P50,2007,48090,-0.030282898958659103
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7111722,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2006,314029,-0.02015633356701105
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7125135,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2006,387181,-0.006220257833643909
"High-throughput annotation of glycan mass spectra     DESCRIPTION (provided by applicant): The correct functioning of many proteins depends on glycosylation, the addition of sugar molecules (glycans) to selected amino acids in the protein. For example, cancer cells have different glycosylation patterns than ordinary cells, and there is strong evidence that glycoproteins on the surface of egg cells play an essential role in sperm binding. Despite the importance of glycosylation, there are as yet no reliable, high-throughput methods for determining the identity and location of glycans. Glycan identification is currently a manual procedure for experts, involving a combination of chemical assays and mass spectrometry. The automation of the process would have a significant impact on our understanding of this important biological process. The proposed project aims to invent chemical procedures, algorithms, and software for high-throughput analysis of glycan mass spectrometry data. The goal is to bring glycan analysis up to the level of peptide analysis within 3 years. In contrast to peptide analysis, which can leverage genomics data, glycan analysis requires the incorporation of expert knowledge of synthetic pathways, in order to limit the huge number of theoretical combinations of monosaccharides to the much smaller number that are actually synthesized in nature. The project will have to develop novel representations for the evolving expert knowledge, because an exhaustive list- analogous to the human genome- is not currently known. Along with expert knowledge, the project will develop and validate machine learning and statistical techniques for glycan identification. In particular, the project will develop methods for internally calibrating spectra, and will learn fragmentation patterns that can statistically distinguish different types of glycosidic linkages.         n/a",High-throughput annotation of glycan mass spectra,7071180,R01GM074128,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'chemical structure', 'chemical synthesis', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'glycosylation', 'high throughput technology', 'mass spectrometry', 'mathematics', 'matrix assisted laser desorption ionization', 'polysaccharides', 'structural biology', 'technology /technique development']",NIGMS,PALO ALTO RESEARCH CENTER,R01,2006,324279,-0.013763925022365654
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7106109,K22LM008389,"['clinical research', 'heart arrest', 'model']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2006,135000,-0.025588133277304814
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7015019,R01NS051826,"['Alzheimer&apos', 's disease', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain morphology', 'human data', 'image enhancement', 'image guided surgery /therapy', 'magnetic resonance imaging', 'mathematical model', 'prenatal growth disorder']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2006,289590,-0.013773160977459259
"Classification Algorithms for Chemical Compounds Computational techniques that build models to correctly assign chemical compounds to various classes of interests have extensive applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or non-toxic, and filtering out drug-like compounds from large compound libraries. The overall goal of this proposal is to develop substructure-based classification algorithms for chemical compound datasets. The key elements of these algorithms are that they (i) utilize highly efficient substructure discovery algorithms to mine the chemical compounds and discover all substructures that can be critical for the classification task, (ii) use multiple criteria to generate a set of substructure-based features that simultaneously simplify the compounds' representation while retaining and exposing the features that are responsible for the specific classification problem, and (iii) build predictive models by employing kernel-based methods that take into account the relationships between these substructures at different levels of granularity and complexity, as well as information provided by traditional descriptors. n/a",Classification Algorithms for Chemical Compounds,7127208,R01LM008713,"['artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'conformation', 'drug classification', 'mathematical model', 'mathematics']",NLM,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,276362,-0.015001363327228037
"Computer System for Functional Analysis of Genomic Data    DESCRIPTION (provided by applicant): In two previous stages of this project, both funded by the National Institute of General Medical Sciences and carried out successfully, we developed GeneWays, a completely automated system that efficiently distills information about molecular interactions from an astronomical number of full-text biomedical articles. The next logical stage of the project is to carry this system from the computational laboratory into a practical, useful, and even indispensable tool that researchers can use to solve complex problems currently posed in experimental medicine and biology. The central hypothesis of our work on GeneWays has been that our computational tools will generate biological predictions of a quality sufficiently high that the biomedical community will invest in serious experimental validation. Specifically, we propose the following. 1. We will improve significantly the precision and recall of the GeneWays system. 2. We will develop and implement a probabilistic belief-network formalism?a belief-graph relative of the Bayesian network formalism that allows us to place and update beliefs on both the vertices and the edges of the graph for probabilistic reasoning over the large collection of facts in the GeneWays database. We will develop and implement a coordinated collection of methods for computing and updating beliefs on individual nodes and edges of the belief graph. 3. We will develop and implement a mathematical framework for incorporating pathway information into a genetic- linkage analysis formalism in such a way that each piece of pathway knowledge includes a specified degree of confidence. 4. We will process an enormous collection of texts, such as open-access biomedical journals, PubMed abstracts, and the GeneWays corpus, and thus will build a comprehensive GeneHighWays database. We will make the GeneHighWays database easily and freely accessible to academic researchers through a web interface. We will evaluate the new version of the GeneWays system and the GeneHighWays database for the quality of data, performance of the mathematical methods, and quality of the interface.           n/a",Computer System for Functional Analysis of Genomic Data,7148274,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,303688,-0.02993433562409526
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,7107885,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2006,385718,-0.012960793822708323
"Spatial Modeling in Glaucoma DESCRIPTION (provided by applicant): This career training proposal is to train Michael D. Twa, OD, MS as an independent clinician-scientist. A five year training program is proposed, consisting of formal coursework in vision science, specific training in computer science and image processing, and mentoring in the application of these skills to clinical outcomes research in glaucoma. In September 2003, NIH announced a new ""Roadmap"" to accelerate advances in biomedical research for the 21st century. Three areas listed in this Roadmap are relevant to this research proposal: (1) Interdisciplinary research training. (2) Clinical research informatics. (3) Development of enabling technologies for improved assessment of clinical outcomes. The Roadmap emphasizes coordinated strategies to develop both technological and human resources to take full advantage of multidisciplinary and translational research opportunities. This proposal addresses the stated training objectives at an individual level.  Glaucoma is a leading cause of blindness. Visual field assessment and optic nerve head imaging (confocal scanning laser tomography) are commonly used to diagnose the disease and monitor its progression, yet there is considerable controversy about how to interpret and make best use of this information. Currently, raw data from these observations are reduced to statistical indices that are meant to summarize clinically meaningful features and provide a basis for classifying test results as normal or not. Unfortunately, these indices may sacrifice other relevant features in the data for interpretability.  We will use mathematical modeling methods (polynomial modeling, spline fitting and wavelet analysis) to quantify patterns in visual field data and topographic images of the optic nerve head. We will use features derived from these modeling methods to apply novel pattern recognition techniques from computer and information sciences-decision trees and non-linear regression analysis-and then compare these techniques to current methods to identify glaucoma. By improving current methods of analysis we can provide a more quantitative basis for clinical decisions, and offer greater consistency and objectivity on data interpretation. The long-term objective of this proposal is to translate advances in computer and information sciences to the analysis of clinical outcomes research in glaucoma and other eye diseases. n/a",Spatial Modeling in Glaucoma,7015012,K23EY016225,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer assisted diagnosis', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'glaucoma', 'glaucoma test', 'human data', 'image processing', 'mathematical model', 'model design /development', 'neuroimaging', 'optic nerve', 'patient oriented research', 'tomography', 'visual fields']",NEI,OHIO STATE UNIVERSITY,K23,2006,142642,-0.03590349070760854
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7007706,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2006,153843,-0.0008234539938611731
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7012338,R01AI064913,"['B lymphocyte', 'T lymphocyte', 'allergens', 'aminoacid', 'antigens', 'artificial intelligence', 'bioinformatics', 'chemical property', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'dengue virus', 'immunoglobulin E', 'major histocompatibility complex', 'mathematical model', 'molecular biology information system', 'peptides', 'physical property', 'protein binding', 'stereochemistry', 'structural biology']",NIAID,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,2006,258041,-0.025310464235478555
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7056185,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,145987,-0.018092880234941107
"The RPI Exploratory Center for Cheminformatics (RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics (RMI),7125575,P20HG003899,"['Internet', 'NIH Roadmap Initiative tag', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2006,377226,0.0025574761126720413
"24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS    DESCRIPTION (provided by applicant):    This conference grant (R13) application requests funds to partially cover the cost of planning, organizing, publicizing and hosting the 24th Annual Symposium on Nonhuman Primate Models for AIDS. The symposium will be held October 4-7, 2006, at the Omni Hotel at CNN Center in downtown Atlanta, Georgia, and will be hosted by the Yerkes National Primate Research Center, Emory University. This meeting is the premier forum for the presentation and exchange of the most recent scientific advances in AIDS research utilizing the nonhuman primate model. The latest findings in primate pathogenesis, immunology, genomics, virology, vaccines and therapeutics will be presented. It is anticipated more than 300 scientists from the United States and other countries will attend. The symposium will encompass five half-day scientific sessions and an evening poster session. The scientific sessions will be: Virology, Pathogenesis, Immunology, Vaccines and Therapeutics/Genomics. Each session will have an invited Chair, a scientific leader in the field, who will give a 30-minute state-of-the-field presentation to open the session, and a Co-Chair from the Scientific Committee, who will moderate the session and entertain questions. In addition, there will be an invited keynote speaker and a banquet speaker, who will address scientific approaches and concerns regarding the global AIDS crisis and related issues of public health. A Scientific Program Committee consisting of eight-ten members drawn from the Yerkes/Emory community and other institutions will review abstracts and assign oral or poster presentations for each of the scientific sessions. Committee members will include leaders in the field from a variety of scientific disciplines. Criteria for selection of oral presentations will include relevance of the topic as well as originality and quality of the information contained in the abstract. Those giving talks will be invited to submit their presentations in manuscript form for publication in the Journal of Medical Primatology. A poster session will include meritorious presentations that cannot be accommodated in one of the platform sessions. A local Organizing Committee will handle arrangements and logistics for the symposium. Feedback from the participants will be obtained through written questionnaires or oral comments to members of the organizing committee. This format has been successfully followed using NCRR support for the previous Annual symposium.           n/a",24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS,7114527,R13RR022961,"['AIDS', 'Primates', 'disease /disorder model', 'meeting /conference /symposium', 'travel']",NCRR,EMORY UNIVERSITY,R13,2006,63089,-0.0036872124347641596
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7104243,U54EB005149,"['NIH Roadmap Initiative tag', 'bioimaging /biomedical imaging', 'bioinformatics', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2006,3809481,0.0054898891084355595
"Nation Center: Multi-Scale Study- Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7126050,U54CA121852,"['NIH Roadmap Initiative tag', 'bioinformatics', 'cell biology', 'computational biology', 'cooperative study', 'genome']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2006,3747227,0.0072927901178108144
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6916483,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,321788,-0.02015633356701105
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7052491,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2005,180862,-0.006220257833643909
"High-throughput annotation of glycan mass spectra     DESCRIPTION (provided by applicant): The correct functioning of many proteins depends on glycosylation, the addition of sugar molecules (glycans) to selected amino acids in the protein. For example, cancer cells have different glycosylation patterns than ordinary cells, and there is strong evidence that glycoproteins on the surface of egg cells play an essential role in sperm binding. Despite the importance of glycosylation, there are as yet no reliable, high-throughput methods for determining the identity and location of glycans. Glycan identification is currently a manual procedure for experts, involving a combination of chemical assays and mass spectrometry. The automation of the process would have a significant impact on our understanding of this important biological process. The proposed project aims to invent chemical procedures, algorithms, and software for high-throughput analysis of glycan mass spectrometry data. The goal is to bring glycan analysis up to the level of peptide analysis within 3 years. In contrast to peptide analysis, which can leverage genomics data, glycan analysis requires the incorporation of expert knowledge of synthetic pathways, in order to limit the huge number of theoretical combinations of monosaccharides to the much smaller number that are actually synthesized in nature. The project will have to develop novel representations for the evolving expert knowledge, because an exhaustive list- analogous to the human genome- is not currently known. Along with expert knowledge, the project will develop and validate machine learning and statistical techniques for glycan identification. In particular, the project will develop methods for internally calibrating spectra, and will learn fragmentation patterns that can statistically distinguish different types of glycosidic linkages.         n/a",High-throughput annotation of glycan mass spectra,6916805,R01GM074128,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'chemical structure', 'chemical synthesis', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'glycosylation', 'high throughput technology', 'mass spectrometry', 'mathematics', 'matrix assisted laser desorption ionization', 'polysaccharides', 'structural biology', 'technology /technique development']",NIGMS,PALO ALTO RESEARCH CENTER,R01,2005,353067,-0.013763925022365654
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,6916728,R01NS051826,"['Alzheimer&apos', 's disease', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain morphology', 'human data', 'image enhancement', 'image guided surgery /therapy', 'magnetic resonance imaging', 'mathematical model', 'prenatal growth disorder']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2005,292728,-0.013773160977459259
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6946761,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2005,100000,-0.028300939981835888
"Classification Algorithms for Chemical Compounds Computational techniques that build models to correctly assign chemical compounds to various classes of interests have extensive applications in pharmaceutical research and are used extensively at various phases during the drug development process. These techniques are used to solve a number of classification problems such as predicting whether or not a chemical compound has the desired biological activity, is toxic or non-toxic, and filtering out drug-like compounds from large compound libraries. The overall goal of this proposal is to develop substructure-based classification algorithms for chemical compound datasets. The key elements of these algorithms are that they (i) utilize highly efficient substructure discovery algorithms to mine the chemical compounds and discover all substructures that can be critical for the classification task, (ii) use multiple criteria to generate a set of substructure-based features that simultaneously simplify the compounds' representation while retaining and exposing the features that are responsible for the specific classification problem, and (iii) build predictive models by employing kernel-based methods that take into account the relationships between these substructures at different levels of granularity and complexity, as well as information provided by traditional descriptors. n/a",Classification Algorithms for Chemical Compounds,6965348,R01LM008713,"['artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'conformation', 'drug classification', 'mathematical model', 'mathematics']",NLM,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,283196,-0.015001363327228037
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6929696,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2005,206378,-0.005478186270566639
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,-0.030123313227952354
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6949109,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2005,395000,-0.012960793822708323
"Spatial Modeling in Glaucoma DESCRIPTION (provided by applicant): This career training proposal is to train Michael D. Twa, OD, MS as an independent clinician-scientist. A five year training program is proposed, consisting of formal coursework in vision science, specific training in computer science and image processing, and mentoring in the application of these skills to clinical outcomes research in glaucoma. In September 2003, NIH announced a new ""Roadmap"" to accelerate advances in biomedical research for the 21st century. Three areas listed in this Roadmap are relevant to this research proposal: (1) Interdisciplinary research training. (2) Clinical research informatics. (3) Development of enabling technologies for improved assessment of clinical outcomes. The Roadmap emphasizes coordinated strategies to develop both technological and human resources to take full advantage of multidisciplinary and translational research opportunities. This proposal addresses the stated training objectives at an individual level.  Glaucoma is a leading cause of blindness. Visual field assessment and optic nerve head imaging (confocal scanning laser tomography) are commonly used to diagnose the disease and monitor its progression, yet there is considerable controversy about how to interpret and make best use of this information. Currently, raw data from these observations are reduced to statistical indices that are meant to summarize clinically meaningful features and provide a basis for classifying test results as normal or not. Unfortunately, these indices may sacrifice other relevant features in the data for interpretability.  We will use mathematical modeling methods (polynomial modeling, spline fitting and wavelet analysis) to quantify patterns in visual field data and topographic images of the optic nerve head. We will use features derived from these modeling methods to apply novel pattern recognition techniques from computer and information sciences-decision trees and non-linear regression analysis-and then compare these techniques to current methods to identify glaucoma. By improving current methods of analysis we can provide a more quantitative basis for clinical decisions, and offer greater consistency and objectivity on data interpretation. The long-term objective of this proposal is to translate advances in computer and information sciences to the analysis of clinical outcomes research in glaucoma and other eye diseases. n/a",Spatial Modeling in Glaucoma,6863529,K23EY016225,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer assisted diagnosis', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'glaucoma', 'glaucoma test', 'human data', 'image processing', 'mathematical model', 'model design /development', 'neuroimaging', 'optic nerve', 'patient oriented research', 'tomography', 'visual fields']",NEI,OHIO STATE UNIVERSITY,K23,2005,143096,-0.03590349070760854
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7125331,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2005,152083,-0.0008234539938611731
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,6916795,R01AI064913,"['B lymphocyte', 'T lymphocyte', 'allergens', 'aminoacid', 'antigens', 'artificial intelligence', 'bioinformatics', 'chemical property', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'dengue virus', 'immunoglobulin E', 'major histocompatibility complex', 'mathematical model', 'molecular biology information system', 'peptides', 'physical property', 'protein binding', 'stereochemistry', 'structural biology']",NIAID,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,2005,264250,-0.025310464235478555
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6931476,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2005,392500,0.013094035150783103
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,6922406,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,174500,-0.018092880234941107
"The RPI Exploratory Center for Cheminformatics(RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics(RMI),7032113,P20HG003899,"['Internet', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2005,375639,0.0025574761126720413
"Computer cluster for computational biology DESCRIPTION (provided by applicant):    The present application aims to establish a computer Cluster for Computational Biology and Bioinformatic (CCBB). The cluster will consists of 256 dual nodes connected with Giganet switches to enable rapid communication between the processors. The cluster will enable the integration of the two approaches and make it possible to effectively address the highly demanding computational tasks of the field. It will serve a small group of investigators, supported by the NIH, and their close collaborators. The hardware needs of computational biology and bioinformatic applications, and of the team of investigators listed in this application can be summarized as follows:   1. Significant computer power for complex and expensive simulations.   2. Large storage capacity for the whole cluster (shared) and (separately) for the individual nodes.   3. Large and rapidly accessible memory for effective statistical analysis, application of machine learning techniques, and biological discovery.   4. Fast network for information updates across the network.   In addition CCBB will have high level of databases and software integration including   1. Updates of important ""mirrors"" of shared databases (such as NR, swissprot, human EST, human genome, protein databank, etc.)   2. Local installation and frequent upgrade of widely used software packages (e.g. BLAST, Pfam, CHARMm etc.)   3. Help in porting novel software for optimal use on the CCBB hardware platform.   The combined unification of optimal hardware and software for computational biology and bioinformatic will make the new cluster; an outstanding resource for NIH related research n/a",Computer cluster for computational biology,6877645,S10RR020889,"['bioinformatics', 'biomedical equipment purchase', 'computational biology', 'computer network', 'computer program /software', 'computer system hardware', 'computers']",NCRR,CORNELL UNIVERSITY ITHACA,S10,2005,500000,-0.0046904245450682495
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),6950028,U54EB005149,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2005,3800000,0.0054898891084355595
"Nation Center: Multi-Scale Study- Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7012104,U54CA121852,"['bioinformatics', 'cell biology', 'computational biology', 'cooperative study', 'genome']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2005,3758967,0.0072927901178108144
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6799187,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,321983,-0.02015633356701105
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6805962,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2004,100000,-0.028300939981835888
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6703756,R44CA093112,"['artificial intelligence', 'clinical research', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'mathematics', 'statistics /biometry']",NCI,"CYTEL, INC",R44,2004,411387,0.015697236946741928
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6783420,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2004,200515,-0.005478186270566639
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,-0.030123313227952354
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,-0.030123313227952354
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6797879,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2004,395000,-0.012960793822708323
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6721300,R37GM030998,"['DNA', 'artificial intelligence', 'biochemical evolution', 'computational neuroscience', 'computer assisted sequence analysis', 'computer simulation', 'gene frequency', 'genetic models', 'mathematical model', 'method development', 'model design /development', 'natural selections', 'nucleic acid sequence', 'species difference', 'statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2004,161792,-0.01184416496116517
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6787778,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2004,392500,0.013094035150783103
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6733529,R01LM007273,"['Internet', 'behavioral /social science research tag', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'confidentiality', 'data management', 'decision making', 'health care facility information system', 'health care policy', 'human data', 'human rights', 'information dissemination', 'information retrieval', 'mathematical model', 'medical records', 'model design /development', 'patient oriented research', 'statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2004,406979,-0.008767707084389441
"Tree Ensemble Regression and Classification Methods    DESCRIPTION (provided by applicant):    This SBIR aims to produce next generation classification and regression software based upon ensembles of decision trees: bagging, random forests, and boosting. The prediction accuracy of these methods has caused much excitement in the machine learning community, and both challenges and complements the data modeling culture prevalent among biostatisticians. Recent research extends the methodology to likelihood based methods used in biostatistics, leading to models for survival data and generalized forest models. Generalized forest models extend regression forests in the same way that generalized linear models extend linear models.      This software would apply broadly, including to medical diagnosis, prognostic modeling, and detecting cancer; and for modeling patient characteristics like blood pressure, discrete responses in clinical trials, and count data.      Phase I work will prototype software for survival data, and investigate the performance of ensemble methods on simulated and real data. For survival applications, we will assess out-of-bag estimates of performance, and investigate measures of variable importance and graphics that help clinicians understand the results. Experience writing prototypes and using them on data will lead to a preliminary software design that serves as the foundation of Phase II work.      Phase II will expand upon this work to create commercial software. We will research and implement algorithms for a wider range of applications including generalized forest models, classification, and least squares regression. We will also implement robust loss criteria that enable good performance on noisy data, and make adaptations to handle large data sets.      This proposed software will enable medical researchers to obtain high prediction accuracy, and complement traditional tools like discriminant analysis, linear and logistic regression models, and the Cox model.         n/a",Tree Ensemble Regression and Classification Methods,6832086,R43CA105724,"['clinical research', 'computer assisted medical decision making', 'computer graphics /printing', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'method development', 'model design /development', 'neoplasm /cancer classification /staging', 'neoplasm /cancer diagnosis', 'neoplasm /cancer remission /regression', 'prognosis', 'statistics /biometry']",NCI,INSIGHTFUL CORPORATION,R43,2004,99937,0.008266069472411432
"National Alliance for Medical Imaging Computing (RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance for Medical Imaging Computing (RMI),6847712,U54EB005149,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2004,100000,0.0054898891084355595
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6682996,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' automated data processing', ' chemical structure', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' crystallization', ' data collection methodology /evaluation', ' image processing', ' mathematics', ' method development', ' protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,298672,-0.02015633356701105
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6617906,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,UNIVERSITY OF NORTH CAROLINA CHAPEL HILL,R01,2003,170109,0.011489644555159884
"BioMediator: Biologic Data Integration & Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration & Analysis System,6681249,R01HG002288,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' information retrieval', ' molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2003,100000,-0.028300939981835888
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6587476,R44CA093112,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' human data', ' mathematical model', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R44,2003,400084,0.015697236946741928
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6658916,R33GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2003,200824,-0.005478186270566639
"Software to Handle Missing Values in Large Data DESCRIPTION (provided by applicant):    This SBIR aims to produce commercial software for handling missing data in large data sets, where the goal is data mining and knowledge discovery. There may be a large number of subjects, variables, or both. Examples include microarray data, surveys, genomic data, and high throughput screening data.      Handling missing data is one important step of careful data preparation, which is key to the success of an entire project. Missing values often arise in medical data. This is an obstacle because many data mining tools either require complete data or are not robust to missing data.      Principled methods of handling missing data are computationally intensive. Therefore computational feasibility is a challenge to handling missing values in large data sets.      Phase I work will explore strategies such as sampling, constraining parameters, and monotone data algorithms for model based techniques. Factor analysis and multivariate linear mixed effects models will be used to reduce the number of parameters. A variable-by-variable approach using a popular data mining technique, recursive partitioning, will also be used to impute missing values.      For each of the methods, we will write prototype software and test performance on missing data patterns simulated on real data. Several ad hoc techniques will serve as a baseline for comparison.   Experience writing prototypes and using them in simulations will lead to preliminary software design that will serve as the foundation of Phase II work.       This proposed software will enable medical researchers to gain more from their data mining efforts: maximally extracting information and achieving unbiased predictions, despite missing data. n/a",Software to Handle Missing Values in Large Data,6690119,R43RR017862,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' human data', ' mathematical model', ' statistics /biometry']",NCRR,INSIGHTFUL CORPORATION,R43,2003,99847,0.012009993716692941
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,-0.030123313227952354
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6663283,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2003,395000,-0.012960793822708323
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6635877,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2003,161792,-0.01184416496116517
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6646557,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2003,237000,-0.009585652488764347
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6690235,P20GM067650,"['animal tissue', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer simulation', ' computer system design /evaluation', ' data management', ' disease /disorder etiology', ' epidemiology', ' functional /structural genomics', ' gene expression', ' human subject', ' informatics', ' interdisciplinary collaboration', ' mathematical model', ' pharmacokinetics', ' science education', ' statistics /biometry', ' technology /technique development', ' therapy', ' training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2003,392500,0.013094035150783103
"Toxicological Evaluation Neuralnet Tools (TENT)  DESCRIPTION (provided by applicant):  YAHSGS' Toxicological Evaluation Neuralnet Tools (TEND is designed to advance the state-of-the-art in the prediction of toxicological end points for new or untested chemicals, drugs, and compounds. TENT deploys computational neural nets (CNN), innovative computational chemistry methods, and modem statistical regression methods into interactive modules that determine (a) a chemical's 3-D structure and physical chemistry properties, (b) Quantitative Structure Activity Relationships, (C) mechanistic modes leading to toxicological responses via microassay database analysis, and (d) a broad spectrum of toxicological properties via CNN 3-D structural similarity analyses. TENTs output includes physical chemistry properties, 3-D structure, predicted toxicological impacts, and confidence level associated with each. It is anticipated that TENT will become one of the primary tools used by (a) researchers in human health and toxicological fields, (b) pharmaceutical companies to screen out drugs early in the development process prior to expending hundreds of millions on clinical in vivo and in vitro testing, (C) by companies developing new chemicals, chemical compounds, and chemically treated materials to determine potential toxicological impacts including those caused by environmental changes during and after usage, (d) companies striving to show compliance with ISO 14000 for materials used in their products, and (e) federal and military organizations for chemicals and materials contemplated for use in their mission areas. Industry experts predict that the market for TENT-type tools and applications will reach $8 -$10 billion by 2006 and three times that amount by 2016. The benefits that the US should receive from TENT could include (a) a greatly enhanced understanding of potential toxicological impacts from pharmaceuticals, chemicals, and chemically treated materials (4 out of 5 chemicals in industrial use currently have not undergone adequate testing due to time and expense), (b) companies will avoid billions of dollars in clinical testing for chemicals and drugs that ultimately fail (the funds saved can be applied to the development of new and better materials that help mankind and the environment that might otherwise go unfunded), and (c) TENT can substantially reduce the number of laboratory animals used for clinical testing.   n/a",Toxicological Evaluation Neuralnet Tools (TENT),6550075,R43ES011918,"['alternatives to animals in research', ' chemical structure function', ' computational neuroscience', ' computer program /software', ' computer simulation', ' method development', ' microarray technology', ' molecular dynamics', ' neurotoxicology', ' statistics /biometry', ' three dimensional imaging /topography', ' toxicant screening']",NIEHS,"YAHSGS, LLC",R43,2003,84450,-0.0066124373886425145
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6620783,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2003,380761,-0.008767707084389441
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6636516,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2003,374063,-0.0027403836338328585
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6605420,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,UNIVERSITY OF NORTH CAROLINA CHAPEL HILL,R01,2002,174567,0.011489644555159884
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6513068,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,DANA-FARBER CANCER INSTITUTE,R01,2002,21871,0.011489644555159884
"Improving Quantum Chemistry Calculations DESCRIPTION (provided by applicant): We propose to extend the functionality of our commercial quantum chemistry program, Q-Chem, to effectively treat molecules containing transition metals. This enhanced capability will provide Q-Chem's end-users with the ability to accurately model complex molecules such as proteins, enzymes, and catalysts of industrial importance. While remarkable progress has been made over the last several years in the accurate modeling of systems containing transition metals, current numerical methods for achieving SCF convergence in these systems are problematic at best, resulting in long execution times or, in some cases, complete failure to find a solution. However, a novel computational technique developed at Q-Chem has been shown to dramatically improve convergence for organic molecules with known SCF convergence problems. We propose to adapt this method for use with transition metals. Our goal is to achieve the same robust SCF convergence that is realized for most organic molecules, thereby greatly increasing productivity and extending the capability of scientists to study molecules such as enzymes and industrial catalysts. During Phase (I, our efforts will be to further extend Q-Chem's capability in the molecular biology arena. This proposal seeks to improve the quantum chemical treatment of molecular systems containing transition metals. Transition metal elements are essential to natural biological processes. The technology developed in this research will enable the computer modeling of those systems that are difficult to handle with the current methodologies and therefore increase of the applications of computational modeling. PROPOSED COMMERCIAL APPLICATION: Transition-metal elements play a vital role in biological systems.  The success of this project will improve the performance of modeling of transition-metal complexes and making the modelings possible for the systems that current algorithms fail.  The resulting work will be made available to researchers in health industry and universities through our commercial software Q-Chem. n/a",Improving Quantum Chemistry Calculations,6484828,R43GM065617,"['artificial intelligence', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' heavy metals', ' mathematical model', ' mathematics', ' model design /development', ' quantum chemistry']",NIGMS,"Q-CHEM, INC.",R43,2002,109642,-0.0010463055058259683
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6549345,R21GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R21,2002,99510,-0.005478186270566639
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6490198,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2002,331492,-0.022898330835905575
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6519073,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2002,161792,-0.01184416496116517
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6554738,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' method development', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2002,395000,-0.012960793822708323
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6526274,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2002,237000,-0.009585652488764347
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6520234,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2002,163400,0.02264773031394493
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6421732,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2002,384388,-0.008767707084389441
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6520333,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2002,363261,-0.0027403836338328585
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6525584,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2002,128860,0.003257172498596203
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6391275,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,215487,-0.023572746217509453
"Markov Chain Monte Carlo and Exact Logistic Regression   DESCRIPTION (provided by applicant): Logistic regression is a very popular           model for the analysis of binary data with widespread applicability in the           physical, behavioral and biomedical sciences. Parameter inference for this           model is usually based on maximizing the unconditional likelihood function.          However unconditional maximum likelihood inference can produce inconsistent          point estimates, inaccurate p-values and inaccurate confidence intervals for         small or unbalanced data sets and for data sets with a large number of               parameters relative to the number of observations. Sometimes the method fails        entirely as no estimates can be found that maximize the unconditional                likelihood function. A methodologically sound alternative approach that has          none of the aforementioned drawbacks is the exact conditional approach in which      one generates the permutation distributions of the sufficient statistics for         the parameters of interest conditional on fixing the sufficient statistics of        the remaining nuisance parameters at their observed values. The major stumbling      block to this approach is the heavy computational burden it imposes. Monte           Carlo methods attempt to overcome this problem by sampling from the reference        set of possible permutations instead of enumerating them all. Two competing          Monte Carlo methods are network based sampling and Markov Chain Monte Carlo          (MCMC) sampling. Network sampling suffers from memory limitations while MCMC         sampling can produce incorrect results if the Markov chain is not ergodic or if      the process is not in the steady state. We propose a novel approach which            combines the network and MCMC sampling, draws upon the strengths of each of          them and overcomes their individual limitations. We propose to implement this        hybrid network-MCMC method in our LogXact software and as an external procedure      in the SAS system.                                                                   PROPOSED COMMERCIAL APPLICATION:  There is great demand for logistic regression software that can handle small, sparse or  unbalanced data sets by exact methods.  Our LogXact package is the only software that  can provide exact inference for data sets which are not ""toy problems"".  Yet even  LogXact quickly breaks down on moderate sized problems.  The new generation of hybrid  network-MCMC algorithms will handle substantially larger problems that nevertheless need  exact inference.  The commercial potential is considerable since such data sets are common  in scientific studies.                                                                                      n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6404971,R43CA093112,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R43,2001,113111,0.017007660695658153
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6343026,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2001,376147,-0.022898330835905575
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6326240,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,DANA-FARBER CANCER INSTITUTE,R01,2001,183883,0.011489644555159884
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6385455,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2001,161792,-0.01184416496116517
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6401728,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2001,237000,-0.009585652488764347
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6387141,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2001,163400,0.02264773031394493
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6484619,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2001,337739,-0.0027403836338328585
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6385653,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2001,127154,0.003257172498596203
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6185231,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,213046,-0.023572746217509453
"SELECTING AMONG MATHEMATICAL MODELS OF COGNITION DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling       of cognition, it is important to have well-justified criteria for choosing       among differing explanations (i.e., models) of observed data.  This project      investigates those criteria as well as their instantiation in five model         selection methods.                                                                                                                                                Two lines of research will be undertaken.  In the first, a thorough              investigation of model complexity will be conducted.  Comprehensive              simulations re intended to determine complexity's contribution to model fit      and to model selection.  An analytical solution will also be sought with the     hope of quantifying model complexity.                                                                                                                             The second line of work examines the utility of each of the five selection       methods in choosing among models in three topic areas in cognitive               psychology (information integration, categorization, connectionist               modeling), the end goal being to identify their merits and shortcomings.                                                                                          Findings should provide a better understanding of model selection than           currently available and serve as a useful guide for researchers comparing        the suitability of quantitative models of cognition.                                                                                                               n/a",SELECTING AMONG MATHEMATICAL MODELS OF COGNITION,6185788,R01MH057472,"['artificial intelligence', ' choice', ' cognition', ' computer simulation', ' information dissemination', ' mathematical model', ' psychometrics']",NIMH,OHIO STATE UNIVERSITY,R01,2000,77332,0.0031997873374644916
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6071498,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2000,364001,-0.022898330835905575
"ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS Predictive models that generate estimate probabilities for medical outcomes have become widely used in health services research, in health policy, and increasingly, for the assessment of health care and for real-time decision support.  Logistic regression models for medical events are central to most probabilistic predictive clinical decision aids and are fundamental to comparative analyses of medical care based on risk-adjusted events.  In such applications, inaccurate assessment of patient risk can have significant health care and health policy implications. New computer-based modeling techniques including generalized additive models, classification trees, and neural networks may potentially capture information that regression methods may miss or misrepresent.  However, these methods use very local information in model construction and may be overfit to the sample data and thus not transport well to new settings.  In years 1-3, we investigated the relative accuracy of predictions made by these modeling methods under a variety of data structures, including the presence of outliers and missing data. For many of these data structures we found that the more ""local"" procedures frequently did not generalize to new test data as well as traditional regression methods.  However, our results suggest that as sample size and data complexity increases the performance of these procedures may substantially improved. Thus, to test these findings under more general conditions, we now propose two additional years of research to 1) rigorously assess the relative predictive performance and transportability of other new innovative modeling methods and of original hybrid model construction methods; 2) systematically investigate the relative predictive performance and model transportability of modeling methods applied to large and complex data structures; and 3) explore and assess procedures for handling outliers and missing data for classification trees and neural networks. The completion of the proposed work will result in the first systematic exploration of the factors affecting the predictive performance of the major modeling methods used to predict medical outcomes, and the comparative performance of models constructed by these methods on the extremely large data sets of the type that are becoming increasing available to researchers.  n/a",ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS,6185210,R01LM005607,"['artificial intelligence', ' computational neuroscience', ' computer assisted medical decision making', ' computer simulation', ' health care facility information system', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' outcomes research', ' prognosis', ' statistics /biometry']",NLM,TUFTS MEDICAL CENTER,R01,2000,270618,-0.013012047760597027
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6131906,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2000,152928,-0.01184416496116517
"CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be       taken to advance our understanding of the genetic and associated co-             variates affecting the inheritance of complex diseases. To that end, this        proposal will develop improved quantitative methods to detect genetic            factors contributing to increased susceptibility to complex disorders and        implement these methods in software for distribution to the research             community.                                                                                                                                                        The methods will concentrate on the use of classification techniques             applied to allele sharing data and other risk factors which affect the           trait. Allele sharing methods for mapping genes will be extended to              include the classification methods known as latent class models, cluster         analysis, and artificial neural networks, as well as a novel use of              logistic regression Co-variates such as gender, parental diagnosis, or           other concomitant factors will be systematically studied through                 applications to both stimulated and existing data sets. An additional goal       is to determine the optimal distribution of relative pairs (e.g. siblings,       first cousins) for these methods. Of great importance to this proposal is        the development of well-documented, user-friendly software and                   documentation which will be distributed to the scientific community via          the Internet. Existing software developed by the PI will be extensively          expanded for latent class models. Existing cluster analysis software will        be modified and combined for ease of use.                                                                                                                         This proposal consists of theoretical exploration, computer simulation,          data analysis, and software development. First, solutions of theoretical         questions relating to classification techniques will be pursued; second,         adaptation of computer programs to implement the analytic methods, and           investigation into alternative research strategies will be accomplished.         The new strategies will be applied to stimulated data, and finally, to           existing data sets of pedigrees in which a complex trait has been                diagnosed. Findings from this research may contribute to the ability to          locate susceptibility loci in complex traits and to the clarification of         those etiological mechanisms responsible for susceptibility.                      n/a",CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI,6168495,R01AA012239,"['alleles', ' analytical method', ' artificial intelligence', ' biomedical resource', ' computer program /software', ' computer simulation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' family genetics', ' gene environment interaction', ' gene expression', ' genetic disorder', ' genetic disorder diagnosis', ' genetic mapping', ' genetic markers', ' genetic susceptibility', ' human data', ' mathematical model', ' model design /development', ' quantitative trait loci', ' statistics /biometry']",NIAAA,WASHINGTON UNIVERSITY,R01,2000,180260,-0.0027807896410373335
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6090912,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2000,214602,0.02264773031394493
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6193019,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN,R01,2000,478050,-0.0027403836338328585
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6180399,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2000,150497,0.003257172498596203
"Deep learning for population genetics Project Summary The revolution in genome sequencing technologies over the past 15 years has created an explosion of population genomic data but has left in its wake a gap in our ability to make sense of data at this scale. In particular, whereas population genetics as a field has been traditionally data-limited, the massive volume of current sequencing means that previously unanswerable questions may now be within reach. To capitalize on this flood of information we need new methods and modes of analysis.  In the past 5 years the world of machine learning has been revolutionized by the rise of deep neural networks. These so-called deep learning methods offer incredible flexibility as well as astounding improvements in performance for a wide array of machine learning tasks, including computer vision, speech recognition, and natural language processing. This proposal aims to harness the great potential of deep learning for population genetic inference.  In recent years our group has made great strides in using supervised machine learning for population genomic analysis (reviewed in Schrider and Kern 2018). However, this work has focused primarily on using more traditional machine learning methods such as random forests. As we argue in this proposal, DNA sequence data are particularly well suited for modern deep learning techniques, and we demonstrate that the application of these methods can rapidly lead to state-of-the-art performance in very difficult population genetic tasks such as estimating rates of recombination. The power of these methods for handling genetic data stems in part from their ability to automatically learn to extract as much useful information as possible from an alignment of DNA sequences in order to solve the task at hand, rather than relying on one or more predefined summary statistics which are generally problem-specific and may omit information present in the raw data.  In this proposal we lay out a systematic approach for both empowering the field with these tools and understanding their shortcomings. In particular, we propose to design deep neural networks for solving population genetic problems, and incorporate successful networks into user-friendly software tools that will be shared with the community. We will also investigate a variety of methods for estimating the uncertainty of predictions produced by deep learning methods; this area is understudied in machine learning but of great importance to biological researchers who require an accurate measure of the degree of uncertainty surrounding an estimate. Finally, we will explore the impact of training data misspecification—wherein the data used to train a machine learning method differ systematically from the data to which it will be applied in practice. We will devise techniques to mitigate the impact of such misspecification in order to ensure that our tools will be robust to the complications inherent in analyzing real genomic data sets. Together, these advances have the potential to transform the methodological landscape of population genetic inference. Project Narrative Deep learning has revolutionized such disparate fields as computer vision, natural language processing, and speech recognition. In this proposal we aim to harness the great potential of deep learning for population genetic inference. We will design, implement, and apply novel deep learning methods and provide open source software for others to both use and build upon, thereby producing valuable tools for the genetics researchers at large.",Deep learning for population genetics,9976348,R01HG010774,"['Algorithms', 'Area', 'Biological', 'Biology', 'Classification', 'Code', 'Communities', 'Computer Vision Systems', 'Computer software', 'DNA Sequence', 'Data', 'Development', 'Ensure', 'Floods', 'Genetic', 'Genetic Recombination', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Image', 'Lead', 'Learning', 'Left', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Natural Language Processing', 'Natural Selections', 'Nature', 'Performance', 'Population', 'Population Explosions', 'Population Genetics', 'Process', 'Program Development', 'Publishing', 'Research Personnel', 'Sequence Alignment', 'Software Tools', 'Techniques', 'Technology', 'Training', 'Trees', 'Uncertainty', 'Ursidae Family', 'Work', 'base', 'computational chemistry', 'convolutional neural network', 'deep learning', 'deep neural network', 'design', 'empowered', 'flexibility', 'genetic information', 'genome sequencing', 'genomic data', 'infancy', 'innovation', 'learning classifier', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'network architecture', 'neural network', 'neural network architecture', 'next generation', 'novel', 'open source', 'random forest', 'recurrent neural network', 'research and development', 'speech recognition', 'statistics', 'stem', 'success', 'supervised learning', 'tool', 'tool development', 'user friendly software']",NHGRI,UNIVERSITY OF OREGON,R01,2020,529154,-0.009624854735888045
"MegaTox for analyzing and visualizing data across different screening systems Project Summary Computational toxicology aims to use rules, models and algorithms based on prior data for specific endpoints, to enable the prediction of whether a new molecule will possess similar liabilities or not. Our recent efforts have used sources like PubChem and ChEMBL to build predictive models for different toxicity-related and drug discovery endpoints. Our Phase I SBIR proposal called MegaTox will provide toxicity machine learning models developed with different algorithms for 40-50 in vitro and in vivo toxicity datasets. We propose using this technology to generate machine learning models for predicting potential compounds against either TGF- a target for countering chlorine induced lung inflammation as well as the adenosine A1 receptor to identify agonists as potential anticonvulsants. In addition, we can also compile molecules that can reactivate acetylcholinesterase which would enable the potential to discover medical countermeasures to address nerve agent and pesticide poisoning. We will access multiple machine learning approaches and validate these Bayesian or other machine learning models (including Linear Logistic Regression, AdaBoost Decision Tree, Random Forest, Support Vector Machine and deep neural networks (DNN) of varying depth) with our own in-house technology for these selected targets. We will aim for ROC values greater than 0.75 and MCC and F1 scores that are acceptable (>0.3). These models will be used to virtually screen FDA approved drugs, clinical candidates, commercially available drugs or other molecules. We will select up to 50 molecules to be tested using in vitro assays alongside controls for each target. These combined efforts should in the first instance provide commercially viable treatments which will be used to experimentally validate our computational models that can be shared with the medical countermeasures scientific community. In summary, we are proposing to build and validate models for targets based on public databases, select compounds for testing, create proprietary data and use this as a starting point for further optimization of compounds if needed. Our goal is to identify at least one promising compound for each target that we then pursue and protect our IP. We will pursue additional grant funding to take these medical countermeasures through additional in vitro and in vivo preclinical studies. Ultimately, we will license our products to larger companies for development prior to clinical trials. Project Narrative There is an urgent need to develop medical countermeasures (MCM) to address pulmonary agents, nerve agents and organophosphorus pesticides. Our approach leverages public and private data to build machine learning models for different targets involved in the physiological effects of the aforementioned agents. We then use these computational models to select new molecules to test in vitro. Our approach builds on our MegaTox approach focused on modeling toxicology targets to specifically focus on identifying compounds for TGF-β and Adenosine A1 as well as potential AChE reactivators. This computational approach will be validated using in vitro testing and offers several advantages to identify potential novel or repurposed molecules as MCM including speed and cost-effectiveness.",MegaTox for analyzing and visualizing data across different screening systems,10094026,R43ES031038,"['Acetylcholinesterase', 'Ache', 'Address', 'Adenosine', 'Adenosine A1 Receptor', 'Agonist', 'Algorithms', 'Anticonvulsants', 'Chlorine', 'Clinical Trials', 'Communities', 'Computer Models', 'Data', 'Data Set', 'Databases', 'Decision Trees', 'Development', 'FDA approved', 'Funding', 'Goals', 'Grant', 'In Vitro', 'Licensing', 'Logistic Regressions', 'Lung Inflammation', 'Machine Learning', 'Modeling', 'Pesticides', 'Pharmaceutical Preparations', 'Phase', 'Physiological', 'Privatization', 'PubChem', 'Small Business Innovation Research Grant', 'Source', 'Speed', 'System', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Transforming Growth Factor alpha', 'Transforming Growth Factor beta', 'base', 'clinical candidate', 'computational toxicology', 'cost effectiveness', 'deep neural network', 'drug discovery', 'in vitro Assay', 'in vitro testing', 'in vivo', 'medical countermeasure', 'nerve agent', 'novel', 'pesticide poisoning', 'preclinical study', 'predictive modeling', 'pulmonary agents', 'random forest', 'screening', 'support vector machine', 'virtual']",NIEHS,"COLLABORATIONS PHARMACEUTICALS, INC.",R43,2020,124915,-0.021135542606627093
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10224492,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,233900,0.003249296768356611
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10017950,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,513041,0.003249296768356611
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,9952803,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2020,92359,-0.012412718412974278
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9979659,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'MeSH Thesaurus', 'Measures', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'large scale data', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'public repository', 'specific biomarkers']",NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2020,467177,0.001539555166720194
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9941090,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2020,605875,-0.005069713361399266
"Center for Machine Learning in Urology PROJECT SUMMARY We propose to establish an Exploratory Center for Interdisciplinary Research in Benign Urology at the Children’s Hospital of Philadelphia (CHOP) and the University of Pennsylvania (Penn), the central mission of which is to apply machine learning to improve the understanding of the pathophysiology, diagnosis, risk stratification, and prediction of treatment responses of benign urological disease among children and adults. The proposed CHOP/Penn Center for Machine Learning in Urology (CMLU) addresses critical structural and scientific barriers that impede the development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. Structurally, urologic research occurs in silos, with little interaction among investigators that study different diseases or different populations (e.g. pediatric and adult). Scientifically, analysis of imaging and other types complex data is limited by inter-observer variability, and incomplete utilization of available information. This proposal overcomes these barriers by applying cutting-edge approaches in machine learning to analyze CT images that are routinely obtained for evaluation of individuals with kidney stone disease. Central to the CHOP/Penn CMLU is the partnership of urologists and experts in machine learning, which will bring a new approach to generating knowledge that advances research and clinical care. In addition, the CMLU will expand the urologic research community by providing a research platform and standalone machine learning executables that could be applied to other datasets. The Center’s mission will be achieved through the following Aims, with progress assessed through systematic evaluation: Aim 1. To expand the research base investigating benign urological disease. We will establish a community with the research base, particularly with the KURe, UroEpi programs, other P20 Centers, and O’Brien Centers. We will build this community by providing mini-coaching clinics to facilitate application of machine learning to individual projects, developing an educational hub for synchronous and asynchronous engagement with the research base, and making freely available all source codes and standalone executables for all machine learning tools. Aim 2. To improve prediction of ureteral stone passage using machine learning of CT images. The CMLU has developed deep learning methods that segment and automate measurement of urinary stones and adjacent renal anatomy. In the Research Project, we will compare these methods to existing segmentation methods and the current gold standard of manual measurement. We will then extract informative features from thousands of CT scans to predict the probability of spontaneous passage of ureteral stones for children and adults evaluated in the CHOP and Penn healthcare systems. Aim 3. To foster collaboration in benign urological disease research across levels of training and centers through an Educational Enrichment Program. We will amplify interactions across institutions and engage investigators locally and nationally by providing summer research internships, and interinstitutional exchange program, and an annual research symposium. PROJECT NARRATIVE The proposed CHOP/Penn O’Brien Center for Machine Learning in Urology addresses critical structural and scientific barriers that impede development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. This application overcomes these barriers by applying cutting- edge approaches in machine learning to analyze complex imaging data for individuals with kidney stone disease.The Center’s strategic vision of using machine learning to generate knowledge that improves diagnosis, risk stratification strategies, and prediction of outcomes among children and adults will be achieved through the implementation of a Educational Enrichment Program and a Research Project.",Center for Machine Learning in Urology,10133362,P20DK127488,"['Address', 'Adult', 'Algorithms', 'Anatomy', 'Area', 'Benign', 'Characteristics', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Clinical Investigator', 'Code', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Educational Status', 'Evaluation', 'Fostering', 'Functional disorder', 'Funding', 'Future', 'Gold', 'Healthcare Systems', 'Image', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Internships', 'Interobserver Variability', 'Investigation', 'Kidney', 'Kidney Calculi', 'Knowledge', 'Lead', 'Longevity', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Mission', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pediatric Hospitals', 'Pennsylvania', 'Philadelphia', 'Population', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Probability', 'Publishing', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk stratification', 'Site', 'Source Code', 'Structure', 'Students', 'Techniques', 'United States National Institutes of Health', 'Universities', 'Urinary Calculi', 'Urologic Diseases', 'Urologist', 'Urology', 'Vision', 'Visit', 'X-Ray Computed Tomography', 'base', 'clinical care', 'complex data ', 'deep learning', 'deep neural network', 'design', 'experience', 'feature selection', 'human error', 'improved', 'interdisciplinary collaboration', 'interest', 'learning strategy', 'novel strategies', 'outcome prediction', 'peer', 'programs', 'routine imaging', 'senior faculty', 'skills', 'summer research', 'symposium', 'tool', 'urologic', 'web page']",NIDDK,CHILDREN'S HOSP OF PHILADELPHIA,P20,2020,358890,-0.03254314170795866
"Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points Project Summary Biomedical data science data modeling is relevant to a plethora of informatics research activities, such as natural language processing, machine learning, artificial intelligence, and predictive analytics. As Electronic Health Record systems become more advanced and more mature, with the potential to incorporate a wide and diverse array of data from genomics to mobile health (mHealth) applications, the scope and nature of the biomedical data science questions researchers ask become broader. Concomitantly, the answers to their questions have the potential to impact the care of millions of patients—getting the answers right, proactively, is high stakes. However, in data modeling currently, there is no bioethics framework to guide the process of mapping key decision points and recording the rationale for choices made. Making data modeling decision points, as well as the reasoning behind them, explicit would have a twofold impact on improving biomedical data science by: 1. Enhancing transparency and reproducibility and maximizing the value of data science research and 2. Supporting the ability to assess decision points and rationales in terms of their most crucial ethical ramifications. Research in this area is particularly timely amid the interest in, and enthusiasm for, leveraging Big Data sources in the service of improving patient population health and the health of the general public. The National Institutes of Health (NIH) recently released a strategic plan for data science; there is no better time than now to create an initial bioethical framework to inform common data modeling decision points. The improvements in data quality that will derive from decision point mapping and bioethical review will enhance efforts to apply data models across a range of high-impact areas, from predictive analytics to support clinical decision-making to robust trending models in population health to better inform local, regional, and national health policies and resource allocation. To develop this initial bioethics framework, we will use well- established qualitative research methods (interviews, focus groups, and in-person deliberation) to map the decision points in biomedical data modeling research and document the rationales invoked to support those decisions (Aim 1 key informant interviews); assess those data science decision points and decision-making rationales for their bioethical ramifications (Aim 2 focus groups); and create an initial bioethics data modeling framework (Aim 3 deliberative meeting). This study would be the first to provide a bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe. This approach directly supports core scientific values of inclusivity, transparency, accountability, and reproducibility that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global. Project Narrative This study would be the first to develop an initial bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe—not least because poorly developed data models have the potential to impact adversely the health of individuals, groups, and communities. Currently, there is limited conversation around potential bioethics issues in data modeling, and as yet no implementable guidance on how biomedical data science modeling research activities should occur. The initial ethics framework developed by this study would provide a roadmap to ensure that data modeling decision points are documented and their ethical ramifications considered at the outset of model creation, thus supporting core scientific values of inclusivity, accountability, reproducibility, and transparency that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global.",Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points,10039527,R21HG011277,"['Accountability', 'Address', 'Area', 'Artificial Intelligence', 'Big Data', 'Bioethical Issues', 'Bioethics', 'Bioethics Consultants', 'Caring', 'Clinical', 'Communities', 'Data', 'Data Science', 'Data Scientist', 'Data Sources', 'Decision Making', 'Development', 'Electronic Health Record', 'Ensure', 'Ethical Issues', 'Ethical Review', 'Ethics', 'Focus Groups', 'Fostering', 'General Population', 'Health', 'Health Resources', 'Health system', 'Individual', 'Informatics', 'Interview', 'Machine Learning', 'Maps', 'Methods', 'Mobile Health Application', 'Modeling', 'National Health Policy', 'Natural Language Processing', 'Nature', 'Output', 'Patients', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Qualitative Research', 'Reproducibility', 'Research', 'Research Activity', 'Research Methodology', 'Research Personnel', 'Resource Allocation', 'Role', 'Services', 'Social Environment', 'Strategic Planning', 'Structure', 'System', 'Time', 'Trust', 'United States National Institutes of Health', 'Walking', 'base', 'biomedical data science', 'clinical decision support', 'clinical decision-making', 'data modeling', 'data quality', 'data tools', 'ethical legal social implication', 'genomic data', 'high standard', 'improved', 'individual patient', 'informant', 'interest', 'interoperability', 'meetings', 'model development', 'patient population', 'population health', 'programs', 'public trust', 'tool', 'trend', 'usability']",NHGRI,"HASTINGS CENTER, INC.",R21,2020,100000,-0.00023250337339247298
"Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine Project Summary  The objective of this proposal is to provide a robust course of training for Gilmer Valdes, PhD, DABR, a candidate with an excellent foundation in clinical and machine learning research, to enable him to become an independent investigator. The proposed research aims to address a tradeoff between interpretability and accuracy of modern machine learning algorithms which limits their use in clinical practice. The candidate’s central hypothesis is that the current tradeoff is not a law of nature but rather a limitation of current interpretable machine learning algorithms. Towards proving this hypothesis, the candidate, leading a multidisciplinary team, have developed unique mathematical frameworks (MediBoost and the Conditional Interpretable Super Learner) to build interpretable and accurate models. The proposed research will I) implement and extensively benchmark these frameworks and II) use the algorithms develop to solve three clinical problems where potentially suboptimal models are currently used to make clinical decisions: 1) predicting mortality in the Intensive Care Unit, 2) predicting risk of Hospital Acquired Venous Thromboembolism, 3) predicting which prostate cancer patients benefit the most from adjuvant radiotherapy. The candidate’s training and research plan, multidisciplinary by nature, takes advantage of the proximity of UC San Francisco, Stanford and UC Berkeley and proposes a training plan that cannot be easily replicated elsewhere. Recognizing the multidisciplinary nature of the work proposed, the author will be mentored and work closely with a stellar committee from three institutions and different scientific areas (Machine Learning, Biostatistics, Statistics, Hospital Medicine, Cancer Research and Quality Assurance in Medicine): Jerome H. Friedman PhD (Stanford Statistics Department), Mark Van der Laan PhD (Berkeley Biostatistics and Statistics Department), Mark Segal (UCSF Epidimiology and Biostatistics Deparments), Andrew Auerbach MD (UCSF Medicine Department), Felix Y. Feng MD (UCSF Radiation Oncology),and Timothy D. Solberg PhD (UCSF Radiation Oncology). This committee will be coordinated by Dr Solberg. The candidate also counts with a strong a multidisciplinary team of collaborators. Successful completion of the proposed research will develop the next generation of accurate and interpretable Machine Learning algorithms and solve three important clinical problems where linear models are currently used in clinical settings. This proposal has wide-ranging implications across the healthcare spectrum. The intermediate-term goal is for the candidate to acquire the knowledge, technical skills and expertise necessary to submit a successful R01 proposal. PROJECT NARRATIVE Current state of the art machine learning algorithms have a marked tradeoff between accuracy and interpretability. In medicine, where errors can have a dire consequence and knowledge representation and validation is as relevant as accuracy, the development of accurate and interpretable algorithms is of paramount importance. My research project will address a critical public health need by developing machine learning algorithms that are both accurate and interpretable, and apply them to solve specific clinical problems.",Development of Accurate and Interpretable Machine Learning Algorithms for their application in Medicine,9989861,K08EB026500,"['Address', 'Adjuvant Radiotherapy', 'Algorithms', 'Area', 'Benchmarking', 'Biometry', 'Cancer Patient', 'Classification', 'Clinical', 'Collection', 'Communities', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Doctor of Philosophy', 'Foundations', 'Goals', 'Healthcare', 'Hospitals', 'Institution', 'Intensive Care Units', 'Knowledge', 'Label', 'Laws', 'Libraries', 'Limb structure', 'Linear Models', 'Machine Learning', 'Malignant neoplasm of prostate', 'Mathematics', 'Mediating', 'Medical', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Modernization', 'Nature', 'Patient Triage', 'Patients', 'Performance', 'Physicians', 'Pneumonia', 'Polynomial Models', 'Public Health', 'Radiation Oncology', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk', 'San Francisco', 'Survival Analysis', 'Technical Expertise', 'Testing', 'Training', 'Trees', 'Validation', 'Work', 'anticancer research', 'artificial neural network', 'asthmatic patient', 'classification trees', 'clinical decision-making', 'clinical practice', 'design', 'improved', 'information organization', 'machine learning algorithm', 'medical specialties', 'mortality', 'multidisciplinary', 'neural network', 'next generation', 'novel', 'quality assurance', 'random forest', 'regression trees', 'standard care', 'statistics', 'structured data', 'task analysis', 'theories', 'venous thromboembolism']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2020,182232,-0.02506039993886277
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,10020414,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'feature selection', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2020,388750,-0.010263360100326944
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy Project Summary/Abstract Artificial intelligence on genomic/healthcare data that is performed jointly between multiple collaborating institutions relies on a trust model but can accelerate genomic medicine research and facilitate quality improvement. To conduct such machine learning while protecting patient privacy and reducing security risks, we are developing blockchain-based privacy-preserving learning methods in a K99/R00 study supported by the National Human Genome Research Institute (NHGRI). However, our previous design of privacy-preserving learning on private blockchain assumed “semi-honesty” as the underlying adversary assumption. That is, we assume that each participating site is curious yet very careful and honest, such that it would only submit correct predictive models. Nevertheless, in real world this assumption may be too optimistic; the models submitted could be an old one due to network latency or malicious users may try to create fake models, which can in turn lead to bioethical concerns and reduce the incentives for genomic/clinical institutions to participate in the collaborative predictive modeling. Therefore, the capability to detect, assess and prevent “model misconducts” is critical to increase the integrity/reliability of machine learning. To address this issue, we consider the following 3 types of model misconducts: (1) model plagiarism, of which a site becomes a free-rider and just submits a copy of a model from the other sites, trying to hide their own information and inspect models from other sites; (2) model fabrication, of which a site mocks up a model, trying to hide information and disturb the machine learning process; and (3) model falsification, of which a site tweaks its model a bit, trying to just disturb the learning process. For each type of the model misconducts, we are interest in how to detect these misconducts of another site, how to assess the losses of machine learning results due to misconducts, and how to prevent these model misconducts. Our aims include (a) detecting model misconducts using model properties, (b) assessing model misconducts losses via model simulation, and (c) preventing model misconducts based on whole model history. The innovative components to our proposed project include (i) summarizing various types of model misconduct, (ii) developing a complete strategy to handle the model misconduct, and (iii) providing a generalizable approach to mitigate bioethical concerns for collaborative machine learning. Project Narrative Artificial intelligence performed jointly between multiple collaborating institutions can accelerate genomic medicine research and facilitate quality improvement, but relies on a trust model which may be too optimistic in real-world setting. In this project, we plan to develop a comprehensive detection, assessment and prevention mechanism to address the potential bioethical risks brought by misconducts of model plagiarism, fabrication, and falsification. The proposed study can supplement the considerations of model misconducts for our original project of privacy-preserving learning on blockchain.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,10130868,R00HG009680,"['Address', 'Artificial Intelligence', 'Bioethics', 'Budgets', 'Calibration', 'Clinical', 'Data', 'Data Set', 'Detection', 'Digit structure', 'Discrimination', 'Event', 'Genomic medicine', 'Genomics', 'Healthcare', 'Incentives', 'Institution', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pattern', 'Plagiarism', 'Prevention', 'Privacy', 'Privatization', 'Process', 'Property', 'Randomized', 'Recording of previous events', 'Research', 'Risk', 'Security', 'Site', 'Sum', 'Testing', 'Time', 'Trust', 'base', 'blockchain', 'design', 'distributed ledger', 'innovation', 'interest', 'learning strategy', 'models and simulation', 'patient privacy', 'predictive modeling', 'prevent', 'privacy preservation', 'statistics']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,102049,-0.0015593049798411532
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9921505,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'data fusion', 'deep learning', 'design', 'feature extraction', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neural network classifier', 'neurophysiology', 'novel', 'post-trauma', 'predictive modeling', 'prevent', 'random forest', 'support vector machine', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,245552,-0.004090958117407291
"Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics The human microbiota plays an important role in health and disease, and its therapeutic manipulation is being actively investigated for a wide range of diseases that span every NIH institute. Our microbiota are inherently dynamic, and analyzing these time-dependent properties is key to robustly linking the microbiota to disease, and predicting the effects of therapies targeting the microbiota; indeed, longitudinal microbiome data is being acquired with increasing frequency, and is a major component of many NIH-funded projects. However, there is currently a dearth of computational tools for analyzing microbiome time-series data, which presents several special challenges including high measurement noise, irregular and sparse temporal sampling, and complex dependencies between variables. The objective of this proposal is to introduce new capabilities, improve on, and provide state-of-the-art implementations of tools for analyzing dynamics, or patterns of change in microbiome time-series data. The tools we develop will use Bayesian machine learning methods, which are well-recognized for their strong conceptual and practical advantages, particularly in biomedical domains. Tools will be rigorously tested and validated on synthetic and real human microbiome data, including publicly available datasets and those from collaborators providing 16S rRNA sequencing, metagenomic, and metabolomics data. We propose three specific aims. For Aim 1, we will develop integrated Bayesian machine learning tools for predicting population dynamics of the microbiome and its responses to perturbations. These tools will include a new model that simultaneously learns groups of microbes with similar interaction structure and predicts their behavior over time, and that incorporates prior phylogenetic information. The model will be further improved by incorporating stochastic microbial dynamics and errors in measurements throughout the model. For Aim 2, we will develop Bayesian machine learning tools to predict host status from microbiome dynamics. The tools will learn easily interpretable, human-readable rules that predict host status from microbiome time-series data, and will be further extended to handle a variety of longitudinal study designs. For Aim 3, we will engineer our microbiome dynamics analysis software tools for optimal performance, ease-of- use, maintainability, extensibility, and dissemination to the community. In total, the proposed work will yield a suite of contemporary software tools for analyzing microbiome dynamics, with expected broad use and major impact. The software will allow investigators to answer important scientific and translational questions about the microbiome, including discovering which microbial taxa or their metagenomes are affected over time by perturbations such as changes in diet or invasion by pathogens; predicting the effects of these perturbations over time, including changes in composition or stability of the gut microbiota; and finding temporal signatures in multi-‘omic microbiome data that predict disease risk in the human host. The human microbiota, or collection of micro-organisms living on and within us, plays an important role in health, and when disrupted or abnormal, may contribute to many types of diseases including infections, kidney diseases, bowel diseases, diabetes, heart diseases, arthritis, allergies, brain diseases, and cancer. Sophisticated computer-based tools are needed to make sense of human microbiota data, particularly time- series data, which can yield important insights into how our microbiomes change over time. This work will develop new and improved computer-based tools for analyzing microbiota time-series data, which will be made freely available and will enable scientists to increase our fundamental knowledge about how our microbiota affect us and ultimately to apply this knowledge to prevent and treat human illnesses.",Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics,10015315,R01GM130777,"['16S ribosomal RNA sequencing', 'Affect', 'Algorithms', 'Antibiotics', 'Arthritis', 'Autoimmunity', 'Bayesian learning', 'Behavior', 'Biological Markers', 'Biological Models', 'Brain Diseases', 'Cardiovascular Diseases', 'Childhood', 'Clostridium difficile', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Dependence', 'Diabetes Mellitus', 'Diet', 'Disease', 'Engineering', 'Environmental Exposure', 'Frequencies', 'Funding', 'Health', 'Heart Diseases', 'Human', 'Human Microbiome', 'Hypersensitivity', 'Infection', 'Institutes', 'Intervention', 'Intestines', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Learning', 'Link', 'Longitudinal Studies', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Metagenomics', 'Microbe', 'Modeling', 'Names', 'Noise', 'Oligosaccharides', 'Outcome', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Play', 'Population Dynamics', 'Property', 'Pythons', 'Readability', 'Research Design', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Series', 'Shotguns', 'Software Engineering', 'Software Tools', 'Speed', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Work', 'base', 'computerized tools', 'design', 'disorder risk', 'dynamic system', 'gut microbiota', 'human data', 'human microbiota', 'human subject', 'improved', 'insight', 'learning algorithm', 'machine learning method', 'man', 'metabolomics', 'metagenome', 'microbial', 'microbiome', 'microbiome analysis', 'microbiome sequencing', 'microbiota', 'microorganism', 'nervous system disorder', 'novel', 'open source', 'pathogen', 'predictive tools', 'prevent', 'recurrent infection', 'response', 'software development', 'targeted treatment', 'tool']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R01,2020,312939,0.006591634814488331
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9878070,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'machine learning method', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,591130,-0.006252749219229742
"A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER) Many promising peripheral neuromodulation techniques have been proposed to treat lower urinary tract (LUT) dysfunction, but our lack of predictive models has forced the community (including the PI’s lab) to explore the vast parameter space of nerve targets, stimulation parameterizations, and electrode designs empirically in animal experiments by trial and error. This type of exploratory experimentation is the only current method of optimizing, personalizing, or discovering novel LUT neuromodulation techniques. Motivated by this clinical need, our long-term goal for this work is to predict the effects of neuromodulation on the LUT. To move toward this goal, we propose to develop a new modeling framework that integrates disparate biophysics models through machine learning, thereby emulating an entire organ system through a process we call Biomechanistic Learning Augmentation of Deep Differential Equation Representations (BLADDER). We will develop and use the general BLADDER framework to create an organ-level model of the normal healthy LUT throughout its filling and voiding cycles, including non-volitional neural reflex control over the bladder and urethra. Our focus on neural reflex control and organ-level scales ensures that, if successful, the BLADDER LUT model will be poised to predict effects of neuromodulation using computational studies, which so far has been impossible due to the complexity of the LUT. The BLADDER framework unites multiple individual mechanistic models (each accounting for a component function of an organ system) by using deep recurrent neural networks (RNN) to learn the appropriate coupling dynamics linking each component model. The combination of mechanistic and machine learning models under a single framework allows us to harness the advantages of both: mechanistic models excel at interpretability but suffer from a lack of scalability (becoming intractable at the level of organ systems), while machine learning models are excellent at scale but lack generalizability and insights for hypothesis generation. The BLADDER framework will scale up mechanistic models to the level of systems physiology by linking tractable model components together using a supervisory RNN, allowing the BLADDER framework to deliver both interpretability and scale. We will draw on existing SPARC datasets in the cat (e.g., Bruns and Gaunt), existing publicly available data in rat, and generate new data in the rat to construct a training dataset for the supervisory RNN. We will further draw from already published small-scale mechanistic models, validated on human and animal data, for the mechanistic components of the BLADDER LUT model. The formal process of identifying these models and datasets, and checking their validity and robustness, will clearly reveal the deficits and strengths in our theoretical and experimental understanding of the LUT in a straightforward and rational way. We will use the 10 Simple Rules to vet mechanistic models for inclusion in the BLADDER LUT model and compile a public inventory for the neurourology community. Major task 1 (Q1-2): Identify available datasets and candidate mechanistic models from published literature. Major deliverables are a public database and a whitepaper detailing the state of the field and prospects for modeling and experimental work. Major Task 2 (Q1-3): Demonstrate proof of concept of BLADDER framework. Major deliverables are a publicly available code linking two LUT component models via supervisory RNN and a report on suitable RNN architectures based on fully described dynamical systems. Major Task 3 (Q3-6): Create a multi-component BLADDER model. Major deliverables are code used to link separate mechanistic LUT models via the supervisory RNN, and an in vivo rat dataset to fill in critical measurables for the machine learning training set. Major Task 4 (Q6-8): Deploy the fully operational BLADDER model of the LUT, including autonomously predicted neural reflex control. Major deliverables are publicly available codes and datasets, and a hypothesis-driven computational experiment to predict simple interventions. n/a",A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER),10206953,OT2OD030524,"['Accounting', 'Animal Experiments', 'Bladder', 'Clinical', 'Code', 'Communities', 'Coupling', 'Data', 'Data Set', 'Databases', 'Differential Equation', 'Electrodes', 'Ensure', 'Equipment and supply inventories', 'Felis catus', 'Functional disorder', 'Generations', 'Goals', 'Individual', 'Intervention', 'Learning', 'Link', 'Literature', 'Lower urinary tract', 'Machine Learning', 'Measurable', 'Methods', 'Modeling', 'Nerve', 'Organ', 'Peripheral', 'Physiology', 'Process', 'Publishing', 'Rattus', 'Reflex control', 'Reporting', 'System', 'Techniques', 'Training', 'Urethra', 'Work', 'animal data', 'base', 'biophysical model', 'body system', 'computer studies', 'design', 'dynamic system', 'experimental study', 'human data', 'in vivo', 'insight', 'neural network architecture', 'neuroregulation', 'novel', 'predictive modeling', 'recurrent neural network', 'relating to nervous system', 'scale up']",OD,FLORIDA INTERNATIONAL UNIVERSITY,OT2,2020,1025141,-0.01589841133952847
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9884791,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical data science', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'patient health information', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2020,92070,-0.03142342850753444
"Novel machine learning approaches for improving structural discrimination in cryo-electron tomography Project Summary Cellular cryo-electron tomography (Cryo-ET) has made possible the observation of cellular organelles and macromolecular complexes at nanometer resolution with native conformations. The rapid increasing amount of Cryo-ET data available however brings along some major challenges for analysis which we will timely ad- dress in this proposal. We will design novel data-driven machine learning algorithms for improving structural discrimination and resolution. In particular, we have the following speciﬁc aims: (1) We will develop a novel Autoencoder and Iterative region Matching (AIM) algorithm for marker-free alignment of image tilt-series to re- construct tomograms with improved resolution; (2) We will develop a saliency-based auto-picking algorithm for better detecting macromolecular complexes, and combine it with an innovative 2D-to-3D framework to further improve structure detection accuracy; (3) We will design an end-to-end convolutional model for pose-invariant clustering of subtomograms. This model will produce an initial clustering which will be reﬁned by a new subto- mogram averaging algorithm that automatically down-weights subtomograms of noise and little contribution; (4) We will perform experimental evaluations by using previously reported bacterial secretion systems and mito- chondrial ultrastructures datasets to improve the ﬁnal resolution. Implementing algorithms in Aims 1-3, we will develop a user-friendly open-source graphical user interface -tom to directly beneﬁt the scientiﬁc community.  -tom will be systematically compared with existing software including IMOD, EMAN2, and Relion on simulated and benchmark datasets. To facilitate distribution, -tom will be integrated into existing software platforms Sci- pion and TomoMiner. Our data-driven algorithms and software not only will facilitate and accelerate the future use of Cryo-ET, but also can be readily used on analyzing the existing large amounts of Cryo-ET data to im- prove our understanding of the structure, function, and spatial organization of macromolecular complexes in situ. Project Narrative This project will create a system of machine learning algorithms to accelerate and facilitate the use and re-use of the rapidly accumulating Cryo-ET datasets. For easy use, we will develop an open-source GUI -Tom (to be disseminated into the Scipion and TomoMiner software platforms) that streamlines the new approaches from the initial tomogram reconstruction step to the ﬁnal subtomogram averaging step. We will validate the performance of our system by applying it on published Cryo-ET datasets and monitor the improvement of the ﬁnal results.",Novel machine learning approaches for improving structural discrimination in cryo-electron tomography,9973462,R01GM134020,"['3-Dimensional', 'Algorithmic Software', 'Algorithms', 'Back', 'Benchmarking', 'Biological Process', 'Cells', 'Communities', 'Computer Analysis', 'Computer software', 'Cryo-electron tomography', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Discrimination', 'Evaluation', 'Future', 'Gaussian model', 'Group Structure', 'Hour', 'Image', 'In Situ', 'Knowledge', 'Laplacian', 'Literature', 'Machine Learning', 'Macromolecular Complexes', 'Manuals', 'Methods', 'Mitochondria', 'Modeling', 'Molecular Conformation', 'Monitor', 'Neurophysiology - biologic function', 'Noise', 'Organelles', 'Performance', 'Process', 'Publishing', 'Reporting', 'Resolution', 'Series', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tomogram', 'Weight', 'Work', 'autoencoder', 'automated algorithm', 'base', 'deep learning', 'design', 'falls', 'feature detection', 'graphical user interface', 'improved', 'innovation', 'insight', 'machine learning algorithm', 'nano', 'nanometer resolution', 'novel', 'novel strategies', 'open source', 'particle', 'pi-Mesons', 'programs', 'reconstruction', 'success', 'user-friendly']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,342970,0.008736265182703783
"A novel computing framework to automatically process cardiac valve image data and predict treatment outcomes PROJECT SUMMARY  There is a massive amount of clinical three-dimensional (3D) cardiac image data available today in numerous hospitals, but such data has been considerably underutilized in both clinical and engineering analyses of cardiac function. These 3D data offers unique and valuable information, allowing researchers to develop innovative, personalized approaches to treat diseases. Furthermore, using these 3D datasets as input to computational models can facilitate a population-based analysis that can be used to quantify uncertainty in treatment procedures, and can be utilized for virtual clinical trials for innovative device development. However, there are several critical technical bottlenecks preventing simulation-based clinical evaluation a reality: 1) difficulty in automatic 3D reconstruction of thin complex structures such as heart valve leaflets from clinical images, 2) computational models are constructed without mesh correspondence, which makes it challenging to run batch simulations and conduct large patient population data analyses due to inconsistencies in model setups, and 3) computing time is long, which inhibits prompt feedback for clinical use.  A potential paradigm-changing solution to the challenges is to incorporate machine learning algorithms to expedite the geometry reconstruction and computational analysis procedures. Therefore, the objective of this proposal is to develop a novel computing framework, using advanced tissue modeling and machine learning techniques, to automatically process pre-operative clinical image data and predict post-operative clinical outcomes. Transcatheter aortic valve replacement (TAVR) intervention will serve as a testbed for the modeling methods. In Aim 1, we will develop novel shape dictionary learning (SDL) based methods for automatic reconstruction of TAVR patient aortic valves. Through the modeling process, mesh correspondence will be established across the patient geometric models. The distribution and variation of TAVR patient geometries will be described by statistical shape models (SSMs). In Aim 2, population-based FE analysis of the TAVR procedure will be conducted on thousands of virtual patient models generated by the SSMs (Aim 1). A deep neural network (DNN) will be developed and trained to learn the relationship between the TAVR FE inputs and outputs. Successful completion of this study will result in a ML-FE surrogate for TAVR analysis, combining the automated TAVR patient geometry reconstruction algorithms and the trained DNN, to provide fast TAVR biomechanics analysis without extensive re-computing of the model. Furthermore, the algorithms developed in this study can be generalized for other applications and devices. PROJECT NARRATIVE Current clinical image modalities can be utilized to develop patient-specific computational models to pre-operatively plan transcatheter aortic valve replacement (TAVR) procedures. However, the computational modeling and simulation processes are time-consuming, which limits clinical translatability. Thus, the objective of this proposal is to develop algorithms using machine learning techniques to rapidly process and predict TAVR computational simulation outcomes directly from clinical image data.",A novel computing framework to automatically process cardiac valve image data and predict treatment outcomes,9973167,R01HL142036,"['3-Dimensional', 'Adverse event', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Attention', 'Biomechanics', 'Biomedical Computing', 'Clinical', 'Clinical Engineering', 'Complex', 'Computer Analysis', 'Computer Models', 'Computer Simulation', 'Consumption', 'Coronary Occlusions', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Device Designs', 'Device or Instrument Development', 'Devices', 'Dictionary', 'Disease', 'Elements', 'Evaluation', 'Extravasation', 'Feedback', 'Finite Element Analysis', 'Generations', 'Geometry', 'Goals', 'Guidelines', 'Heart Valves', 'Hospitals', 'Hour', 'Human', 'Image', 'Intervention', 'Laboratories', 'Language', 'Learning', 'Left ventricular structure', 'Machine Learning', 'Manuals', 'Methods', 'Mitral Valve', 'Modeling', 'Outcome', 'Output', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Postoperative Period', 'Problem Sets', 'Procedures', 'Process', 'Property', 'Research Personnel', 'Response Elements', 'Running', 'Rupture', 'Sampling', 'Shapes', 'Statistical Data Interpretation', 'Stents', 'Structure', 'Techniques', 'Testing', 'Thinness', 'Time', 'Tissue Model', 'Training', 'Translations', 'Treatment outcome', 'Uncertainty', 'Variant', 'X-Ray Computed Tomography', 'algorithm training', 'aortic valve', 'aortic valve replacement', 'ascending aorta', 'base', 'calcification', 'clinical application', 'clinical imaging', 'clinical practice', 'clinically translatable', 'deep learning', 'deep neural network', 'heart function', 'heart imaging', 'imaging modality', 'improved', 'innovation', 'machine learning algorithm', 'models and simulation', 'novel', 'patient population', 'personalized approach', 'population based', 'prevent', 'reconstruction', 'research clinical testing', 'simulation', 'speech recognition', 'time resolved data', 'two-dimensional', 'virtual', 'virtual clinical trial']",NHLBI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2020,383601,-0.013739946571871694
"Deep Learning Algorithms for FreeSurfer Abstract FreeSurfer is a tool for the analysis of Magnetic Resonance Imaging (MRI) that has proven to be a flexible and powerful technology for quantifying the effects of many conditions, including numerous neurological disorders, on human brain anatomy, connectivity, vasculature, chemical composition, physiology and function. In the past 20 years, these open source tools have been developed to accurately and automatically segment an array of brain structures and have become the core analysis infrastructure for the Alzheimer’s Disease NeuroImaging Initiative (ADNI). In this project, we seek the resources to radically increase the speed, accuracy and flexibility of these tools, taking advantage of exciting new results in Deep Learning. This will enable us to more accurately quantify neuroanatomical changes that are critical to diagnosing, staging and assessing the efficacy of potential therapeutic interventions in diseases such as Alzheimer’s. This includes the generation of documentation, tutorials, unit tests, regression tests and system tests to harden the tools and make them usable by clinicians and neuroscientists, and finally the distribution and support of the data, manual labelings and tools to the more than 40,000 researchers that use FreeSurfer through our existing open source mechanism. In addition, we will analyze the entire Alzheimer’s Disease NeuroImaging Initiative dataset and return it for public release, including a set of manually labeled data that can be used to optimize Deep Learning tools for Alzheimer’s Disease over the next decade. Relevance Successful completion of the proposed project will increase the usability and accuracy of our publicly available segmentation tools, and open up new possibilities, such as integrating them into the MRI scanner and rapidly detecting Alzheimer’s-related changes. These new capabilities well enable other studies to significantly increase their ability to detect AD and other disease effects in research settings as well as phase II and phase III clinical trials due to the radical increase in speed of the new tools, enabling them to be applied to a diverse set of MRI contrasts and much larger datasets, rapidly and accurately. Further, they will allow rapid application of cutting-edge analyses to the ongoing Alzheimer’s Disease NeuroImaging Initiative dataset, improving the ability to extract early biomarkers of this devastating disease.",Deep Learning Algorithms for FreeSurfer,9970009,R01AG064027,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Chemicals', 'Code', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Engineering', 'Ensure', 'Excision', 'Functional Magnetic Resonance Imaging', 'Future', 'Generations', 'Hour', 'Human', 'Image', 'Infrastructure', 'Label', 'Licensing', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Memory', 'Modeling', 'Neurobiology', 'Pattern', 'Phase II Clinical Trials', 'Phase III Clinical Trials', 'Physiology', 'Population', 'Procedures', 'Publishing', 'Recording of previous events', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Rest', 'Sensitivity and Specificity', 'Speed', 'Staging', 'Stream', 'Structure', 'Surface', 'System', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Validation', 'Variant', 'Work', 'base', 'contrast imaging', 'convolutional neural network', 'cranium', 'deep learning', 'deep learning algorithm', 'early detection biomarkers', 'flexibility', 'high resolution imaging', 'human disease', 'improved', 'large datasets', 'morphometry', 'nervous system disorder', 'neuroimaging', 'novel', 'open source', 'prevent', 'prototype', 'skills', 'spatial relationship', 'support tools', 'tool', 'usability', 'web site', 'wiki']",NIA,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,649026,-0.0785370437532797
"Large-scale annotation-free disease correlation analysis of the iHMP Project Summary We will work with the iHMP data resource to apply novel tools and data analysis methodologies to the challenge of disease association between large microbiome data sets, Inflammatory Bowel Disease, and the onset of diabetes. We will start with an annotation-free approach using k-mers to preprocess IBD and diabetes cohorts. We then will apply a novel scaling technology implemented in the sourmash software to reduce the data set size by a factor of 2000, rendering it tractable to machine learning approaches. We next will use random forests to determine a subset of predictive k-mers, and will measure their accuracy on validation data sets not used in the initial training. Finally, we will annotate the predictive k-mers using all available genome databases as well as a novel method to infer the metagenomic presence of accessory genomes of known genomes. Our outcomes will include a catalog of microbial genomes that correlate with IBD subtype and the onset of diabetes, as well as automated workflows to apply similar approaches to other data sets. Project Narrative We propose to work with the iHMP data, a large central microbiome resource, to study disease correlations with inflammatory bowel disease and diabetes. We will work to associate specific microbial species with the disease conditions. We will also produce resources that will help other researchers perform similar studies.",Large-scale annotation-free disease correlation analysis of the iHMP,10112077,R03OD030596,"['Catalogs', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diabetes Mellitus', 'Disease', 'Ecology', 'Genbank', 'Genome', 'Human', 'Immunoglobulin Variable Region', 'Inflammatory Bowel Diseases', 'Machine Learning', 'Measures', 'Metadata', 'Metagenomics', 'Methodology', 'Methods', 'Onset of illness', 'Organism', 'Outcome', 'Reporting', 'Research Personnel', 'Resources', 'Technology', 'Training', 'Update', 'Validation', 'Variant', 'Viral', 'Work', 'cohort', 'data resource', 'genome database', 'member', 'metagenome', 'metatranscriptome', 'metatranscriptomics', 'microbial', 'microbial genome', 'microbiome', 'novel', 'random forest', 'tool']",OD,UNIVERSITY OF CALIFORNIA AT DAVIS,R03,2020,304918,0.001530096265162093
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9963080,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Alzheimer&apos', 's disease pathology', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Models', 'Computer software', 'Data', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genetic testing', 'genome-wide', 'genomic data', 'genomic locus', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'large scale data', 'machine learning algorithm', 'machine learning method', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2020,410000,-0.07122624888029289
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9969467,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Models', 'Computer software', 'Computers', 'Custom', 'Data', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'algorithm development', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'large datasets', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,614363,0.016537015761885286
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10058463,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data reuse', 'data sharing', 'data visualization', 'data warehouse', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,627034,0.004836658096277491
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9923688,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'contig', 'dark matter', 'deep learning', 'deep learning algorithm', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'machine learning method', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'statistical and machine learning', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,276700,0.007258757341071691
"Improving Population Representativeness of the Inference from Non-Probability Sample Analysis SUMMARY The critical role of population-representativeness for estimating disease incidence and prevalence has been widely accepted in epidemiologic studies. Improving population representativeness of nonprobability samples, such as samples of volunteers in epidemiologic studies or electronic health records, however, has received little attention by biostatisticians or epidemiologists. In this project, we propose two innovative “pseudoweight” construction methods: 1) two-step matching, and 2) calibration, under an adapted exchangeability assumption, for unbiased estimation of disease incidence and prevalence in the target population. The proposed methods, combined with machine learning methods for propensity score estimation, will achieve significant bias reduction, especially when selection into nonprobability samples is driven by complex relationships between the covariates. We will quantify the bias reduced by the proposed “pseudoweights”, numerically and empirically, on the estimation of disease incidence and prevalence in the target population. Monte Carlo simulation studies are designed under varying degrees of departure from the adapted exchangeability assumption to evaluate the bias of the proposed estimates. The robustness of the proposed estimators against varying sample sizes, number of clusters in survey, and complexities of the true propensity score modeling will be investigated in scenarios that differ by levels of non-linearity, non-additivity and correlations between covariates in the true propensity model. Using data from National Institutes of Health and the American Association of Retired Persons (NIH-AARP, a nonprobability cohort sample) data and the US National Health Interview Survey (NHIS, a probability survey sample), the proposed methods will be applied to estimate the prevalence of self-reported diseases and all-cause or all-cancer mortality rates for people aged 50-71 in the US. To test our methods, we will purposely select outcome variables that are available in both the NIH-AARP and the NHIS. Thus, the amount of bias in NIH-AARP estimates corrected by the proposed pseudoweights can be quantified in practice, assuming the weighted NHIS estimate is true. The proposed methods, although motivated by the volunteer-based epidemiological studies, have wide applications outside of epidemiology, such as electronic health records or web surveys. The results from this project can be used by epidemiologists and health policy makers to improve the understanding of the health-related characteristics in the general population. Computer software that implements the proposed methods will be made available for public use. PROJECT NARRATIVE The project proposes innovative “pseudoweights” construction methods for nonprobability samples, such as samples of volunteers in epidemiologic studies or electronic health records, to improve their population representativeness. The project will quantify the amount of bias reduced by the proposed “pseudoweights,” numerically and empirically, on the estimation of population parameters such as disease incidence and prevalence. The result can be used by epidemiologists and health policy makers to improve the understanding of the health related characteristics in the general population.",Improving Population Representativeness of the Inference from Non-Probability Sample Analysis,10046869,R03CA252782,"['American', 'Attention', 'Calibration', 'Characteristics', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Disease', 'Effectiveness', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Equilibrium', 'General Population', 'Health', 'Health Policy', 'Incidence', 'Internet', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monte Carlo Method', 'National Health Interview Survey', 'Outcome', 'Patient Self-Report', 'Policy Maker', 'Population', 'Prevalence', 'Probability', 'Probability Samples', 'Research', 'Role', 'Sample Size', 'Sampling', 'Source', 'Surveys', 'Target Populations', 'Testing', 'Trees', 'United States National Institutes of Health', 'Weight', 'aged', 'base', 'cohort', 'complex data ', 'design', 'epidemiology study', 'flexibility', 'improved', 'innovation', 'machine learning method', 'mortality', 'random forest', 'retiree', 'software development', 'volunteer']",NCI,"UNIV OF MARYLAND, COLLEGE PARK",R03,2020,154500,-0.013092102060428476
"In-silico prediction of protein-peptide interactions. IN-SILICO PREDICTION OF PROTEIN-PEPTIDE INTERACTIONS Automated docking methods are used extensively for gaining a mechanistic understanding of the molecular interactions underpinning cellular processes. While these tools work well for small molecules they perform poorly for peptides and cannot handle Intrinsically Disordered Proteins (IDPs) which play very important roles in these processes. The goal of this project is the development of an efficient and practical peptide docking software, useful for designing therapeutic peptides and gaining insight into IDPs binding ordered proteins. The proposed software supports biomedical applications ranging from investigating chemical pathways to designing and optimizing therapeutic molecules for diseases such as cancer and metabolic disorders. Under the previous award we developed and released a new method for docking fully-flexible peptides with up to 20 standard amino acids: AutoDock CrankPep (ADCP). We showed that it outperforms current state-of-the-art docking methods. For the next award, we propose to: 1) further develop ADCP to support docking IPDs with up to 70 amino acids and improve support for therapeutic peptides containing modified amino acids and complex macrocycles; 2) develop peptide-specific scoring functions to increase docking success rates and methods for predicting the free energy of binding of peptides. This will be done by exploiting the latest advances in statistical potentials for docking, as well as applying machine-learning techniques; 3) test and validate the software on our datasets, community benchmarks, and through our collaborations with outstanding biologists working on biomedical applications spanning from designing drugs for thrombosis and influenza, to modeling IDPs interacting with globular proteins; and 4) document the software and release it under an open source license on a regular basis along with datasets we compile and update on regularly. The proposed research will occur in the context of collaborations with experimental biologists working on highly relevant biomedical projects and providing experimental feedback and validation. In addition, this project will benefit from various collaborations with experts in the fields of computational biology, applied mathematics and artificial intelligence. This docking software tool will be developed by applying best practices in software engineering and be implemented as a modular, extensible, component-based software framework for peptide docking. This docking engine will be part of the widely used AutoDock software suite. The ability to model complexes formed by proteins and fully-flexible peptides or IDPs is in high demand and will greatly extend the range of peptide-based therapeutic approaches for which automated docking can be successfully applied. It will also support gaining insights into interactions of IDPs with proteins. As such, it will impact the research of many medicinal chemists and biologist and extend the use of computational tools to a wider community of scientists, thereby supporting the advancement of biomedical research. Automated docking is a workhorse for rational drug design, however, applying these methods to peptides has remained challenging, thus impeding the designing of therapeutic peptides and the study of Intrinsically Disordered Proteins (IDP) binding to their ordered partners. During the prior funding period, we made substantial progress toward peptide docking, resulting in a new docking engine: AutoDock CrankPep, which outperforms state-of-the-art docking methods for linear and cyclic peptides with up to 20 standard amino acids. We propose to further develop AutoDock CrankPep to support docking of therapeutic peptides with modified amino acids as well as IDPs with up to 70 amino acids, creating a practical docking tool for peptides that will impact the research of many computational and medicinal chemists and biologist, contribute to our understanding of biological processes, and significantly advance biomedical research.",In-silico prediction of protein-peptide interactions.,10116950,R01GM096888,"['Amino Acids', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'Benchmarking', 'Binding', 'Binding Proteins', 'Biological', 'Biological Availability', 'Biological Process', 'Biomedical Research', 'Cell physiology', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Cyclic Peptides', 'Data Set', 'Development', 'Disease', 'Docking', 'Documentation', 'Drug Design', 'Educational workshop', 'Feedback', 'Free Energy', 'Funding', 'Goals', 'Half-Life', 'Influenza', 'Insulin', 'Libraries', 'Licensing', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Mediating', 'Metabolic Diseases', 'Methods', 'Modeling', 'Modernization', 'Mutate', 'Pathway interactions', 'Peptides', 'Performance', 'Peripheral', 'Permeability', 'Pharmaceutical Preparations', 'Play', 'Process', 'Production', 'Property', 'Proteins', 'Renaissance', 'Research', 'Role', 'Scientist', 'Signal Pathway', 'Software Engineering', 'Software Framework', 'Software Tools', 'Specificity', 'Structure', 'Study models', 'Techniques', 'Testing', 'Therapeutic', 'Thrombosis', 'Toxic effect', 'Training', 'Update', 'Validation', 'Work', 'base', 'combinatorial', 'computerized tools', 'computing resources', 'design', 'flexibility', 'globular protein', 'graphical user interface', 'improved', 'improved functioning', 'in silico', 'insight', 'interest', 'interoperability', 'novel', 'open source', 'peptide drug', 'predictive tools', 'programs', 'protein protein interaction', 'receptor', 'screening', 'small molecule', 'success', 'symposium', 'therapeutic target', 'tool', 'translational study', 'virtual screening']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2020,399375,-0.04005122429160314
"Using machine learning to predict odor characteristics from molecular structure PROJECT SUMMARY/ABSTRACT We cannot yet look at a chemical structure and predict if the molecule will have an odor, much less what character it will have. The goal of the proposed research is to apply machine learning to predict perceptual characteristics from chemical features of molecules. The specific aims of the proposal will determine (1) which molecules are odorous , and (2) what data are needed to model odor character. Building a highly predictive model requires two key ingredients: high-quality data and a sound modeling approach. High-quality data must be accurate (ratings are consistent and describe true odor properties) and detailed (ratings describe even small differences in odor properties). We have collected human psychophysical data on a diverse set of molecules and have trained a model to predict if a molecule has an odor, but pilot data identified odorous contaminants that limit model training and measurement of model accuracy. In Aim 1, I will apply my background in analytical chemistry to evaluate the accuracy of the data, using gas chromatography to identify and correct errors caused by chemical contaminants. In Aim 2, I will apply my experience in human sensory evaluation to measure and compare the consistency and the degree of detail in ratings that can be achieved with different sensory methods and subject training procedures. By executing my training plan, I will develop the skills in statistical programming and machine learning needed to employ a sound modeling approach to these problems. The model constructed in Aim 1 will enable prediction of odor classification (odor/odorless) for any molecule and thus define which molecules are perceptually relevant. Predicting odor character is a far more complex challenge – while a molecule can have only one of two odor classifications (odor or odorless) it may elicit any number of diverse odor character attributes (fruity, floral, musky, sweet, etc.). Descriptive Analysis (DA) is the gold standard method for generating accurate and detailed sensory profiles, but this method is time-consuming. We estimate that an odor character dataset will be large enough (“model-ready”) to predict odor character with approximately 10,000 molecules and that it would require more than 30,000 hours of human subject evaluation, or approximately 6 years for the typical trained panel, to produce this dataset using DA. Before we invest the time and resources, it is responsible to evaluate the relative data quality of more rapid sensory methods. The results of Aim 2 are expected to determine the best approach for generating a model-ready dataset by quantifying trade-offs in degree of detail (data resolution), rating consistency, and method speed of five candidate sensory methods. Together, these aims represent a significant step forward in linking chemical recipe to human odor perception, an advancement that supports the NIDCD goal of understanding normal olfactory function (how stimulus relates to percept) and has many potential applications in foods (what composition of molecules should be present to produce a target aroma percept). PROJECT NARRATIVE Currently, scientists cannot predict whether a molecule will have an odor and, if so, what odor characteristics it will have based on its chemical structure. The goal of this project is to develop predictive models linking chemical composition to odor characteristics. These models will advance our understanding of the human olfactory system and help design strategies for improving the aroma and palatability of healthy foods.",Using machine learning to predict odor characteristics from molecular structure,10142097,F32DC019030,"['Address', 'Analytical Chemistry', 'Characteristics', 'Chemical Structure', 'Chemicals', 'Chemistry', 'Classification', 'Collection', 'Complex', 'Consumption', 'Data', 'Data Set', 'Descriptor', 'Development', 'Evaluation', 'Food', 'Fruit', 'Gas Chromatography', 'Goals', 'Gold', 'Health Food', 'Hour', 'Human', 'Human Resources', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Mass Fragmentography', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Structure', 'National Institute on Deafness and Other Communication Disorders', 'Odors', 'Olfactory Pathways', 'Palate', 'Perception', 'Positioning Attribute', 'Procedures', 'Programmed Learning', 'Property', 'Protocols documentation', 'Psychophysics', 'Quality Control', 'Recipe', 'Research', 'Research Technics', 'Resolution', 'Resources', 'Sampling', 'Science', 'Scientist', 'Sensory', 'Smell Perception', 'Speed', 'Stimulus', 'Structure', 'Testing', 'Time', 'Training', 'Work', 'base', 'data quality', 'design', 'experience', 'food science', 'human subject', 'improved', 'machine learning algorithm', 'model building', 'predictive modeling', 'prevent', 'rapid technique', 'skills', 'sound']",NIDCD,MONELL CHEMICAL SENSES CENTER,F32,2020,67446,-6.444209662916579e-06
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9847973,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data cleaning', 'data ecosystem', 'data reuse', 'data sharing', 'data standards', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2020,489919,0.0027991605880493225
"Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication,10111658,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'data standards', 'data streams', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF ARKANSAS AT FAYETTEVILLE,R01,2020,491992,-0.03173125741594523
"Using machine learning techniques to characterize the Metabolomics Workbench Dataset PROJECT SUMMARY/ABSTRACT  Mass spectrometry in combination with chromatography provides a powerful approach to characterize small molecules produced in cells, tissues and other biological systems. In essence, measured metabolites provide a functional readout of cellular state, allowing novel biological studies that advance our understanding of health and disease. Currently, the main bottleneck in metabolomics is determining the chemical identities associated with the spectral signatures of measured masses. Despite the growth of spectral databases and advances in annotation tools that recommend the chemical structure that best explains each signature, the large majority of measured masses cannot be assigned a chemical identity. There is now consensus that gleaning partial information regarding the measured spectra in terms of chemical substructure or chemical classification can inform biological studies. This consensus is reflected in the newly updated reporting standards for metabolite annotation as proposed by the Metabolite Identification Task Group of the Metabolomics Society. As we show in our Preliminary Results, spectral characterization results in “features” that can enhance performance in machine-learning tasks such as annotation.  This work aims to enhance the use and value of the metabolomics dataset in Metabolomics Workbench by: (1) developing machine-learning tools trained on this dataset to characterize unknown spectra, and (2) adding characterization information to the Metabolomics Workbench dataset. In Aim 1, we identify spectral patterns (motifs) that can represent chemically meaningful groupings of peaks within the spectra (e.g., peaks associated with aromatic substructures, loss of a substructure fragment, etc.). We utilize neural topic models that use variational inference to identify such motifs. We expect such models to offer computational speedups and to identify more chemically coherent motifs when compared to earlier implementations of topic modeling. We generate motifs across all spectra in the Metabolomics Workbench and provide annotations for each spectrum.  In Aim 2, we map spectral signatures to chemical ontology classes. As ontologies are hierarchical and as a molecule can be associated with multiple classes at different hierarchical levels of an ontology, we cast this mapping problem as a hierarchical multi-label classification problem and use neural networks to implement such a classifier. The classifier will be trained using the Metabolomics Workbench dataset. Learned motifs from Aim 1 will be used as additional input features to improve classification. We expect that the developed classifier can be used by others to elucidate measurements of unidentified molecules with chemical ontology classes, or to generate ontology terms that can be used as features in downstream machine-learning tasks. Relevance to Public Health The project proposes to investigate machine learning techniques to enhance the utility of a Common Fund data set hosted through the Metabolomics Workbench. This data set consists of biologically relevant molecules and information about their structural composition and their mass spectrometry signatures. We anticipate that our techniques will result in annotating and adding information to the data set, which in turn will advance discoveries in biomedical research and have direct benefits to human health.",Using machine learning techniques to characterize the Metabolomics Workbench Dataset,10111982,R03OD030601,"['Biochemical', 'Biological', 'Biomedical Research', 'Catalogs', 'Cells', 'Chemical Structure', 'Chemicals', 'Chromatography', 'Classification', 'Complement', 'Computational Technique', 'Computing Methodologies', 'Consensus', 'Consumption', 'Coupled', 'Data', 'Data Set', 'Databases', 'Disease', 'Funding', 'Gas Chromatography', 'Gene Expression', 'Glean', 'Goals', 'Grouping', 'Growth', 'Health', 'Histidine', 'Human', 'Ions', 'Label', 'Liquid Chromatography', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Modeling', 'Molecular', 'Molecular Structure', 'Nature', 'Ontology', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Public Health', 'Reporting', 'Research Personnel', 'Sampling', 'Societies', 'Structure', 'Taxonomy', 'Techniques', 'Time', 'Tissues', 'Training', 'Update', 'Validation', 'Variant', 'Vocabulary', 'Work', 'annotation  system', 'biological systems', 'biomarker discovery', 'cost', 'functional outcomes', 'improved', 'metabolomics', 'neural network', 'novel', 'protein expression', 'relating to nervous system', 'response', 'small molecule', 'tool']",OD,TUFTS UNIVERSITY MEDFORD,R03,2020,263120,0.007642770631929408
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,-0.0033539208013623205
"Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution Project Summary/Abstract It is hard to overstate the importance of monoclonal antibodies in the life sciences. Antibodies are critical tools in biomedical research and diagnostics (e.g. western blotting, immunoprecipitation, cytometry, biomarker discovery, and histology), are one of the most rapidly growing class of therapeutics, and are the basis for myriad new strategies in cancer therapy, such as checkpoint inhibitors that are revolutionizing treatment. Unfortunately, current methods for the generation of custom antibodies, including animal immunization and phage display, are slow, costly, inaccessible to most researchers, and often unsuccessful. We propose Autonomously EvolvinG Yeast-displayed antibodieS (AEGYS), a system for the continuous and rapid evolution of high-quality antibodies against custom antigens that requires only the simple culturing of yeast cells. We believe this can be achieved by combining cutting-edge generative machine learning algorithms for antibody library design with a new technology for in vivo continuous evolution and a yeast antigen-presenting cell that we will engineer. If successful, AEGYS should have a transformative impact across the whole of biomedicine by turning monoclonal antibody generation into a rapid, scalable, and accessible process where any lab with standard molecular biology capabilities can generate custom antibodies on demand simply by “immunizing” a test tube of yeast cells with an antigen. We anticipate that this democratization of antibody generation will also result in an explosion of crowdsourced antibody sequence data that will train our machine learning algorithms to design better antibody libraries for AEGYS, starting a virtuous cycle. We ourselves will use AEGYS to generate a panel of subtype- and conformation-specific nanobodies against biogenic amine receptors including those that respond to acetylcholine, adrenaline, dopamine, and other neurotransmitters, so that we can understand their role in neurobiology and addiction.! Project Narrative This proposal will provide a system for the scalable continuous evolution and computational design of antibodies against user-selected antigens. Antibodies are critical tools in medical research and are the basis for numerous therapies, but the generation of custom antibodies against new targets is a difficult and specialized task. The system proposed will turn antibody generation into a routine and widely accessible process for researchers in almost any field.","Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution",10021311,R01CA260415,"['Acetylcholine', 'Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antibody Formation', 'Antigen Targeting', 'Antigen-Presenting Cells', 'Antigens', 'Architecture', 'Area', 'Back', 'Biogenic Amine Receptors', 'Biological Sciences', 'Biomedical Research', 'Cell Surface Receptors', 'Cells', 'Chemistry', 'Clinic', 'Collection', 'Communities', 'Cultured Cells', 'Custom', 'Cytometry', 'Data', 'Data Set', 'Detergents', 'Diagnostic', 'Directed Molecular Evolution', 'Docking', 'Dopamine', 'Elements', 'Engineering', 'Epidemic', 'Epinephrine', 'Evolution', 'Explosion', 'G-Protein-Coupled Receptors', 'Generations', 'Genes', 'Genetic', 'Histology', 'Human', 'Hybridomas', 'Image', 'Immune checkpoint inhibitor', 'Immune system', 'Immunization', 'Immunize', 'Immunoglobulin Fragments', 'Immunoprecipitation', 'Libraries', 'Machine Learning', 'Medical Research', 'Medicine', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Monoclonal Antibodies', 'Neuraxis', 'Neurobiology', 'Neurosciences', 'Neurotransmitters', 'Nobel Prize', 'Outcome', 'Pathogen detection', 'Phage Display', 'Pharmaceutical Preparations', 'Pheromone', 'Play', 'Problem Solving', 'Process', 'Production', 'Protein Engineering', 'Proteins', 'Proteome', 'Public Health', 'Reagent', 'Research', 'Research Personnel', 'Role', 'Signal Transduction', 'Specificity', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Training', 'Tube', 'Update', 'V(D)J Recombination', 'Western Blotting', 'Yeasts', 'addiction', 'antibody engineering', 'antibody libraries', 'antigen binding', 'base', 'biomarker discovery', 'cancer therapy', 'cost', 'crowdsourcing', 'decision research', 'design', 'empowered', 'experimental study', 'follow-up', 'improved', 'in vivo', 'innovation', 'insight', 'interest', 'machine learning algorithm', 'nanobodies', 'new technology', 'novel', 'receptor', 'response', 'scaffold', 'structural biology', 'tool']",NCI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2020,1690552,-0.015886460654132704
"Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways ﻿    DESCRIPTION (provided by applicant): This project aims to develop new statistical machine learning methods for metabolomics data from diverse platforms, including targeted and unbiased/global mass spectrometry (MS), labeled MS experiments for measuring metabolic ﬂux and Nuclear Magnetic Resonance (NMR) platforms. Unbiased MS and NMR proﬁling studies result in identifying a large number of unnamed spectra, which cannot be directly matched to known metabolites and are hence often discarded in downstream analyses. The ﬁrst aim develops a novel kernel penalized regression method for analysis of data from unbiased proﬁling studies. It provides a systematic framework for extracting the relevant information from unnamed spectra through a kernel that highlights the similarities and differences between samples, and in turn boosts the signal from named metabolites. This results in improved power in identiﬁcation of named metabolites associated with the phenotype of interest, as well as improved prediction accuracy. An extension of this kernel-based framework is also proposed to allow for systematic integration of metabolomics data from diverse proﬁling studies, e.g. targeted and unbiased MS proﬁling technologies. The second aim pro- vides a formal inference framework for kernel penalized regression and thus complements the discovery phase of the ﬁrst aim. The third aim focuses on metabolic pathway enrichment analysis that tests both orchestrated changes in activities of steady state metabolites in a given pathway, as well as aberrations in the mechanisms of metabolic reactions. The fourth aim of the project provides a uniﬁed framework for network-based integrative analysis of static (based on mass spectrometry) and dynamic (based on metabolic ﬂux) metabolomics measurements, thus providing an integrated view of the metabolome and the ﬂuxome. Finally, the last aim implements the pro- posed methods in easy-to-use open-source software leveraging the R language, the capabilities of the Cytoscape platform and the Galaxy workﬂow system, thus providing an expandable platform for further developments in the area of metabolomics. The proposed software tool will also provide a plug-in to the Data Repository and Coordination Center (DRCC) data sets, where all regional metabolomics centers supported by the NIH Common Funds Metabolomics Program deposit curated data. PUBLIC HEALTH RELEVANCE: Metabolomics, i.e. the study of small molecules involved in metabolism, provides a dynamic view into processes that reﬂect the actual physiology of the cell, and hence offers vast potential for detection of novel biomarkers and targeted therapies for complex diseases. However, despite this potential, the development of computational methods for analysis of metabolomics data lags the rapid growth of metabolomics proﬁling technologies. The current application addresses this need by developing novel statistical machine learning methods for integrative analysis of static and dynamic metabolomics measurements, as well as easy-to-use open-source software to facilitate the application of these methods.",Machine Learning Tools for Discovery and Analysis of Active Metabolic Pathways,9899255,R01GM114029,"['Address', 'Adoption', 'Anabolism', 'Area', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Assay', 'Cardiovascular Diseases', 'Cell physiology', 'Cells', 'Characteristics', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Deposition', 'Detection', 'Development', 'Diabetes Mellitus', 'Disease', 'Environment', 'Environmental Risk Factor', 'Equilibrium', 'Funding', 'Galaxy', 'Homeostasis', 'Knowledge', 'Label', 'Language', 'Letters', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methodology', 'Methods', 'Names', 'Network-based', 'Nuclear Magnetic Resonance', 'Pathway interactions', 'Phase', 'Phenotype', 'Plug-in', 'Procedures', 'Process', 'Prognostic Marker', 'Proteomics', 'Reaction', 'Sampling', 'Signal Transduction', 'Software Tools', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization', 'Work', 'base', 'biological systems', 'biomarker discovery', 'data warehouse', 'diagnostic biomarker', 'diverse data', 'experimental study', 'flexibility', 'high dimensionality', 'improved', 'insight', 'interest', 'machine learning method', 'metabolome', 'metabolomics', 'new technology', 'novel', 'novel diagnostics', 'novel marker', 'open source', 'programs', 'public health relevance', 'rapid growth', 'response', 'small molecule', 'statistical and machine learning', 'targeted treatment', 'tool', 'transcriptomics']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,336869,0.020232428570069322
"National Resource for Network Biology (NRNB) OVERALL - PROJECT SUMMARY The mission of the National Resource for Network Biology (NRNB) is to advance the science of biological networks by creating leading-edge bioinformatic methods, software tools and infrastructure, and by engaging the scientific community in a portfolio of collaboration and training opportunities. Much of biomedical research is dependent on knowledge of biological networks of multiple types and scales, including molecular interactions among genes, proteins, metabolites and drugs; cell communication systems; relationships among genotypes and biological and clinical phenotypes; and patient and social networks. NRNB-supported platforms like Cytoscape are among the most widely used software tools in biology, with tens of thousands of active users, enabling researchers to apply network concepts and data to understand biological systems and how they are reprogrammed in disease.  NRNB’s three Technology Research and Development projects introduce innovative concepts with the potential to transform network biology, transitioning it from a static to a dynamic science (TR&D 1); from flat network diagrams to multi-scale hierarchies of biological structure and function (TR&D 2); and from descriptive interaction maps to predictive and interpretable machine learning models (TR&D 3). In previous funding periods our technology projects have produced novel and highly cited approaches, including network-based biomarkers for stratification of disease, data-driven gene ontologies assembled completely from network data, and deep learning models of cell structure and function built using biological networks as a scaffold.  During the next period of support, we introduce dynamic regulatory networks formulated from single-cell transcriptomics and advanced proteomics data (TR&D 1); substantially improved methodology for the study of hierarchical structure and pleiotropy in biological networks (TR&D 2); and procedures for using networks to seed machine learning models of drug response that are both mechanistically interpretable and transferable across biomedical contexts (TR&D 3). These efforts are developed and applied in close collaboration with outside investigators from 19 Driving Biomedical Projects who specialize in experimental generation of network data, disease biology (cancer, neuropsychiatric disorders, diabetes), single-cell developmental biology, and clinical trials. TR&Ds are also bolstered by 7 Technology Partnerships in which NRNB scientists coordinate technology development with leading resource-development groups in gene function prediction, mathematics and algorithm development, and biomedical databases. Beyond these driving collaborations, we continually support a broader portfolio of transient (non-driving) research collaborations; organize and lead international meetings including the popular Network Biology track of the Intelligent Systems for Molecular Biology conference; and deliver a rich set of training opportunities and network analysis protocols. OVERALL - PROJECT NARRATIVE We are all familiar with some of the components of biological systems – DNA, proteins, cells, organs, individuals – but understanding biological systems involves more than just cataloging its component parts. It is critical to understand the many interactions of these parts within systems, and how these systems give rise to biological functions and responses and determine states of health and disease. The National Resource for Network Biology provides the scientific community with a broad platform of computational tools for the study of biological networks and for incorporating network knowledge in biomedical research.",National Resource for Network Biology (NRNB),9937486,P41GM103504,"['Area', 'Automobile Driving', 'Beds', 'Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Biomedical Technology', 'Cataloging', 'Catalogs', 'Cell Communication', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Conceptions', 'DNA', 'Data', 'Data Set', 'Databases', 'Developmental Cell Biology', 'Diabetes Mellitus', 'Disease', 'Disease stratification', 'Drug Modelings', 'Ecosystem', 'Educational workshop', 'Event', 'Expert Systems', 'Feedback', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic Risk', 'Genomics', 'Genotype', 'Goals', 'Health', 'Individual', 'Infrastructure', 'International', 'Knowledge', 'Lead', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mentors', 'Methodological Studies', 'Methods', 'Mission', 'Modeling', 'Molecular Biology', 'National Institute of General Medical Sciences', 'Network-based', 'Ontology', 'Organ', 'Pathway Analysis', 'Patients', 'Pharmaceutical Preparations', 'Phase Transition', 'Phenotype', 'Positioning Attribute', 'Procedures', 'Proteins', 'Proteomics', 'Protocols documentation', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Running', 'Science', 'Scientist', 'Seeds', 'Services', 'Social Network', 'Software Tools', 'Structure', 'Students', 'System', 'Technology', 'Testing', 'Tissues', 'Training', 'Visual', 'Visualization', 'Work', 'algorithm development', 'biological systems', 'clinical phenotype', 'cloud storage', 'computational platform', 'computerized tools', 'deep learning', 'gene function', 'genomics cloud', 'improved', 'innovation', 'interoperability', 'lens', 'mathematical algorithm', 'meetings', 'method development', 'multi-scale modeling', 'neuropsychiatric disorder', 'next generation', 'novel', 'pleiotropism', 'prediction algorithm', 'programs', 'protein metabolite', 'response', 'scaffold', 'single cell analysis', 'software infrastructure', 'symposium', 'technology development', 'technology research and development', 'tool', 'training opportunity', 'transcriptome', 'transcriptomics']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",P41,2020,1995376,-0.012574440059411773
"Computational Techniques for Advancing Untargeted Metabolomics Analysis PROJECT SUMMARY/ABSTRACT Detecting and quantifying products of cellular metabolism using mass spectrometry (MS) has already shown great promise in biomarker discovery, nutritional analysis and other biomedical research fields. Despite recent advances in analysis techniques, our ability to interpret MS measurements remains limited. The biggest challenge in metabolomics is annotation, where measured compounds are assigned chemical identities. The annotation rates of current computational tools are low. For several surveyed metabolomics studies, less than 20% of all compounds are annotated. Another contributing factor to low annotation rates is the lack of systematic ways of designing a candidate set, a listing of putative chemical identities that can be used during annotation. Relying on exiting databases is problematic as considering the large combinatorial space of molecular arrangements, there are many biologically relevant compounds not catalogued in databases or documented in the literature. A secondary yet important challenge is interpreting the measurements to understand the metabolic activity of the sample under study. Current techniques are limited in utilizing complex information about the sample to elucidate metabolic activity. The goal of this project is to develop computational techniques to advance the interpretation of large-scale metabolomics measurements. To address current challenges, we propose to pursue three Aims: (1) Engineering candidate sets that enhance biological discovery. (2) Developing new techniques for annotation including using deep learning and incremental build out methods to recommend novel chemical structures that best explain the measurements. (3) Constructing probabilistic models to analyze metabolic activity. Each technique will be rigorously validated computationally and experimentally using chemical standards. Two detailed case studies on the intestinal microbiota will allow us to further validate our tools. Microbiota-derived metabolites have been detected in circulation and shown to engage host cellular pathways in organs and tissues beyond the digestive system. Identifying these metabolites is thus critical for understanding the metabolic function of the microbiota and elucidating their mechanisms. The complex test cases will challenge our techniques, provide feedback during development, and allow us to further disseminate our techniques. We will work closely with early adopters of our tools, as proposed in supporting letters, to further validate our tools and encourage wide adoption. All proposed tools will be open source and made accessible through the web. Our tools promise to change current practices in interpreting metabolomics data beyond what is currently possible with databases, current annotation tools, statistical and overrepresentation analysis, or combinations thereof. The use of machine learning and large data sets as proposed herein defines the most promising research direction in metabolomics analysis. PROJECT NARRATIVE  Untargeted Metabolomics is a recently developed technique that allows the measurement of thousands of molecules in a biological sample. This work proposes several novel computational techniques that address limitations of current metabolomics analysis tools. We anticipate that this work will advance discoveries in biomedical research and have direct benefits to human health.",Computational Techniques for Advancing Untargeted Metabolomics Analysis,10022125,R01GM132391,"['Address', 'Adoption', 'Biological', 'Biomedical Research', 'Blood Circulation', 'Case Study', 'Chemical Structure', 'Chemicals', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Engineering', 'Ensure', 'Feedback', 'Goals', 'Health', 'Human', 'Internet', 'Intestines', 'Label', 'Letters', 'Literature', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'MeSH Thesaurus', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Structure', 'Nutritional', 'Organ', 'Pathway interactions', 'Performance', 'Play', 'Probability', 'Property', 'PubChem', 'PubMed', 'Public Domains', 'Research', 'Research Personnel', 'Role', 'Running', 'Sampling', 'Statistical Models', 'Structure', 'Surveys', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Uncertainty', 'Validation', 'Work', 'annotation  system', 'base', 'biomarker discovery', 'chemical standard', 'combinatorial', 'computerized tools', 'cost', 'dark matter', 'deep learning', 'design', 'drug development', 'drug discovery', 'experimental study', 'gastrointestinal system', 'gut microbiota', 'interest', 'large datasets', 'metabolome', 'metabolomics', 'microbiota', 'microbiota metabolites', 'neural network', 'novel', 'nutrition', 'open source', 'physical property', 'small molecule', 'tool']",NIGMS,TUFTS UNIVERSITY MEDFORD,R01,2020,378983,0.013531956441384631
"Computational Techniques for Advancing Untargeted Metabolomics Analysis PROJECT SUMMARY/ABSTRACT Detecting and quantifying products of cellular metabolism using mass spectrometry (MS) has already shown great promise in biomarker discovery, nutritional analysis and other biomedical research fields. Despite recent advances in analysis techniques, our ability to interpret MS measurements remains limited. The biggest challenge in metabolomics is annotation, where measured compounds are assigned chemical identities. The annotation rates of current computational tools are low. For several surveyed metabolomics studies, less than 20% of all compounds are annotated. Another contributing factor to low annotation rates is the lack of systematic ways of designing a candidate set, a listing of putative chemical identities that can be used during annotation. Relying on exiting databases is problematic as considering the large combinatorial space of molecular arrangements, there are many biologically relevant compounds not catalogued in databases or documented in the literature. A secondary yet important challenge is interpreting the measurements to understand the metabolic activity of the sample under study. Current techniques are limited in utilizing complex information about the sample to elucidate metabolic activity. The goal of this project is to develop computational techniques to advance the interpretation of large-scale metabolomics measurements. To address current challenges, we propose to pursue three Aims: (1) Engineering candidate sets that enhance biological discovery. (2) Developing new techniques for annotation including using deep learning and incremental build out methods to recommend novel chemical structures that best explain the measurements. (3) Constructing probabilistic models to analyze metabolic activity. Each technique will be rigorously validated computationally and experimentally using chemical standards. Two detailed case studies on the intestinal microbiota will allow us to further validate our tools. Microbiota-derived metabolites have been detected in circulation and shown to engage host cellular pathways in organs and tissues beyond the digestive system. Identifying these metabolites is thus critical for understanding the metabolic function of the microbiota and elucidating their mechanisms. The complex test cases will challenge our techniques, provide feedback during development, and allow us to further disseminate our techniques. We will work closely with early adopters of our tools, as proposed in supporting letters, to further validate our tools and encourage wide adoption. All proposed tools will be open source and made accessible through the web. Our tools promise to change current practices in interpreting metabolomics data beyond what is currently possible with databases, current annotation tools, statistical and overrepresentation analysis, or combinations thereof. The use of machine learning and large data sets as proposed herein defines the most promising research direction in metabolomics analysis. PROJECT NARRATIVE  Untargeted Metabolomics is a recently developed technique that allows the measurement of thousands of molecules in a biological sample. This work proposes several novel computational techniques that address limitations of current metabolomics analysis tools. We anticipate that this work will advance discoveries in biomedical research and have direct benefits to human health.",Computational Techniques for Advancing Untargeted Metabolomics Analysis,10145183,R01GM132391,"['Address', 'Adoption', 'Biological', 'Biomedical Research', 'Blood Circulation', 'Case Study', 'Chemical Structure', 'Chemicals', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Engineering', 'Ensure', 'Feedback', 'Goals', 'Health', 'Human', 'Internet', 'Intestines', 'Label', 'Letters', 'Literature', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'MeSH Thesaurus', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Structure', 'Nutritional', 'Organ', 'Pathway interactions', 'Performance', 'Play', 'Probability', 'Property', 'PubChem', 'PubMed', 'Public Domains', 'Research', 'Research Personnel', 'Role', 'Running', 'Sampling', 'Statistical Models', 'Structure', 'Surveys', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Uncertainty', 'Validation', 'Work', 'annotation  system', 'base', 'biomarker discovery', 'chemical standard', 'combinatorial', 'computerized tools', 'cost', 'dark matter', 'deep learning', 'design', 'drug development', 'drug discovery', 'experimental study', 'gastrointestinal system', 'gut microbiota', 'interest', 'large datasets', 'metabolome', 'metabolomics', 'microbiota', 'microbiota metabolites', 'neural network', 'novel', 'nutrition', 'open source', 'physical property', 'small molecule', 'tool']",NIGMS,TUFTS UNIVERSITY MEDFORD,R01,2020,10920,0.013531956441384631
"Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry PROJECT SUMMARY  Cellular interactions with the environment form the basis of health and disease for all organisms. Exposure to nutrients, toxins, and neighboring cells trigger coordinated molecular responses that impact cell function and metabolism in a beneficial, adaptive, or detrimental manner. Although the benefits of multicellularity for the formation of complex tissue structures or the function of entire organ systems has been long appreciated, it has only recently been understood that microbial inhabitants of vertebrates also have a tremendous impact on host cell function and dysfunction. Despite this, an understanding of these interactions has not moved beyond simple associations, and there are virtually no molecular technologies available that adequately define how a complex microbial ecosystem impacts host cell function, or how the host response to microbial colonization affects the bacterial community. This gap in knowledge is striking when one considers the broad and significant impact that microbes have on human health. In this application, we propose to expressly fill this knowledge gap through development of a novel multimodal imaging pipeline that will provide 3-dimensional information on the molecular heterogeneity of microbial communities and the immune response at the host-pathogen interface.  This proposal combines our expertise in immunology, infection biology, mass spectrometry, small animal imaging, machine learning, and computer vision to develop an integrated multimodal visualization method for studying infectious disease. Our unique approach will computationally combine ultra-high speed (~50px/s) MALDI-TOF images, ultra-high mass resolution (>200,000 resolving power) MALDI FTICR IMS, metal imaging by LA-ICP-IMS, high-spatial resolution optical microscopy, and MR imaging using data-driven image fusion. This strategy will enable 3-D molecular images to be generated for thousands of elements, metabolites, lipids, and proteins with an unprecedented combination of chemical specificity and spatial fidelity more than 50x faster than is currently possible. We will use this next-generation imaging capability to (i) define the heterogeneous microbial subpopulations throughout the 3-D volume of a S. aureus community, (ii) uncover the host molecules that form the abscess and accumulate to restrict microbial growth in murine models, and (iii) elucidate molecular markers that differentiate in vivo biofilms at the host-pathogen interface, between abscesses at various stages of progression, and under distinct degrees of nutrient stress. These studies will uncover new targets for therapeutic intervention and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research. PROJECT NARRATIVE This proposal will enable detailed views of the molecular components of infectious disease with unprecedented resolution through the development of a multimodal, 3-dimensional imaging platform. The proposed technologies will improve throughput and molecular specificity, enable automated high-precision and high-accuracy image alignment, and allow for descriptions of molecular signals in 3-D through the fusion of multi-modal imaging data. These studies will uncover targets for therapeutic intervention and antibiotic development and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research.",Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry,9989025,R01AI138581,"['3-Dimensional', 'Abscess', 'Affect', 'Animal Model', 'Animals', 'Anterior nares', 'Antibiotics', 'Antibodies', 'Architecture', 'Awareness', 'Bacteria', 'Bacterial Infections', 'Bacterial Proteins', 'Behavior', 'Biology', 'Biomedical Research', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Cellular Metabolic Process', 'Chemicals', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Custom', 'Data', 'Development', 'Diagnosis', 'Differentiation Antigens', 'Dimensions', 'Disease', 'Ecosystem', 'Elements', 'Environment', 'Exposure to', 'Fourier transform ion cyclotron resonance', 'Functional disorder', 'Genus staphylococcus', 'Glean', 'Growth', 'Health', 'Health Promotion', 'Heterogeneity', 'Histology', 'Human', 'Image', 'Imaging technology', 'Immune', 'Immune response', 'Immunology', 'Imprisonment', 'Individual', 'Infection', 'Infectious Diseases Research', 'Integration Host Factors', 'Knowledge', 'Label', 'Lesion', 'Lipids', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mass Spectrum Analysis', 'Metals', 'Methodology', 'Methods', 'Microbe', 'Microbial Biofilms', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nutrient', 'Optics', 'Organism', 'Pathogenesis', 'Physiological', 'Population', 'Process', 'Proteins', 'Reagent', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Spatial Distribution', 'Specificity', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Speed', 'Staphylococcus aureus', 'Stress', 'Structure', 'Techniques', 'Technology', 'Therapeutic Intervention', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Tissues', 'Toxin', 'Vertebrates', 'Visualization', 'Work', 'animal imaging', 'bacterial community', 'base', 'body system', 'commensal bacteria', 'experimental study', 'host colonization', 'imaging capabilities', 'imaging detection', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'innovation', 'interest', 'microbial', 'microbial colonization', 'microbial community', 'microscopic imaging', 'molecular imaging', 'molecular marker', 'mouse model', 'multimodality', 'neutrophil', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'protein expression', 'response', 'supervised learning', 'targeted treatment', 'virtual']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2020,612684,-0.027582009515359343
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,-0.01689336472728789
"Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data Project Abstract/Summary Our interdisciplinary research team will develop algorithms to accelerate the detection of respiratory virus outbreaks at an unprecedented local scale in US cities. We propose to advance outbreak detection by combining machine learning data integration methods and spatial models of disease transmission. The dynamic models that will be developed will provide mechanistic engines for distinguishing typical from atypical disease trends and the optimization methods evaluate the informativeness of data sources to achieve specified public health goals through the rapid evaluation of diverse input data sources. Working with local healthcare and public health leaders, we will translate the algorithms into user-friendly online tools to support preparedness plans and decision-making. Our proposed research is organized around three major aims. In Aim 1, we will apply machine learning and signal processing methods to build systems that track the earliest indicators of emerging outbreaks within seven US cities. We will evaluate non-clinical data reflecting early and mild symptoms as well as clinical data covering underserved communities and geographic and demographic hotspots for viral emergence. In Aim 2, we will develop sub-city scale models reflecting the syndemics of co-circulating respiratory viruses and chronic respiratory diseases (CRD) that can exacerbate viral infections. We will infer viral transmission rates and socio-environmental risk cofactors by fitting the model to respiratory disease data extracted from millions of electronic health records (EHRs) for the last nine years. We will then partner with clinical and EHR experts to translate our models into the first outbreak detection system for severe respiratory viruses that incorporates EHR data on CRDs. Using machine learning techniques, we will further integrate other surveillance, environmental, behavioral and internet predictor data sources to maximize the accuracy, sensitivity, speed and population coverage of our algorithms. In Aim 3, we will develop an open-access Python toolkit to facilitate the integration of next generation data into outbreak surveillance models. This project will produce practical early warning algorithms for detecting emerging viral threats at high spatiotemporal resolution in several US cities, elucidate socio-geographic gaps in current surveillance systems and hotspots for viral emergence, and provide a robust design framework for extrapolating these algorithms to other US cities. Project Narrative We will develop innovative algorithms for detecting emerging respiratory viruses within US cities. To do so, we will model the syndemic dynamics of respiratory viruses and chronic respiratory diseases and apply machine learning to combine geospatial data that track early indicators of emerging threats. Working with local public health and healthcare collaborators, we will translate this research into practical tools for addressing socio- geographic gaps in surveillance and accelerating the detection, prevention and mitigation of severe outbreaks.","Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data",9946212,R01AI151176,"['Absenteeism', 'Address', 'African', 'Age', 'Algorithm Design', 'Algorithms', 'Area', 'Articulation', 'Bayesian Method', 'Behavioral', 'Caring', 'Chronic', 'Chronic Disease', 'Cities', 'Climate', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Communities', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Disease model', 'Ebola', 'Electronic Health Record', 'Ensure', 'Epidemic', 'Evaluation', 'Geography', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Location', 'Lung diseases', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mexico', 'Modeling', 'Neighborhoods', 'Pollution', 'Population', 'Prevention', 'Public Health', 'Pythons', 'Readiness', 'Reporting', 'Research', 'Resolution', 'Risk', 'Rural', 'Schools', 'Sentinel', 'Series', 'Signal Transduction', 'Social Environment', 'Specific qualifier value', 'Speed', 'Subgroup', 'Surveillance Modeling', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Translating', 'Uncertainty', 'Validation', 'Viral', 'Virus', 'Virus Diseases', 'Visualization', 'Work', 'World Health Organization', 'austin', 'base', 'cofactor', 'comorbidity', 'dashboard', 'data acquisition', 'data integration', 'design', 'digital', 'disease transmission', 'diverse data', 'epidemiologic data', 'epidemiological model', 'experimental study', 'flexibility', 'global health', 'health care availability', 'health goals', 'high risk', 'high risk population', 'influenza outbreak', 'influenzavirus', 'innovation', 'insight', 'metropolitan', 'next generation', 'novel', 'outcome prediction', 'pandemic disease', 'public health intervention', 'respiratory virus', 'school district', 'signal processing', 'simulation', 'social media', 'sociodemographic group', 'socioeconomics', 'sound', 'spatiotemporal', 'stem', 'tool', 'transmission process', 'trend', 'user-friendly', 'viral transmission']",NIAID,YALE UNIVERSITY,R01,2020,611043,-0.02553259890932373
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9961522,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2020,15000,-0.003915398038826289
"Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1 PROJECT SUMMARY Previous studies showed discrepancies of health and behavior prevalence between American Indians (AI) population and other racial or ethnic groups. Most health surveys have certain limitations when studying AI population due to the small sample sizes for AI population. Data collected by AI Tribal Epidemiology Centers (TECs) provides an excellent opportunity to conduct research for AI population due to sufficient sample size and extensive information. However, most surveys conducted by TECs used non-probability sampling design (e.g. convenient sample) due to its lower cost and increased time efficiency. Non-probability sample may suffer from sampling, coverage and nonresponse errors without further proper adjustments. Such difficulties greatly hampers the analysis of AI population in health and behavior research. Our general hypothesis is that data integration by combining information from non-probability and probability samples can reduce sampling, coverage and nonresponse errors in original non-probability sample. The Goal of this project is to develop an accurate and robust data integration methodology for AI population analysis specifically tailored to health and behavior research. During the past years, we have 1) studied data integration using calibration and parametric modeling approaches; 2) investigated machine learning and propensity score modeling methods in survey sampling and other fields; and 3) assembled an experienced team of multi-disciplinary team of experts. In this project, we propose to capitalize on our expertise and fulfill the following Specific Aims: Aim 1. Develop a data integration approach using machine learning and propensity score modeling We will develop machine learning and propensity score based data integration approaches to combine information from non-probability and probability samples. Compared to existing methods (i.e., Calibration, Parametric approach), our proposed approaches are more robust against the failure of underlying model assumptions. The inference is more general and multi-purpose (e.g. one can estimate most parameters such as means, totals and percentiles). Simulation studies will be performed to compare our proposed methods with other existing methods. A computing package will be built to implement the method in other settings. Aim 2. Evaluate the accuracy and robustness of the proposed method in AI health and behavior research We will use real data to validate the proposed methods in terms of accuracy and robustness to the various data types. The performance will also be assessed by comparing with results from existing data integration methods such as calibration and parametric modeling approaches. The planned study takes advantage of a unique data source and expands the impact of the Indian Health Service (IHS)-funded research. We expect this novel integration method will vertically advance the field by facilitating the analysis based on non-probability sample, which can provide in-depth understanding regarding the AI population health and behavior studies. Project Narrative The overall goal of this R21 project is to develop an accurate, robust and multi-purpose data integration methodology for AI population (non-probability sample) analysis specifically tailored to health and behavior research such as diabetes and smoking. The code implementing the proposed method will be released and is general enough to be applied to AI population studies of other fileds. The success of this study will vertically advance the field by facilitating the AI population analysis, which can provide a better guidance and new insights on the future precision personalized prevention and treatment of certain diseases.",Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1,10063407,R21MD014658,"['Adult', 'Age', 'American', 'American Indians', 'Behavioral', 'Behavioral Risk Factor Surveillance System', 'Calibration', 'Censuses', 'Code', 'Communities', 'Community Surveys', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Disease', 'Epidemiology', 'Ethnic group', 'Event', 'Failure', 'Funding', 'Future', 'General Population', 'Geographic state', 'Goals', 'Health', 'Health Fairs', 'Health Surveys', 'Health behavior', 'High Prevalence', 'Kansas', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Not Hispanic or Latino', 'Oklahoma', 'Performance', 'Population', 'Population Analysis', 'Population Study', 'Prevalence', 'Probability', 'Probability Samples', 'Publishing', 'Race', 'Research', 'Research Personnel', 'Respondent', 'Risk Factors', 'Sample Size', 'Sampling', 'Smoking', 'Surveys', 'Target Populations', 'Testing', 'Texas', 'Time', 'Tobacco', 'Training', 'United States Indian Health Service', 'Weight', 'Work', 'Youth', 'base', 'behavioral study', 'cigarette smoking', 'cluster computing', 'cost', 'data integration', 'data quality', 'design', 'experience', 'improved', 'individualized prevention', 'innovation', 'insight', 'multidisciplinary', 'novel', 'personalized medicine', 'population health', 'simulation', 'smoking prevalence', 'success', 'therapy development', 'tribal health']",NIMHD,UNIVERSITY OF OKLAHOMA HLTH SCIENCES CTR,R21,2020,115176,-0.0024981190837742463
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9874005,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,360227,0.010629840215813294
"Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios PROJECT SUMMARY The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four months represents a nearly unprecedented challenge for the infectious disease modelling community. This pandemic has benefitted from huge volumes of data being generated, but the rate of dissemination of these data has often outpaced existing data pipelines. While the last decade has seen significant advances in real-time infectious disease forecasting — spurred by rapid growth in data and computational methods — these methods have primarily focused on seasonal endemic diseases based, are based on historical data, and so do not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leverage the wealth of surveillance data at fine spatial granularity, together with associated information about policy interventions and environmental conditions over space and time, to reason directly about the mechanisms to forecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must use sound statistical and epidemiological principles and be flexible and computationally efficient to provide real- time forecasts to guide public health decision-making and respond to changing aspects of this global crisis. The central research activities of this project are (1) to develop scalable, computationally efficient Bayesian hierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2) to design models and conduct analyses to draw robust inference about the effectiveness of interventions in impacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base for continued responses to COVID-19 and future pandemics. PUBLIC HEALTH NARRATIVE The SARS-CoV-2 pandemic is an emerging public health crisis. A fundamental challenge is how to turn data into evidence that can inform decision-making about managing resources, improving health outcomes, and controlling further spread of SARS-CoV-2. Real-time forecasting and flexible mechanistic models to understand the disease dynamics can provide policy-makers tools to manage public response. The goal of the proposed research is to adapt existing statistical modeling frameworks and develop new ones for making forecasts of COVID-19 in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios,10150377,R35GM119582,"['2019-nCoV', 'Award', 'Budgets', 'COVID-19', 'Calibration', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Contracts', 'Data', 'Data Collection', 'Data Reporting', 'Data Sources', 'Decision Making', 'Dengue', 'Dengue Fever', 'Development', 'Disease', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Endemic Diseases', 'Epidemic', 'Epidemiology', 'Evaluation Methodology', 'Future', 'Goals', 'Grant', 'Health', 'Hospitalization', 'Infection', 'Influenza', 'Influenza prevention', 'Intervention', 'Machine Learning', 'Methods', 'Modeling', 'Natural experiment', 'Outcome', 'Performance', 'Policies', 'Policy Maker', 'Programmed Learning', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Resources', 'Series', 'Social Distance', 'Standardization', 'Statistical Methods', 'Statistical Models', 'Structure', 'Testing', 'Thailand', 'Time', 'Work', 'base', 'data dissemination', 'data pipeline', 'deep learning', 'evidence base', 'flexibility', 'human pathogen', 'improved', 'infectious disease model', 'influenza epidemic', 'machine learning method', 'model design', 'novel', 'pandemic disease', 'pathogen', 'predictive modeling', 'rapid growth', 'real world application', 'reproductive', 'response', 'seasonal influenza', 'sound', 'statistical and machine learning', 'surveillance data', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2020,78507,-0.020643918321372252
"Image-guided robot for high-throughput microinjection of Drosophila embryos PROJECT SUMMARY This proposal is submitted in response to the NIH Development of Animal Models and Related Biological Materials for Research (R21) program. The proposal develops an image-guided robotic platform that performs the automated delivery of molecular genetic tools and non-genetically encoded reagents such as chemical libraries, fluorescent dyes to monitor cellular processes, functionalized magnetic beads, or nanoparticles into thousands of Drosophila embryos in a single experimental session. The proposed work builds on recent engineering innovations in our collaborative group which has developed image-guided robotic systems that can precisely interface with single cells in intact tissue. The two Specific Aims provide for a systematic development of the proposed technologies. AIM 1 first engineers a robotic platform (‘Autoinjector’) that can scan and image Drosophila embryos in arrays of egg laying plates. We will utilize machine learning algorithms for automated detection of embryos, followed by thresholding and morphology analysis to detect embryo centroids and annotate injection sites. In AIM 2, we will utilize microprocessor-controlled fluidic circuits for programmatic delivery of femtoliter to nanoliter volumes of reagents into individual embryos. We will quantify the efficacy of the Autoinjector by comparing the survival, fertility, and transformation rates of transposon or PhiC31-mediated transgenesis to manual microinjection datasets. Finally, we will demonstrate the efficient delivery of sgRNAs and mutagenesis in the presence of Cas9. This project fits very well within the goals of the program by engineering a novel tool for producing and improving animal models. The Autoinjector will accelerate Drosophila research and empower scientists to perform novel experiments and genome-scale functional genomics screens that are currently too inefficient or labor intensive to be conducted on a large scale and may additionally enable other novel future applications. PROJECT NARRATIVE This proposal develops a technology platform that will enable automated microinjection of molecular genetic tools and non-genetically encoded tools such as chemical libraries, fluorescent dyes, functionalized magnetic beads, or nanoparticles, into thousands of Drosophila embryos in a single experimental session. The successful development of this technology will empower Drosophila biologists to perform screens and develop new applications that are currently too inefficient or labor intensive to contemplate and will accelerate research into the function of the nervous system and the molecular and genetic underpinnings of numerous diseases in this important animal model.",Image-guided robot for high-throughput microinjection of Drosophila embryos,9989196,R21OD028214,"['Animal Model', 'Biocompatible Materials', 'Biological Assay', 'Caliber', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Collection', 'Computer Vision Systems', 'Cryopreservation', 'Data Set', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Drosophila melanogaster', 'Embryo', 'Engineering', 'Expenditure', 'Exploratory/Developmental Grant', 'Fertility', 'Fluorescent Dyes', 'Future', 'Gene Transfer Techniques', 'Genetic', 'Goals', 'Guide RNA', 'Image', 'Individual', 'Injections', 'Investigation', 'Laboratories', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Mediating', 'Methods', 'Microinjections', 'Microprocessor', 'Microscope', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Morphology', 'Motivation', 'Mutagenesis', 'Needles', 'Nervous System Physiology', 'Performance', 'Process', 'Reagent', 'Research', 'Resources', 'Robot', 'Robotics', 'Scanning', 'Scientist', 'Signaling Molecule', 'Site', 'Space Perception', 'System', 'Technology', 'Tissues', 'Transgenes', 'Transgenic Organisms', 'United States National Institutes of Health', 'Work', 'animal model development', 'automated algorithm', 'base', 'biological research', 'cost', 'egg', 'experience', 'experimental study', 'functional genomics', 'gene product', 'genetic manipulation', 'genome-wide', 'image guided', 'improved', 'innovation', 'machine learning algorithm', 'magnetic beads', 'mutant', 'mutation screening', 'nanolitre', 'nanoparticle', 'novel', 'novel strategies', 'programs', 'response', 'robotic system', 'screening', 'small molecule libraries', 'stem', 'technology development', 'tool']",OD,UNIVERSITY OF MINNESOTA,R21,2020,222618,-0.003386462310788058
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9997917,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Infrastructure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'feature detection', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'patient health information', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2020,1339073,-0.007816809302741054
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9920181,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,249000,-8.362175047626606e-05
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,10016840,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data curation', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'large scale data', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2020,330502,0.013500281385887188
"The Effects of Insurance Benefit Design Innovation on Patient Health Abstract My research in health economics has focused on how information and targeted consumer cost-sharing influences how patients choose providers and the financial savings of incentivizing patients to choose low-price providers. I have also examined the opposite side of the market, how patient use of information and targeted consumer incentives spurs provider price competition. These topics provided the framework for my research as a PhD student in Health Economics at the University of California, Berkeley and I continue to build on these topics while a policy researcher at the RAND Corporation. A natural next step for my career is to expand this line of research but in a more in-depth manner and using more advanced statistical methods. Performing mentored research in these areas will help me successfully make the transition from directed to independent research. The proposed study will help me to (1) contribute to a deeper understanding of patient health effects of an innovative insurance benefit design that is particularly relevant for the aging population; (2) continue to build capabilities working with large medical claims data sets and develop expertise in innovative statistical methods from different disciplines; (3) gain training in aging-related health-services research; (4) expand my exposure to the aging, health economics, and health services research communities; and (5) develop my abilities as an independent health services researcher and build the foundations to successfully compete for R01-level grants.  In this project, I propose to examine whether reference pricing for colonoscopies and pharmaceuticals decreases adherence to recommended colorectal cancer screening and medication therapies among the near- elderly population. I will also examine the impact of reference pricing on patient health outcomes and the aging process. To do so, I intend to apply novel machine-learning statistical methods that have been recently developed in the computer science and statistics fields. As part of this proposal, I have built a formal training plan to develop expertise in these methods. This project will provide me with the flexibility and support to develop a long-term research agenda that focuses on using innovative statistical methods to evaluate the comprehensive effects of consumer cost- sharing programs. Although this study focuses on a single cost- sharing program, reference pricing, the skills I gain through this award will allow me to independently lead evaluations of future benefit designs. The application of machine-learning methods to the setting of reference pricing will provide a framework that I or other researchers can use to evaluate other insurance benefit designs or alternative patient populations. ! Narrative An increasingly popular insurance benefit design, reference pricing, provides targeted financial incentives for consumers to receive care at low-cost providers. While the financial savings from reference pricing programs are well-known, the health impacts have yet to be studied. The proposed career grant will apply machine learning techniques to develop a long-term research agenda focused on understanding the patient health effects of reference pricing for colonoscopies and medication therapies, which are services that are especially relevant for the aging population. !",The Effects of Insurance Benefit Design Innovation on Patient Health,9891936,K01AG061274,"['Accident and Emergency department', 'Adherence', 'Admission activity', 'Adult', 'Advisory Committees', 'Age', 'Aging', 'Area', 'Award', 'Behavior', 'Big Data', 'California', 'Caring', 'Chronic', 'Chronic Disease', 'Colonoscopy', 'Communities', 'Cost Sharing', 'Data Set', 'Deductibles', 'Development', 'Diabetes Mellitus', 'Discipline', 'Elderly', 'Employee', 'Evaluation', 'Exposure to', 'Foundations', 'Future', 'Grant', 'Health', 'Health Benefit', 'Health Care Costs', 'Health Services', 'Health Services Research', 'Healthcare', 'Heart Diseases', 'Heart Rate', 'Heterogeneity', 'Hospitals', 'Incentives', 'Individual', 'Inpatients', 'Insurance', 'Insurance Benefits', 'Insurance Carriers', 'Internal Medicine', 'Journals', 'Lead', 'Link', 'Machine Learning', 'Medical', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'New England', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Policies', 'Population', 'Preventive service', 'Price', 'Process', 'Provider', 'Publications', 'Publishing', 'Quality of life', 'Research', 'Research Methodology', 'Research Personnel', 'Retirement', 'Savings', 'Screening for cancer', 'Services', 'Side', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Training', 'Universities', 'Work', 'aging population', 'asthmatic patient', 'career', 'colorectal cancer screening', 'comorbidity', 'compliance behavior', 'computer science', 'cost', 'design', 'doctoral student', 'financial incentive', 'flexibility', 'health data', 'health economics', 'health plan', 'improved', 'innovation', 'machine learning method', 'mortality', 'novel', 'patient population', 'programs', 'response', 'semiparametric', 'skills', 'statistical and machine learning', 'statistics', 'treatment effect']",NIA,RAND CORPORATION,K01,2020,130228,-0.019728192632007542
"Mechanism-Driven Virtual Adverse Outcome Pathway Modeling for Hepatotoxicity PROJECT SUMMARY/ABSTRACT  Experimental animal and clinical testing to evaluate hepatotoxicity demands extensive resources and long turnaround times. Utilization of computational models to directly predict the toxicity of new compounds is a promising strategy to reduce the cost of drug development and to screen the multitude of industrial chemicals and environmental contaminants currently lacking safety assessments. However, the current computational models for complex toxicity endpoints, such as hepatotoxicity, are not reliable for screening new compounds and face numerous challenges. Our recent studies have shown that traditional Quantitative Structure-Activity Relationship modeling is applicable for relatively simple properties or toxicity endpoints with a clear mechanism, but fails to address complex bioactivities such as hepatotoxicity. The primary objective of this proposal is to develop novel mechanism-driven Virtual Adverse Outcome Pathway (vAOP) models for the fast and accurate assessment of hepatotoxicity in a high-throughput manner The resulting vAOP models will be experimentally validated using a complement of in vitro and ex vivo testing. We have generated a preliminary vAOP model based on the antioxidant response element (ARE) pathway that has undergone initial validation and refinement using in vitro testing. To this end, our project will generate novel predictive models for hepatotoxicity by applying 1) a virtual cellular stress pathway model to mechanism profiling and assessment of new compounds; 2) computational predictions to fill in the missing data for specific targets within the pathway; 3) in vitro experimental validation with three complementary bioassays; and 4) ex vivo experimental validation with pooled primary human hepatocytes capable of biochemical transformation. The scientific approach of this study is to develop a universal modeling workflow that can take advantage of all available short-term testing information, obtained from both computational predictions using novel machine learning approaches and in vitro experiments, for target compounds of interest. We will validate and use our modeling workflow to directly evaluate the hepatotoxicity of new compounds and prioritize candidates for validation in pooled primary human hepatocytes. The resulting workflow will be disseminated via a web portal for public users around the world with internet access. Importantly, this study will pave the way for the next generation of chemical toxicity assessment by reconstructing the modeling process through a combination of big data, computational modeling, and low cost in vitro experiments. To the best of our knowledge, the implementation of this project will lead to the first publicly available mechanisms-driven modeling and web- based prediction framework for complex chemical toxicity based on publicly-accessible big data. These deliverables will have a significant public health impact by not only prioritizing compounds for safety testing or new chemical development, but also revealing toxicity mechanisms. PROJECT NARRATIVE Hepatotoxicity is a leading safety concern in the development of new chemicals. We will create virtual “Adverse Outcome Pathway” models that will directly evaluate the hepatotoxicity potentials of chemicals using massive public toxicity data. The primary deliverable of this project will be a publically-accessible, web-based search engine to evaluate new chemicals for risk of hepatotoxicity.",Mechanism-Driven Virtual Adverse Outcome Pathway Modeling for Hepatotoxicity,9864299,R01ES031080,"['Address', 'Animal Model', 'Animal Testing', 'Antioxidants', 'Big Data', 'Biochemical', 'Biological', 'Biological Assay', 'Biological Markers', 'Cellular Stress', 'Chemical Injury', 'Chemical Structure', 'Chemicals', 'Clinical', 'Complement', 'Complex', 'Computer Models', 'Computer software', 'Computers', 'Cryopreservation', 'Custom', 'Data', 'Data Pooling', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Drug Costs', 'Ensure', 'Environment', 'Environmental Pollution', 'Evaluation', 'Face', 'Generations', 'Hepatocyte', 'Hepatotoxicity', 'Human', 'In Vitro', 'Industrialization', 'Injury', 'Internet', 'Libraries', 'Liver', 'Luciferases', 'Machine Learning', 'Marketing', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Nutraceutical', 'Online Systems', 'Pathway interactions', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Population', 'Process', 'Property', 'Proteomics', 'PubChem', 'Public Health', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Personnel', 'Resources', 'Response Elements', 'Risk', 'Safety', 'Signal Transduction', 'Source', 'Statutes and Laws', 'System', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Toxicology', 'Translating', 'Validation', 'Vertebrates', 'adverse outcome', 'base', 'candidate validation', 'cell injury', 'combat', 'computational toxicology', 'computer framework', 'computerized tools', 'cost', 'data mining', 'deep neural network', 'design', 'developmental toxicity', 'drug development', 'endoplasmic reticulum stress', 'experimental study', 'hepatocellular injury', 'improved', 'in vitro Assay', 'in vitro testing', 'in vivo', 'interest', 'knowledge base', 'large datasets', 'liver injury', 'next generation', 'novel', 'pre-clinical', 'predictive modeling', 'reproductive toxicity', 'research clinical testing', 'safety assessment', 'safety testing', 'screening', 'search engine', 'tool', 'toxicant', 'transcriptomics', 'virtual', 'web portal']",NIEHS,RUTGERS THE STATE UNIV OF NJ CAMDEN,R01,2020,465692,-0.0124839172910615
"An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics Immune-repertoire sequence, which consists of an individual's millions of unique antibody and T-cell receptor (TCR) genes, encodes a dynamic and highly personalized record of an individual's state of health. Our long- term goal is to develop the computational models and tools necessary to read this record, to one day be able diagnose diverse infections, autoimmune diseases, cancers, and other conditions directly from repertoire se- quence. The key problem is how to find patterns of specific diseases in repertoire sequence, when repertoires are so complex. Our hypothesis is that a combination of bottom-up (sequence-level) and top-down (systems- level) modeling can reveal these patterns, by encoding repertoires as simple but highly informative models that can be used to build highly sensitive and specific disease classifiers. In preliminary studies, we introduced two new modeling approaches for this purpose: (i) statistical biophysics (bottom-up) and (ii) functional diversity (top-down), and showed their ability to elucidate patterns related to vaccination status (97% accuracy), viral infection, and aging. Building on these studies, we will test our hypothesis through two specific aims: (1) We will develop models and classifiers based on the bottom-up approach, statistical biophysics; and (2) we will de- velop the top-down approach, functional diversity, to improve these classifiers. To achieve these aims, we will use our extensive collection of public immune-repertoire datasets, beginning with 391 antibody and TCR da- tasets we have characterized previously. Our team has deep and complementary expertise in developing computational tools for finding patterns in immune repertoires (Dr. Arnaout) and in the mathematics that under- lie these tools (Dr. Altschul), with additional advice available as needed regarding machine learning (Dr. AlQuraishi). This proposal is highly innovative for how our two new approaches address previous issues in the field. (i) Statistical biophysics uses a powerful machine-learning method called maximum-entropy modeling (MaxEnt), improving on past work by tailoring MaxEnt to learn patterns encoded in the biophysical properties (e.g. size and charge) of the amino acids that make up antibodies/TCRs; these properties ultimately determine what targets antibodies/TCRs can bind, and therefore which sequences are present in different diseases. (ii) Functional diversity fills a key gap in how immunological diversity has been measured thus far, by factoring in whether different antibodies/TCRs are likely to bind the same target. This proposal is highly significant for (i) developing an efficient, accurate, generative, and interpretable machine-learning method for finding diagnostic patterns in repertoire sequence; (ii) applying a robust mathematical framework to the measurement of immuno- logical diversity; (iii) impacting clinical diagnostics; and (iv) adding a valuable new tool for integrative/big-data medicine. The expected outcome of this proposal is an integrated pair of robust and well validated new tools/models for classifying specific disease exposures directly from repertoire sequence. This proposal in- cludes plans to make these tools widely available, to maximize their positive impact across medicine. The proposed research is relevant to public health because B cells/antibodies and T cells play vital roles across such a vast range of health conditions, from infection, to autoimmunity, to cancer, that the ability to de- code what they are doing would be an important step forward for diagnosing these conditions. The proposed research is relevant to the NIH's mission of fostering fundamental creative discoveries, innovative research strategies, and their applications as a basis for ultimately protecting and improving health, specifically relating to the diagnosis of human diseases.",An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics,10050030,R01AI148747,"['Address', 'Affect', 'Aging', 'Amino Acid Motifs', 'Amino Acids', 'Antibodies', 'Autoimmune Diseases', 'Autoimmunity', 'B-Lymphocytes', 'Base Sequence', 'Big Data', 'Binding', 'Biophysics', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Code', 'Collection', 'Complex', 'Computer Models', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Entropy', 'Fostering', 'Gene Frequency', 'Genes', 'Goals', 'Health', 'Human', 'Immune', 'Immunology', 'Individual', 'Infection', 'Influenza vaccination', 'Intuition', 'Learning', 'Letters', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Physics', 'Play', 'Population Heterogeneity', 'Privatization', 'Property', 'Public Health', 'Reading', 'Reporting', 'Research', 'Role', 'Sample Size', 'Sampling', 'Sampling Errors', 'Signs and Symptoms', 'Speed', 'Statistical Study', 'System', 'T-Cell Receptor', 'T-Cell Receptor Genes', 'T-Lymphocyte', 'Testing', 'United States National Institutes of Health', 'Vaccination', 'Virus Diseases', 'Work', 'base', 'biophysical properties', 'clinical diagnostics', 'computerized tools', 'diagnostic accuracy', 'human disease', 'immunological diversity', 'improved', 'information model', 'innovation', 'machine learning method', 'multidisciplinary', 'multilevel analysis', 'novel', 'novel strategies', 'tool']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,535171,-0.019861395370276393
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,10002330,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data curation', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2020,653481,0.008043049687097197
"Multi-Study Integer Programming Methods for Human Voltammery Project Summary/Abstract  The development of treatments for addiction requires the characterization of neural mechanisms underlying reward. Studying reward in humans requires assays that can detect changes in neurotransmitter levels with high chemical specificity. Recently, fast-scan cyclic voltammetry (FSCV) has been implemented in humans to measure dopamine with high temporal and spatial resolution. This technological achievement was enabled in large part through the novel application of machine learning methods. FSCV relies on statistical tools since FSCV records an electrochemical response which must be converted into concentration estimates via a statistical model. The validity of the scientific conclusions from human FSCV studies therefore depends heavily on the reliability of these statistical models to generate accurate dopamine concentration estimates.  In human FSCV, models are fit on in vitro training sets as making in vivo training sets in humans is infeasible. Producing accurate estimates thus requires that models trained on in vitro training sets generalize to in vivo brain recordings. Combining data from multiple training sets is the standard approach human FSCV researchers have employed to improve model generalizability. This proposal extends work that shows that multi-study machine learning methods improve dopamine concentration estimates by combining training sets from different electrodes such that the resulting average signal (“cyclic voltammogram” or CV) is similar to the average CV of the electrode used in the brain. However, this approach relies on random resampling. This is problematic because the randomness limits the extent to which estimate accuracy can be improved and the slow speed of the resampling approach precludes the generation of estimates during data collection, which is critical to experiment success.  This proposal details the development of methods that leverage mixed integer programming to optimally generate training sets that combine data from multiple electrodes. By generating training sets that are specifically tailored to the electrode used for brain measurements, one can vastly improve dopamine concentration estimate accuracy. The speed of the integer programming methods will enable the use of this approach during data collection. This work will include validation of the methods on in vitro data as well as on data from published in vivo and slice experiments in rodents. By applying methods to published optogenetic experiments, one can compare estimates from the proposed methods and from standard methods. The asymptotic properties of the proposed methods will be characterized analytically assuming a linear mixed effects model and empirically through application of the methods to data simulated under this model.  This work will be conducted at the highly collaborative and innovative Harvard School of Public Health. The fellowship will support growth in statistical, computing and collaborative skills, and prepare the trainee for a productive career as a biostatistics professor who develops methods for neuroscience and addiction research. Project Narrative  Fast-scan cyclic voltammetry in humans offers an invaluable tool to study the neural mechanisms underlying reward by allowing for sub-second detection of dopamine during cognitive-behavioral tasks. However, conducting voltammetry in humans presents distinct statistical challenges that must be overcome to ensure optimal dopamine concentration estimates. We propose novel statistical methods that use mixed integer optimization and extend preliminary work that shows multi-study machine learning methods substantially improve dopamine concentration estimate accuracy.",Multi-Study Integer Programming Methods for Human Voltammery,10067624,F31DA052153,"['Achievement', 'Address', 'Algorithms', 'Behavioral', 'Biological Assay', 'Biometry', 'Brain', 'Cells', 'Chemicals', 'Cognitive', 'Complex Mixtures', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Dopamine', 'Electrodes', 'Ensure', 'Fellowship', 'Generations', 'Goals', 'Grant', 'Growth', 'Human', 'In Vitro', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Neurosciences', 'Neurotransmitters', 'Nucleus Accumbens', 'Performance', 'Periodicity', 'Property', 'Public Health Schools', 'Publications', 'Publishing', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Rewards', 'Rodent', 'Scanning', 'Scheme', 'Signal Transduction', 'Slice', 'Specificity', 'Speed', 'Statistical Computing', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Training', 'Validation', 'Work', 'addiction', 'algorithm training', 'career', 'effective therapy', 'experimental study', 'improved', 'in vivo', 'innovation', 'insight', 'machine learning method', 'method development', 'multiple data sources', 'neuromechanism', 'novel', 'optogenetics', 'predictive modeling', 'professor', 'relating to nervous system', 'response', 'skills', 'success', 'therapy development', 'tool']",NIDA,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2020,37235,-0.0005259997615046115
"Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions PROJECT SUMMARY The research interests of my group are rooted in explorations of new and useful conceptual models to improve the control and prediction of noncovalent interactions. Our research involves the use of a variety of computational quantum chemical tools, applications of density functional theory (DFT), cheminformatics, and machine-learning methods. A premise of our research is that aromaticity may be used to modulate many types of noncovalent interactions (such as hydrogen bonding, π-stacking, anion-π interactions). The reciprocal relationship we find, between “aromaticity” in molecules and the strengths of “noncovalent interactions,” is surprising especially since they are typically considered as largely separate ideas in chemistry. The innovation of this research is that it will enable use of intuitive “back-of-the-envelope” electron-counting rules (such as the 4n+2πe Hückel rule for aromaticity) to make predictions of experimental outcomes regarding the impact of noncovalent interactions. A five-year goal is to realize the use of our conceptual models in real synthetic examples prepared by our experimental collaborators. My research vision is to bridge discoveries of innovative concepts to their practical impacts for biomedical and biomolecular research. PROJECT NARRATIVE This research proposal includes four projects that are jointly motivated by the challenge to control and predict noncovalent interactions in organic and biomolecular systems. The proposed work involves applications of a variety of computational quantum chemical tools and synergistic investigations with experimental collaborators. We seek to identify new and useful concepts to guide experimental designs of novel “non-natural” molecular systems (e.g., receptors, biosensors, and hydrogels) that have potential biomedical applications.",Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions,10016376,R35GM133548,"['Anions', 'Back', 'Biosensor', 'Chemicals', 'Chemistry', 'Electrons', 'Experimental Designs', 'Goals', 'Hydrogels', 'Hydrogen Bonding', 'Intuition', 'Investigation', 'Modeling', 'Molecular', 'Outcome', 'Plant Roots', 'Research', 'Research Project Summaries', 'Research Proposals', 'System', 'Vision', 'Work', 'cheminformatics', 'density', 'improved', 'innovation', 'interest', 'machine learning method', 'novel', 'quantum computing', 'receptor', 'theories', 'tool']",NIGMS,UNIVERSITY OF HOUSTON,R35,2020,377200,-0.004169493309363108
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9889134,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated algorithm', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'machine learning method', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2020,356625,0.014723992563666022
"High-Performance Compute Cluster for Comprehensive Cancer and Infectious Diseases Research Project Summary/Abstract Fred Hutch respectfully requests funds to upgrade the current 523-node, 3328-core high performance computing (HPC) cluster, which was created in 2004 and expanded in 2006, 2010 (S10 funds), 2013, 2015 (S10 funds), and 2018. 456 end of life nodes (1824 cores) will be replaced and 144 nodes (3456 cores) will be added, creating a new 211-node, 4996-core system with an overall 50% increase in core count, 60% increase in processing power, and more than 100% increase in memory capacity over the old system. The expanded capacity will enable deep and efficient analysis of our research studies and accommodate 20% annual growth in computing intensive research, much of which is not possible on the current cluster. The core user group for the new HPC cluster consists of at least 37 NIH-funded research groups participating in this proposal, however as much as 85 groups use the cluster regularly. Their biomedical research is aimed at eradicating cancer and other diseases and dependent on computationally intensive technical approaches such as development of novel statistical analysis or machine learning methods, for example for assessing immune correlates to facilitate vaccine development, analyzing large scale clinical trials or to develop software tools for the analysis of large-scale immunological datasets, DNA and RNA sequencing, modeling prostate cancer outcomes, studies of the human microbiome, modeling of cancers, mRNA, miRNA, and structural variant detection, structural biology with Cryo- EM, modeling of infectious agents and pandemics, computational modeling, prediction and design of macromolecular structures and interactions, identifying drivers of neoplasia and an international consortium improving colorectal cancer detection using GWAS, whole genome sequencing and genome-wide gene- environment (GxE) studies as well as research in diabetes, mhealth and cardiovascular diseases. Several of the Major Users at Fred Hutch are currently experiencing substantial delays in accomplishing their work using the current cluster. Others have projects that cannot be done at all on the existing instrument. (see Research Projects section for details). The Scientific Computing department (SciComp) has operated the current HPC cluster for more than 10 years and has a staff with a combined experience of over 150 years. The proposed new HPC cluster will be installed in available space in a Fred Hutch datacenter. The expanded cluster will address both immediate and future needs of our user community, supporting NIH-funded research at Fred Hutch. Funded research at our Center will greatly benefit from the increased data-processing capacity and improved performance of the requested HPC cluster, including applications of machine learning to the study of clinical trial efficacy, comparison of immune system receptors to identify responses to specific pathogens/diseases, modeling of carcinogenesis. Besides multiple infectious diseases Fred Hutch researches all types of cancer and our computationally intensive investigators tend to focus on colon, prostate, brain, Barrett’s esophagus, lung and liquid tumors such as leukemia. Project Narrative (Public Health Relevance) We are requesting an expansion of our high-performance computing (HPC) cluster to provide capacity for the growing computational needs in a broad range of biomedical research studies at our Center. Access to fast and reliable computational power is critically important for analyzing the exploding amounts of data produced by large-scale clinical and epidemiological studies, as well as scientific instruments such as genome sequencers and electron microscopes. The prevention, detection, and treatment of cancer, HIV, and other life-threatening diseases are major areas of NIH-funded research at our Center that will greatly benefit from the improved performance of the requested HPC cluster, including large-scale clinical and epidemiologic studies of various cancers, modeling of vaccine efficacy, and infectious disease transmission and proteomic-based biomarker discovery.",High-Performance Compute Cluster for Comprehensive Cancer and Infectious Diseases Research,9940345,S10OD028685,"['Address', 'Barrett Esophagus', 'Biomedical Research', 'Brain', 'Cancer Model', 'Cardiovascular Diseases', 'Clinical Trials', 'Colon', 'Communicable Diseases', 'Communities', 'Computer Models', 'Cryoelectron Microscopy', 'DNA sequencing', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Disease model', 'Environment', 'Funding', 'Future', 'Genes', 'Growth', 'High Performance Computing', 'Human Microbiome', 'Immune', 'Immune system', 'Immunologics', 'Infectious Agent', 'Infectious Diseases Research', 'International', 'Liquid substance', 'Lung', 'Machine Learning', 'Malignant Neoplasms', 'Memory', 'Messenger RNA', 'MicroRNAs', 'Modeling', 'Molecular Structure', 'Neoplasms', 'Performance', 'Prostate', 'Prostate Cancer Outcomes Study', 'Research', 'Research Personnel', 'Research Project Grants', 'Software Tools', 'Statistical Data Interpretation', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'cancer type', 'carcinogenesis', 'cluster computing', 'colorectal cancer screening', 'computerized data processing', 'design', 'efficacy clinical trial', 'end of life', 'experience', 'genome sequencing', 'genome wide association study', 'genome-wide', 'improved', 'instrument', 'leukemia', 'mHealth', 'machine learning method', 'microbiome research', 'novel', 'pandemic disease', 'pathogen', 'prostate cancer model', 'receptor', 'research study', 'response', 'scientific computing', 'software development', 'structural biology', 'transcriptome sequencing', 'tumor', 'vaccine development', 'variant detection', 'whole genome']",OD,FRED HUTCHINSON CANCER RESEARCH CENTER,S10,2020,2000000,-0.03338955371174152
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9900790,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'feature selection', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'machine learning method', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistical and machine learning', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2020,293003,0.0005675679414603596
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,10002249,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2020,593966,-0.027423666432595514
"Designing neutralization antibodies against Sars-Cov-2 Project Summary COVID-19 has become a worldwide pandemic whose rapid spread and mortality rate threatens millions of lives and the global economic system. Developing effective treatment such as neutralization antibodies is an urgent need. We propose here to develop a new method to design antibodies strongly bind to the SARS-CoV-2 receptor binding domain (RBD) that is necessary for viral entrance to human cells. We will develop a novel approach that combines directed evolution, deep sequencing and interpretable neural network models to efficiently identify strong and specific antibodies. This method will allow analyzing large sequencing data sets of antibody variants against the SARS-CoV-2 RBD in order to derive superior binders that do not exist in the original library. Iteration through directed evolution and computational design will efficiently identify neutralization antibody candidates that can be used as potent therapeutics to treat COVID-19. Narrative: Developing neutralization antibodies is critical to provide effective treatment for Covid-19.",Designing neutralization antibodies against Sars-Cov-2,10173204,R21AI158114,"['2019-nCoV', 'Affinity', 'Amino Acids', 'Antibodies', 'Binding', 'COVID-19', 'Cells', 'Cessation of life', 'Clinical Trials', 'Consumption', 'Data', 'Data Set', 'Development', 'Directed Molecular Evolution', 'Economics', 'Epitopes', 'Future', 'Gene Library', 'Histones', 'Human', 'Human Engineering', 'Immunoglobulin G', 'Lead', 'Libraries', 'Machine Learning', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Mutate', 'Mutation', 'Nature', 'Network-based', 'Neural Network Simulation', 'Peptides', 'Positioning Attribute', 'Process', 'Reporting', 'Resistance', 'Screening procedure', 'Solubility', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Variant', 'Viral', 'Virus', 'Virus Diseases', 'base', 'clinical efficacy', 'data archive', 'deep learning', 'deep sequencing', 'design', 'drug candidate', 'effective therapy', 'machine learning method', 'mortality', 'mutant', 'neural network', 'neutralizing antibody', 'novel strategies', 'pandemic disease', 'receptor binding', 'screening', 'trend']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2020,433750,-0.012627387464232965
"Accelerating Community-Driven Medical Innovation with VTK Abstract Thousands of medical researchers around the world use VTK —the Visualization Toolkit— an open-source, freely available software development toolkit providing advanced 3D interactive visualization, image processing and data analysis algorithms. They either use VTK directly in their in-house research applications or indirectly via one of the multitude of medical image analysis and bioinformatics applications that is built using VTK: Osirix, 3D Slicer, BioImageXD, MedINRIA, SCIRun, ParaView, and others. Furthermore, VTK also provides 3D visualizations for clinical applications such as BrainLAB’s VectorVision surgical guidance system and Zimmer’s prosthesis design and evaluation platform. VTK has been downloaded many hundreds of thousands of times since its initial release in 1993. Considering its broad distribution and prevalent use, it can be argued that VTK has had a greater impact on medical research, and patient care, than any other open-source visualization package.  This proposal is in response to the multitude of requests we have been receiving from the VTK medical community. The aims are as follows:  1. Aim 1: Adaptive visualization framework: Produce an integrated framework that supports  visualization applications that balance server-side and client-side processing depending on data size,  analysis requirements, and the user platform (e.g., phone, tablet, or GPU-enabled desktop).  2. Aim 2: Integrated, interactive applications: Extend VTK to support a diversity of programming  paradigms ranging from C++ to JavaScript to Python and associated tools such as Jupyter Notebooks,  integrating with emerging technologies such as deep learning technologies.  3. Aim 3: Advanced rendering, including AR/VR: Target shader-based rendering systems and AR/VR  libraries that achieve high frame rates with minimal latency for ubiquitous applications that combine  low-cost, portable devices such as phones, ultrasound transducers, and other biometric sensors for  visually monitoring, guiding, and delivering advanced healthcare.  4. Aim 4: Infrastructure, Outreach, and Validation: Engage the VTK community and the proposed  External Advisory Board during the creation and assessment of the proposed work and corresponding  modern, digital documentation in the form of videos and interactive web-based content. Project Narrative The Visualization Toolkit (VTK) is an open source, freely available software library for the interactive display and processing of medical images. It is being used in most major medical imaging research applications, e.g., 3D Slicer and Osirix, and in several commercial medical applications, e.g., BrainLAB’s VectorVision surgical guidance system. VTK development began in 1993 and since then an extensive community of users and developers has grown around it. However, the rapid advancement of cloud computing, GPU hardware, deep learning algorithms, and VR/AR systems require corresponding advances in VTK so that the research and products that depend on VTK continue to deliver leading edge healthcare technologies. With the proposed updates, not only will existing applications continue to provide advanced healthcare, but new, innovative medical applications will also be inspired.",Accelerating Community-Driven Medical Innovation with VTK,9910382,R01EB014955,"['3-Dimensional', 'Adopted', 'Algorithmic Analysis', 'Algorithms', 'Bioinformatics', 'Biomechanics', 'Biomedical Technology', 'Biometry', 'Client', 'Cloud Computing', 'Cloud Service', 'Code', 'Communities', 'Computational Geometry', 'Computer software', 'Data', 'Data Analyses', 'Development', 'Devices', 'Documentation', 'Emerging Technologies', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Explosion', 'Foundations', 'Funding', 'Grant', 'Health Technology', 'Healthcare', 'Hybrids', 'Image Analysis', 'Industry', 'Infrastructure', 'Internet', 'Language', 'Letters', 'Libraries', 'Licensing', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modernization', 'Monitor', 'Online Systems', 'Operative Surgical Procedures', 'Patient Care', 'Prevalence', 'Process', 'Prosthesis Design', 'Publications', 'Pythons', 'Research', 'Research Personnel', 'Resources', 'Side', 'Surveys', 'System', 'Tablets', 'Techniques', 'Technology', 'Telephone', 'TensorFlow', 'Testing', 'Time', 'Training', 'Ultrasonic Transducer', 'Update', 'Validation', 'Virtual and Augmented reality', 'Visual', 'Visualization', 'Work', 'base', 'clinical application', 'cloud based', 'computerized data processing', 'cost', 'deep learning', 'deep learning algorithm', 'design', 'digital', 'health care delivery', 'image processing', 'innovation', 'interest', 'learning strategy', 'meetings', 'new technology', 'open source', 'outreach', 'point of care', 'portability', 'processing speed', 'real world application', 'response', 'sensor', 'software development', 'statistics', 'success', 'supercomputer', 'synergism', 'three-dimensional visualization', 'tool', 'trend', 'web services']",NIBIB,"KITWARE, INC.",R01,2020,510157,-0.004732505087275002
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,10017004,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'clinical center', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'mHealth', 'machine learning method', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2020,725375,-0.028204534928390493
"S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450 This application is requesting funds to purchase the Aperio™ GT-450 digital pathology slide scanner from Leica Biosystems. The requested instrumentation will be located in the Pathology and Biobanking Core of the Lester and Sue Smith Breast Center at Baylor College of Medicine (BCM). The predominant use of the Aperio scanner will be research-based whole slide imaging (WSI) and analysis of patient specimens, patient-derived xenograft (PDX) cancer models, and pre-clinical investigations on various animal- and cell-line model systems. All user projects have large sample cohorts that require high throughput, high-resolution scanning and image analysis. High capacity and improved scanning with dynamic focusing makes the GT-450 microscope scanner well-suited and the most cost-effective for use in the proposed projects. An underlying theme in the studies selected for Aperio scanner-supported services integrates novel biomarker and molecular pathway discovery with spatial morphological characterization, a necessary process to investigate heterogeneity in disease states. This instrument leverages high-throughput scanning capability with open-source, fully customizable machine-learning analytics to meet the evolving needs of investigators at Baylor College of Medicine, in particular faculty groups studying mechanisms of cancer cell dynamics and the development of new therapeutic targets. Expansion of systems biology and precision medicine research is an essential component of the college’s strategic roadmap. The Aperio GT-450 is critically needed as we modernize our laboratory offerings and capabilities; the acquisition of this digital scanner will strengthen existing research programs underway and establish new, collaborative research opportunities and directions within Baylor College of Medicine and surrounding institutions. To address the growing demand for integrating quantitative spatial assessment of biomarkers with molecular pathway discovery, we request funding for the Leica Aperio GT-450 digital microscope scanner. This instrument will facilitate research that seeks to better understand molecular mechanisms of tumorigenesis in the context of its spatial environment and will be critical for the development of clinically correlative biomarkers for next generation precision medicine research initiatives at Baylor College of Medicine.",S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450,9940426,S10OD028671,"['Animals', 'Biological Models', 'Breast', 'Cancer Model', 'Cell Line', 'Development', 'Disease', 'Faculty', 'Funding', 'Grant', 'Heterogeneity', 'Image Analysis', 'Institution', 'Laboratories', 'Machine Learning', 'Medicine', 'Microscope', 'Modernization', 'Molecular', 'Morphology', 'Pathology', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Slide', 'Specimen', 'Systems Biology', 'Xenograft procedure', 'base', 'biobank', 'cancer cell', 'clinical investigation', 'cohort', 'college', 'cost effective', 'digital', 'digital pathology', 'improved', 'instrument', 'instrumentation', 'new therapeutic target', 'novel marker', 'open source', 'pre-clinical', 'precision medicine', 'programs', 'whole slide imaging']",OD,BAYLOR COLLEGE OF MEDICINE,S10,2020,477043,-0.021880986983055437
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,-0.025670393728455705
"Developing novel technologies that ensure privacy and security in biomedical data science research Data science holds the promise of enabling new pathways to discovery and can improve the understanding, prevention and treatment of complex disorders such as cancer, diabetes, substance abuse, etc., which are significantly on the rise. The promise of data science can be fully realized only when collected data can be collaboratively shared and analyzed. However, the widespread increases in healthcare data breaches due to inappropriate access as well as the increasing number of novel privacy attacks restrict institutions from sharing data. Indeed, in some cases, the results of the analysis can themselves lead to significant privacy harm. The success of the data commons depends on ensuring the maximal access to data, subject to all of the patient privacy requirements including those mandated by legislation, and all of the constraints of the organization collecting the data itself. While there are existing solutions that can solve parts of the problem, there are significant challenges in truly incorporating these into comprehensive working solutions that are usable by the biomedical research community, and new challenges brought on by modern techniques such as deep learning. The long-term goal of this research is to develop technologies that can holistically enable data sharing while respecting privacy and security considerations and to ensure that they are implemented in existing platforms that have widespread acceptance in the research community. Towards this, the objective of this project is to develop complementary solutions for risk inference, distributed learning, and access control that can enable different modalities of data sharing. The problems studied are general in nature and will evolve depending on research successes and new impediments that arise. The proposed program of research is significant since lack of access to biomedical data can lead to fragmentation of care, resulting in higher economic and social costs, and is a significant impediment to biomedical research. The project will result in open-source, freely available software tools that will be integrated into widely used data collection, cohort identification, and distributed analytics platforms. There are several ongoing collaborations that will serve as initial pilot customers to provide use cases, identify the requirements, evaluate results, and in general validate the developed solutions. Project Narrative Statement of Relevance to Public Health Being able to ensure privacy and security while enabling data sharing and analysis is critical to pave the way forward for public health research and improve our understanding of diseases. The proposed work will address the challenges that impede the use of data across all of the different modalities of data sharing. The integration into existing platforms will ensure that the developed models, tools, and solutions directly impact the research community and improve public health interventions.",Developing novel technologies that ensure privacy and security in biomedical data science research,9851602,R35GM134927,"['Address', 'Biomedical Research', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Science', 'Diabetes Mellitus', 'Disease', 'Economics', 'Ensure', 'Goals', 'Healthcare', 'Institution', 'Lead', 'Learning', 'Malignant Neoplasms', 'Modality', 'Modeling', 'Modernization', 'Nature', 'Pathway interactions', 'Prevention', 'Privacy', 'Public Health', 'Research', 'Risk', 'Security', 'Software Tools', 'Statutes and Laws', 'Substance abuse problem', 'Techniques', 'Technology', 'Work', 'biomedical data science', 'care fragmentation', 'cohort', 'cost', 'data sharing', 'deep learning', 'improved', 'new technology', 'novel', 'open source', 'patient privacy', 'programs', 'public health intervention', 'public health research', 'social', 'success', 'tool']",NIGMS,RUTGERS THE STATE UNIV OF NJ NEWARK,R35,2020,382108,0.005056022434454374
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9851853,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'multidimensional data', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2020,161422,-0.07212322481595698
"Center for Modeling Complex Interactions Biomedical problems are innately complex, and their solutions require input from many fields. Many centers focus on a single disease or organ system. By contrast, the Center for Modeling Complex Interactions focuses on an approach that can address many biomedical problems: team-based, interdisciplinary research centered around modeling. Our goal is to support and facilitate biomedical discovery by integrating modeling into interdisciplinary research. Modeling improves research at all stages—hypothesis formulation, experimental design, analysis, and interpretation. It provides a unifying language by which exchange of ideas can highlight commonalities and uncover unforeseen connections between problems. Formalization of ideas into this unifying language also improves rigor and reproducibility. We define modeling broadly to include everything from deterministic and stochastic mathematical approaches, to physical and computational models of three- dimensional objects, to agent-based and machine learning approaches where exact solutions are not possible. We seek to support modelers by increasing their numbers, and by giving them opportunities to play on interdisciplinary teams. We seek to support empiricists by giving them access to relevant modeling expertise, and by creating a community and a culture to facilitate interdisciplinary research. In Phase I, the Center for Modeling Complex Interactions created the intellectual, cultural, and physical environment to promote team- based, interdisciplinary research. In Phase II, we will build on that foundation by maintaining a strong interdisciplinary culture to foster collaboration among people who might otherwise never connect, and by adding additional faculty to expand our modeling expertise. We have four Aims: 1) Support faculty to carry out model-based, interdisciplinary biomedical research and increase their competitiveness for external funding. Research in the Center is carried out in the context of Working Groups—zero-barrier, interdisciplinary, goal- focused teams that meet regularly to get work done. Supported research includes three Research Projects, Pilot Projects, Modeling Access Grants, and ad hoc teams. Our comprehensive plan for proposal preparation improves grantsmanship, and our staff assists with submission and grant management. 2) Increase University of Idaho’s faculty participation in biomedical research. We will add six new faculty as a commitment to this Phase II COBRE and attract broader participation from across the University. 3) Extend the reach of the Modeling Core into new areas of modeling to capitalize on emerging opportunities. The Modeling Core accelerates interdisciplinary research by placing Core Fellows at the hub of the research community. We have added new Core Initiatives in machine learning and geospatial modeling to stimulate research in these areas with high potential for future growth. 4) Establish a path to long-term sustainability under the umbrella of the Institute for Modeling Collaboration & Innovation. The major hurdle for sustainability is to maintain a robust Modeling Core. We have developed a business plan that calls for us to diversify our funding sources to include institutional, state, and private support to supplement federal grants. Human health is determined by interactions of complex biological systems at multiple scales, from the ecological to the biophysical; these are layered with spatial and temporal variation. To decipher these systems requires predictive modeling, coupled with strong empirical work, to be guided by and to feed the models. The Center for Modeling Complex Interactions generates model-based biomedical research and connects people who might otherwise never interact, which enhances the strong interdisciplinary culture of the University of Idaho.",Center for Modeling Complex Interactions,10026000,P20GM104420,"['Address', 'Area', 'Biological', 'Biomedical Research', 'Biophysics', 'Businesses', 'Centers of Research Excellence', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Models', 'Coupled', 'Data', 'Development', 'Disease', 'Ensure', 'Experimental Designs', 'Faculty', 'Feedback', 'Formulation', 'Fostering', 'Foundations', 'Funding', 'Funding Agency', 'Future', 'Generations', 'Goals', 'Grant', 'Growth', 'Health', 'Holly', 'Home environment', 'Human', 'Human Resources', 'Idaho', 'Incubators', 'Individual', 'Infrastructure', 'Institutes', 'Interdisciplinary Study', 'Language', 'Lead', 'Machine Learning', 'Modeling', 'Outcome', 'Phase', 'Physical environment', 'Pilot Projects', 'Play', 'Population', 'Postdoctoral Fellow', 'Preparation', 'Privatization', 'Progress Reports', 'Property', 'Reproducibility', 'Research', 'Research Institute', 'Research Project Grants', 'Research Support', 'Schedule', 'Scientist', 'Structure', 'Students', 'System', 'Testing', 'Time', 'Training', 'Uncertainty', 'Universities', 'Ursidae Family', 'Work', 'base', 'biological systems', 'body system', 'complex biological systems', 'experience', 'faculty support', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'mathematical methods', 'member', 'next generation', 'novel strategies', 'physical model', 'predictive modeling', 'spatial temporal variation', 'success', 'three-dimensional modeling', 'undergraduate student', 'working group']",NIGMS,UNIVERSITY OF IDAHO,P20,2020,2172986,0.01068447722923236
"Determination of structure, dynamics and energetics of enzyme reactions Project Summary  Understanding enzyme mechanisms is of paramount importance from both the basic biophysics perspective of understanding life processes and the role of enzymes in diseases. To achieve a detailed understanding of enzyme catalysis, the effects of protein structure and dynamics on the reaction energetics need to be elucidated. We propose a combined computational and experimental approach that combines the synthetic, computational and structural biology expertise of a team of investigators that has been working together for >15 years to create a “molecular movie” where the position, movement and energy of every atom in the system followed over the entire reaction pathway. The proposal exploits the emerging convergence of timescales accessible by molecular simulation using GPUs and time resolved structural biology. Specific Aim 1 describes the simulation of the complete reaction pathway of Pseudomonas mevalonii (Pm) HMGCoA Reductase (HMGR) and will use transition state force fields (TSFFs) generated by the quantum guided molecular mechanics method to allow the µsec MD simulations of the chemical steps. TSFFs not only circumvent the well-known boundary problem of QM/MM, but are also 102-104 times faster. This allows a realistic modeling of the coupling of µsec dynamics and catalysis that was demonstrated in the last grant period to be essential for understanding the reaction. Together with accelerated MD simulations of the conformational changes involved in the reaction using standard force fields, these computational studies cover the fsec to µsec timescale. In Specific Aim 2, the computational results will be merged with the results of a three-tiered approach to obtain structural snapshots with progressively increasing time resolution: (i) “Frozen” intermediates that map out the overall pathway on long timescales, (ii) time resolved Laue crystallography using pH jump initiation on the msec timescale and (iii) use of photocaged substrates to allow time resolved Laue experiments on the µsec timescale. This approach will be applied to the study of HMGR, an enzyme of high biophysical and biomedical significance that has a complex reaction mechanism involving three chemical steps, six large-scale conformational changes and two cofactor exchange steps. The project is highly innovative because it (i) uses a combination of MD simulations using TSFFs and time resolved crystallography to span timescales of at least 12 orders of magnitude, (ii) iteratively couples the Markov State analysis of long timescale trajectories to the Singular Value Decomposition used to analyze time resolved crystallography data, thus providing new tools to generate and experimentally validate trial structures (iii) applies global optimization and machine learning techniques to allow the automated fitting of TSFFs for proteins, which will enhance the application of this powerful method to other proteins and (iv) provides new photocaged substrates for the study of enzyme mechanisms to the chemical biology community. All tool compounds, methods and codes developed in this project will be made available to the scientific community. Public Health Statement  The detailed study of enzyme mechanisms is a cornerstone of biophysical chemistry that, while basic in nature, has had a major impact on human health including the development of new mechanism-based drugs for a range of diseases and an understanding of the mechanism of action for existing drugs that allows the design of combination therapies. The combination of Laue crystallography and long-scale MD simulations will allow simultaneous studies of structure, dynamics and energetic studies with unprecedented detail. The application to HMG CoA Reductase, arguably the single most important drug target in western industrialized countries, will demonstrate the applicability of the methodology to an enzyme of high mechanistic complexity. 1","Determination of structure, dynamics and energetics of enzyme reactions",9897100,R01GM111645,"['Active Sites', 'Anti-Bacterial Agents', 'Biochemistry', 'Biological', 'Biology', 'Biophysics', 'Catalysis', 'Chemicals', 'Cholesterol', 'Code', 'Collaborations', 'Combined Modality Therapy', 'Communities', 'Complex', 'Computational Biology', 'Computing Methodologies', 'Couples', 'Coupling', 'Crystallization', 'Crystallography', 'Data', 'Developed Countries', 'Development', 'Disease', 'Drug Targeting', 'Enzymatic Biochemistry', 'Enzymes', 'Equilibrium', 'Free Energy', 'Freezing', 'Goals', 'Grant', 'Health', 'Human', 'Hydroxymethylglutaryl-CoA reductase', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Movement', 'Mutagenesis', 'Nature', 'Oxidoreductase', 'Pathway interactions', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Protein Dynamics', 'Proteins', 'Pseudomonas', 'Public Health', 'Reaction', 'Research Personnel', 'Resolution', 'Roentgen Rays', 'Role', 'Running', 'Science', 'Structure', 'System', 'Techniques', 'Time', 'Validation', 'Work', 'base', 'biophysical chemistry', 'cofactor', 'computer studies', 'design', 'electron density', 'enzyme mechanism', 'experience', 'experimental study', 'improved', 'innovation', 'machine learning method', 'millisecond', 'molecular dynamics', 'molecular mechanics', 'molecular scale', 'movie', 'new therapeutic target', 'particle', 'protein structure', 'quantum', 'simulation', 'structural biology', 'synthetic biology', 'theories', 'tool']",NIGMS,UNIVERSITY OF NOTRE DAME,R01,2020,335664,-0.034007438496180355
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9916801,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer’s disease biomarker', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'diverse data', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multidimensional data', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2020,341471,-0.05925774429233081
"Imaging Molecular Level Details of Collagen Fibers by VSFG Microscopy In this proposed project, we plan to fill the knowledge gap of the relationships between microscopic self-assembled structures, collagen-molecule interactions and macroscopic fiber morphologies of type-I collagen, the primary component of most human tissues and a commonly used biomaterial for tissue engineering. By investigating collagen-water and collagen-protein interactions in in vitro systems that mimic basic aspects of physiologically relevant three- dimensional fibrillar tissue architectures, we aim to fill knowledge gaps in fundamental collagen research. We will achieve this goal by developing a hyperspectral imaging technique – vibrational sum frequency generation (VSFG) microscopy – at high repetition rates (400 kHz) and apply it to collagen. The long-term vision is to develop new biophysics methods to reveal molecular-level structures and interactions for pericellular space research and other complex biological environments, and eventually applying it to study various pericellular environment related diseases. In order to correlate spectral features to microscopic and macroscopic structures of type I collagen, we plan to apply machine-learning techniques to analyze our data and extract spectral signatures of collagen’s micro/macrostructures. We will two major scientific focuses: (A) understanding molecular signatures of microscopic self-assembly fibrils structures and its relationship to the macroscopic morphology (plan 1 and 2); and (B) investigating molecular level collagen-molecule interactions (plan 3 and 4). Specific plans include:  1. Obtaining hyperspectral VSFG images of collagen tissues to study their morphology in a  label free and non-invasive manner  2. Establishing molecular spectral signatures of self-assembled collagen fibril structures  3. Understanding collagen-water interaction in first solvation layer of collagen fibers.  4. Imaging spatial locations of chemicals and peptides that interact with collagens. If successful, the significance is that a label free, vibrational mode specific imaging technique specific for pericellular space will be available, which can reveal molecular level insights of collagen structures and its interactions with surrounding molecules, pertinent to fibrosis and cell— pericellular space interaction related diseases. This proposed project contributes to the scope of NIGMS by developing new technology to reveal fundamental molecular-level principle, mechanism and signatures related to morphology of collagen I at both micro- and macroscopic scales, and collagen-molecule interactions, laying foundations for biophysical/biochemical principles for future biomedical applications related to collagens. This proposed development of vibrational sum frequency generation microscopy, in the short term, will spatially resolve collagen tissues with chemical structure and molecular interaction information in a complicated environment. Machine learning and simulation approaches will be employed to build a data base to convert hyperspectral images of collagen into a spatial map with microscopic structures and molecular interaction information. In the long term, the fundamental biochemical knowledge learned from this development will lay foundations for rationally design biomedical approaches to monitor and control pericellular spaces and its interaction with cells, and further advance treatment to diseases related to it.",Imaging Molecular Level Details of Collagen Fibers by VSFG Microscopy,10028946,R35GM138092,"['3-Dimensional', 'Architecture', 'Binding', 'Biochemical', 'Biocompatible Materials', 'Biological', 'Biophysics', 'Cells', 'Chemical Structure', 'Chemicals', 'Collagen', 'Collagen Fiber', 'Collagen Fibril', 'Collagen Type I', 'Complex', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Fiber', 'Fibrosis', 'Foundations', 'Frequencies', 'Future', 'Generations', 'Goals', 'Image', 'Imaging Techniques', 'In Vitro', 'Knowledge', 'Label', 'Location', 'Machine Learning', 'Maps', 'Microscopic', 'Microscopy', 'Molecular', 'Molecular Profiling', 'Monitor', 'Morphology', 'National Institute of General Medical Sciences', 'Peptides', 'Physiological', 'Proteins', 'Research', 'Structure', 'Sum', 'System', 'Techniques', 'Tissue Engineering', 'Tissues', 'Vision', 'Water', 'biophysical techniques', 'design', 'human tissue', 'image reconstruction', 'insight', 'molecular imaging', 'new technology', 'self assembly', 'simulation', 'vibration']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R35,2020,393958,-0.027333977567261157
"Pacific Northwest Advanced Compound Identification Core OVERALL SUMMARY The capability to chemically identify thousands of metabolites and other chemicals in clinical samples will revolutionize the search for environmental, dietary, and metabolic determinants of disease. By comparison to near-comprehensive genetic information, comparatively little is understood of the totality of the human metabolome, largely due to insufficiencies in molecular identification methods. Through innovations in computational chemistry and advanced ion mobility separations coupled with mass spectrometry, we propose to overcome a significant, long standing obstacle in the field of metabolomics: the absence of methods for accurate and comprehensive identification of metabolites without relying on data from analysis of authentic chemical standards. A paradigm shift in metabolomics, we will use gas-phase molecular properties that can be both accurately predicted computationally and consistently measured experimentally, and which can thus be used for comprehensive identification of the metabolome without the need for authentic chemical standards. The outcomes of this proposal directly advance the mission and goals of the NIH Common Fund by: (i) transforming metabolomics science by enabling consideration of the totality of the human metabolome through optimized identification of currently unidentifiable molecules, eventually reaching hundreds of thousands of molecules, and (ii) developing standardized computational tools and analytical methods to increase the national capacity for biomedical researchers to identify metabolites quickly and accurately. This work is significant because it enables comprehensive and confident chemical measurement of the metabolome. This work is innovative because it utilizes an integrated quantum-chemistry and machine learning computational pipeline to accurately predict physical-chemical properties of metabolites coupled to measurements. OVERALL NARRATIVE This project will utilize integrated quantum-chemistry and machine learning computational computational approaches coupled with advanced instrumentation to characterize the human metabolome, and identify currently unidentifiable molecules without the use of authentic chemical standards. Results from these studies will contribute to the goal of understanding diseases, and the tools and resources will be made publically available for biomedical researchers.",Pacific Northwest Advanced Compound Identification Core,9968331,U2CES030170,"['Adoption', 'Algorithms', 'Analytical Chemistry', 'Attributes of Chemicals', 'Biological', 'Biological Markers', 'Biomedical Research', 'Chemical Structure', 'Chemicals', 'Clinical', 'Communities', 'Computers and Advanced Instrumentation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Databases', 'Dependence', 'Diet', 'Disease', 'Educational workshop', 'Engineering', 'Exposure to', 'Funding', 'Gases', 'Genetic', 'Goals', 'High Performance Computing', 'Human', 'Isotopes', 'Libraries', 'Liquid substance', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Molecular', 'Outcome', 'Pacific Northwest', 'Phase', 'Predictive Analytics', 'Probability', 'Procedures', 'Property', 'Reference Standards', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Science', 'Serum', 'Source', 'Standardization', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Uncertainty', 'United States National Institutes of Health', 'Urine', 'Work', 'analytical method', 'base', 'chemical property', 'chemical standard', 'comparative', 'computational chemistry', 'computational pipelines', 'computerized tools', 'dark matter', 'drug candidate', 'drug discovery', 'experience', 'genetic information', 'human disease', 'improved', 'in silico', 'innovation', 'instrumentation', 'ion mobility', 'metabolome', 'metabolomics', 'non-genetic', 'novel', 'novel therapeutics', 'programs', 'quantum chemistry', 'small molecule libraries', 'stereochemistry', 'tool']",NIEHS,BATTELLE PACIFIC NORTHWEST LABORATORIES,U2C,2020,1024120,0.015429367831007406
"Pacific Northwest Advanced Compound Identification Core OVERALL SUMMARY The capability to chemically identify thousands of metabolites and other chemicals in clinical samples will revolutionize the search for environmental, dietary, and metabolic determinants of disease. By comparison to near-comprehensive genetic information, comparatively little is understood of the totality of the human metabolome, largely due to insufficiencies in molecular identification methods. Through innovations in computational chemistry and advanced ion mobility separations coupled with mass spectrometry, we propose to overcome a significant, long standing obstacle in the field of metabolomics: the absence of methods for accurate and comprehensive identification of metabolites without relying on data from analysis of authentic chemical standards. A paradigm shift in metabolomics, we will use gas-phase molecular properties that can be both accurately predicted computationally and consistently measured experimentally, and which can thus be used for comprehensive identification of the metabolome without the need for authentic chemical standards. The outcomes of this proposal directly advance the mission and goals of the NIH Common Fund by: (i) transforming metabolomics science by enabling consideration of the totality of the human metabolome through optimized identification of currently unidentifiable molecules, eventually reaching hundreds of thousands of molecules, and (ii) developing standardized computational tools and analytical methods to increase the national capacity for biomedical researchers to identify metabolites quickly and accurately. This work is significant because it enables comprehensive and confident chemical measurement of the metabolome. This work is innovative because it utilizes an integrated quantum-chemistry and machine learning computational pipeline to accurately predict physical-chemical properties of metabolites coupled to measurements. OVERALL NARRATIVE This project will utilize integrated quantum-chemistry and machine learning computational computational approaches coupled with advanced instrumentation to characterize the human metabolome, and identify currently unidentifiable molecules without the use of authentic chemical standards. Results from these studies will contribute to the goal of understanding diseases, and the tools and resources will be made publically available for biomedical researchers.",Pacific Northwest Advanced Compound Identification Core,10260964,U2CES030170,"['Adoption', 'Algorithms', 'Analytical Chemistry', 'Attributes of Chemicals', 'Biological', 'Biological Markers', 'Biomedical Research', 'Chemical Structure', 'Chemicals', 'Clinical', 'Communities', 'Computers and Advanced Instrumentation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Databases', 'Dependence', 'Diet', 'Disease', 'Educational workshop', 'Engineering', 'Exposure to', 'Funding', 'Gases', 'Genetic', 'Goals', 'High Performance Computing', 'Human', 'Isotopes', 'Libraries', 'Liquid substance', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Molecular', 'Outcome', 'Pacific Northwest', 'Phase', 'Predictive Analytics', 'Probability', 'Procedures', 'Property', 'Reference Standards', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Science', 'Serum', 'Source', 'Standardization', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Uncertainty', 'United States National Institutes of Health', 'Urine', 'Work', 'analytical method', 'base', 'chemical property', 'chemical standard', 'comparative', 'computational chemistry', 'computational pipelines', 'computerized tools', 'dark matter', 'drug candidate', 'drug discovery', 'experience', 'genetic information', 'human disease', 'improved', 'in silico', 'innovation', 'instrumentation', 'ion mobility', 'metabolome', 'metabolomics', 'non-genetic', 'novel', 'novel therapeutics', 'programs', 'quantum chemistry', 'small molecule libraries', 'stereochemistry', 'tool']",NIEHS,BATTELLE PACIFIC NORTHWEST LABORATORIES,U2C,2020,152500,0.015429367831007406
"N3C & All of Us Research Program Collaborative Project Project Summary/Abstract The COVID-19 pandemic presents unprecedented clinical and public health challenges. Though institutions collect large amounts of clinical data about COVID-19 cases, these datasets individually might not be diverse enough to draw population level conclusions. Also, statistical, machine learning, and causal analyses are most successful with large-scale data beyond what is available in any given organization. To tackle this problem, NCATS introduced the National COVID Cohort Collaborative (N3C), an open science, community-based initiative to share patient level data for analysis. The initiative requires participating institutions to share information about their COVID-19 patients in a standard-driven way, including demographics, vital signs, diagnoses, laboratory results, medications, and other treatments. The data from multiple institutions will be merged and consolidated, and access will be provided to investigators through a centralized analytical platform. The COVID-19 data sharing collaboration with the N3C initiative offers a mechanism to initiate collaborations with other NIH sponsored data sharing programs, such as the All of Us Research Program (AoURP). This administrative supplement will support efforts to clean and standardize data at VCU, and to transfer it to the N3C data repository. The supplement will also assist in introducing new services at the Wright Center to support our investigators to use the N3C resources. It will also enable collaboration with the AoURP by establishing a pipeline to collect and transmit consented patients' EHR data and by building on existing community outreach pathways to recruit additional participants for the AoURP. The project will be overseen by the PI/Executive Committee and supervised by the Director of Research Informatics. Procedures and services developed at our local CTSA hub will be shared and disseminated to the CTSA network. Project Narrative NIH/NCATS has been working on the National COVID Cohort Collaborative (N3C), which aims to build a centralized national data resource to be used by the research community to study the COVID-19 pandemic and identify potential treatments as the pandemic continues to evolve. The COVID-19 data sharing collaboration with the N3C initiative also offers a mechanism to initiate collaborations with the All of Us Research Program (AoURP). This administrative supplement will support the creation and management of a data extraction and transfer pipeline to the N3C and AoURP data repositories from VCU.",N3C & All of Us Research Program Collaborative Project,10217339,UL1TR002649,"['Administrative Supplement', 'All of Us Research Program', 'COVID-19', 'COVID-19 pandemic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Outreach', 'Consent', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Ecosystem', 'Effectiveness', 'Funding Opportunities', 'Goals', 'Health', 'Health Status', 'Individual', 'Informatics', 'Infrastructure', 'Institution', 'Laboratories', 'Outcomes Research', 'Participant', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Procedures', 'Public Health', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Services', 'Supervision', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'base', 'biomedical informatics', 'clinical center', 'cohort', 'coronavirus disease', 'data resource', 'data sharing', 'data standards', 'data warehouse', 'demographics', 'design', 'improved', 'informatics infrastructure', 'innovation', 'large scale data', 'multi-site trial', 'network informatics', 'open data', 'pandemic disease', 'parent grant', 'programs', 'recruit', 'response', 'statistical and machine learning', 'tool']",NCATS,VIRGINIA COMMONWEALTH UNIVERSITY,UL1,2020,346608,-0.013789013867425242
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9992596,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,20899,0.005704982489860433
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9899262,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,430738,0.005704982489860433
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,10000112,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'ZIKA', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'data infrastructure', 'data modeling', 'data streams', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'heterogenous data', 'improved', 'innovation', 'mathematical methods', 'multiple data sources', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2020,365601,0.009746786964973388
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9856493,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'data exchange', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual machine', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,339098,0.015366060773164082
"A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth This project provides a data science framework and a toolbox of best practices for systematic and reproducible data-driven methods for validating and deriving RDoC constructs with relevance to psychopathology. Despite recent advances in methods for data-driven constructs, results are often hard to reproduce using samples from other studies. There is a lack of systematic statistical methods and analytical design for enhancing reproducibility. To fill this gap, we will develop a data science framework, including novel scalable algorithms and software, to derive and validate RDoC constructs. Although the proposed methods will generally apply to all RDoC domains and constructs, we focus specifically on furthering understanding of the RDoC domains of cognitive control (CC) and attention (ATT) constructs implicated in attention deficit disorder (ADHD) and obsessive-compulsive disorder (OCD). Our application will use multi-modal neuroimaging, behavioral, and clinical/self-report data from large, nationally representative samples from the on Adolescent Brain Cognitive Development (ABCD) study and multiple local clinical samples with ADHD and OCD. Specifically, using the baseline ABCD samples, in aim 1, we will apply and develop methods to assess and validate the current configuration of RDoC for CC and ATT using confirmatory latent variable modeling. We will implement and develop new unsupervised learning methods to construct new computational-driven, brain-based domains from multi-modal image data. In Aim 2, We will introduce network analysis (via Gaussian graphical models) to characterize heterogeneity in the interrelationship of RDoC measurements due to observed characteristics (i.e., age and sex). We will further model the heterogeneity of the population due to unobserved characteristics by introducing the data-driven precision phenotypes, which are the subgroup of participants with similar RDoC dimensions. We propose a Hierarchical Bayesian Generative Model and scalable algorithm for simultaneous dimension reduction and identify precision phenotypes. The model also serves as a tool to transfer information from the community sample ABCD to local clinical enriched studies. In aim 3, we will utilize the follow-up samples from ABCD and local clinical enriched data sets to validate the results from Aims 1 and 2 and assess the clinical utility of the precision phenotypes in predicting psychological development in follow-up time. Our project will provide a suite of analytical tools to validate existing RDoC constructs and derive new, reproducible constructs by accounting for various sources of heterogeneity. To advance the understanding of psychopathology using dimensional constructs of measurements from multiple units of analysis, we propose reproducible statistical framework for validating and deriving RDoC constructs with relevance to psychopathology. We will use multi-modal neuroimaging, behavioral and clinical/self-report data from multiple samples to develop this framework. The design of our study consists of analyzing large, nationally representative samples, validating the results in local clinically enriched samples, and transfer information from the large community samples to local clinical samples.",A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth,10058921,R01MH124106,"['11 year old', 'Accounting', 'Adolescent', 'Age', 'Algorithmic Software', 'Algorithms', 'Attention', 'Attention Deficit Disorder', 'Base of the Brain', 'Behavioral', 'Brain', 'Characteristics', 'Child', 'Chronology', 'Clinical', 'Clinical Data', 'Communities', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Dimensions', 'Ensure', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Goals', 'Heterogeneity', 'Image', 'Knowledge', 'Learning', 'Link', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Obsessive-Compulsive Disorder', 'Participant', 'Pathway Analysis', 'Patient Self-Report', 'Phenotype', 'Population Heterogeneity', 'Prediction of Response to Therapy', 'Psychological Transfer', 'Psychopathology', 'Reproducibility', 'Reproducibility of Results', 'Research Domain Criteria', 'Sampling', 'Source', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'Time', 'Variant', 'Youth', 'age effect', 'analytical tool', 'autoencoder', 'base', 'biological sex', 'cognitive control', 'cognitive development', 'deep learning', 'design', 'follow up assessment', 'follow-up', 'high dimensionality', 'independent component analysis', 'insight', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'multimodality', 'network models', 'neuroimaging', 'novel', 'psychologic', 'response', 'sex', 'tool', 'unsupervised learning']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2020,710101,0.0025946352985279602
"Meta-analysis in human brain mapping This is the competing renewal of R01MH074457-13, which sustains the BrainMap Project (www.brainmap.org). The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data. To date, the BrainMap Project has designed and populated two coordinate-based databases: 1) a task-activation repository (TA DB); and, 2) a voxel-based morphometry repository (VBM DB). The TA DB contains >17,200 experiments, collectively representing > 78,000 subjects and > 110 task- activation paradigms. The VBM DB contains > 3,100 experiments, collectively representing > 81,000 subjects with > 80 psychiatric, neurologic and developmental disorders with ICD-10 coding. The BrainMap Project has created, optimized and validated an integrated pipeline of multi-platform (Javascript), open-access tools to curate (Scribe), filter and retrieve (Sleuth), analyze (GingerALE), visualize (Mango) and interpret analysis output (BrainMap meta-data plugins for Mango). Several network-modeling approaches have been applied to BrainMap data -- MACM, independent components analysis (ICA), graph theory modeling (GTM), author-topic modeling (ATM), structural equation modeling (SEM), and connectivity-based parcellation (CBP) – but none are yet pipeline components. Utilization of these CBMA resources is substantial: BrainMap software, data and meta-data have been used in > 825 peer-reviewed publications. Of these, > 350 were published within the current funding period (April 2015-March 2019; brainmap.org/pubs). In this competing renewal, four tool- development aims are proposed, each of which extends this high-impact research resource. Aim 1. Database Expansion. BrainMap data repositories will be expanded. Aim 2. Meta-analytic Network Modeling. Network modeling will be added to the BrainMap pipeline. Aim 3. Large-Scale Simulations, Comparisons and Validations. Data simulations, characterizations and validations will be performed. Aim 4. Meta-data Inferential tools. Tools for mining BrainMap’s location-linked meta-data will be expanded. Data Sharing Plan. BrainMap data, meta-data, pipeline tools, and templates created by whole-database modeling (e.g., ICA and ATM network masks) are shared at BrainMap.org. Of all new data entries, more than half are contributed by BrainMap users, i.e., community data sharing via BrainMap.org. For community-coded entries, the BrainMap team provides curation and quality control. Comprehensive database images (database dumps) are available to tool developers through Collaborative Use Agreements. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data  sets, metadata, computational tools, and related resources that enable coordinate-­based meta-­analyses  (CBMA), meta-­analytic connectivity modeling (MACM), meta-­data informed interpretation (“decoding”) of  imaging results, and meta-­analytic priors for mining (including machine learning) primary (per-­subject)  neuroimaging data.    ",Meta-analysis in human brain mapping,10056029,R56MH074457,"['Agreement', 'Area', 'Brain', 'Brain Mapping', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Set', 'Databases', 'Disease', 'Educational workshop', 'Equation', 'Functional disorder', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Institution', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Java', 'Link', 'Location', 'Machine Learning', 'Mango - dietary', 'Masks', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Output', 'Peer Review', 'Plug-in', 'Publications', 'Publishing', 'Quality Control', 'Research Domain Criteria', 'Resources', 'Rest', 'Site', 'Software Framework', 'Specificity', 'Structure', 'Training', 'Universities', 'Validation', 'base', 'candidate marker', 'computerized tools', 'data pipeline', 'data sharing', 'data warehouse', 'design', 'developmental disease', 'experimental study', 'graph theory', 'independent component analysis', 'interest', 'large scale simulation', 'morphometry', 'nervous system disorder', 'network architecture', 'network models', 'neuroimaging', 'neuropsychiatric disorder', 'repository', 'simulation', 'tool', 'tool development']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R56,2020,543396,-0.0037005061835752974
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,10002192,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'data standards', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2020,427122,0.01886946853885739
"Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design PROJECT SUMMARY/ABSTRACT Molecular simulation is a powerful tool to predict the properties of biomolecules, interpret biophysical experiments, and design small molecules or biomolecules with therapeutic utility. However, a number of obstacles have impeded the development of quantitative, cloud-scale research workﬂows involving biomolecular simulation. Two main ob- stacles are the insufﬁcient accuracy of current atomistic models for biomolecules and small molecule therapeutics and the lack of interoperability in simulation toolchains used in both academic and industrial biomolecular research. Our original R01, “Open Data-driven Infrastructure for Building Biomolecular Force Fields for Predictive Bio- physics and Drug Design,” seeks to solve the ﬁrst problem. It helps fund our effort, the Open Force Field Initiative (https://openforceﬁeld.org) to develop open, extensible, and shared software and data infrastructure, implementing statistically robust methods of parameterizing force ﬁelds and choosing new force ﬁelds in a statistically sound manner. This work is designed to create not just a new generation of force ﬁelds, but an open technology to continue advancing force ﬁeld science. However, even with improved molecular models, putting together complete workﬂows of biomolecular simulations involves interfacing substantial numbers of different tools. However the majority of the existing molecular simulation workﬂows are mutually incompatible, with differing representations of the molecular models. The Open Force Field Initiative effort already includes the development of molecular data structures that we can ex- port into existing molecular simulation tools. We propose to extend the existing scope of our R01 to create an extensible common molecular simulation representation and translators to and from this representation. Such a set of tools will immediately make it signiﬁcantly easier to combine the disparate workﬂows developed for different sets of molecular simulation tools. Researchers will be able to set up and build the biophysical simulations using their usual tools, but run and analyze them with currently incompatible tools, enabling better matching of computational resources and methods to problems. It will help avoid trapping in a single software framework, and enable combinations of functionalities previously impossible without substantial developer time and effort. We will (Aim 1) work with partners to generalize our modular, extensible object model for representing parameterized biomolecular systems in a manner that accommodates the force ﬁeld terms currently supported by most popular biomolecular simulation packages. We will engineer it to be extensible to advanced interaction forms, such as polarizability and other multibody terms, and machine learning models for intermolecular forces. We will (Aim 2): enable easy conversion between components of molecular simulation workﬂows by allowing other molecular simulation packages to easily store their representations in this data model, developing converters that can import/export this object model to multiple popular ﬁle formats, focusing initially on OpenMM, AMBER, CHARMM, and GROMACS. We will demonstrate the utility of this interface in cloud-ready workﬂows. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life carry out their functions and to design new medications. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms. This supplement will make it much easier for molecular simulation workﬂows to interoperate with each other in large-scale workﬂows.",Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design,10166314,R01GM132386,"['Affinity', 'Binding', 'Biophysics', 'COVID-19', 'Collaborations', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA', 'Development', 'Drug Design', 'Ecosystem', 'Engineering', 'Funding', 'Generations', 'Human', 'Individual', 'Industrialization', 'Infrastructure', 'Language', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Motion', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Problem Solving', 'Property', 'Proteins', 'Pythons', 'RNA', 'Readability', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Science', 'Scientist', 'Software Framework', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'Writing', 'biomaterial interface', 'computing resources', 'data infrastructure', 'data modeling', 'design', 'experimental study', 'file format', 'improved', 'interoperability', 'molecular modeling', 'open data', 'simulation', 'small molecule', 'small molecule therapeutics', 'software infrastructure', 'sound', 'structured data', 'tool']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,225000,0.006373795728119353
"IEEE International Symposium on Biomedical Imaging (ISBI) 2020 Project Summary This R13 application will provide travel support for competitively selected U.S.-based students, postdoctoral fellows, and junior faculty to present their work at the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 to be held April 4-8, 2020 at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa. ISBI is a scientific conference dedicated to mathematical, algorithmic, and computational aspects of biological and biomedical imaging, across all scales of observation (from microscopic to whole-body imaging) and is sponsored by both the IEEE Signal Processing Society (SPS) and the IEEE Engineering in Medicine and Biology Society (EMBS). It attracts approximately 600-700 attendees each year involved in biomedical imaging research and development from academic institutions, government laboratories, and private companies. Since its inception in 2002, ISBI has become a leading international conference bringing together researchers from diverse algorithmic fields, applications, modalities, and size scales, to facilitate cross-fertilization of ideas. ISBI, like other IEEE SPS and EMBS conferences, requires submission and review of four-page papers which are peer-reviewed much like journal articles. In addition to oral and poster presentations of peer-reviewed papers in multiple tracks during the main conference, ISBI 2020 will also include special student events, tutorials, a “clinical day” emphasizing multi-disciplinary presentations/collaborations, plenary talks, workshops, and onsite grand challenges. Recipients of the travel awards will be competitively selected based on need and scientific excellence. U.S.- based students, postdoctoral fellows, and junior faculty with accepted papers will be eligible to apply. We will be particularly supportive in providing travel awards to women and under-represented groups to help increase the diversity of the attendees. We anticipate that the travel awards will provide sufficient funding to the awardees to make the cost-benefit ratio for attendance extremely favorable. Through their attendance at ISBI, the awardees will benefit through their exposure to the simultaneous breadth and depth of topics offered at ISBI (presented by a mixture of leaders in the field as well as those early in their careers), their experience of presenting their work at an international conference, and their interactions and discussions with other attendees and leaders in the field. The conference as a whole will also benefit by not only enabling the high-quality work of the attendees to be presented but by also enabling an increased attendance of (and discussions/ideas/interactions with) U.S.-based students, postdoctoral fellows, and early career faculty with diverse backgrounds. Project Narrative This application requests funds to provide travel support for students, postdoctoral fellows, and/or early- career faculty to attend and participate in the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 conference, to be held at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa, April 4-8, 2020. The conference covers many of the mathematical and computational aspects of biological and biomedical imaging problems of high relevance to human health, and hence is of high relevance to the interests of the National Institutes of Health.",IEEE International Symposium on Biomedical Imaging (ISBI) 2020,9914410,R13EB029304,"['Address', 'Algorithms', 'Appointment', 'Area', 'Artificial Intelligence', 'Award', 'Biological', 'Biological Models', 'Biology', 'Biomedical Computing', 'Breeding', 'Budgets', 'Cities', 'Clinical', 'Collaborations', 'Complement', 'Computational algorithm', 'Computer Models', 'Costs and Benefits', 'Data', 'Educational workshop', 'Engineering', 'Event', 'Exposure to', 'Faculty', 'Fertilization', 'Funding', 'Future', 'Generations', 'Goals', 'Government', 'Grant', 'Growth', 'Health', 'Human', 'Image', 'Image Analysis', 'Imaging problem', 'Institution', 'International', 'Iowa', 'Laboratories', 'Location', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Microscopic', 'Modality', 'Modeling', 'Oral', 'Paper', 'Participant', 'Peer Review', 'Postdoctoral Fellow', 'Privatization', 'Recommendation', 'Request for Applications', 'Research', 'Research Personnel', 'Societies', 'Statistical Models', 'Students', 'Training', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'Visualization', 'Women&apos', 's Group', 'Work', 'authority', 'base', 'bioimaging', 'body system', 'career', 'computerized tools', 'cost', 'early-career faculty', 'experience', 'graduate student', 'imaging modality', 'innovation', 'interest', 'journal article', 'mathematical algorithm', 'meetings', 'member', 'multidisciplinary', 'physical model', 'posters', 'programs', 'reconstruction', 'research and development', 'signal processing', 'student participation', 'success', 'supportive environment', 'symposium', 'whole body imaging']",NIBIB,UNIVERSITY OF IOWA,R13,2020,5000,-0.005673343163238931
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domainagnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10057127,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2020,609068,-0.00997019232776762
"Center for Open Bioimage Analysis Project Summary  The Center for Open Bioimage Analysis will serve the cell biology community’s growing need for sophisticated software for light microscopy image analysis. Quantitative image analysis has become an indispensable tool for biologists using microscopy throughout basic biological and biomedical research.  Quantifying images is now a critical, widespread need as imaging experiments continue to grow in scale, size, dimensionality, scope, modality, and complexity. Many biologists are missing out on the quantitative bioimaging revolution due to lack of effective algorithms and/or usable software for their needs, or lack of access to training. The Center brings together the Carpenter laboratory at the Broad Institute and the Eliceiri laboratory at the University of Wisconsin­Madison, and in doing so brings together the two most popular open source bioimage analysis projects, ImageJ (including ImageJ2 and FIJI) and CellProfiler. Through the collaborative development and dissemination of open source image analysis software, as well as training events and resources, the Center will empower thousands of researchers to apply advanced analytics in innovative ways to address new experimental areas.  Building on the team’s expertise developing algorithms and user­friendly software for use in biology under real­world conditions, the Center will focus on two Technology Research and Development (TR&D) projects: deep learning­based image processing, and accessibility of image­processing algorithms for biologists. This work will not occur in isolation at the Center; rather, the Center will nucleate a larger community working on these two areas and serve as a catalyst and organizing force to create software and resources shared by all.  The Driving Biological Projects (DBPs) will serve a major role in driving the TR&D work: our teams are accustomed to working deeply and iteratively on problems side by side and with frequent feedback from biologists. This will ensure that important cell biological problems drive the work of the Center. The DBPs reflect tremendous variety in terms of biological questions, model systems, imaging modalities, and researcher expertise and will ensure robustness of our tools for the widest possible impact on the community. Continuing the teams’ track record with ImageJ and CellProfiler, two mature open source bioimage analysis software projects critical to the work of biologists worldwide, the Center will also assist and train biologists in applying the latest computational techniques to important biological problems involving images.  In short, the need for robust, accurate, and readily usable software is more urgent than ever. The Center for Open Bioimage Analysis will serve as a hub for pioneering new computational strategies for diverse biological problems, translating them into user­friendly software, further developing ImageJ and CellProfiler, and training the biological community to apply advanced software to important and diverse problems in cell biology. Project Narrative Biologists studying a huge variety of diseases and basic biological processes need software to measure cells, tissues, and organisms in microscopy images. We will create the Center for Open Bioimage Analysis which will catalyze the scientific community, creating resources, free software, and training that allow biologists to analyze images using deep learning and other new image processing algorithms, offering improved accuracy, convenience, and reproducibility.",Center for Open Bioimage Analysis,9855767,P41GM135019,"['Address', 'Algorithmic Software', 'Algorithms', 'Area', 'Automobile Driving', 'Benchmarking', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Cellular Structures', 'Cellular biology', 'Characteristics', 'Collaborations', 'Communities', 'Complex', 'Computational Technique', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Data Science', 'Development', 'Dimensions', 'Disease', 'Educational workshop', 'Ensure', 'Event', 'Feedback', 'Hand', 'Image', 'Image Analysis', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Measures', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Modernization', 'Organism', 'Organoids', 'Reproducibility', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Role', 'Savings', 'Scientist', 'Side', 'Software Engineering', 'System', 'Technology', 'Time', 'Tissues', 'Training', 'Translating', 'Universities', 'Wisconsin', 'Work', 'advanced analytics', 'algorithmic methodologies', 'base', 'bioimaging', 'biological research', 'biological systems', 'catalyst', 'deep learning', 'experimental study', 'hackathon', 'image processing', 'imaging modality', 'improved', 'innovation', 'light microscopy', 'microscopic imaging', 'next generation', 'novel', 'open source', 'quantitative imaging', 'research and development', 'skills', 'symposium', 'technology research and development', 'tool', 'user friendly software', 'user-friendly']",NIGMS,"BROAD INSTITUTE, INC.",P41,2020,1835520,-0.004824412761309275
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9979969,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Estrogen receptor positive', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'data standards', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'public repository', 'repository', 'structured data', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2020,511367,-0.004442602507557163
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,0.016438341531768236
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9924432,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device', 'wearable sensor technology']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,298890,-0.01181065832004035
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",10023935,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'automated segmentation', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2020,256578,0.016217469238226753
"Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria PROJECT SUMMARY Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria. Antibiotic-resistant Gram-negative bacterial infections are increasing in incidence and novel antibiotics are urgently needed to combat this growing threat to public health. A major roadblock to the development of novel antibiotics is our poor understanding of the structural features of small molecules that correlate with bacterial penetration and efflux. As a result, while potent biochemical inhibitors can often be identified for new targets, developing them into compounds with whole-cell antibacterial activity has proven challenging. To address this critical problem, we propose herein a comprehensive, multidisciplinary approach to develop quantitative models to predict small-molecule penetration and efflux in Gram-negative bacteria. We have pioneered a general platform for systematic, quantitative evaluation of small-molecule accumulation in bacteria, using label-free LC-MS/MS detection and multivariate cheminformatic analysis. We have also developed unique isogenic strain sets of wild-type, hyperporinated, efflux-knockout, and doubly-compromised E. coli, P. aeruginosa, and A. baumannii that allow us to dissect the individual contributions of outer/inner membrane penetration and active efflux to net accumulation, using a kinetic model that accurately recapitulates available experimental data. Moreover, we have developed machine learning and neural network approaches to QSAR (quantitative structure–activity relationship) modeling of pharmacological properties that will now be used to develop predictive cheminformatic models for Gram-negative accumulation, penetration, and efflux. This project will be carried out by a multidisciplinary SPEAR-GN Project Team (Small-molecule Penetration & Efflux in Antibiotic-Resistant Gram-Negatives, “speargun”) involving the labs of Derek Tan (MSK, PI), Helen Zgurskaya (OU, PI), Bradley Sherborne (Merck, Lead Collaborator), Valentin Rybenkov (OU, Co-I), Adam Duerfeldt (OU, Co-I), Carl Balibar (Merck, Collaborator), and David McLaren (Merck, Collaborator), comprising extensive combined expertise in organic and diversity-oriented synthesis, biochemistry, microbiology, high- throughput screening, mass spectrometry, biophysical modeling, cheminformatics, and medicinal chemistry. Herein, we will design and synthesize chemical libraries with diverse structural and physicochemical properties; analyze their accumulation in the isogenic strain sets in both high-throughput and high-density assay formats; extract kinetic parameters for penetration and efflux from the resulting experimental datasets; develop and validate robust QSAR models for accumulation, penetration, and efflux; and demonstrate the utility of these models in medicinal chemistry campaigns to develop novel Gram-negative antibiotics against three targets. This project will provide a major advance in the field of antibacterial drug discovery, providing powerful enabling tools to the scientific community to address this major threat to public health. PUBLIC HEALTH RELEVANCE Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria. Antibiotic-resistant Gram-negative bacteria pose a growing threat to public health in the U.S. and globally. A major obstacle to the development of new antibiotics to combat such infections is our poor understanding of the chemical requirements for small molecules to enter Gram-negative cells and to avoid ejection by efflux pumps. The proposed comprehensive, multidisciplinary research program aims to develop predictive computational tools to identify such molecules by carrying out large-scale, quantitative analyses of the accumulation of diverse small molecules in Gram-negative bacteria. These tools will then enable medicinal chemistry campaigns to develop novel antibiotics.",Predictive Models for Small-Molecule Accumulation in Gram-Negative Bacteria,9982190,R01AI136795,"['Acinetobacter baumannii', 'Address', 'Algorithmic Software', 'Anti-Bacterial Agents', 'Antibiotic Resistance', 'Antibiotics', 'Architecture', 'Bacteria', 'Biochemical', 'Biochemistry', 'Biological Assay', 'Biological Availability', 'Cells', 'Chemicals', 'Communities', 'Data', 'Data Set', 'Detection', 'Development', 'Effectiveness', 'Escherichia coli', 'Gram-Negative Bacteria', 'Gram-Negative Bacterial Infections', 'Human', 'Incidence', 'Individual', 'Infection', 'Interdisciplinary Study', 'Kinetics', 'Knock-out', 'Label', 'Lead', 'Libraries', 'Machine Learning', 'Mammalian Cell', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Membrane', 'Microbiology', 'Modeling', 'Oral', 'Partner in relationship', 'Penetration', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Pharmacology', 'Property', 'Pseudomonas aeruginosa', 'Public Health', 'Quantitative Evaluations', 'Quantitative Structure-Activity Relationship', 'Role', 'Structure', 'Testing', 'Variant', 'analog', 'base', 'biophysical model', 'cell envelope', 'cheminformatics', 'combat', 'computerized tools', 'density', 'design', 'drug discovery', 'efflux pump', 'high throughput screening', 'improved', 'inhibitor/antagonist', 'interdisciplinary approach', 'kinetic model', 'lead optimization', 'learning network', 'multidisciplinary', 'neural network', 'novel', 'predictive modeling', 'programs', 'prospective', 'public health relevance', 'screening', 'small molecule', 'small molecule libraries', 'success', 'tool']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,R01,2020,1239304,-0.004537977075403326
"Open data-driven infrastructure for building biomolecular force fields for predictive biophysics and drug design PROJECT SUMMARY/ABSTRACT The study of biomolecular interactions and design of new therapeutics requires accurate physical models of the atomistic interactions between small molecules and biological macromolecules. Over the least few decades, molecular mechanics force ﬁelds have demonstrated the potential that physical models hold for quantitative biophysical modeling and predictive molecular design. However, a signiﬁcant technology gap exists in our ability to build force ﬁelds that achieve high accuracy, can be systematically improved in a statistically robust manner, be extended to new areas of chemistry, can model post-translational and covalent modiﬁcations, are able to quantify systematic errors in predictions, and can be broadly applied across a high-performance software packages. In this project, we willl bridge this technology gap to enable new generations of accurate quantitative biomolec- ular modeling and (bio)molecular design for chemical biology and drug discovery. In Aim 1, we will produce a modern, open infrastructure to enable practitioners to rapidly and conveniently construct and employ accurate and statistically robust physical force ﬁelds via automated machine learning methods. In Aim 2, we will construct open, machine-readable experimental and quantum chemical datasets that will accelerate next-generation force ﬁeld development. In Aim 3, we will develop statistically robust Bayesian inference techniques to enable the auto- mated construction of type assignment schemes that avoid overﬁtting and selection of physical functional forms statistically justﬁed by the data. This approach will also provide an estimate of the systematic error in predicted properties arising from uncertainty in parameters or functional form choices—generally the dominant source of error—to be quantiﬁed with little added expense. In Aim 4, we will integrate and apply this infrastructure to produce open, transferable, self-consistent force ﬁelds that achieve high accuracy and broad coverage for modeling small molecule interactions with biomolecules (including unnatural amino or nucleic acids and covalent modiﬁcations by organic molecules), with the ultimate goal of covering all major biomolecules. This research is signiﬁcant in that the technology developed in this project has the potential to radically transform the study of biomolecular phenomena by providing highly accurate force ﬁelds with exceptionally broad chemical coverage via fully consistent parameterization of organic (bio)molecules. In addition, we will produce new tools to automate force ﬁeld creation and tailoring to speciﬁc problem domains, quantify the systematic error in predictions, and identify new data for improving force ﬁeld accuracy. This will greatly improve our ability to study diverse biophysical processes at the molecular level, and to rationally design new small-molecule, protein, and nucleic acid therapeutics. This supplement to the original RO1 is to purchase a small GPU cluster to enable rapid prototyping of many of the approaches that are proposed in this project outlined in the four aims. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life carry out their functions and to design new medications. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms. This equipment supplement will provide computational resources for testing of new techniques throughout the duration of the grant.",Open data-driven infrastructure for building biomolecular force fields for predictive biophysics and drug design,10157034,R01GM132386,"['Address', 'Area', 'Automobile Driving', 'Award', 'Bayesian Analysis', 'Binding', 'Biological', 'Biology', 'Biophysical Process', 'Biophysics', 'Charge', 'Chemicals', 'Chemistry', 'Complex', 'Computer Simulation', 'Computer software', 'DNA', 'Data', 'Data Set', 'Databases', 'Development', 'Drug Design', 'Electrostatics', 'Ensure', 'Equipment', 'Error Sources', 'Generations', 'Goals', 'Grant', 'Heart', 'Individual', 'Infrastructure', 'Investigation', 'Learning', 'Life', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Molecular', 'Nucleic Acids', 'Perception', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Property', 'Proteins', 'RNA', 'Readability', 'Reproducibility', 'Research', 'Roentgen Rays', 'Scheme', 'Science', 'Scientist', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Therapeutic', 'Thermodynamics', 'Training', 'Uncertainty', 'Validation', 'Work', 'base', 'biophysical model', 'chemical synthesis', 'cheminformatics', 'computing resources', 'data infrastructure', 'design', 'drug discovery', 'experience', 'experimental study', 'improved', 'interest', 'machine learning method', 'macromolecule', 'models and simulation', 'molecular mechanics', 'multidisciplinary', 'new technology', 'next generation', 'novel therapeutics', 'nucleic acid-based therapeutics', 'open data', 'open source', 'physical model', 'physical property', 'prototype', 'quantum', 'simulation', 'simulation software', 'small molecule', 'software infrastructure', 'sound', 'tool', 'unnatural amino acids']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,96003,-0.00978838577659992
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,10011756,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'effectiveness evaluation', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2020,247413,0.01542057000872389
"ShapeWorks in the Cloud Project Summary This application is submitted in response to NOT-OD-20-073 as an administrative supplement to the parent award R01AR076120 titled: ""Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches."" The form (or shape) of anatomies is the clinical language that describes abnormal mor- phologies tied to pathologic functions. Quantifying such subtle morphological shape changes requires parsing the anatomy into a quantitative description that is consistent across the population in question. For more than 100 years, morphometrics has been an indispensable quantitative tool in medical and biological sciences to study anatomical forms. But its representation capacity is limited to linear distances, angles, and areas. Sta- tistical shape modeling (SSM) is the computational extension of classical morphometric techniques to analyze more detailed representations of complex anatomy and their variability within populations The parent award ad- dresses existing roadblocks for the widespread adoption of SSM computational tools in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM) and its associated suite of open-source software tools, ShapeWorks. ShapeWorks enables learning population-level shape representation via automatic dense placement of homologous landmarks on image segmentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread applicability and impact to medicine and biology are hindered by computational barriers that most existing shape modeling packages face. The goal of this supplement award is to provide supplemental support for Aim 3 of the parent award to leverage best practices in software development and advances in cloud computing to enable researchers with limited computational resources and/or large-scale cohorts to build and execute custom SSM workﬂows us- ing remote scalable computational resources. To achieve this goal, we have developed a plan to enhance the design, implementation, and cloud-readiness of ShapeWorks and augmented our scientiﬁc team to add senior, experienced software engineers/developers who have extensive experience in professional programming, code refactoring, and scientiﬁc computing. This award will provide our team with the support necessary to (Aim 1) de- sign ShapeWorks as a collection of modular and reusable services, (Aim 2) decouple ShapeWorks services from explicitly encoded data sources, and (Aim 3) refactor ShapeWorks to scale efﬁciently on the cloud. All software development will be performed in adherence to software engineering practices and design principles, including coding style, documentation, and version control. The proposed efforts will be released as open-source software in a manner consistent with the principles of reproducible research and the practices of open science. Our long- term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein in addition to the parent award will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. The impact and scientiﬁc value of ShapeWorks have been recognized in a range of applications, including psychology, biological phenotyping, car- diology, and orthopedics. If funded, this supplement will provide support to revise, refactor, and redeploy Shape- Works to take advantage of new cloud computing paradigms, to be robust, sustainable, scalable, and accessible to a broader community, and to address the growing need for shape modeling tools to handle large collections of clinical data and to obtain sufﬁcient statistical power for large shape studies.",ShapeWorks in the Cloud,10166337,R01AR076120,"['Address', 'Adherence', 'Administrative Supplement', 'Adoption', 'Anatomy', 'Applied Research', 'Architecture', 'Area', 'Award', 'Biological', 'Biological Sciences', 'Biology', 'Cardiology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cloud Computing', 'Cloud Service', 'Code', 'Collection', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computer Models', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Databases', 'Disabled Persons', 'Documentation', 'Environment', 'Face', 'Funding', 'Goals', 'Image', 'Imagery', 'Language', 'Learning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'Occupations', 'Online Systems', 'Orthopedics', 'Parents', 'Pathologic', 'Phenotype', 'Population', 'Privatization', 'Psychology', 'Readiness', 'Reproducibility', 'Research', 'Research Personnel', 'Running', 'Scientist', 'Services', 'Shapes', 'Software Design', 'Software Engineering', 'Software Tools', 'Source Code', 'Speed', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'base', 'cohort', 'computational platform', 'computerized tools', 'computing resources', 'data management', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'large datasets', 'model development', 'open data', 'open source', 'particle', 'response', 'scientific computing', 'shape analysis', 'software development', 'statistics', 'tool', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,210000,0.019386316726028877
"Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine A fundamental challenge in precision medicine is to understand the patterns of differentiation between individuals. To address this challenge, we propose to go beyond the traditional `one disease--one model' view of bioinformatics and pursue a new view built upon personalized patient models that facilitates precision medicine by leveraging both commonalities within a patient cohort as well as signatures unique to every individual patient. With the emergence of large-scale databases such as The Cancer Genome Atlas (TCGA), the International Cancer Genome Consortium (ICGC), and the Gene Expression Omnibus (GEO), which collect multi-omic data on many different diseases, a new “pan-omics” and “pan-disease” paradigm has emerged to jointly analyze all patients in a disease cohort while accounting for patient-specific effects. An example of this is the recently released Pan-Cancer Atlas. At the same time, next generation statistical tools to accurately and rigorously draw the necessary inferences are lacking. In this project we propose a series of mathematically rigorous, statistically sound, and computationally feasible approaches to infer sample-specific models, providing a more complete view of heterogeneous datasets. By bringing together ideas from the machine learning, statistics, and mathematical optimization communities, we provide a rigorous framework for precision medicine via sample-specific statistical models. Crucially, we propose to analyze this framework and prove strong theoretical guarantees under weak assumptions--this dramatically distinguishes our framework from much of the existing literature. Towards these goals, we propose the following aims: Aim 1: Discovery of new molecular profiles with sample-specific statistical models. We propose a general framework for inferring sample-specific models with low-rank structure based on the novel concept of distance-matching. This allows us to infer statistical models at the level of a single patient without overfitting, and is general enough to be applied for prediction, classification, and network inference as well as a variety of diseases and phenotypes. Aim 2: Multimodal approaches to personalized diagnosis--contextually interpretable models for actionable clinical decision support. In order to translate these models into practice, we propose a novel interpretable predictive model that supports complex, multimodal data types such as images and text combined with high-level interpretable features such as SNP data, gender, age, etc. This framework simultaneously boosts the accuracy of clinical predictions by exploiting sample heterogeneity while providing human-digestable explanations for the predictions being made. Aim 3: Next-generation precision medicine--algorithms and software for personalized estimation. To put our models into practical use, we will develop new algorithms for interpretable prediction of personalized clinical outcomes and visualization of personalized statistical models. All of our tools will be combined into a user-friendly software package called PrecisionX that will be freely available to researchers and clinicians everywhere. RELEVANCE (See instructions): Personalization with data is a critical challenge whenever decisions must be made at scale, and has applications that go beyond precision medicine; businesses, educational institutions, and financial institutions are among the many players that have acknowledged a stake in this complex problem. We expect the proposed work to provide a rigorous foundation for personalization with large and high-dimensional datasets, finding use throughout the broader scientific community as well as with industry and educational institutions. Alongside our collaboration with Pitt/UPMC, we will work with physicians and data scientists for practical feedback as well as provide training in the methods developed. n/a",Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine,10133782,R01GM140467,"['Accounting', 'Address', 'Age', 'Algorithmic Software', 'Algorithms', 'Atlases', 'Bioinformatics', 'Businesses', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Scientist', 'Data Set', 'Disease', 'Feedback', 'Foundations', 'Gender', 'Gene Expression', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Industry', 'Institution', 'Instruction', 'International', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modeling', 'Molecular Profiling', 'Multiomic Data', 'Outcome', 'Patients', 'Pattern', 'Physicians', 'Portraits', 'Research Personnel', 'Sampling', 'Series', 'Statistical Models', 'Structure', 'Text', 'The Cancer Genome Atlas', 'Time', 'Training', 'Translating', 'Visualization', 'Work', 'base', 'cancer genome', 'clinical decision support', 'clinically actionable', 'cohort', 'disease phenotype', 'heterogenous data', 'high dimensionality', 'individual patient', 'large-scale database', 'molecular modeling', 'multimodal data', 'multimodality', 'next generation', 'novel', 'personalized diagnostics', 'personalized predictions', 'precision medicine', 'predictive modeling', 'sound', 'statistics', 'tool', 'user friendly software']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,305566,-0.001326727545203089
"Computational and Statistical Framework to Model Tissue Shape and Mechanics PROJECT SUMMARY  The morphologic and mechanical characteristics of a tissue are fundamental to understanding the development, homeostasis, and pathology of the human body. During the previous period of funding, we developed statistical shape modeling (SSM) methods and applied these to the study of structural hip disease. We also developed the initial framework to integrate SSM with finite element (FE) analysis to enable the study of shape and mechanics together. If incorporated into clinical practice, SSM and FE analysis could identify features of the anatomy likely responsible for injury, remodeling, or repair. Geometry needed for SSM and FE models is typically generated by segmentation of volumetric imaging data. This step can be painstakingly slow, error prone, and cost prohibitive, which hampers clinical application of these computational techniques. We have created a deep machine learning algorithm ‘DeepSSM’ that uses a convolutional neural network to establish the correspondence model directly from unsegmented images. In Aim 1 we will apply DepSSM to improve clinical understanding of structural hip disease by characterizing differences in anatomy between symptomatic and asymptomatic individuals; these morphometric comparisons will identify anatomic features most telling of disease, thereby guiding improvements in diagnosis. Computational advancements have simplified the process to generate patient-specific FE models, enabling clinically focused research. However, there is no framework to collectively visualize, compare, and interpret (i.e., post-process) results from multiple FE models. Currently, inter-subject comparisons require oversimplifications such as averaging results over subjectively defined regions. In Aim 2 we will develop new post-processing methods to collectively visualize, interpret and statistically analyze FE results across multiple subjects and study groups. We will map FE results to synthetic anatomies representing statistically meaningful distributions using the correspondence model. Statistical parametric mapping will be applied to preserve anatomic detail through statistical testing. We will use our published FE models of hip joint mechanics as the test system. Finally, volumetric images provide a wealth of information that is delivered to physicians in a familiar format. Yet, tools are not available to interpret model data with clinical findings from volumetric images. In Aim 3, we will develop methods that evaluate relationships between shape, mechanics, and clinical findings gleaned from imaging through integrated statistical tests and semi-automatic medical image annotation tools that utilize standard ontologies. Quantitative CT and MRI images of the hip, which estimate bone density and cartilage ultrastructure, respectively, will be evaluated as test datasets. To impart broad impact, we will disseminate our methods to the community as open source software that will call core functionality provided by existing, open source software that has a large user base (FEBio, ShapeWorks). PROJECT NARRATIVE The proposed technology will provide the methodologies necessary to increase the clinical acceptance and applicability of computer models. These models measure three-dimensional tissue shape and estimate tissue mechanics, providing information that cannot be measured conventionally. We will implement these methods into software that can be used by the public free-of-charge.",Computational and Statistical Framework to Model Tissue Shape and Mechanics,9972694,R01EB016701,"['3-Dimensional', 'Adoption', 'Algorithms', 'Anatomy', 'Architecture', 'Bone Density', 'Cardiology', 'Cartilage', 'Characteristics', 'Charge', 'Clinical', 'Communities', 'Computational Technique', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Deformity', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Finite Element Analysis', 'Foundations', 'Funding', 'Geometry', 'Glean', 'Grooming', 'Hip Joint', 'Hip region structure', 'Homeostasis', 'Human Pathology', 'Human body', 'Image', 'Individual', 'Injury', 'Intuition', 'Libraries', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphology', 'Neurology', 'Ontology', 'Orthopedics', 'Pathology', 'Patient imaging', 'Patients', 'Performance', 'Physicians', 'Procedures', 'Process', 'Publishing', 'Quantitative Evaluations', 'Research', 'Resources', 'Scheme', 'Shapes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Training', 'Validation', 'X-Ray Computed Tomography', 'annotation  system', 'base', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data modeling', 'disease diagnosis', 'improved', 'in vivo', 'machine learning algorithm', 'novel', 'open source', 'predictive modeling', 'preservation', 'relating to nervous system', 'repaired', 'shape analysis', 'simulation', 'three-dimensional modeling', 'tool']",NIBIB,UNIVERSITY OF UTAH,R01,2020,563658,0.005567108930309749
"Open data-driven infrastructure for building biomolecular force fields for predictive biophysics and drug design PROJECT SUMMARY/ABSTRACT The study of biomolecular interactions and design of new therapeutics requires accurate physical models of the atomistic interactions between small molecules and biological macromolecules. Over the least few decades, molecular mechanics force ﬁelds have demonstrated the potential that physical models hold for quantitative biophysical modeling and predictive molecular design. However, a signiﬁcant technology gap exists in our ability to build force ﬁelds that achieve high accuracy, can be systematically improved in a statistically robust manner, be extended to new areas of chemistry, can model post-translational and covalent modiﬁcations, are able to quantify systematic errors in predictions, and can be broadly applied across a high-performance software packages. In this project, we aim to bridge this technology gap to enable new generations of accurate quantitative biomolec- ular modeling and (bio)molecular design for chemical biology and drug discovery. In Aim 1, we will produce a modern, open infrastructure to enable practitioners to rapidly and conveniently construct and employ accurate and statistically robust physical force ﬁelds via automated machine learning methods. In Aim 2, we will construct open, machine-readable experimental and quantum chemical datasets that will accelerate next-generation force ﬁeld development. In Aim 3, we will develop statistically robust Bayesian inference techniques to enable the auto- mated construction of type assignment schemes that avoid overﬁtting and selection of physical functional forms statistically justﬁed by the data. This approach will also provide an estimate of the systematic error in predicted properties arising from uncertainty in parameters or functional form choices—generally the dominant source of error—to be quantiﬁed with little added expense. In Aim 4, we will integrate and apply this infrastructure to produce open, transferable, self-consistent force ﬁelds that achieve high accuracy and broad coverage for modeling small molecule interactions with biomolecules (including unnatural amino or nucleic acids and covalent modiﬁcations by organic molecules), with the ultimate goal of covering all major biomolecules. This research is signiﬁcant in that the technology developed in this project has the potential to radically transform the study of biomolecular phenomena by providing highly accurate force ﬁelds with exceptionally broad chemical coverage via fully consistent parameterization of organic (bio)molecules. In addition, we will produce new tools to automate force ﬁeld creation and tailoring to speciﬁc problem domains, quantify the systematic error in predictions, and identify new data for improving force ﬁeld accuracy. This will greatly improve our ability to study diverse biophysical processes at the molecular level, and to rationally design new small-molecule, protein, and nucleic acid therapeutics. This approach will bring statistical rigor to the ﬁeld of force ﬁeld construction and application by providing a means to make data-driven decisions, while enhancing reproducibility by enabling it to become a rigorous and reproducible science using a fully open infrastructure and datasets. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life do their jobs. They also use simulations to help design new medications – compounds that can bind and inﬂuence the behavior of these molecules of life, and thereby block diseases at the molecular level. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms.",Open data-driven infrastructure for building biomolecular force fields for predictive biophysics and drug design,9887804,R01GM132386,"['Address', 'Area', 'Automobile Driving', 'Bayesian Analysis', 'Binding', 'Biological', 'Biology', 'Biophysical Process', 'Biophysics', 'Charge', 'Chemicals', 'Chemistry', 'Complex', 'Computer Simulation', 'Computer software', 'DNA', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Design', 'Electrostatics', 'Ensure', 'Error Sources', 'Generations', 'Goals', 'Heart', 'Individual', 'Infrastructure', 'Investigation', 'Learning', 'Life', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Molecular', 'Nucleic Acids', 'Occupations', 'Perception', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Property', 'Proteins', 'RNA', 'Readability', 'Reproducibility', 'Research', 'Roentgen Rays', 'Scheme', 'Science', 'Scientist', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Technology', 'Temperature', 'Therapeutic', 'Thermodynamics', 'Training', 'Uncertainty', 'Validation', 'Work', 'base', 'behavior influence', 'biophysical model', 'chemical synthesis', 'cheminformatics', 'data infrastructure', 'design', 'drug discovery', 'experience', 'experimental study', 'improved', 'interest', 'machine learning method', 'macromolecule', 'models and simulation', 'molecular mechanics', 'multidisciplinary', 'new technology', 'next generation', 'novel therapeutics', 'nucleic acid-based therapeutics', 'open data', 'open source', 'physical model', 'physical property', 'quantum', 'simulation', 'simulation software', 'small molecule', 'software infrastructure', 'sound', 'tool', 'unnatural amino acids']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,675565,-0.006676734248937844
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9888378,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelial', 'Epithelium', 'Esophageal Adenocarcinoma', 'Esophageal Tissue', 'Esophagitis', 'Esophagus', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'intelligent algorithm', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'stem cells', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2020,171720,-0.029829720754620932
"Multi-Resolution Docking Methods for Electron Microscopy Summary In the past decade, we have witnessed a revolutionary progress in camera technology and the attainable resolution of macromolecular assemblies via cryogenic electron microscopy (cryo-EM) and in the development of computational algorithms that relate the resulting 3D maps to atomic resolution structures. Whereas single- particle cryo-EM today is capable of directly solving atomic structures of biomolecular assemblies in isolation, electron tomography (ET) in unstained frozen-hydrated samples is widely used to capture the 3D organization of supramolecular complexes in their native (organelle, cell, or tissue) environments. We have identified three inter-related research areas where our computational modeling experience (historically rooted in pre-revolution multi-scale approaches) offers the biggest value to today's post-revolution EM community: (1) medium resolution cryo-EM modeling, (2) the segmentation and denoising of cryo-ET data, and (3) the validation of atomic models and their corresponding maps. The first aim is an extension of promising new ideas in flexible fitting as well as secondary structure prediction for medium resolution maps, which have been our key research areas in the past. medium resolution (5-10Å) maps are still widely used in EM and can be of significant biological importance. This is particularly true in the case of cryo-ET maps, which are harder to read than single particle cryo-EM maps because they often exhibit considerable noise, anisotropic resolution, and anisotropic density variations due to the low dose requirements and the missing wedge in the Fourier space. In the case of tightly packed or crowded macromolecular structures, the fusion of nearby biomolecular densities prevents an automated segmentation of geometric shapes, requiring a labor-intensive manual tracing by human experts. We are currently developing novel computational approaches to provide a more objective strategy for missing wedge correction in homogeneous specimen areas of tomograms. Our hybrid approach combines deconvolution and denoising with template matching in a unified mathematical framework that allows modeling constraints to be imposed in a least-squares optimization process. Our approach can also be extended to the flexible refinement of atomic structures using our damped dynamics flexible fitting approach by tuning the internal point-spread functions to the missing wedge of the ET data. To support these aims, we will quantitatively measure the fitness of an atomic model in local density regions and characterize the fitness of maps with reliable reference structures. The collaborative efforts supported by this grant will include the refinement of cytoskeletal filaments, molecular motors, bacterial chemoreceptor arrays, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established Internet-based mechanisms used by the Situs and Sculptor packages and as plugins for the popular UCSF Chimera graphics program. Project Narrative This project will help biological electron microscopists bridge a broad range of resolution levels, from the atomic to the living organism. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,10120245,R01GM062968,"['3-Dimensional', 'Algorithms', 'Architecture', 'Area', 'Biological', 'Cells', 'Characteristics', 'Chemoreceptors', 'Chimera organism', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational algorithm', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Crowding', 'Cryo-electron tomography', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Databases', 'Deposition', 'Detection', 'Development', 'Docking', 'Dose', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Elements', 'Environment', 'Equilibrium', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Human', 'Hybrids', 'Hydration status', 'Internet', 'Laboratories', 'Least-Squares Analysis', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Mathematics', 'Measures', 'Medical', 'Methods', 'Microscope', 'Modeling', 'Modernization', 'Molecular Motors', 'Molecular Structure', 'Morphologic artifacts', 'Nature', 'Noise', 'Organelles', 'Organism', 'Pattern', 'Plant Roots', 'Research', 'Resolution', 'Sampling', 'Shapes', 'Specimen', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Tomogram', 'Training', 'Validation', 'Variant', 'Visualization software', 'Work', 'algorithmic methodologies', 'automated segmentation', 'base', 'beta pleated sheet', 'computer code', 'cryogenics', 'data warehouse', 'deep learning', 'denoising', 'density', 'electron tomography', 'experience', 'feature detection', 'fitness', 'flexibility', 'fundamental research', 'heuristics', 'high standard', 'image reconstruction', 'improved', 'interest', 'learning network', 'macromolecular assembly', 'novel', 'particle', 'prevent', 'process optimization', 'programs', 'reconstruction', 'structured data', 'theories', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2020,313572,-0.01801617215611697
"Carolina Population Center PROJECT SUMMARY/ABSTRACT The Carolina Population Center requests infrastructure support that will advance population dynamics research at CPC by increasing research impact, innovation, and productivity, supporting the development of junior scientists, and reducing the administrative burden on scientists. Infrastructure support will advance science in three primary research areas: Sexuality, Reproduction, Fertility, and Families; Population, Health, and the Environment; and Inequality, Mobility, Disparities, and Well-Being. Much of the research at CPC draws on large publicly available longitudinal data sets that our faculty have designed and collected, including the National Longitudinal Study of Adolescent to Adult Health, the China Health and Nutrition Survey, newer surveys associated with the Transfer Project, and the Study of the Tsunami Aftermath and Recovery, all of which will continue to be important in work related to our primary research areas over the next five years. These projects embody several themes that have guided research at CPC since the Center's inception. These themes, which will continue to shape our work, are the importance of life course processes and longitudinal data, multi-level processes and measurement of context, interventions and natural experiments as means of learning about causal processes, and the relevance of sociodemographic variables such as age, gender, race- ethnicity, and socioeconomic status for disparities in health and well-being. By embedding these themes, our projects provide data that enable us to address barriers that otherwise impede progress in the population sciences generally, and in our primary research areas in particular. We request support for three cores which in combination will provide an institutional infrastructure that will push populations dynamics research forward by empowering CPC faculty to tackle challenging questions using state of the art measurement techniques and methods. The Administrative Core plans activities that maintain a stimulating intellectual community, streamlines administrative processes so that scientists can focus on research, coordinates activities of the Cores so that services are offered efficiently, and communicates information about research and data more broadly. The Development Core supports early stage investigators and other faculty with exciting new ideas through multiple mechanisms: workshops, access to technical expertise in measurement, and seed grants. The Research Services Core enables scientists to address complex and important population research issues by providing access to state-of-the-art research tools and professional support for programming, survey development, and analysis. NARRATIVE This project will provide infrastructure support for a cutting edge program of research on population dynamics at the Carolina Population Center. Research at the Center will analyze state-of-the art data to address fundamental questions regarding fertility, adolescent health, and links between the environment and health. Special attention will be paid to factors creating health disparities.",Carolina Population Center,10005569,P2CHD050924,"['Address', 'Adolescent', 'Adopted', 'Adult', 'Age', 'Applications Grants', 'Area', 'Attention', 'Biological Markers', 'China', 'Cognitive', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Vision Systems', 'Creativeness', 'Data', 'Data Collection', 'Development', 'Diffuse', 'Educational workshop', 'Environment', 'Ethnic Origin', 'Extramural Activities', 'Faculty', 'Family', 'Fertility', 'Fostering', 'Funding', 'Gender', 'Genetic', 'Grant', 'Hand', 'Health', 'Health Surveys', 'Home environment', 'Inequality', 'Infrastructure', 'Intervention', 'Journals', 'Learning', 'Life Cycle Stages', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mainstreaming', 'Measurement', 'Mentors', 'Methods', 'Natural experiment', 'Nutrition Surveys', 'Personal Satisfaction', 'Phase', 'Policy Making', 'Population', 'Population Dynamics', 'Population Research', 'Population Sciences', 'Postdoctoral Fellow', 'Process', 'Production', 'Productivity', 'Publishing', 'Race', 'Recovery', 'Reproduction', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Schools', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Seeds', 'Services', 'Sexuality', 'Shapes', 'Socioeconomic Status', 'Structure', 'Students', 'Surveys', 'Talents', 'Teacher Professional Development', 'Technical Expertise', 'Techniques', 'Training Programs', 'Tsunami', 'Universities', 'Work', 'adolescent health', 'career', 'collaborative environment', 'cost', 'data access', 'design', 'empowered', 'experience', 'faculty support', 'health disparity', 'innovation', 'interdisciplinary collaboration', 'longitudinal dataset', 'novel strategies', 'population health', 'privacy protection', 'programs', 'research and development', 'response', 'sociodemographic variables', 'success', 'tool']",NICHD,UNIV OF NORTH CAROLINA CHAPEL HILL,P2C,2020,774402,-0.0037455364903358048
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and affect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable of discovering local and global alteration of matter without the need to apriori select an anatomical region of interest. The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image datasets is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focuses on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,10139715,R42MH118845,"['Affect', 'Algorithmic Software', 'Algorithms', 'Anatomy', 'Blood flow', 'Brain', 'Clinical', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Data Set', 'Databases', 'Detection', 'Deterioration', 'Diffuse', 'Disease', 'Drug Screening', 'Goals', 'Grain', 'HIV', 'Image', 'Image Analysis', 'Internet', 'Joints', 'Label', 'Lead', 'Machine Learning', 'Measures', 'Medical Imaging', 'Metabolic', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Neurologic', 'Neurologic Effect', 'Neurosurgeon', 'Online Systems', 'Outcome', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Analysis', 'Population Study', 'Positioning Attribute', 'Process', 'Questionnaires', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Software Validation', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Variant', 'Visualization', 'Washington', 'analysis pipeline', 'base', 'data access', 'data infrastructure', 'deep learning', 'experience', 'gray matter', 'high throughput screening', 'image registration', 'imaging capabilities', 'improved', 'insight', 'interest', 'metabolic rate', 'morphometry', 'nervous system disorder', 'neurodegenerative dementia', 'novel', 'programs', 'prototype', 'regional difference', 'research and development', 'shape analysis', 'software development', 'software infrastructure', 'task analysis', 'tool', 'usability', 'web app', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R42,2020,456359,-0.021172257869676286
"FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control PROJECT SUMMARY In this proposal we plan to contribute addressing the above foundational and operational challenges by advancing the science of influenza modeling and contributing novel methods and data sources that will increase the accuracy and availability of seasonal and pandemic influenza models. To address these challenges, we plan to build on the unique mechanistic spatially structured modeling approaches developed by our consortium, that includes stochastic metapopulation models and fully developed agent-based models nested together in our global epidemic and mobility modeling (GLEAM) approach. The objective of this project is to generate novel and actionable scientific insights from dynamic transmission models of influenza transmission that effectively integrate key socio-demographic indicators of the focus population, as well as a wide spectrum of pharmaceutical and non-pharmaceutical interventions. Our proposed work in specific aim 1 (A1) will leverage our global modeling (from the global to local scale) framework that can be used to explore the multi-year impact of influenza vaccination, antiviral prophylaxis/treatment, and community mitigation during influenza seasons and pandemics. Our specific aim 2 (A2) will focus on using high quality data to model heterogeneous transmission drivers and novel contact pattern stratifications that will allow us to guide mitigation strategies and prioritization for interventions. In our Aim 3 (A3) we will use artificial intelligence approaches to identify interventions that are particularly synergistic and well-suited to particular epidemic scenarios, for seasonal and pandemic influenza. Our overarching goal is to provide a modeling portfolio with flexible and innovative mathematical and computational approaches. We aim to address several questions commonly asked about seasonal and pandemic influenza and match these with analytical methods and outbreak projections. The modeling and data developed in this project can help facilitate and justify transparent public health decisions, while contributing to the definition of standard methods for model selection and validation. Finally, our influenza modeling platform can also benefit the broader network of modeling teams and can be used to improve result sharing and harmonization of modeling approaches. The objective of this proposal is to advance the science of modeling and contribute novel methods and data analytics tools that will increase the understanding of seasonal and pandemic influenza in the context of the network of modeling teams coordinated by the CDC. To address these challenges, we plan to develop a novel global modeling framework, contribute new data and methods for improve the accuracy and validation of flu modeling approaches, and evolve successful methodologies to advance the analysis of layered intervention with artificial Intelligence.",FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control,10071782,U01IP001137,[' '],NCIRD,NORTHEASTERN UNIVERSITY,U01,2020,371721,0.0174871093902524
"Novel Statistical Inference for Biomedical Big Data Project Summary This project develops novel statistical inference procedures for biomedical big data (BBD), including data from diverse omics platforms, various medical imaging technologies and electronic health records. Statistical inference, i.e., assess- ing uncertainty, statistical signiﬁcance and conﬁdence, is a key step in computational pipelines that aim to discover new disease mechanisms and develop effective treatments using BBD. However, the development of statistical inference procedures for BBD has lagged behind technological advances. In fact, while point estimation and variable selection procedures for BBD have matured over the past two decades, existing inference procedures are either limited to simple methods for marginal inference and/or lack the ability to integrate biomedical data across multiple studies and plat- forms. This paucity is, in large part, due to the challenges of statistical inference in high-dimensional models, where the number of features is considerably larger than the number of subjects in the study. Motivated by our team's extensive and complementary expertise in analyzing multi-omics data from heterogenous studies, including the TOPMed project on which multiple team members currently collaborate, the current proposal aims to address these challenges. The ﬁrst aim of the project develops a novel inference procedure for conditional parameters in high-dimensional models based on dimension reduction, which facilitates seamless integration of external biological information, as well as biomedical data across multiple studies and platforms. To expand the application of this method to very high-dimensional models that arise in BBD applications, the second aim develops a data-adaptive screening procedure for selecting an optimal subset of relevant variables. The third aim develops a novel inference procedure for high-dimensional mixed linear models. This method expands the application domain of high-dimensional inference procedures to studies with longitu- dinal data and repeated measures, which arise commonly in biomedical applications. The fourth aim develops a novel data-driven procedure for controlling the false discovery rate (FDR), which facilitates the integration of evidence from multiple BBD sources, while minimizing the false negative rate (FNR) for optimal discovery. Upon evaluation using ex- tensive simulation experiments and application to multi-omics data from the TOPMed project, the last aim implements the proposed methods into easy-to-use open-source software tools leveraging the R programming language and the capabilities of the Galaxy workﬂow system, thus providing an expandable platform for further developments for BBD methods and tools. Public Health Relevance Biomedical big data (BBD), including large collections of omics data, medical imaging data, and electronic health records, offer unprecedented opportunities for discovering disease mechanisms and developing effective treatments. However, despite their tremendous potential, discovery using BBD has been hindered by computational challenges, including limited advances in statistical inference procedures that allow biomedical researchers to investigate uncon- founded associations among biomarkers of interest and various biological phenotypes, while integrating data from multiple BBD sources. The current proposal bridges this gap by developing novel statistical machine learning methods and easy-to-use open-source software for statistical inference in BBD, which are designed to facilitate the integration of data from multiple studies and platforms.",Novel Statistical Inference for Biomedical Big Data,9969887,R01GM133848,"['Address', 'Adoption', 'Behavioral', 'Big Data Methods', 'Biological', 'Biological Assay', 'Biological Markers', 'Code', 'Collection', 'Communities', 'Computer software', 'Data', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Galaxy', 'Genetic study', 'Goals', 'Heart', 'Imaging technology', 'Individual', 'Linear Models', 'Measurement', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Outcome', 'Phenotype', 'Procedures', 'R programming language ', 'Research Personnel', 'Sample Size', 'Scientist', 'Screening procedure', 'Software Tools', 'Structure', 'System', 'Testing', 'Trans-Omics for Precision Medicine', 'Uncertainty', 'Work', 'base', 'big biomedical data', 'computational pipelines', 'data integration', 'design', 'diverse data', 'effective therapy', 'experimental study', 'heterogenous data', 'high dimensionality', 'interest', 'machine learning method', 'member', 'novel', 'open source', 'public health relevance', 'screening', 'simulation', 'statistical and machine learning', 'structured data', 'tool', 'treatment strategy', 'user friendly software']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,456980,0.01706731875626722
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9929423,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Visualization', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data tools', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2020,754121,-0.01994368769505162
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary  Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and effect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable to discover local and global alteration of matter without the need to apriori select an anatomical region of interest.  The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image data set is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focus on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,10115288,R41MH118845,"['Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Calibration', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Computer software', 'Data Set', 'Databases', 'Dementia', 'Deterioration', 'Development', 'Diffuse', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Foundations', 'Goals', 'Grain', 'Image', 'Image Analysis', 'Internet', 'Lead', 'Location', 'Machine Learning', 'Medical Imaging', 'Methodology', 'Methods', 'Modality', 'Nature', 'Nerve Degeneration', 'Neurologic', 'Neurologic Effect', 'Online Systems', 'Outcome', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Study', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Structure', 'Technology', 'Temporal Lobe', 'Testing', 'Validation', 'Variant', 'Visualization', 'autism spectrum disorder', 'base', 'clinical Diagnosis', 'experience', 'frontal lobe', 'gray matter', 'high throughput screening', 'image processing', 'image registration', 'imaging capabilities', 'improved', 'interest', 'machine learning method', 'morphometry', 'nervous system disorder', 'predict clinical outcome', 'predictive modeling', 'programs', 'research and development', 'shape analysis', 'software development', 'task analysis', 'tool', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R41,2020,99860,-0.01999072410545296
"Rapid and Precise Molecular Pathway Modelling of the SARS-CoV-1 and SARS-CoV-2 Infection Cycle with Human Host Protein and Therapeutic Interactions Project Summary The Reactome Knowledgebase is a widely used and internationally recognized expert-curated, open-source resource of a broad array of human biological processes and their disease counterparts, coupled to powerful tools for data analysis and display, and integrated with diverse community genomics resources. The work proposed here will add molecular annotations of the COVID-19 infection process mediated by the SARS-CoV-2 coronavirus, interactions between viral components and human host proteins that mediate the severity of viral infection, and the effects of therapeutics and drug-like compounds on both viral and host proteins. The resulting SARS-CoV-2 pathway annotations will provide a framework for pathway- and network-based data analysis and visualization, which will be critical for the interpretation of numerous COVID-19 studies now and in the future. In collaboration with a team of community experts in virology, drug design, and infectious disease, we will assemble information in two stages. First, a draft annotation will associate relevant SARS-CoV-1 and SARS-CoV- 2 viral and host cell proteins with each stage of the infection process and the host response to it. These annotations will be immediately useful for identifying additional relevant interacting proteins, for assessing possible effects of variation in the host or viral proteins on specific steps of viral infection, and for identifying possible drug targets. In the second stage, the SARS-CoV-2 map will be annotated more extensively to fill in molecular details of each step in these processes and to highlight differences in the processes mediated by SARS- CoV-2 virus and related coronaviruses. This annotation process will continue for the duration of the project to incorporate newly validated molecular details as they are uncovered by the research community. All the data, code and tools developed by this project will be open source and open. Project Narrative Using long-established Reactome Knowledgebase standards for authorship, curation and peer-review of molecular pathway data from published and unpublished sources, we will annotate the molecular details of the processes by which SARS-CoV-2 and SARS-CoV-1 coronaviruses infect human cells. This will include the interactions among viral and host proteins, the expression of viral proteins likely to trigger innate and adaptive immunity, and an annotation of the molecular effects of drugs on these processes. Our strategy will allow rapid revision and extension of annotations as data accumulate, will support close integration with other community resources, and will generate a valuable resource for community analysis of experimental data sets relevant to COVID-19 disease.",Rapid and Precise Molecular Pathway Modelling of the SARS-CoV-1 and SARS-CoV-2 Infection Cycle with Human Host Protein and Therapeutic Interactions,10165320,U41HG003751,"['2019-nCoV', 'Authorship', 'Biological Process', 'COVID-19', 'Cells', 'Code', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer software', 'Consensus', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Disease', 'Disease Pathway', 'Drug Design', 'Drug Targeting', 'Ensure', 'Future', 'Genomics', 'Human', 'Immune response', 'Infection', 'Innate Immune Response', 'Institutes', 'International', 'Link', 'Literature', 'Manuals', 'Maps', 'Mediating', 'Mining', 'Modeling', 'Molecular', 'Natural Immunity', 'Natural Language Processing', 'Network-based', 'Paper', 'Pathogenicity', 'Pathway interactions', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Proteome', 'Publishing', 'Reaction', 'Research', 'Resources', 'SARS coronavirus', 'Severities', 'Source', 'System', 'Systems Biology', 'Therapeutic', 'Therapeutic Effect', 'Therapeutic Monoclonal Antibodies', 'Variant', 'Viral', 'Viral Proteins', 'Virus', 'Virus Diseases', 'Work', 'adaptive immunity', 'data integration', 'data structure', 'data tools', 'data visualization', 'experimental analysis', 'knowledge base', 'open source', 'protein expression', 'small molecule', 'structured data', 'tool', 'virology']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2020,310292,-0.034423659930582495
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9962426,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Pooling', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'multiple data types', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,323659,-0.01592777280271827
"Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes PROJECT SUMMARY Overview: We will extend and develop implementations of foundational methods for analyzing populations of attributed connectomes. Our toolbox will enable brain scientists to (1) infer latent structure from individual connectomes, (2) identify meaningful clusters among populations of connectomes, and (3) detect relationships between connectomes and multivariate phenotypes. The methods we develop and extend will naturally overcome the challenges inherent in connectomics: high-dimensional non-Euclidean data with multi-level nonlinear interactions. Our implementations will comply with the highest open-source standards by: providing extensive online documentation and extended tutorials, hosting workshops to demonstrate our tools on an annual basis, and merging our implementations into commonly used packages such as scikit-learn [1], scipy [2], and networkx [3]. All of the code we develop is open source. We strive to ensure that our code is shared in accordance with the strictest guiding principles. We chose to implement these algorithms in Python due to its wide adoption in the neuroscience and data science fields. In particular, many other neuroscience tools applicable to connectomics, including NetworkX DiPy, mindboggle, nilearn, and nipy, are also implemented in Python. This will enable researchers to chain our analysis tools onto pre-existing pipelines for data preprocessing and visualization. Nonetheless, we feel that sharing our code in our own public repositories is insufficient for global reach. We have also begun reaching out to developers of the leading data science packages in python, including scipy, sklearn, networkx, scikit-image, and DiPy. For each of those packages, we have informal approval to begin integrating algorithms that we have developed. Those packages are collectively used by >220,000 other packages, so merging our algorithms into those packages will significantly extend our global reach. All researchers investigating connectomics, including all the authors of the 24,000 papers that mention the word “connectome”, will be able to apply state-of-the-art statistical theory and methods to their data. Currently, we have about 150 open source software projects on our NeuroData GitHub organization. Collectively, these projects get about 2,000 downloads and >11,000 views per month. As we incorporate additional functionality as described in this proposal, we expect far more researchers across disciplines and sectors will utilize our software. 20 ​ ​​ ​ ​​ Project Narrative Connectomes are an increasingly important modality for characterizing the structure of the brain, to complement behavior, genetics, and physiology. We and others have developed foundational statistical theory and methods over the last decade for the analysis of networks, networks with edge, vertex, and other attributes, and populations thereof, with preliminary implementations of those tools that we leverage in our laboratory for various application papers. In this project, we will extend our package, called graspy, to be of professional quality, implementing key functionality to include (1) estimating latent structure from attributed connectomes, (2) identifying meaningful clusters among populations of connectomes, and (3) detecting relationships between connectomes and multivariate phenotypes, such as behavior, genetics, and physiology. 18",Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes,10012519,RF1MH123233,"['Adoption', 'Algorithms', 'Behavioral Genetics', 'Brain', 'Code', 'Coin', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Development', 'Discipline', 'Documentation', 'Educational workshop', 'Ensure', 'Foundations', 'Funding', 'Genes', 'Human', 'Image', 'Individual', 'Journals', 'Laboratories', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modality', 'Modernization', 'Motivation', 'Neurosciences', 'Paper', 'Pathway Analysis', 'Phenotype', 'Physiology', 'Population', 'Population Analysis', 'Population Study', 'Property', 'PubMed', 'Publishing', 'Pythons', 'Research Personnel', 'Scientist', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Study', 'Structure', 'Telecommunications', 'Testing', 'Visualization', 'Work', 'brain research', 'connectome', 'data pipeline', 'design', 'high dimensionality', 'high standard', 'open source', 'public repository', 'software development', 'theories', 'tool', 'user-friendly']",NIMH,JOHNS HOPKINS UNIVERSITY,RF1,2020,1246005,-0.0043169896012293174
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,10021018,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2020,255238,-0.0008196221096183337
"Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics Abstract Support is requested for a Keystone Symposia conference entitled Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics, organized by Drs. Jose M. Lora and Timothy K. Lu. The conference will be held in Breckenridge, Colorado from March 29- April 1, 2019. Synthetic Biology tools and principles have matured tremendously over the last decade and have reached extraordinary levels of sophistication, both in eukaryotic and prokaryotic systems. Synthetic biology as a therapeutic modality is starting to enter multiple clinical studies and has the potential to have a significant impact on medicine across a wide range of diseases (e.g., metabolic, immune-mediated, cancer, and neurologic diseases). This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine. While there are conferences that capture synthetic biology in only a few talks mixed in among other various topics, there is a paucity of conferences focused on synthetic biology as drugs to treat disease. However, due to the rapid pace of fundamental scientific advances along with an expanding number of biotechnology companies and emerging clinical studies with synthetic biology at their core, this conference will be highly relevant for a wide audience of scientists both from academia and industry. In addition, other meetings in this field have a highly technology-driven focus on synthetic biology techniques with relatively little attention given to biological and medical context. Ultimately, this Keystone Symposia conference should inspire researchers from diverse backgrounds to discuss synthetic biology via many new angles. PROJECT NARRATIVE Over the past two decades, tremendous advances have been made in the use of biological parts to engineer systems that can effectively direct living cells for a vast variety of purposes (a.k.a. synthetic biology). Synthetic biology is being used to construct more effective therapies in diseases such as cancer, but there are remaining obstacles to the clinical translation of these therapies. This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine.",Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics,9913772,R13EB029305,"['Academia', 'Address', 'Area', 'Attention', 'Biological', 'Biomedical Research', 'Biotechnology', 'Cells', 'Clinical Research', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Colorado', 'Computers', 'Disease', 'Educational workshop', 'Engineering', 'Future', 'Genetic Engineering', 'Genetic Screening', 'Human', 'Immune', 'Industrialization', 'Industry', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Medicine', 'Metabolic', 'Methodology', 'Modality', 'Neurologic', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Preventive', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Scientific Advances and Accomplishments', 'Scientist', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Work', 'clinical application', 'clinical practice', 'clinical translation', 'combinatorial', 'design', 'effective therapy', 'graduate student', 'meetings', 'nervous system disorder', 'next generation', 'novel diagnostics', 'posters', 'symposium', 'synthetic biology', 'targeted treatment', 'tool']",NIBIB,KEYSTONE SYMPOSIA,R13,2020,10000,-0.014858613682005552
"Skyline Targeted Proteomics Environment Development on Skyline started in 2008 to fill a critical need for a software tool that enabled targeted proteomics experiments. Since then, Skyline has grown into an entire ecosystem of tools, expanding well beyond targeted proteomics. The Skyline software ecosystem is one of the most widely used software platforms in all of mass spectrometry, supporting thousands of investigators in their research. The synergy between Skyline software development and its vast and thriving user community uniquely generate exciting new opportunities for quantitative mass spectrometry. Skyline has been a key factor in the success and growth of this new field, with Skyline itself becoming one of the most significant software tools in mass spectrometry. Since 2015, we have expanded Skyline software, from just the traditional targeted proteomics experiments that used selected reaction monitoring (SRM) with triple quadrupole (QQQ) mass spectrometers, to broadly encompass ALL types of quantitative proteomics experiments, including data dependent acquisition (DDA) experiments using MS1 peak areas (aka MS1 filtering), targeted tandem mass spectrometry (aka parallel reaction monitoring or PRM) experiments and data independent acquisition (DIA). As of Oct 2019, Skyline has been installed >97,500 times (117% increase since 2015), has over 14,000 registered users (122% increase since 2015) on its website (http://skyline.ms) and is booted up >9,000 times per week (exceeding 17,500 bootups in a single week). The Skyline project has grown beyond the bounds of a single tool. Currently, there are 14 Skyline external tools (55% increase since 2015) that rely on a formalized framework in Skyline and available through its tool store, with more still in development. The prior grant cycle has greatly expanded a community of users and developers working with a common set of tools to analyze quantitative data from all six major mass spectrometry vendors. Specifically, our proposal has five aims. 1) Improve Skyline’s analysis of DDA data, 2) Improve Skyline’s analysis of DIA data, 3) Expand support of new molecule types within Skyline, 4) Support for new quantitative data types, and 5) Provide continued support and training for the Skyline ecosystem. Mass spectrometry has been a fundamental technology for the analysis diverse molecule types in health and disease. Targeted mass spectrometry measurements offer a promising alternative to immunological based assays that are the standard for quantitative protein measurements in clinical and basic research laboratories. Critical to these experiments is our software, Skyline and the associated ecosystem of tools, which have been developed to handle the generation of instrument methods and the subsequent analysis of the resulting data.",Skyline Targeted Proteomics Environment,10049625,R01GM103551,"['Algorithms', 'Area', 'Basic Science', 'Biological Assay', 'Clinical Research', 'Collection', 'Communities', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Discovery', 'Data Scientist', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational workshop', 'Engineering', 'Environment', 'Flow Injection Analysis', 'Funding', 'Generations', 'Grant', 'Health', 'Immunologics', 'Infrastructure', 'Laboratory Research', 'Lipids', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Methods', 'Modification', 'Monitor', 'Peptides', 'Polysaccharides', 'Proteins', 'Proteomics', 'Reaction', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Software Tools', 'Technology', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vendor', 'base', 'biomedical scientist', 'computerized data processing', 'crosslink', 'data acquisition', 'experimental study', 'improved', 'innovation', 'instrument', 'ion mobility', 'mass spectrometer', 'meetings', 'new growth', 'open source', 'search engine', 'small molecule', 'software development', 'success', 'synergism', 'tandem mass spectrometry', 'tool', 'web site', 'webinar']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,258335,0.00818442948556257
"Automated Molecular Identity Disambiguator (AutoMID) PROJECT SUMMARY Small molecules are one of the most important classes of therapeutics alleviating suffering and in many cases death for hundreds of millions of people worldwide. Small molecules also serve as invaluable tools to study biology, often with the goal to validate novel targets for the development of future therapeutic drugs. Reproducibility of experimental results and the interoperability and reusability of resulting datasets depend on accurate descriptions of associated research objects, and most critically on correct representations of small molecules that are tested in biological assays. For example, it is not possible to develop predictive models of protein target - small molecule interactions if their chemical structure representations are not correct. Many factors contribute to errors in reported chemical structures in small molecule screening and omics reference databases, scientific publications, and many other web-based resources and documents. Because of the complexity of representing small molecules chemical structure graphs and the lack of thorough curation, errors are frequently introduced by non-experts and error propagation across different digital research assets is a pervasive problem. To address this challenging problem via a scalable approach, we propose the Automated Molecular Identity Disambiguator (AutoMID). AutoMID will be usable in batch mode at scale via an API, for example to assist chemical structure standardization and registration by maintainers of digital research assets, and also via interactive (UI) mode for everyday researchers to quickly and easily validate or correct their small molecule representations. AutoMID will leverage extensive highly standardized linked databases of chemical structures and associated information including names, synonyms, biological activity and physical properties and their sources / provenance and leverage expert rules and AI to enable reliable disambiguation of chemical structure identities at scale. PROJECT NARRATIVE Small molecules are one of the most important types of drugs. They also serve as invaluable tools to study biology. The complexity of representing chemical graphs and the lack of thorough curation leads to frequent small molecule structure errors, which propagate across digital research assets, impeding their interoperability and reusability. To address this challenging problem, we propose the Automated Molecular Identity Disambiguator (AutoMID). Built on expert knowledge and AI, AutoMID will enable researchers and maintainers of data repositories to reliably identify and resolve ambiguities in chemical structures at scale.",Automated Molecular Identity Disambiguator (AutoMID),9987129,R01LM013391,"['Address', 'Adoption', 'Biological', 'Biological Assay', 'Biology', 'Categories', 'Cessation of life', 'Chemical Structure', 'Chemicals', 'Classification', 'Complex', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Detection', 'Development', 'FAIR principles', 'Future', 'Goals', 'Graph', 'Hand', 'Hybrids', 'In Vitro', 'Individual', 'Knowledge', 'Legal patent', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Metadata', 'Modeling', 'Molecular', 'Molecular Structure', 'Names', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Privatization', 'Property', 'Proteins', 'Publications', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Standardization', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Training', 'base', 'cheminformatics', 'data harmonization', 'data modeling', 'data warehouse', 'design', 'digital', 'high throughput screening', 'improved', 'in silico', 'in vivo', 'interoperability', 'knowledge curation', 'novel', 'online resource', 'physical property', 'predictive modeling', 'relational database', 'screening', 'small molecule', 'software systems', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2020,293345,-0.0231722664913543
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,0.0028255913192442176
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9970407,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2020,453846,-0.014870697150628625
"Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences Abstract A major approach in causal inference literature aimed at mitigating bias due to unmeasured confounding is the so- called instrumental variable (IV) design which relies on identifying a variable which (i) influences the treatment process, (ii) has no direct effect on the outcome other than through the treatment, and (iii) is independent of any unmeasured confounder. IV methods are very well developed and widely used in social and health science, although validity of IV inferences may not be reliable if any of required assumptions (i)-(iii) is violated. This proposal aims to develop (a) new IV methods robust to violation of any of (i)-(iii); (b) New negative control methods that can be used to detect and sometimes to nonparametrically account for unmeasured confounding bias; (c) New bracketing methods for partial inference about causal effects in comparative interrupted time series studies. The proposed methods will be used to address current scientific queries in three major substantive public health areas:(1) to understand the health effects of air pollution; (2) to quantify the causal effects of modifiable risk factors for Alzheimer's disease and related disorders; (3) To uncover the mechanism by which a randomized package of interventions produced a substantial reduction of HIV incidence in a recent major cluster randomized trial of treatment as prevention in Botswana, Africa. Our proposal will provide the best available analytical methods to date to resolve confounding concerns in these high impact public health applications and more broadly in observational studies in the health sciences. Summary This proposal aims to develop new causal inference methods to tame bias due to hidden confounding factors in obser- vational studies as well as in randomized experiments subject to non-adherence. The proposed methods are firmly grounded in modern semiparametric theory which will be used to obtain more robust and efficient inferences about causal effects in a broad range of public health applications including in Epidemiology of Aging, Environmental Health Epidemiology and HIV/AIDS Prevention.",Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences,9859751,R01AG065276,"['AIDS prevention', 'Address', 'Adherence', 'Africa', 'Aging', 'Air Pollution', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Area', 'Blood Pressure', 'Botswana', 'Clinical Treatment', 'Cluster randomized trial', 'Data', 'Diabetes Mellitus', 'Disease', 'Environmental Health', 'Epidemiology', 'Genetic', 'HIV', 'Health', 'Health Sciences', 'Incidence', 'Interruption', 'Intervention', 'Learning', 'Linkage Disequilibrium', 'Literature', 'Machine Learning', 'Masks', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Observational Study', 'Outcome', 'Participant', 'Prevention', 'Process', 'Public Health', 'Public Health Applications Research', 'Randomized', 'Research Design', 'Research Personnel', 'Risk Factors', 'Series', 'Social Sciences', 'Testing', 'Thromboplastin', 'Time', 'ambient air pollution', 'analytical method', 'c new', 'comparative', 'design', 'experimental study', 'genetic variant', 'high dimensionality', 'intervention effect', 'modifiable risk', 'mortality', 'novel', 'pleiotropism', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'uptake', 'user friendly software']",NIA,UNIVERSITY OF PENNSYLVANIA,R01,2020,502013,-0.05319084466907506
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,-0.015855078853345295
"Modeling the Incompleteness and Biases of Health Data Modeling the Incompleteness and Biases of Health Data Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. Existing efforts for missing health data imputation often focus on only cross-sectional correlation (e.g., correlation across subjects or across variables) but neglect autocorrelation (e.g., correlation across time points). Moreover, they often focus on modeling incompleteness but neglect the biases in health data. Modeling both the incompleteness and bias may contribute to better understanding of health data and better support clinical decision making. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets. Aim 1 introduces the MICA framework to jointly consider cross-sectional correlation and auto-correlation. In Aim 2, we will augment MICA to be bias-aware (hence BAMICA) to account for biases stemmed from multiple roots such as healthcare process and use them as features in imputing missing health data. This augmentation is achieved by a novel recurrent neural network architecture that keeps track of both evolution of health data variables and bias factors. In Aim 3, we will supplement unstructured clinical notes to structured health data for modeling incompleteness and biases using a novel architecture of graph neural network on top of memory network. We will apply graph neural networks to process clinical notes in order to learn proper representations as input to the memory networks for imputation and downstream predictive modeling tasks. Depending on the clinical problem and data availability, not all modules may be needed. Thus our proposed BAMICA framework is designed to be flexible and consists of selectable modules to meet some or all of the above needs. In summary, our proposal bridges a key knowledge gap in jointly modeling incompleteness and biases in health data and utilizes unstructured clinical notes to supplement and augment such modeling in order to better support predictive modeling and clinical decision making. We will demonstrate generalizability by experimenting on four large clinical and cohort study datasets, and by scaling up to the eMERGE network spanning 11 institutions nationwide. We will disseminate the open-source framework. The principled and flexible framework generated by this project will bring significant methodological advancement and have a direct impact on enhancing discovery from health data. Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets.",Modeling the Incompleteness and Biases of Health Data,9941499,R01LM013337,"['Adoption', 'Algorithms', 'Architecture', 'Awareness', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Collection', 'Communities', 'Computer software', 'Critical Care', 'Data', 'Data Collection', 'Data Set', 'Dependence', 'Derivation procedure', 'Development', 'Diagnostic', 'Diagnostic tests', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Evolution', 'Functional disorder', 'General Hospitals', 'Goals', 'Graph', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Hour', 'Individual', 'Inpatients', 'Institution', 'Intuition', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Learning', 'Measurement', 'Medical', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Procedures', 'Process', 'Protocols documentation', 'Regimen', 'Research', 'Research Personnel', 'Resources', 'Role', 'Schedule', 'Structure', 'Symptoms', 'System', 'Test Result', 'Testing', 'Time', 'Training', 'Validation', 'clinical decision support', 'clinical decision-making', 'data mining', 'data quality', 'design', 'experimental study', 'flexibility', 'health care service utilization', 'health data', 'improved', 'lifetime risk', 'machine learning algorithm', 'neglect', 'neural network', 'neural network architecture', 'novel', 'open source', 'patient population', 'personalized diagnostics', 'personalized therapeutic', 'predictive modeling', 'recurrent neural network', 'scale up', 'social health determinants', 'stem', 'structured data', 'text searching', 'tool', 'trait']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,348397,-0.005306633101830954
"Estimating Mediation Effects in Prevention Studies The purpose of this competing continuation grant proposal is to develop, evaluate and apply  methodological and statistical procedures to investigate how prevention programs change outcome  variables. These mediation analyses assess the link between program effects on the constructs targeted  by a prevention program and effects on the outcome. As noted by many researchers and federal  agencies, mediation analyses identify the most effective program components and increase  understanding of the underlying mechanisms leading to changing outcome variables. Information from  mediation analysis can make interventions more powerful, more efficient, and shorter. The P. I. of this grant received a one-year NIDA small grant and four multi-year grants to develop and evaluate mediation  analysis in prevention research. This work led to many publications and innovations. The proposed  five-year continuation focuses on the further development and refinement of exciting new mediation  analysis statistical developments. Four statistical topics represent next steps in this research and include  analytical and simulation research as well as applications to etiological and prevention data. The work expands on our development of causal mediation and Bayesian mediation methods that hold great promise for mediation analysis. In Study 1, practical causal mediation and Bayesian mediation analyses  for research designs are developed and evaluated. This approach will clarify methods and develop  approaches for dealing with violation of testable and untestable assumptions. Study 2 investigates  important measurement issues for the investigation of mediation. This work will focus on methods to identify critical facets of mediating variables, approaches to understanding whether mediators and  outcomes are redundant, and develop methods for studies with big data. Study 3 continues the development and evaluation of new longitudinal mediation methods for ecological momentary assessment data and other studies with massive data collection. These new methods promise to more accurately model change over time for both individuals and groups of individuals. Study 4 develops methods to  uncover subgroups in mediation analysis including causal mediation methods, multilevel models, and new  approaches based on residuals for identifying individuals for whom mediating processes differ in  effectiveness from other individuals. For each study, we will investigate unique issues with mediation analysis of prevention data including methods for small N and also massive data collection (big data), the RcErLitEicVaANl rCoEle(Soeef imnsetruacstiounrse):ment for mediating mechanisms, and the application of the growing literature on  causal methods and Bayesian methods. Study 5 applies new statistical methods to data from several NIH  The project further develops a method, statistical mediation analysis, that extracts more information from  funded prevention studies providing important feedback about the usefulness of the methods. Study 6  research. Mediation analysis explains how and why prevention and treatments are successful. Mediation  disseminates new information about mediation analysis through our website and other media, by  analysis improves prevention and treatment so that their effects are greater and even cost less. communication with researchers, and publications from the project. n/a",Estimating Mediation Effects in Prevention Studies,9851457,R37DA009757,"['Address', 'Alcohol or Other Drugs use', 'Applications Grants', 'Bayesian Method', 'Behavioral Mechanisms', 'Big Data', 'Biological Models', 'Communication', 'Complex', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Ecological momentary assessment', 'Educational workshop', 'Effectiveness', 'Etiology', 'Evaluation', 'Feedback', 'Funding', 'Grant', 'Individual', 'Individual Differences', 'Intervention', 'Investigation', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Outcome', 'Persons', 'Prevention', 'Prevention Research', 'Prevention program', 'Principal Investigator', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Residual state', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Testing', 'Time', 'Translating', 'United States National Institutes of Health', 'Work', 'base', 'computer program', 'cost', 'data space', 'design', 'dynamic system', 'improved', 'innovation', 'interest', 'longitudinal design', 'model design', 'multilevel analysis', 'novel strategies', 'programs', 'simulation', 'successful intervention', 'theories', 'therapy design', 'tool', 'treatment research', 'web site']",NIDA,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R37,2020,382893,0.004599147218338719
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9969443,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data standards', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'large datasets', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'public repository', 'repository', 'research and development', 'software development', 'software infrastructure', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,158388,0.00532311996938296
"Reconstruction of heterogeneous and small macromolecules by cyro-EM PROJECT SUMMARY Single-particle electron cryomicroscopy (cryo-EM) has recently joined X-ray crystallography and NMR spectroscopy as a high-resolution structural method for biological macromolecules. In addition, cryo-EM produces images of individual molecules, and therefore has the potential to resolve conformational changes. The proposal aims to develop new algorithms and software for extending the application of cryo-EM to molecules that are either too small or too flexible to be mapped by existing computational tools for cryo-EM. This extension requires solving two of the most challenging computational problems posed by cryo-EM. First, mapping the structural variability of macromolecules is widely recognized as the main computational challenge in cryo-EM. Structural variations are of great significance to biologists, as they provide insight into the functioning of molecular machines. Existing computational tools are limited to a small number of distinct conformations, and therefore are incapable of tackling highly mobile biomolecules with multiple, continuous spectra of conformational changes. The first area of investigation in this project is the development of a computational framework to analyze continuous variability. The proposed approach is based on a new mathematical representation of continuously changing structures and its efficient estimation using Markov chain Monte Carlo (MCMC) algorithms. MCMC algorithms have found great success in many other scientific disciplines, yet they have been mostly overlooked for cryo-EM single particle analysis. Second, a major limiting factor for present cryo-EM studies is the molecule size. Images of small molecules (below ~50kDa) have too little signal to allow existing methods to provide valid 3-D reconstructions. It is commonly believed that cryo-EM cannot be used for molecules that are too small to be reliably detected and picked from micrographs. Challenging that widespread belief, the second area of investigation focuses on developing a groundbreaking approach for reconstructing small molecules directly from micrographs without particle picking. The new approach is based on autocorrelation analysis and completely bypasses particle picking and orientation assignment and requires just one pass over the data. The single-pass approach opens new possibilities for real-time processing during data acquisition. PROJECT NARRATIVE Determining structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, and a first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to significantly increase the power of structure-determination using electron cryomicroscopy (cryo-EM). Importantly, our methods will broaden the application of cryo-EM to molecules that are either too small or too flexible to be mapped by existing techniques.",Reconstruction of heterogeneous and small macromolecules by cyro-EM,9943364,R01GM136780,"['3-Dimensional', 'Algorithmic Software', 'Algorithms', 'Area', 'Belief', 'Biological', 'Biological Process', 'Bypass', 'Collaborations', 'Complex', 'Computational algorithm', 'Computer software', 'Cryoelectron Microscopy', 'Crystallization', 'Data', 'Data Set', 'Detection', 'Development', 'Diffusion', 'Dimensions', 'Discipline', 'Drug Design', 'Fostering', 'G-Protein-Coupled Receptors', 'Heterogeneity', 'Human Genome', 'Image', 'Individual', 'Institution', 'Investigation', 'Ion Channel', 'Ion Pumps', 'Machine Learning', 'Maps', 'Markov Chains', 'Markov chain Monte Carlo methodology', 'Mathematics', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Molecular Motors', 'Molecular Weight', 'Motion', 'NMR Spectroscopy', 'Names', 'Noise', 'Particle Size', 'Phase', 'Polymerase', 'Preparation', 'Proteins', 'Pythons', 'Research', 'Resolution', 'Ribosomes', 'Roentgen Rays', 'Sampling', 'Signal Transduction', 'Spliceosomes', 'Structural Protein', 'Structure', 'Techniques', 'Time', 'Uncertainty', 'Update', 'Variant', 'Work', 'X-Ray Crystallography', 'base', 'computer framework', 'computerized data processing', 'computerized tools', 'data acquisition', 'expectation', 'flexibility', 'high dimensionality', 'improved', 'insight', 'interest', 'macromolecule', 'molecular mass', 'novel strategies', 'open source', 'particle', 'programs', 'protein complex', 'protein structure', 'receptor', 'reconstruction', 'small molecule', 'statistics', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2020,328440,-0.011123334038015624
"Methods for determination of glycoprotein glycosylation similarities among disease states Abstract This application addresses NIGMS PAR-17-045 “Focused Technology Research and Development (R01)”. This initiative supports projects that focus solely on development of technologies with the potential to enable biomedical research. Dysregulation of the cellular microenvironment occurs in cancers, neurodevelopmental and neuropsychiatric diseases. Known as the matrisome, the set of extracellular matrix and cell surface molecules control the availability of growth factors to cellular receptors and the mechanical-physical properties of the cell microenvironment. Currently, the limited understanding of regulation of matrisome glycosylation hinders understanding of the roles of glycosylation-dependent matrisome networks in the basic mechanisms necessary for targeted intervention of many diseases. Matrisome function depends on networks of interaction among glycosylated proteins and glycan-binding lectins. It is not possible using present proteomics and glycoproteomics methods to compare using rigorous statistics similarities of glycoproteins that differ by disease-related changes in site-specific glycosylation. We propose to develop technologies to meet this need. Present proteomics methods quantify proteins using a few representative peptides per gene product; sequence coverage for most proteins is low. Such low sequence coverage does not suffice to reconstruct the predominant glycosylated proteoforms active in a biological context. We propose to develop technologies to compare glycoprotein similarities among biological sample sets. To do this, we will develop MS acquisition and bioinformatics methods for rapid, sensitive and reproducible mapping of glycoprotein glycosylation to enable statistically rigorous comparison of glycoprotein similarities. By making these technologies available, we will enable a new level of understanding of the roles of matrisome networks in human diseases. Project narrative The matrisome consists of glycosylated extracellular matrix and cell surface proteins that surround cells and support normal physiological activity. While it is known that glycosylation changes during disease processes, it has not been possible to quantitatively compare glycoprotein structure among biological samples. We aim to develop technologies to meet this need.",Methods for determination of glycoprotein glycosylation similarities among disease states,9995540,R01GM133963,"['Address', 'Algorithms', 'Atherosclerosis', 'Autoimmune Diseases', 'Binding', 'Bioinformatics', 'Biological', 'Biological Process', 'Biomedical Research', 'Brain', 'Brain region', 'CSPG3 gene', 'Cell Surface Proteins', 'Cell surface', 'Cells', 'Chondroitin Sulfate Proteoglycan', 'Collagen', 'Complex', 'Core Protein', 'Data', 'Data Set', 'Disease', 'Dissociation', 'Electron Transport', 'Environment', 'Enzymes', 'Extracellular Matrix', 'Family', 'Functional disorder', 'Genes', 'Glycopeptides', 'Glycoproteins', 'Growth Factor', 'Growth Factor Receptors', 'Heart', 'Heparitin Sulfate', 'Intelligence', 'Intervention', 'Ions', 'Knowledge', 'Lectin', 'Liquid Chromatography', 'Machine Learning', 'Malignant Neoplasms', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Neurodegenerative Disorders', 'Pathway interactions', 'Peptides', 'Physiological', 'Polysaccharides', 'Process', 'Protein Glycosylation', 'Proteins', 'Proteoglycan', 'Proteomics', 'Receptor Protein-Tyrosine Kinases', 'Regulation', 'Reproducibility', 'Role', 'Sampling', 'Signal Pathway', 'Site', 'Structure', 'Technology', 'Tissues', 'aggrecan', 'bioinformatics tool', 'brevican', 'cell growth', 'data acquisition', 'data to knowledge', 'extracellular', 'gene product', 'glycoprotein structure', 'glycoproteomics', 'glycosylation', 'human disease', 'hydrophilicity', 'neuropsychiatric disorder', 'pathogen', 'physical property', 'rapid technique', 'receptor', 'statistics', 'technology development', 'technology research and development', 'versican']",NIGMS,BOSTON UNIVERSITY MEDICAL CAMPUS,R01,2020,420750,-0.031209875801652943
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9855035,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'disease phenotype', 'driver mutation', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'machine learning method', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistical learning', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,305167,-0.00283188936337735
"Computational Methods for Enhancing Privacy in Biomedical Data Sharing Project Summary Data sharing is essential to modern biomedical data science. Access to a large amount of genomic and clinical data can help us better understand human genetics and its impact on health and disease. However, the sensitive nature of biomedical information presents a key bottleneck in data sharing and collection efforts, limiting the utility of these data for science. The goal of this project is to leverage cutting-edge advances in cryptography and information theory to develop innovative computational frameworks for privacy-preserving sharing and analysis of biomedical data. We will draw upon our recent success in developing secure pipelines for collaborative biomedical analyses to address the imminent need to share sensitive data securely and at scale.  Practical adoption of existing privacy-preserving techniques in biomedicine has thus far been largely limited due to two major pitfalls, which this project overcomes with novel technical advances. First, emerging cryptographic data sharing frameworks, which promise to enable collaborative analysis pipelines that securely combine data across multiple institutions with theoretical privacy guarantees, are too costly to support complex and large-scale computations required in biomedical analyses. In this project, we will build upon recent advances in cryptography (e.g., secure distributed computation, pseudorandom correlation, zero-knowledge proofs) to significantly enhance the scalability and security of cryptographic biomedical data sharing pipelines. Second, existing approaches that locally transform data to protect sensitive information before sharing (e.g. de-identification techniques) either offer insufficient levels of protection or require excessive perturbation in order to ensure privacy. We will draw upon recent tools from information theory to develop effective local privacy protection methods that achieve superior utility-privacy tradeoffs on a range of biomedical data including genomes, transcriptomes, and medical images by directly exploiting the latent correlation structure of the data.  To promote the use of our privacy techniques, we will create production-grade software of our tools and publicly release them. We will also actively participate in international standard-setting organizations in genomics, e.g. GA4GH and ICDA, to incorporate our insights into community guidelines for biomedical privacy. Successful completion of these aims will result in computational methods and software tools that open the door to secure sharing and analysis of massive sets of sensitive genomic and clinical data. Our long-term goal is to broadly enable data sharing and collaboration efforts in biomedicine, thus empowering researchers to better understand the molecular basis of human health and to drive translation of new biological insights to the clinic. Project Narrative Rapidly-growing volume of biomedical datasets around the world promises to enable unprecedented insights into human health and disease. However, increasing concerns for individual privacy severely limited the extent of data sharing in the field. This project draws upon cutting-edge tools from cryptography and information theory to develop effective privacy- preserving methods for collecting, sharing, and analyzing sensitive biomedical data to empower advances in genomics and medicine.",Computational Methods for Enhancing Privacy in Biomedical Data Sharing,10017554,DP5OD029574,"['Address', 'Adoption', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Clinic', 'Clinical Data', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Disease', 'Electronic Health Record', 'Engineering', 'Enhancement Technology', 'Ensure', 'Foundations', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Genetics', 'Image', 'Individual', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'International', 'Knowledge', 'Letters', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mainstreaming', 'Mathematics', 'Medical Genetics', 'Medical Imaging', 'Mentorship', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Pattern', 'Pharmacology', 'Policies', 'Polynomial Models', 'Preservation Technique', 'Privacy', 'Privatization', 'Production', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Science', 'Secure', 'Security', 'Software Tools', 'Structure', 'Techniques', 'Technology', 'Translations', 'Vision', 'Work', 'analysis pipeline', 'base', 'biomedical data science', 'computer framework', 'computing resources', 'cost', 'cryptography', 'data sharing', 'design', 'empowered', 'experience', 'experimental study', 'genetic analysis', 'genome wide association study', 'genomic data', 'infancy', 'innovation', 'insight', 'novel', 'privacy preservation', 'privacy protection', 'software development', 'statistics', 'structured data', 'success', 'task analysis', 'tool', 'transcriptome', 'transcriptomics', 'web server']",OD,"BROAD INSTITUTE, INC.",DP5,2020,392799,-0.009391023792375582
"Alliance for Regenerative Rehabilitation Research & Training 2.0 (AR3T 2.0) OVERALL: ABSTRACT  The scope of regenerative medicine encompasses the repair, regeneration, and replacement of defective, injured, and diseased tissues and organs. The success of regenerative therapies is dependent, at least in part, on a favorable microenvironment in which the regenerative processes occur. Technological innovations and a deepened mechanistic understanding of how these microenvironmental signals influence tissue regeneration has drawn attention to the critical importance of the clinical field with foundations in the application of physical, thermal, and electrical stimuli to promote functional restoration—rehabilitation. We propose that the fields of regenerative medicine and rehabilitative science are inextricably intertwined, an intersection of disciplines that we and others have termed Regenerative Rehabilitation. To realize the full potential of Regenerative Rehabilitation, there is a need for formalized mechanisms that promote the interaction of basic scientists with rehabilitation specialists. During the initial funding cycle, the Alliance for Regenerative Rehabilitation Research & Training (AR3T) built a national network of investigators and programs that has helped to expand scientific knowledge, expertise and methodologies across the domains of regenerative medicine and rehabilitation. This proposal seeks funding for AR3T 2.0, in which we will build on successes achieved and lessons learned over the initial period of support with the goal of being even more responsive to the needs of the greater community. Six specific aims define a framework upon which we will achieve our goals. AR3T will provide education and drive the science underlying Regenerative Rehabilitation by: 1) Providing didactic programs that expose rehabilitation researchers to cutting-edge investigations and state-of-the-art technologies in the field of regenerative medicine (Didactic Aim); 2) Cultivating collaborative opportunities between renowned investigators in the fields of regenerative medicine and rehabilitation (Collaborations Aim); 3) Coordinating a pilot funding program to support novel lines of Regenerative Rehabilitation research (Pilot Funding Aim); 4) Developing and validating technologies to advance the measurement and use of the regenerative rehabilitation programs (Technology Aim); 5) Promoting our center’s expertise to a broad community of trainees, investigators, and clinicians (Promotion Aim); 6) Carefully monitoring and evaluating the effectiveness of our program will ensure that we are successful in achieving our goals (Quality Control Aim). Administrative note: In the preparation of this proposal, we made every effort to present a comprehensive and detailed plan for achieving our goals while minimizing redundancy. Therefore, in multiple places, we refer the reader to specific components of the application, rather than repeating text. We appreciate the time and effort the reviewers devote to the evaluation of the proposals.  Sincerely, Fabrisia, Tom and Mike PROJECT NARRATIVE  Regenerative Rehabilitation is the integration of principles and approaches across the fields of rehabilitation science and regenerative medicine. The integration of these two fields will increase the efficiency of interventions designed to optimize physical functioning to the benefit of a wide range of individuals with disabilities. The Alliance for Regenerative Rehabilitation Research & Training (AR3T) 2.0 will build on the momentum gained over the first cycle of funding with the goal of continuing to illuminate and seize opportunities to expand scientific knowledge, expertise and methodologies in the domain of Regenerative Rehabilitation.",Alliance for Regenerative Rehabilitation Research & Training 2.0 (AR3T 2.0),9967689,P2CHD086843,"['Accountability', 'Activities of Daily Living', 'Age', 'Attention', 'Awareness', 'Basic Science', 'Biocompatible Materials', 'Clinical', 'Collaborations', 'Communities', 'Congenital Abnormality', 'Country', 'Data Analyses', 'Development', 'Disabled Persons', 'Discipline', 'Disease', 'Documentation', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'In Vitro', 'Incubators', 'Individual', 'Injury', 'Intervention', 'Investigation', 'Journals', 'Knowledge', 'Laboratories', 'Machine Learning', 'Marketing', 'Measurement', 'Mechanics', 'Mentors', 'Methodology', 'Methods', 'Mission', 'Monitor', 'Natural regeneration', 'Organ', 'Performance', 'Physical Function', 'Pre-Clinical Model', 'Preparation', 'Process', 'Quality Control', 'Reader', 'Regenerative Medicine', 'Rehabilitation therapy', 'Research', 'Research Design', 'Research Personnel', 'Research Training', 'Resources', 'Science', 'Scientist', 'Series', 'Signal Transduction', 'Specialist', 'Stimulus', 'Structure', 'Systems Analysis', 'Technology', 'Text', 'Time', 'Tissues', 'Training', 'Trauma', 'Treatment Efficacy', 'Update', 'career', 'effectiveness evaluation', 'falls', 'functional restoration', 'gait examination', 'healing', 'injured', 'innovation', 'interest', 'investigator training', 'multidisciplinary', 'new technology', 'novel', 'novel strategies', 'pre-clinical', 'programs', 'regenerative', 'regenerative therapy', 'rehabilitation research', 'rehabilitation science', 'repaired', 'response', 'sabbatical', 'social media', 'success', 'symposium', 'technological innovation', 'therapy design', 'tissue regeneration', 'webinar']",NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P2C,2020,1059198,-0.008325998748545747
"Methods for determination of glycoprotein glycosylation similarities among disease states Abstract This application addresses NIGMS PAR-17-045 “Focused Technology Research and Development (R01)”. This initiative supports projects that focus solely on development of technologies with the potential to enable biomedical research. Dysregulation of the cellular microenvironment occurs in cancers, neurodevelopmental and neuropsychiatric diseases. Known as the matrisome, the set of extracellular matrix and cell surface molecules control the availability of growth factors to cellular receptors and the mechanical-physical properties of the cell microenvironment. Currently, the limited understanding of regulation of matrisome glycosylation hinders understanding of the roles of glycosylation-dependent matrisome networks in the basic mechanisms necessary for targeted intervention of many diseases. Matrisome function depends on networks of interaction among glycosylated proteins and glycan-binding lectins. It is not possible using present proteomics and glycoproteomics methods to compare using rigorous statistics similarities of glycoproteins that differ by disease-related changes in site-specific glycosylation. We propose to develop technologies to meet this need. Present proteomics methods quantify proteins using a few representative peptides per gene product; sequence coverage for most proteins is low. Such low sequence coverage does not suffice to reconstruct the predominant glycosylated proteoforms active in a biological context. We propose to develop technologies to compare glycoprotein similarities among biological sample sets. To do this, we will develop MS acquisition and bioinformatics methods for rapid, sensitive and reproducible mapping of glycoprotein glycosylation to enable statistically rigorous comparison of glycoprotein similarities. By making these technologies available, we will enable a new level of understanding of the roles of matrisome networks in human diseases. Project narrative The matrisome consists of glycosylated extracellular matrix and cell surface proteins that surround cells and support normal physiological activity. While it is known that glycosylation changes during disease processes, it has not been possible to quantitatively compare glycoprotein structure among biological samples. We aim to develop technologies to meet this need.",Methods for determination of glycoprotein glycosylation similarities among disease states,10135316,R01GM133963,"['Address', 'Algorithms', 'Atherosclerosis', 'Autoimmune Diseases', 'Binding', 'Bioinformatics', 'Biological', 'Biological Process', 'Biomedical Research', 'Brain', 'Brain region', 'CSPG3 gene', 'Cell Surface Proteins', 'Cell surface', 'Cells', 'Chondroitin Sulfate Proteoglycan', 'Collagen', 'Complex', 'Core Protein', 'Data', 'Data Set', 'Disease', 'Dissociation', 'Electron Transport', 'Environment', 'Enzymes', 'Extracellular Matrix', 'Family', 'Functional disorder', 'Genes', 'Glycopeptides', 'Glycoproteins', 'Growth Factor', 'Growth Factor Receptors', 'Heart', 'Heparitin Sulfate', 'Intelligence', 'Intervention', 'Ions', 'Knowledge', 'Lectin', 'Liquid Chromatography', 'Machine Learning', 'Malignant Neoplasms', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Neurodegenerative Disorders', 'Pathway interactions', 'Peptides', 'Physiological', 'Polysaccharides', 'Process', 'Protein Glycosylation', 'Proteins', 'Proteoglycan', 'Proteomics', 'Receptor Protein-Tyrosine Kinases', 'Regulation', 'Reproducibility', 'Role', 'Sampling', 'Signal Pathway', 'Site', 'Structure', 'Technology', 'Tissues', 'aggrecan', 'bioinformatics tool', 'brevican', 'cell growth', 'data acquisition', 'data to knowledge', 'extracellular', 'gene product', 'glycoprotein structure', 'glycoproteomics', 'glycosylation', 'human disease', 'hydrophilicity', 'neuropsychiatric disorder', 'pathogen', 'physical property', 'rapid technique', 'receptor', 'statistics', 'technology development', 'technology research and development', 'versican']",NIGMS,BOSTON UNIVERSITY MEDICAL CAMPUS,R01,2020,100000,-0.031209875801652943
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10050843,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk stratification', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,759918,-0.004306788345293212
