text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9743225,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2019,528283,0.06464282653907949
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9614770,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genetic Diseases', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,538700,0.06464282653907949
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9770622,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,380000,0.0049764941359981925
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9123422,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Natural Products', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'novel therapeutics', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2016,419732,0.047167375048918214
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9126755,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,40625,0.047167375048918214
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9117880,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,379655,0.047167375048918214
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.       PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.         ",The Transporter Classification Database (TCDB),8628843,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'genome annotation', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,293768,0.031867405009281466
"In silico identification of phyto-therapies     DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits.                RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.  ",In silico identification of phyto-therapies,8749705,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R01,2014,389869,0.047167375048918214
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.       PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.         ",The Transporter Classification Database (TCDB),8447507,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'genome annotation', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,290465,0.031867405009281466
"A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology    DESCRIPTION (provided by applicant): The non-medical use of pharmaceutical opioids has been identified as one of the fastest growing forms of drug abuse in the U.S. There is a critical need to enhance current epidemiological monitoring, early warning, and post-marketing surveillance systems by providing additional and more timely data. The World Wide Web has been identified as one of the ""leading edge"" data sources for detecting patterns and changes in drug use practices. Many websites provide a venue for individuals to freely share their own experiences, post questions, and offer comments about different drugs. Such User Generated Content (UGC) can be used as a very rich data source to study knowledge, attitudes, and behaviors related to illicit drugs. To harness the full potential of the Web for drug abuse research, the field needs to develop a highly automated way of accessing, extracting, and analyzing Web-based data related to illicit drug use. This exploratory R21 is a multi-principal investigator, collaborative effort between researchers at the Center for Interventions, Treatment and Addictions Research (CITAR) and the Center for Knowledge-Enabled Information Services and Science (Kno.e.sis) at Wright State University. The purpose of this Web-based study is to apply cutting-edge information processing techniques, such as the Semantic Web, Natural Language Processing, and Machine Learning, to qualitative and quantitative content analysis of user generated content to achieve the following aims: 1) Describe drug users' knowledge, attitudes, and behaviors related to the illicit use of Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine); 2) Identify and describe temporal patterns of the illicit use of these drugs as reflected on web-based forums. To collect data, the study will use websites that allow for the free discussion of illicit drugs, contain information on illicit prescription drug use, and are accessible for public viewing. The study will generate new information about the practices of buprenorphine abuse and will contribute to the advancement of public health and substance abuse research by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data. Automated information extraction methods applied in this study will enhance current early warning and epidemiological surveillance systems and could advance qualitative and Web-based research methods in other areas of public health.        Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.            ",A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology,8269958,R21DA030571,"['Accident and Emergency department', 'Archives', 'Area', 'Behavior', 'Buprenorphine', 'Code', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Data', 'Data Sources', 'Drug Rehabilitation Centers', 'Drug abuse', 'Drug usage', 'Drug user', 'Early identification', 'Epidemiologic Monitoring', 'Epidemiologic Studies', 'Epidemiology', 'Face', 'Health', 'Health Knowledge, Attitudes, Practice', 'Health Professional', 'Heroin Dependence', 'Hospitals', 'Illicit Drugs', 'Individual', 'Information Sciences', 'Information Services', 'Information Systems', 'Internet', 'Intervention', 'Intervention Studies', 'Interview', 'Knowledge', 'Label', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Monitor', 'NIH Program Announcements', 'Naloxone', 'Natural Language Processing', 'Online Systems', 'Opioid', 'Overdose', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Poison Control Centers', 'Policies', 'Population', 'Prevention', 'Principal Investigator', 'Process', 'Public Health', 'Published Comment', 'Qualitative Research', 'Reading', 'Reliance', 'Research', 'Research Methodology', 'Research Personnel', 'Sampling', 'Selection Bias', 'Self Disclosure', 'Semantics', 'Source', 'Substance abuse problem', 'Subutex', 'Surveillance Methods', 'Surveys', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'addiction', 'buprenorphine abuse', 'computer based Semantic Analysis', 'design', 'emotional disclosure', 'empowered', 'experience', 'informant', 'information processing', 'misuse of prescription only drugs', 'population survey', 'post-market', 'prescription drug abuse', 'programs', 'response', 'social', 'tool', 'trend', 'web site', 'working group']",NIDA,WRIGHT STATE UNIVERSITY,R21,2012,182500,0.01707692067099906
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8515555,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2012,290837,0.043623947058157427
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.      PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.           PROJECT NARRATIVE:  TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.",The Transporter Classification Database (TCDB),8230584,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Genome', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,303730,0.025225243365216796
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8496251,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2012,40000,0.1078849780309828
"A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology    DESCRIPTION (provided by applicant): The non-medical use of pharmaceutical opioids has been identified as one of the fastest growing forms of drug abuse in the U.S. There is a critical need to enhance current epidemiological monitoring, early warning, and post-marketing surveillance systems by providing additional and more timely data. The World Wide Web has been identified as one of the ""leading edge"" data sources for detecting patterns and changes in drug use practices. Many websites provide a venue for individuals to freely share their own experiences, post questions, and offer comments about different drugs. Such User Generated Content (UGC) can be used as a very rich data source to study knowledge, attitudes, and behaviors related to illicit drugs. To harness the full potential of the Web for drug abuse research, the field needs to develop a highly automated way of accessing, extracting, and analyzing Web-based data related to illicit drug use. This exploratory R21 is a multi-principal investigator, collaborative effort between researchers at the Center for Interventions, Treatment and Addictions Research (CITAR) and the Center for Knowledge-Enabled Information Services and Science (Kno.e.sis) at Wright State University. The purpose of this Web-based study is to apply cutting-edge information processing techniques, such as the Semantic Web, Natural Language Processing, and Machine Learning, to qualitative and quantitative content analysis of user generated content to achieve the following aims: 1) Describe drug users' knowledge, attitudes, and behaviors related to the illicit use of Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine); 2) Identify and describe temporal patterns of the illicit use of these drugs as reflected on web-based forums. To collect data, the study will use websites that allow for the free discussion of illicit drugs, contain information on illicit prescription drug use, and are accessible for public viewing. The study will generate new information about the practices of buprenorphine abuse and will contribute to the advancement of public health and substance abuse research by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data. Automated information extraction methods applied in this study will enhance current early warning and epidemiological surveillance systems and could advance qualitative and Web-based research methods in other areas of public health.      PUBLIC HEALTH RELEVANCE: Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.              Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.            ",A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology,8190799,R21DA030571,"['Accident and Emergency department', 'Archives', 'Area', 'Behavior', 'Buprenorphine', 'Code', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Data', 'Data Sources', 'Drug Rehabilitation Centers', 'Drug abuse', 'Drug usage', 'Drug user', 'Early identification', 'Epidemiologic Monitoring', 'Epidemiologic Studies', 'Epidemiology', 'Face', 'Health', 'Health Knowledge, Attitudes, Practice', 'Health Professional', 'Heroin Dependence', 'Hospitals', 'Illicit Drugs', 'Individual', 'Information Sciences', 'Information Services', 'Information Systems', 'Internet', 'Intervention', 'Intervention Studies', 'Interview', 'Knowledge', 'Label', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Monitor', 'NIH Program Announcements', 'Naloxone', 'Natural Language Processing', 'Online Systems', 'Opioid', 'Overdose', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Poison Control Centers', 'Policies', 'Population', 'Prevention', 'Principal Investigator', 'Process', 'Public Health', 'Published Comment', 'Qualitative Research', 'Reading', 'Reliance', 'Research', 'Research Methodology', 'Research Personnel', 'Sampling', 'Selection Bias', 'Self Disclosure', 'Semantics', 'Source', 'Substance abuse problem', 'Subutex', 'Surveillance Methods', 'Surveys', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'addiction', 'buprenorphine abuse', 'computer based Semantic Analysis', 'design', 'emotional disclosure', 'empowered', 'experience', 'informant', 'information processing', 'misuse of prescription only drugs', 'population survey', 'post-market', 'prescription drug abuse', 'programs', 'response', 'social', 'tool', 'trend', 'web site', 'working group']",NIDA,WRIGHT STATE UNIVERSITY,R21,2011,219000,0.028134272239588348
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,8062287,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Comorbidity', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Health Sciences', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Schools', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2011,140381,0.027522424807179667
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,8075593,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2011,274386,0.08127655406465112
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8034342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2011,332732,0.043623947058157427
"The Transporter Classification Database (TCDB)    DESCRIPTION (provided by applicant): Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, and individual transporter.  In this competitive renewal of GM0077402, we propose to broaden and deepen our efforts to expand, update, automate and interlink TCDB. We will generate new data concerning transport proteins, design new machine learning approaches for data, and introduce procedures for making functional predictions of uncharacterized transporters. This last effort will derive reliable new biological knowledge from a variety of sources, including phylogeny, motif, domain, operon and regulon analyses.  Our Specific Aims are as follows:  1. To develop software for automatic text mining and information extraction.  2. To conduct bioinformatic analyses and molecular biological experiments for TC knowledge expansion.  3. To interconnect TCDB bidirectionally with other relevant databases, thereby creating a  ""network"" of knowledge from current ""island"" databases.  4. To use multiple approaches to derive reliable functional predictions as guides for future  research.  5. To utilize a newly formed TCDB advisory board and establish a plan for modernization and  sustainability. These goals are top priorities for rendering TCDB increasingly useful to the scientific community.      PUBLIC HEALTH RELEVANCE: TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.           TCDB is a database providing the worldwide scientific community with systematized information about proteins that catalyze transmembrane transport of salts, nutrients, toxins, drugs and macromolecules. It is the only IUBMB approved system for classifying transport proteins. Funding of this proposal will allow the maintenance and further development of TCDB, interlinking with related databases, expansion of machine learning approaches for information acquisition, and introduction of approaches for predicting the functions of uncharacterized proteins.         ",The Transporter Classification Database (TCDB),8042509,R01GM077402,"['Adopted', 'Adoption', 'Algorithms', 'Animals', 'Binding Sites', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biology', 'Carrier Proteins', 'Cells', 'Cistrons', 'Classification', 'Communities', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Digital Libraries', 'Ecosystem', 'Escherichia coli', 'Eukaryota', 'Family', 'Funding', 'Future', 'Genome', 'Goals', 'Hereditary Disease', 'Homeostasis', 'Housing', 'Human Genetics', 'Individual', 'Information Resources', 'Institutes', 'International', 'Internet', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Literature', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Multi-Drug Resistance', 'Names', 'Nutrient', 'Online Mendelian Inheritance In Man', 'Operon', 'Organelles', 'Organism', 'Paper', 'Pharmaceutical Preparations', 'Phylogenetic Analysis', 'Phylogeny', 'Physiological', 'Physiology', 'Plants', 'Postdoctoral Fellow', 'Procedures', 'Prokaryotic Cells', 'Protein Binding', 'Proteins', 'PubMed', 'Recruitment Activity', 'Regulon', 'Research Personnel', 'Resources', 'Salts', 'Secure', 'Seeds', 'Signal Transduction', 'Source', 'Structure', 'Students', 'System', 'Technology', 'Time', 'Toxin', 'Transmembrane Transport', 'Update', 'Work', 'base', 'design', 'drug discovery', 'improved', 'link protein', 'macromolecule', 'novel', 'novel strategies', 'pathogen', 'protein transport', 'research study', 'software development', 'text searching', 'tool', 'transmission process', 'tumor']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,324495,0.025568788158491493
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8138357,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2011,128304,0.1078849780309828
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8325815,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2011,100000,0.1078849780309828
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7936991,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,BOSTON CHILDREN'S HOSPITAL,RC1,2010,491408,0.045537737233620874
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7802898,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Comorbidity', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2010,136911,0.027522424807179667
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7851323,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2010,278345,0.08127655406465112
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,7772342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2010,326303,0.043623947058157427
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,8129905,R43HG005046,"['Address', 'Benefits and Risks', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2010,35000,0.04966476737413231
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7921043,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2010,133651,0.1078849780309828
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7918614,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'clinical practice', 'forgetting', 'innovation', 'natural language', 'preference', 'speech processing', 'symposium']",NLM,UNIVERSITY OF CHICAGO,R01,2009,185745,0.04550011559377706
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7842799,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,MAYO CLINIC ROCHESTER,RC1,2009,497477,0.045537737233620874
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7597080,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2009,138161,0.027522424807179667
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7920591,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2009,54000,0.027522424807179667
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7656692,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,282304,0.08127655406465112
"Textpresso: information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso: information retrieval and extraction system for biological literature,7583249,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Body of uterus', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2009,320000,0.043623947058157427
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7672256,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Readability', 'Reader', 'Reading', 'Self Care', 'Self Management', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'fourth grade', 'improved', 'instrument', 'literacy', 'ninth grade', 'patient oriented', 'prevent', 'programs', 'tenth grade', 'tool', 'web site']",NIDDK,UNIVERSITY OF UTAH,R01,2009,398216,0.02438149846366168
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,7748337,R43HG005046,"['Address', 'Benefits and Risks', 'Body of uterus', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2009,119499,0.04966476737413231
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7692401,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2009,135000,0.1078849780309828
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7928674,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2009,55179,0.1078849780309828
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7675157,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Language', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'day', 'forgetting', 'innovation', 'preference', 'speech processing', 'symposium']",NLM,UNIVERSITY OF CHICAGO,R01,2008,354823,0.04550011559377706
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7364629,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physical Dialysis', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Range', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Standards of Weights and Measures', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'computer science', 'cost effectiveness', 'data management', 'desire', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'prescription document', 'prescription procedure', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2008,134890,0.027522424807179667
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7432910,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Eukaryotic Cell', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'desire', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,273906,0.08127655406465112
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7475712,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,438476,0.02438149846366168
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7671784,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,48482,0.02438149846366168
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7217497,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Language', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'day', 'forgetting', 'innovation', 'preference', 'speech processing', 'symposium']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,361696,0.04550011559377706
"Technology Development for a MolBio Knowledge-Base Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges adsing from the proliferation of high- throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent iadvances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB. n/a",Technology Development for a MolBio Knowledge-Base,7473405,R01LM008111,"['Address', 'Area', 'Biology', 'Class', 'Clinical', 'Collection', 'Data', 'Databases', 'Evaluation', 'Facility Construction Funding Category', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Human', 'Indium', 'Informatics', 'Information Resources', 'Information Resources Management', 'Investments', 'Knowledge', 'Knowledge Base (Computer)', 'Laboratories', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Metabolism', 'Methods', 'Metric System', 'Molecular', 'Molecular Biology', 'Mus', 'Names', 'Natural Language Processing', 'Numbers', 'Ontology', 'Persons', 'Pharmaceutical Preparations', 'Plug-in', 'Proteins', 'Purpose', 'Semantics', 'Source', 'SwissProt', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'base', 'concept', 'data integration', 'gene function', 'heuristics', 'instrumentation', 'knowledge base', 'success', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,234058,0.06459798084497893
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7209599,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physical Dialysis', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Range', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Standards of Weights and Measures', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'computer science', 'cost effectiveness', 'data management', 'desire', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'prescription document', 'prescription procedure', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2007,112534,0.027522424807179667
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7303652,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,435682,0.02438149846366168
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance  existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of  health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted  readability levels with no critical information loss, using statistical natural language processing  techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive  impact on reader comprehension. We will use as a test bed for our system a general internal medicine  clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public. n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7492453,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,47853,0.021603856397211775
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methodswithin the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data,  earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user  nterface to support access to information resources for enhanced decision-making by practitioners. The ong-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3  Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technologywork; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C). n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7284424,P01HK000027,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2007,1889501,0.03897044309147919
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,7123058,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,611872,0.06742123287489532
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,0.051245354930860285
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,0.05212813230488494
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7101997,R01LM008799,"['clinical research', 'human', 'language', 'memory disorders', 'model', 'physicians', 'training', 'voice']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,372499,0.04550011559377706
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,7033080,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2006,133459,0.03727891910186499
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6953701,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2005,613495,0.06742123287489532
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The overall goal of this proposal is to develop an information integration architecture and associated tools to support rapid integration of data and knowledge from distributed heterogeneous data sources. The architecture aims to play a significant role in extracting coherent knowledge bases for biomedical research and improving the accuracy, completeness and quality of the extracted knowledge. Towards achieving these goals, the proposed scalable architecture includes new innovative generalized integration algorithms and tools for the generation of mediators to capture the functional behavior of data sources, semantic representation of data sources to support automated generation of integration agents, and optimization of integrated data queries. The information integration architecture keeps pace with the evolving Internet-based XML electronic data interchange, semantic web services, and web services discovery standards. Thus, leveraging the Internet technologies and standards for the purpose of providing lasting state-of-the-art solutions for information integration. In addition, the proposed architecture is inherently scalable in terms of the number of data sources that can be integrated, the number of users of the integrated system, and the range of biomedical problems that can be tackled. During phase I of the project, prototypes of the proposed integration algorithms and tools will be developed as proofs of concept and to form the foundation for evaluation and pilot testing of the proposed integration mechanisms, using private and public data sources, in terms of scalability and integration capabilities.         n/a",Information Integration of Heterogeneous Data Sources,6881960,R43RR018667,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'information system analysis', 'information systems', 'mathematics']",NCRR,"INFOTECH SOFT, INC.",R43,2005,260661,0.05838861956680804
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,0.051245354930860285
"Linking Information, Families and Technology (LIFT) DESCRIPTION (provided by applicant): KIT Solutions, a private firm specializing in developing intelligent, Knowledge-based Information Technology (KIT) solutions for the field of health and human services will partner with the University of Pittsburgh, Office of Child Development (OCD) to develop a Web-based, interactive software application of a Family Support Management Information System (FS MIS) for nationwide dissemination. This innovation is called Linking Information, Families and Technology (LIFT).  Family support centers, like other human service programs and agencies across the country, are being required to implement best practices and document the impact of their services to funders, policy makers, and the community. However, most family centers do not have a state-of-the-art web-based information system available to them that integrates best practice, expert knowledge, and daily management functions. The proposed LIFT system will address these critical needs and has great potential for nationwide commercial distribution. The combination of KIT'S proven record of developing knowledge based information technology and OCD's over 20 years of research and practice in family support services will greatly enhances the chance of success for this business venture. In Phase I of the project, we will produce prototype software demonstrating the benefit, usability, and feasibility of a web-based, interactive, intelligent system for use by family support centers across the nation. The extent of which LIFT enables family center staff to build skill, capacity, access information and expert knowledge, to enhance their work will be the focus of this phase. In Phase II, we will fully develop the prototype LIFT to a commercial grade web application for nationwide dissemination and further validate the commercial potential and impact of LIFT, using a quasiexperimental design, which will involve a large number of users across multiple sites. In Phase III, we will seek private funding for marketing the system to the national market. We intend to use the Microsoft.Net Platform and follow XML web service concepts to develop the proposed innovation. Collection of a subscription fee will be used to support the maintenance and future development of the system. n/a","Linking Information, Families and Technology (LIFT)",6990440,R43HD049229,"['Internet', 'artificial intelligence', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'family', 'focus groups', 'human subject', 'information dissemination', 'social service']",NICHD,"KIT SOLUTIONS, INC.",R43,2005,104545,0.03370249746425042
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,0.05212813230488494
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS    DESCRIPTION (provided by applicant): The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methods within the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data, earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user interface to support access to information resources for enhanced decision-making by practitioners. The long-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3 Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technology work; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C).             n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7084856,P01CD000261,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2005,1270432,0.03955156477857092
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6865478,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2005,162750,0.03727891910186499
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6822280,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2004,577307,0.06742123287489532
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,0.051245354930860285
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,337653,0.11677412443069725
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,0.05212813230488494
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6777517,R43EB000608,"['archives', 'clinical research', 'computer data analysis', 'computer system design /evaluation', 'confidentiality', 'data management', 'health care policy', 'health related legal', 'human data', 'imaging /visualization /scanning', 'information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2004,149200,0.01620139864679953
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6709988,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2004,161668,0.03727891910186499
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6637557,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2003,148818,0.04693203380596569
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6744998,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2003,191306,0.07926138650198387
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6636465,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,327818,0.11677412443069725
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6738628,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,159000,0.11677412443069725
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6694270,R43EB000608,"['archives', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data management', ' health care policy', ' health related legal', ' human data', ' imaging /visualization /scanning', ' information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2003,250800,0.01620139864679953
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6530779,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2002,144484,0.04693203380596569
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6526728,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2002,372289,0.07688499148966262
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6520265,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,318270,0.11677412443069725
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6649647,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,232118,0.11677412443069725
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6258188,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2001,140274,0.04693203380596569
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6388359,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2001,362594,0.07688499148966262
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6363593,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2001,306158,0.07926138650198387
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6495900,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,35096,0.11677412443069725
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6387173,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,309000,0.11677412443069725
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6031661,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2000,354198,0.07688499148966262
ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS No abstract available n/a,ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS,6361843,01LM003528,"['anatomy', ' artificial intelligence', ' vocabulary development for information system']",NLM,UNIVERSITY OF WASHINGTON,N01,2000,228331,0.043610815897799506
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,0.07901369960416772
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,0.07901369960416772
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6165092,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2000,297119,0.07926138650198387
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6132622,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,300000,0.11677412443069725
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6323962,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,186611,0.11677412443069725
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6344145,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,141192,0.11677412443069725
"Identifying False HPV-Vaccine Information and Modeling Its Impact on Risk Perceptions PROJECT SUMMARY/ABSTRACT Human papillomavirus (HPV) is the most common sexually transmitted infection in the United States, with over 30,000 new HPV-related-cancers are diagnosed annually. Although HPV vaccines have been approved by the Food and Drug Administration (FDA) since 2006 and recommended for routine vaccination for school-age girls and boys, vaccination rates remain low. One reason that has contributed to low vaccination rates is incorrect “risk perceptions” around HPV vaccines such as the high perceived risks of adverse events or side effects from the HPV vaccine. Incorrect risk perceptions are often rooted in the false information about HPV vaccines that people are exposed to in their daily life, including social media. The impact of social media on health information is substantial. Negative social-media HPV-vaccine information has been found to have an association with low vaccination coverage. Given the negative consequences of false information, there is a need to develop a robust and scalable way to detect false HPV-vaccine information before it propagates and negatively impacts behavior. The overarching goal of the proposed research is to build a model to identify false HPV-vaccine information on Twitter, demonstrate its impact on individual risk perceptions and measure its underlying mechanisms on risk perception formation. We propose a novel approach to leverage machine learning, natural language processing, network analysis, crowdsourcing/expert data annotation, psycholinguistic analysis and statistical modeling to investigate the false HPV-vaccine information collectively (in terms of its detection and propagation patterns) and individually (in terms of its impact and underlying cognitive mechanisms). Our study will first build a computational model to detect false HPV-vaccine information on Twitter. By modeling the domain-specific HPV- vaccine related text content, information-veracity related linguistic features, individual and collective user behaviors, and dissemination patterns, our model will be able to detect false HPV-vaccine information before it gets verified and spreads widely. We will then investigate the impact of false HPV-vaccine information on risk perceptions around HPV vaccination operationalized by natural language processing methods and a developed HPV-vaccine Risk Lexicon. We will further conduct psycholinguistic analysis on the false HPV-vaccine information and use statistical modeling to uncover the underlying mechanism of risk perceptions. Our study will make a critical and timely contribution to identifying the false HPV-vaccine information and its impact, which has the potential to be applied to other health topics. This proposed project will also address the National Cancer Institute priorities in promoting HPV vaccines and combating misinformation in cancer prevention and control. PROJECT NARRATIVE The uptake of human papillomavirus (HPV) vaccine remains low in part because of incorrect perceptions of vaccination risks, which has been linked to the spread of false HPV-vaccine information. The proposed study seeks to build a computational model to detect false HPV-vaccine information on social media (Twitter) and determine its impact on risk perceptions of the HPV vaccine. The findings will provide important contributions to understand the impact of false health information on HPV vaccination behavior and could be expanded to other health topics.",Identifying False HPV-Vaccine Information and Modeling Its Impact on Risk Perceptions,9954963,R21CA237483,"['Address', 'Affect', 'Age', 'Anxiety', 'Attitude', 'Behavior', 'Cancer Control', 'Categories', 'Cognitive', 'Communication', 'Comprehension', 'Computer Models', 'Data', 'Decision Making', 'Detection', 'Diagnosis', 'Electronic cigarette', 'Event', 'Exposure to', 'Fright', 'Goals', 'Harm Reduction', 'Health', 'Human Papilloma Virus Vaccination', 'Human Papilloma Virus Vaccine', 'Human Papilloma Virus-Related Malignant Neoplasm', 'Human Papillomavirus', 'Human papilloma virus infection', 'Individual', 'Information Dissemination', 'Knowledge', 'Lesion', 'Life', 'Linguistics', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Misinformation', 'Modeling', 'National Cancer Institute', 'Natural Language Processing', 'Neural Network Simulation', 'Participant', 'Pathway Analysis', 'Patients', 'Pattern', 'Perception', 'Plant Roots', 'Politics', 'Property', 'Psycholinguistics', 'Psychological reinforcement', 'Research', 'Risk', 'Safety', 'School-Age Population', 'Semantics', 'Sexually Transmitted Diseases', 'Source', 'Statistical Models', 'Text', 'Time', 'Twitter', 'United States', 'United States Food and Drug Administration', 'Vaccination', 'Vaccines', 'Work', 'adverse event risk', 'boys', 'cancer diagnosis', 'cancer prevention', 'combat', 'crowdsourcing', 'deep learning', 'girls', 'high risk', 'information model', 'information processing', 'multilevel analysis', 'news', 'novel strategies', 'premalignant', 'prevent', 'recurrent neural network', 'response', 'risk perception', 'side effect', 'social media', 'theories', 'uptake']",NCI,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R21,2020,211659,-0.0021639143726726803
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10057158,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'structured data', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2020,795873,0.026226749114043667
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9955351,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical data science', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'large scale data', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,517554,0.06464282653907949
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10237828,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Information Retrieval', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2020,500000,0.0049764941359981925
