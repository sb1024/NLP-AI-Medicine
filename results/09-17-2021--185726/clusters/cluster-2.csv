text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space Project Summary  An extensible analysis platform will be developed to accurately perform the automated genotyping of PCR/capillary electrophoresis (CE) traces for multiple disease-associated short tandem repeater (STR) assays. This study will evaluate the feasibility of developing generalizable and adaptive molecular analysis models, and will ultimately establish a new paradigm for deep learning analytical tools in the molecular diagnostic space.  Advanced machine learning strategies will be applied to interpret genotypes of inherited disorders caused by genetically unstable STR DNA sequences. STRs have traditionally been difficult to investigate due to their length (on the order of kilobases) and low sequence complexity, which elude detection by traditional and next- generation sequencing technologies. However, advances in PCR/CE technology have enabled the amplification and fragment sizing of STR DNA fragments, advancing clinical research and diagnostic test development for several neurodegenerative disorders, such as fragile X syndrome and amyotrophic lateral sclerosis. Despite these advances, the analysis of PCR/CE data from assays targeting STRs remains a manual, burdensome, and subjective process. There is a clear need to create a system that can scale with the development of new assays, and the proposed approach utilizes modern breakthroughs in artificial intelligence to fulfill that need.  This method will leverage recent advances in representation learning to establish a generalized and adaptive framework for automated PCR/CE annotation that can scale to new assays and improve automatically with the inclusion of new data. The project will benefit from Asuragen’s experience in optimizing repeat-primed chemistries to develop and commercialize multiple high performance assays including the AmplideX PCR/CE FMR1 kit. Importantly, the proposed modeling strategy will borrow-strength across multiple established PCR/CE assays and generalize to future PCR/CE assays for novel STR disease associated biomarkers. This system will be paramount to enabling a continuous learning platform wherein computationally-assisted annotation of PCR/CE assays can be continuously improved and integrated in to clinical research tools and diagnostics. Project Narrative  We are developing AmplideX DeepNet, an artificial intelligence-based analysis system that can accurately perform computationally-assisted analysis of molecular diagnostic assays. The proposed system will build upon recent breakthroughs in artificial intelligence to allow it to easily adapt to new assays and to continue to improve. The system will be applied to assays for several disorders, including fragile X syndrome, amyotrophic lateral sclerosis (ALS), myotonic dystrophy, and Huntington’s disease, and will provide a number of benefits over current analysis methods by reducing turn-around time for assay results and assuring reproducible reporting between operators and labs.","AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space",9678895,R43GM128498,"['Alleles', 'American', 'Amyotrophic Lateral Sclerosis', 'Artificial Intelligence', 'Automated Annotation', 'Biological Assay', 'Biological Markers', 'C9ORF72', 'Capillary Electrophoresis', 'Chemistry', 'Clinical', 'Clinical Research', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'FMR1', 'FMR1 repeat', 'Fragile X Syndrome', 'Future', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Guidelines', 'Hand', 'Hereditary Disease', 'Heritability', 'Huntington Disease', 'Interruption', 'Learning', 'Length', 'Machine Learning', 'Manuals', 'Medical Genetics', 'Methods', 'Modeling', 'Modernization', 'Molecular Analysis', 'Myotonic Dystrophy', 'Neurodegenerative Disorders', 'Nucleotides', 'Pathogenicity', 'Performance', 'Phase', 'Process', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Running', 'Sampling', 'Short Tandem Repeat', 'System', 'Systems Analysis', 'Technology', 'Testing', 'Time', 'Training', 'analysis pipeline', 'analytical tool', 'automated analysis', 'base', 'clinical diagnostics', 'cohort', 'computer framework', 'deep learning', 'design', 'diagnostic assay', 'experience', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'heuristics', 'human-in-the-loop', 'improved', 'instrumentation', 'learning progression', 'learning strategy', 'medical schools', 'molecular diagnostics', 'nervous system disorder', 'next generation sequencing', 'novel', 'research and development', 'success', 'tool']",NIGMS,"ASURAGEN, INC.",R43,2019,269217,-0.004412857702446458
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9664620,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2019,573396,0.002923785511543789
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9704042,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoke', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'imaging biomarker', 'improved', 'learning strategy', 'machine learning algorithm', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2019,189000,-0.05127002175938055
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9756832,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Trauma', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'deep learning', 'design', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neurophysiology', 'novel', 'predictive modeling', 'prevent', 'random forest', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,250346,0.0037814030761402235
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9731367,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'machine learning algorithm', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2019,410000,-0.059292188103816375
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9753295,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Machine Learning', 'Marines', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'contig', 'dark matter', 'deep learning', 'deep learning algorithm', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,279949,-0.0022233768356027055
"Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications Abstract Atrial fibrillation (AF) is a pervasive disease which affects over 30 million individuals worldwide, in whom it is associated with morbidity and mortality, yet for which therapeutic outcomes are suboptimal. One major limitation to mechanistic and clinical advances in AF is its taxonomy, which is based on number of days of detected AF rather than increasingly reported functional and personalized mechanisms. I reasoned that a digital and scalable AF taxonomy, based on interactions of anatomic and functional factors and clinical features, may better guide existing therapy and catalyze future mechanistic and therapeutic advances. I set out to create a predictive tool to guide therapy in AF patients using machine learning of rich mechanistic data from a large multicenter registry of patients undergoing ablation. I hypothesized that clinically actionable AF phenotypes can be defined by statistical clustering between electrophysiologic features, anatomic regions and clinical indices, that can be uncovered by physiological and statistical quantification and machine learning. I have two Specific Aims: 1) To construct a multimodal digital atlas of atrial fibrillation which registers functional indices at absolute and relative spatial locations in both atria from a multicenter registry, and make this atlas available as an open-source software resource. This deliverable will uniquely map the probability that specific mechanisms will be relevant to AF in a specific patient of given clinical characteristics. Novel pathophysiological phenotypes will be defined via probabilistic interactions in these individual components. 2) To develop a predictive tool using machine learning to estimate the likelihood that ablation at any site(s) will contribute to success tailored to individual characteristics, by learning clusters of electrophysiologic features, clinical indices, and anatomic regions in a training population and applying it to a validation cohort from a large multicenter registry. This project uses state-of-the-art computational tools and statistical methods that may reconcile divergent AF mechanistic hypotheses to define novel functional AF phenotypes and guide therapy. In the process, I will be mentored by world leading mentors, in an extraordinary training environment to facilitate this development into an independent physician-scientist in bioengineering-heart rhythm medicine. Project Narrative This research provides an avenue to define atrial fibrillation in an actionable classification rooted in pathophysiologic and mechanistic observations. Such a classification scheme would further our understanding and refine our conversation about complex arrhythmia in cardiac tissue. Only an understanding at this level is will provide truly effective and safe treatments of each individual patient’s arrhythmic condition.","Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications",9772892,F32HL144101,"['3-Dimensional', 'Ablation', 'Affect', 'Anatomy', 'Anti-Arrhythmia Agents', 'Applications Grants', 'Arrhythmia', 'Atlases', 'Atrial Fibrillation', 'Biomedical Engineering', 'Cardiac', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Communities', 'Comorbidity', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Electrophysiology (science)', 'Enrollment', 'Environment', 'Faculty', 'Foundations', 'Freedom', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Heart Atrium', 'Individual', 'Injury', 'Language', 'Learning', 'Location', 'Machine Learning', 'Maps', 'Measurable', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Mission', 'Morbidity - disease rate', 'Obstructive Sleep Apnea', 'Patients', 'Pharmacotherapy', 'Phenotype', 'Physicians', 'Physiological', 'Plant Roots', 'Population', 'Probability', 'Procedures', 'Process', 'Pulmonary veins', 'Randomized Clinical Trials', 'Registries', 'Reporting', 'Research', 'Resources', 'Scientist', 'Site', 'Statistical Methods', 'Structure', 'Taxonomy', 'Testing', 'Therapeutic', 'Therapy trial', 'Tissues', 'Training', 'Translations', 'United States National Institutes of Health', 'Validation', 'base', 'clinically actionable', 'cohort', 'computer science', 'computerized tools', 'convolutional neural network', 'deep learning', 'digital', 'disease classification', 'health care service utilization', 'heart rhythm', 'improved outcome', 'indexing', 'individual patient', 'mortality', 'multimodality', 'novel', 'open source', 'patient registry', 'patient response', 'patient stratification', 'predictive tools', 'success', 'supervised learning', 'therapy outcome', 'tool', 'trial design']",NHLBI,STANFORD UNIVERSITY,F32,2019,66778,-0.0004350312120058127
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,-0.0075210835275757466
"A Clinical Trial Enrichment Tool Based on Subgroups Defined by Machine Learning Predictive Models ABSTRACT Neurodegenerative disorders, including amyotrophic lateral sclerosis (ALS), Friedreich's ataxia (FA), multiple sclerosis (MS), Duchenne muscular dystrophy (DMD), Alzheimer’s disease (AD), Parkinson’s disease (PD), and Huntington’s disease (HD) are characterized by heterogeneous disease progression. Efforts to identify responder subgroups may uncover subgroups that are more homogeneous in disease-related features than the full study population. As a result, a subgroup may exhibit a statistically significant effect size. However, current methodologies for subgroup analysis are limited by the relatively small number of prognostic and predictive indicators that can be used to describe subgroups. These methods are not well suited to describing subgroups with reduced heterogeneity in disease progression, or in identifying indicators for multifactorial diseases. We have developed and submitted a patent application for a novel subgroup analysis method based on grouping participants with similar predicted disease progression profiles and analyzing nearest neighbor subgroups within a clinical trial. We call this method Detectable Effect Cluster(DEC) analysis. In our phase 1 and phase 2 SBIR grants, we used ALS as a model disease to develop our API product that uses machine learning disease models to improve trial arm randomization and provide covariates for statistical analysis. In the ongoing phase 2 grant we are expanding our disease offerings to include AD. PD and HD. Building on a set of ALS disease progression models that we have previously developed and validated, we seek in this grant application to develop a novel prototype machine-learning based subgroup analysis application that we plan on adding to our product offerings. During this proposed phase 1 grant, we will address research-level questions regarding the nature of the subgroups defined using DEC analysis including how to define confidence intervals of our DEC-clusters, and estimated bounds for using prediction-thresholds as selection criteria for a confirmatory clinical trial. Finally, we will apply DEC analysis to three publicly-available clinical trial data sets in an attempt to identify subgroups with significant treatment effects. Aim 1: We will apply methods used in image analysis for identifying statistically significant subgroups to address the  multiplicity issue inherent in DEC Analysis. Aim 2: We will use statistical methods to model the confidence intervals of a power analysis in which DEC cluster  based selection criteria would be used for a confirmatory trial. Aim 3: We will isolate records from PRO-ACT that include whether a patient was treated with riluzole and two other  publicly available recent ALS datasets to test the application of DEC Analysis. Origent’s current suite of products will answer drug development needs of a full portfolio of neurodegenerative diseases. Ultimately, we see a series of machine learning applications aimed at solving drug development issues for multiple disease areas, including orphan diseases. These models and applications will vastly increase the speed and efficiency of drug development, resulting in faster, cheaper, more efficient drug trials that yield numerous new medications to ease human pain and suffering. NARRATIVE This work will develop Detectable Effect Cluster (DEC) analysis, a novel machine-learning based method of subgroup analysis. DEC analysis shows great promise in identifying patient subgroups with statistically significant drug effects within larger, more heterogeneous, failed therapeutic clinical trials. DEC analysis has the potential to rescue a drug that otherwise would have been discarded as a drug that does not provide therapeutic benefit, when, in fact, the opposite is true.",A Clinical Trial Enrichment Tool Based on Subgroups Defined by Machine Learning Predictive Models,9846759,R43MH122925,"['Address', 'Alzheimer&apos', 's Disease', 'Amyotrophic Lateral Sclerosis', 'Applications Grants', 'Area', 'Autoimmune Diseases', 'Cardiology', 'Clinical', 'Clinical Drug Development', 'Clinical Trials', 'Cluster Analysis', 'Communicable Diseases', 'Confidence Intervals', 'Data', 'Data Set', 'Disease', 'Disease Progression', 'Disease model', 'Drug Industry', 'Duchenne muscular dystrophy', 'Exhibits', 'Friedreich Ataxia', 'Grant', 'Grouping', 'Health', 'Heterogeneity', 'Hot Spot', 'Human', 'Huntington Disease', 'Image Analysis', 'Legal patent', 'Letters', 'Lung diseases', 'Machine Learning', 'Maps', 'Metabolic Diseases', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Nature', 'Neurodegenerative Disorders', 'Neurologic', 'Non-linear Models', 'Pain', 'Parkinson Disease', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Plant Roots', 'Prognostic Marker', 'Randomized', 'Rare Diseases', 'Records', 'Research', 'Riluzole', 'Risk', 'Scientist', 'Secondary to', 'Selection Criteria', 'Series', 'Small Business Innovation Research Grant', 'Speed', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Clinical Trial', 'Training', 'Work', 'arm', 'base', 'cohort', 'drug development', 'drug testing', 'experience', 'improved', 'interest', 'nervous system disorder', 'novel', 'off-patent', 'oncology', 'patient subsets', 'power analysis', 'predictive modeling', 'prognostic', 'prototype', 'research and development', 'response', 'study population', 'systems research', 'tool', 'treatment effect']",NIMH,"ORIGENT DATA SCIENCES, INC.",R43,2019,222907,-0.027770599102823736
"Integrating Machine Learning and Genomic Approaches to Understand Cerebral Small Vessel Disease Pathogenesis from White Matter Hyperintensity Patterns PROJECT SUMMARY  As a neurointensivist and neurologist at Washington University School of Medicine in St. Louis (WUSM), my career goal is to develop an independent research program as a computational biologist capable of using advanced bioinformatics and statistical methods to integrate analysis of large-scale neuroimaging and genetic data, with the aim of deepening understanding of the biological mechanisms influencing cerebral small vessel disease (SVD) and identifying new targets for therapeutic development. As a first step towards this goal, I have designed an innovative proposal that combine machine-learning (ML) methods and integrated imaging genetic analyses of large-scale neuroimaging and genetic data to improve characterization of SVD disease mechanisms.  The clinical, imaging, and etiologic heterogeneity of SVD have impeded efforts to uncover the pathophysiology of this common and debilitating neurological disease. White matter hyperintensities (WMH), a major imaging endpoint of SVD, are comprised of multiple SVD pathologic processes. Growing evidence suggests location-specific vulnerability of brain parenchyma to different underlying SVD pathologic processes, in which spatially localized WMH patterns may reflect distinct SVD etiologies. Characterizing WMH spatial pattern variations in SVD will not only provide insights into underlying pathogenesis, such as vascular amyloid deposition, arteriolosclerosis, and other less well defined or as-yet unknown disease mechanisms, but also lead to creation of novel imaging biomarkers of these SVD pathologic processes. This proposal addresses a key inadequacy, as existing WMH pattern definitions are determined empirically and cannot distinguish overlapping SVD etiologies and risk factors. In this proposal, I aim to capture WMH spatial pattern variations that reflect distinct SVD etiologies in an unbiased manner, by applying clustering analysis/ML methods to structural MRI data to create novel etiology-specific SVD imaging phenotypes. Moreover, given that genetics influence the variance of WMH, I will integrate genetic analyses of these WMH patterns to uncover novel mechanisms that influence SVD pathogenesis. My preliminary data demonstrate the feasibility of identifying data-driven WMH spatial pattern variations, which are specific to distinct SVD etiologies, and allow detection of genetic risk variants that may help inform SVD pathologic processes.  My career plan leverages the extensive resources and exceptional environments at WUSM, under the guidance of a multidisciplinary mentorship team with expertise across diverse fields including cerebrovascular physiology, neuroimaging, informatics, genetics, and machine learning (Drs. Jin-Moo Lee, Daniel Marcus, Carlos Cruchaga and Yasheng Chen). In this Career Development Award, I propose to: 1) determine distinct WMH spatial patterns that can discriminate underlying SVD pathology and/or risk factors by applying pattern analysis ML methods to structural MRI data from three unique cohorts (n=2,710) enriched for different SVD pathologies (Aim 1a), and examining if the ML-defined WMH patterns segregate individuals by well-defined SVD risk factors as biologic validation (Aim 1b), and 2) identify genetic variants (Aim 2a) associated with WMH patterns that reflect diverse pathologic processes influencing SVD using genome wide association and gene-based analyses; replicate the top variants (Aim 2b) in an independent population-based cohort (n=21,708); and use advanced bioinformatics tools to uncover new biologic pathways associated with WMH spatial patterns (Aim 2c).  This research proposal and accompanying development plan with focused training in machine learning, neuroimaging, and multivariate methods for integrated imaging genetics analysis, will build on my background in genetics towards a career investigating cerebrovascular disorders using translational bioinformatics. This Award will provide me with the necessary training to evolve into an independent investigator with a computational research program that can integrate large imaging and genetics datasets to derive results that are highly relevant to the prevention and treatment of cerebrovascular disease in my clinical patient population. PROJECT NARRATIVE Cerebral small vessel disease (SVD) is one of the most prevalent neurological conditions in older adults, and a leading cause of stroke and cognitive impairment, for which existing treatment and preventative options have been limited or lack efficacy. This project’s goal is to enhance our knowledge of the complex disease processes underlying SVD using machine learning and integrating individual imaging and genetic data from over 20,000 adults. By understanding these disease processes, we can design novel methods to more effectively treat and prevent stroke and dementia.",Integrating Machine Learning and Genomic Approaches to Understand Cerebral Small Vessel Disease Pathogenesis from White Matter Hyperintensity Patterns,9886424,K23NS110927,"['3-Dimensional', 'Address', 'Adult', 'Affect', 'Alzheimer&apos', 's Disease', 'Amyloid deposition', 'Archives', 'Arteries', 'Award', 'Bioinformatics', 'Biological', 'Biology', 'Blood Vessels', 'Brain', 'Categories', 'Cerebral Amyloid Angiopathy', 'Cerebral hemisphere hemorrhage', 'Cerebral small vessel disease', 'Cerebrovascular Disorders', 'Cerebrovascular Physiology', 'Clinical', 'Cluster Analysis', 'Complex', 'Computational Technique', 'Data', 'Data Set', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Disease', 'Disease Progression', 'Distal', 'Elderly', 'Environment', 'Etiology', 'Failure', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Risk', 'Genetic Variation', 'Genomic approach', 'Genotype', 'Goals', 'Grant', 'Heritability', 'Heterogeneity', 'Hypertension', 'Image', 'Impaired cognition', 'Individual', 'Informatics', 'Intervention', 'Investigation', 'Ischemic Stroke', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Lobar', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mentorship', 'Methods', 'Microvascular Dysfunction', 'Neurologic', 'Neurologist', 'Pathogenesis', 'Pathogenicity', 'Pathologic', 'Pathologic Processes', 'Pathology', 'Pathway interactions', 'Pattern', 'Phenotype', 'Play', 'Population', 'Prevention', 'Process', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Risk Factors', 'Role', 'Statistical Methods', 'Stroke', 'Stroke prevention', 'Structure', 'Testing', 'Training', 'Universities', 'Validation', 'Variant', 'Washington', 'White Matter Hyperintensity', 'arteriole', 'base', 'biobank', 'bioinformatics tool', 'brain parenchyma', 'career', 'cohort', 'design', 'disorder subtype', 'effective therapy', 'genetic analysis', 'genetic association', 'genetic variant', 'genome wide association study', 'genome-wide', 'hypertension control', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'learning strategy', 'medical schools', 'multidisciplinary', 'nervous system disorder', 'neuroimaging', 'new therapeutic target', 'novel', 'patient population', 'population based', 'programs', 'racial and ethnic', 'risk variant', 'serial imaging', 'stroke therapy', 'success', 'therapeutic development', 'tool', 'treatment strategy', 'unsupervised learning']",NINDS,WASHINGTON UNIVERSITY,K23,2019,179997,-0.027538786478815906
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9773141,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2019,360843,-0.017342924907988427
"Bioinformatics for post-traumatic stress Project Summary/Abstract Maladaptive complications following trauma, including post-traumatic stress (PTS), are highly prevalent in both veterans and civilians, and have been difficult to accurately diagnose, manage and treat. Debate regarding diagnostic criteria and the need to represent the full spectrum of inter-connected features contributing to psychopathology has spawned the development of the Research Domain Criteria (RDoC) by the National Institute of Mental Health (NIMH). RDoC is a developing framework to help guide the discovery and validation of new dimensions of mental health disorders and their relationships to underlying biological mechanisms. NIMH now has a rich federated database that currently houses raw data from RDoC-sponsored clinical research, and clinical trial data from the National Database of Clinical Trials (NDCT) with information that may help to unlock the complex and overlapping relationships between symptoms of PTS and the underlying biomarkers to fuel improvements on diagnostic and therapeutic frameworks for trauma recovery. The proposed project will apply bioinformatics and machine learning analytical tools to these large, heterogeneous datasets to identify and validate new research dimensions of trauma-related psychopathology and treatment response trajectories and their predictors. Aim 1 will develop an in silico trauma patient population by integrating data from diverse sources, including cross-sectional and observational longitudinal clinical studies housed within available data repositories for trauma and other related mental health research. Data will include medical history, demographics, diagnostic tests, clinical outcomes, psychological assessments, genomics, imaging, and other relevant study and meta-data. Aim 2 will identify multiple dimensions of PTS diagnostic criteria, using a combination of unsupervised dimension-reduction statistical methods, internal and external cross-validation, and supervised hypothesis testing of predictive models to understand the heterogeneous subtypes of PTS. Aim 3 will deploy unsupervised machine learning methods, such as topological data analysis and hierarchical clustering, to identify unique clusters of patients based on symptomatology to develop clustering methods for precision mapping of PTS patients based on disease severity. Aim 4 will use supervised machine learning techniques for targeted predictive analytics focused on identifying treatment responders from the NDCT, and identification of latent variables that predict treatment response. The results of the proposed research project will greatly enrich the field of computational psychiatry research to identify conserved dimensions associated with the complex relationships of psychopathology and precision treatment planning following exposure to traumatic events. Project Narrative A recent restructuring of diagnostic and research criteria for psychiatric disorders has been implemented to promote greater understanding of the biological mechanisms involved in the development of complex mental health disorders. The proposed project aims to apply bioinformatics and machine learning analytics to large datasets from trauma-exposed patients to identify and validate dimensions of post-traumatic stress (PTS), relevant biological predictors, and precision treatment response trajectories.",Bioinformatics for post-traumatic stress,9760006,R01MH116156,"['Bioinformatics', 'Biological', 'Biological Markers', 'Categories', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Database', 'Complex', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Dimensions', 'Disease', 'Exposure to', 'Genomics', 'Growth', 'Image', 'Laboratories', 'Linear Models', 'Linear Regressions', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Medical History', 'Mental Health', 'Mental disorders', 'Metadata', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nervous System Trauma', 'Neurocognitive', 'Observational Study', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Principal Component Analysis', 'Psychiatry', 'Psychopathology', 'Recovery', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Research Project Grants', 'Severity of illness', 'Source', 'Statistical Methods', 'Supervision', 'Symptoms', 'Syndrome', 'Techniques', 'Testing', 'Therapeutic', 'Trauma', 'Trauma Research', 'Trauma patient', 'Trauma recovery', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Work', 'accurate diagnosis', 'analytical tool', 'base', 'biobehavior', 'combat', 'computational platform', 'data archive', 'data mining', 'data sharing', 'data warehouse', 'demographics', 'federated computing', 'guided inquiry', 'hands-on learning', 'indexing', 'innovation', 'insight', 'interest', 'learning strategy', 'multimodality', 'patient population', 'patient subsets', 'post-traumatic stress', 'precision medicine', 'predictive modeling', 'predictive test', 'psychologic', 'research and development', 'research study', 'response', 'statistics', 'stress related disorder', 'supervised learning', 'symptomatology', 'tool', 'trauma exposure', 'traumatic event', 'treatment planning', 'treatment responders', 'treatment response', 'unsupervised learning', 'vector']",NIMH,UNIVERSITY OF MINNESOTA,R01,2019,501996,0.007822155860175834
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9774249,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2019,332952,-0.007884935431126188
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9729808,K08HL136928,"['Affect', 'Bioinformatics', 'Biological', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Diseases', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Multiomic Data', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'learning strategy', 'miRNA expression profiling', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2019,172800,-0.0034766174080266355
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9742524,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'learning strategy', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'patient subsets', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'supervised learning', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,659674,0.001391543287281894
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9741121,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'side effect', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,281932,0.012572489144159316
"Centralized assay datasets for modelling support of small drug discovery organizations Project Summary We have recently used computational models to identify the protease inhibitor indinavir (used as an antiviral for HIV) as a positive allosteric modulator at the α7-nicotinic acetylcholine receptor. We now propose to build on this discovery as well as other publications describing compounds that are positive allosteric modulators of the α7- nicotinic acetylcholine receptor. We propose identifying further compounds that possess activity against this or one of 9 other targets implicated in Alzheimer’s disease using a combination of Bayesian machine learning and in vitro assays. Generating such data will enable us to potentially provide more specific compounds as well as building datasets that can be used to build predictive models to identify additional compounds with activity at these Alzheimer’s targets. These combined efforts should in the first instance provide commercially viable treatments which will be used to experimentally validate our computational models that can be shared with the Alzheimer’s disease scientific community. We are proposing to build and validate models based on public databases, select compounds for testing and use the data generated as a starting point for further optimization. Project Narrative Alzheimer’s disease (AD) is one of the most common neurodegenerative disorders that causes dementia and it is characterized by amyloid deposition of a 39-42 AA peptide (Aβ) processed from the amyloid precursor protein (APP) and neurofibrillary tangles (NFT). Many palliative drugs are available for the disease but there is still an urgent need for curative drugs with greater efficacy. We need to understand the key factors involved in disease progression and their suitability as drug targets for discovering new drugs against Alzheimer's disease. We hence propose to study several of these targets for Alzheimer’s disease involving all the key steps in the pathway, including several old and new targets. We will use our ‘Assay Central’ software to compile structure-activity data for building computational models, that can be used to selected compounds to test activity in vitro as a starting point for further optimization.",Centralized assay datasets for modelling support of small drug discovery organizations,9881110,R44GM122196,"['Alzheimer&apos', 's Disease', 'Amyloid beta-Protein', 'Amyloid beta-Protein Precursor', 'Amyloid deposition', 'Antiviral Agents', 'Bayesian learning', 'Biological Assay', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Databases', 'Dementia', 'Disease', 'Disease Progression', 'Drug Targeting', 'HIV', 'Indinavir', 'Modeling', 'Neurodegenerative Disorders', 'Neurofibrillary Tangles', 'Pathway interactions', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protease Inhibitor', 'Publications', 'Structure', 'Testing', 'alpha-bungarotoxin receptor', 'base', 'drug discovery', 'in vitro Assay', 'in vitro activity', 'novel therapeutics', 'palliative', 'positive allosteric modulator', 'predictive modeling']",NIGMS,"COLLABORATIONS PHARMACEUTICALS, INC.",R44,2019,188759,-0.08268957397787988
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9634069,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2019,293003,0.017837106152089062
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9739188,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Simulation', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in vivo', 'insight', 'intracranial artery', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'multiple omics', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'single photon emission computed tomography', 'spatiotemporal', 'supervised learning', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2019,506426,0.008390115177288274
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis. PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.",Developing Classification Criteria for the Uveitides,9663961,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'age group', 'age related', 'aging population', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,127308,0.009569577569556075
"The Effects of Insurance Benefit Design Innovation on Patient Health Abstract My research in health economics has focused on how information and targeted consumer cost-sharing influences how patients choose providers and the financial savings of incentivizing patients to choose low-price providers. I have also examined the opposite side of the market, how patient use of information and targeted consumer incentives spurs provider price competition. These topics provided the framework for my research as a PhD student in Health Economics at the University of California, Berkeley and I continue to build on these topics while a policy researcher at the RAND Corporation. A natural next step for my career is to expand this line of research but in a more in-depth manner and using more advanced statistical methods. Performing mentored research in these areas will help me successfully make the transition from directed to independent research. The proposed study will help me to (1) contribute to a deeper understanding of patient health effects of an innovative insurance benefit design that is particularly relevant for the aging population; (2) continue to build capabilities working with large medical claims data sets and develop expertise in innovative statistical methods from different disciplines; (3) gain training in aging-related health-services research; (4) expand my exposure to the aging, health economics, and health services research communities; and (5) develop my abilities as an independent health services researcher and build the foundations to successfully compete for R01-level grants.  In this project, I propose to examine whether reference pricing for colonoscopies and pharmaceuticals decreases adherence to recommended colorectal cancer screening and medication therapies among the near- elderly population. I will also examine the impact of reference pricing on patient health outcomes and the aging process. To do so, I intend to apply novel machine-learning statistical methods that have been recently developed in the computer science and statistics fields. As part of this proposal, I have built a formal training plan to develop expertise in these methods. This project will provide me with the flexibility and support to develop a long-term research agenda that focuses on using innovative statistical methods to evaluate the comprehensive effects of consumer cost- sharing programs. Although this study focuses on a single cost- sharing program, reference pricing, the skills I gain through this award will allow me to independently lead evaluations of future benefit designs. The application of machine-learning methods to the setting of reference pricing will provide a framework that I or other researchers can use to evaluate other insurance benefit designs or alternative patient populations. ! Narrative An increasingly popular insurance benefit design, reference pricing, provides targeted financial incentives for consumers to receive care at low-cost providers. While the financial savings from reference pricing programs are well-known, the health impacts have yet to be studied. The proposed career grant will apply machine learning techniques to develop a long-term research agenda focused on understanding the patient health effects of reference pricing for colonoscopies and medication therapies, which are services that are especially relevant for the aging population. !",The Effects of Insurance Benefit Design Innovation on Patient Health,9646811,K01AG061274,"['Accident and Emergency department', 'Adherence', 'Admission activity', 'Adult', 'Advisory Committees', 'Age', 'Aging', 'Area', 'Award', 'Behavior', 'Big Data', 'California', 'Caring', 'Chronic', 'Chronic Disease', 'Colonoscopy', 'Communities', 'Comorbidity', 'Cost Sharing', 'Data Set', 'Deductibles', 'Development', 'Diabetes Mellitus', 'Discipline', 'Elderly', 'Employee', 'Evaluation', 'Exposure to', 'Foundations', 'Future', 'Grant', 'Health', 'Health Benefit', 'Health Care Costs', 'Health Services', 'Health Services Research', 'Healthcare', 'Heart Diseases', 'Heart Rate', 'Heterogeneity', 'Hospitals', 'Incentives', 'Individual', 'Inpatients', 'Insurance', 'Insurance Benefits', 'Insurance Carriers', 'Internal Medicine', 'Journals', 'Lead', 'Link', 'Machine Learning', 'Medical', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'New England', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Policies', 'Population', 'Preventive service', 'Price', 'Process', 'Provider', 'Publications', 'Publishing', 'Quality of life', 'Research', 'Research Methodology', 'Research Personnel', 'Retirement', 'Savings', 'Screening for cancer', 'Services', 'Side', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Training', 'Universities', 'Work', 'aging population', 'asthmatic patient', 'career', 'colorectal cancer screening', 'compliance behavior', 'computer science', 'cost', 'design', 'doctoral student', 'financial incentive', 'flexibility', 'health data', 'health economics', 'health plan', 'improved', 'innovation', 'learning strategy', 'mortality', 'novel', 'patient population', 'programs', 'response', 'semiparametric', 'skills', 'statistics', 'treatment effect']",NIA,RAND CORPORATION,K01,2019,130618,-0.0019945526997384457
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9658873,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,317858,0.022087369998143157
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9694279,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer’s disease biomarker', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multidimensional data', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2019,341691,-0.02994355284113031
"Next Generation Testing Strategies for Assessment of Genotoxicity Project Summary  It is well recognized that current batteries of genetic toxicology assays exhibit two critical deficiencies. First, the throughput capacity of in vitro mammalian cell genotoxicity tests is low, and does not meet current needs. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no consideration for potency, and virtually no information is provided about molecular targets and mechanisms. These deficiencies in hazard characterization prevent genotoxicity data from optimally contributing to modern risk assessments, where this information is essential. We will address these major problems with current in vitro mammalian cell genetic toxicity assays by developing methods and associated commercial assay kits that dramatically enhance throughput capacity, and delineate genotoxicants' primary molecular targets, while simultaneously providing information about potency. Once biomarkers and a family of multiplexed assays have been developed for these purposes, an interlaboratory trial will be performed with prototype assay kits to assess the transferability of the methods. Project Narrative  DNA damage that cannot be faithfully repaired results in gene mutation and/or chromosomal aberrations, and these effects are known to contribute to cancer and other severe diseases. Thus, there is an important need for sensitive assays to evaluate chemicals for genotoxic and other deleterious effects. The work proposed herein will address issues that have plagued genotoxicity assessments for the last several decades: low throughput, lack of potency metrics, and little to no information about molecular targets. We will address these major problems with current genetic toxicity assays by developing new methods and associated commercial assay kits.",Next Generation Testing Strategies for Assessment of Genotoxicity,9807074,R44ES029014,"['Address', 'Affect', 'Aneugens', 'Antioxidants', 'Appearance', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Biological Response Modifiers', 'Bleomycin', 'Caspase', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Chemicals', 'Chromosome abnormality', 'Chromosomes', 'Classification', 'Cleaved cell', 'Colcemid', 'Companions', 'Complex', 'Computer Simulation', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'DNA-PKcs', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Dose', 'Epitopes', 'Etoposide', 'Exhibits', 'Family', 'GADD45A gene', 'Gamma-H2AX', 'Gene Mutation', 'Genetic', 'Goals', 'Harvest', 'Histone H3', 'Human', 'In Vitro', 'Intercalating Agents', 'Investigation', 'Kinetics', 'Label', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Modification', 'Molecular Target', 'Mutagenicity Tests', 'NF-kappa B', 'Nuclear', 'Pathway interactions', 'Phase', 'Physiologic pulse', 'Procedures', 'Protocols documentation', 'Reagent', 'Reference Values', 'Risk Assessment', 'Schedule', 'Scheme', 'Series', 'Stains', 'TP53 gene', 'Testing', 'Time', 'Toxic effect', 'Toxicogenetics', 'Training', 'Validation', 'Work', 'aurora kinase', 'base', 'clastogen', 'computerized tools', 'design', 'experimental study', 'genotoxicity', 'hazard', 'inhibitor/antagonist', 'next generation', 'prediction algorithm', 'prevent', 'prototype', 'random forest', 'repaired', 'response', 'targeted agent', 'tool', 'treatment optimization', 'virtual']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2019,482291,-0.011311930286514673
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9789907,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Infrastructure', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2019,366616,0.012015233652750595
"An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis Project Summary  Improved methods for the bedside diagnosis and evaluation of neuromuscular disorders are needed. One technology that is finding increasing use for this purpose is electrical impedance myography (EIM). In EIM, a very weak, high frequency electrical current is passed through a muscle of interest and the resulting surface voltages are measured. Disease associated alterations in the composition and microstructural features of the muscle produce characteristic changes that can be used to help classify specific conditions and grade disease severity. To date, most studies using EIM analysis have utilized a fairly limited data set for disease assessment. While effective, this approach ignores a great deal of information locked within the impedance data, including those values that can assist in predicting specific muscle features (such as myofiber diameter) and the presence of pathological change (e.g., fat or connective tissue deposition). In addition, as it stands, the data set is challenging for the clinician to understand without a detailed knowledge of impedance theory. Myolex, Inc is a small business concern located in Boston, MA has as its main focus the development of EIM technologies for clinical use. Myolex recently completed a Phase 1 SBIR that demonstrated the potential capability of machine learning based classification algorithms to effectively discriminate healthy muscle from diseased and to discriminate one disease from another. In this proposed work, we will greatly advance this concept by embodying classification algorithms into a powerful new software suite for Myolex’s current EIM system, the mView. Our underlying hypothesis is that EIM data analysis can be automated to the point that classification systems can provide data on disease diagnosis as well as disease severity for improved ease-of-use. We propose to study this hypothesis via 2 specific aims. In Specific Aim 1, we will design a software suite capable of assisting with artifact-free data collection to be incorporated into our current EIM system, the mViewTM. Then using classification paradigms based on a prodigious amount of previous collected data, we will develop an automated data analysis tool to help provide data on disease category as well as microscopic features, muscle based on the impedance data alone using Microsoft’s Azure Cloud platform. In Specific Aim 2, we will test this developed software suite in a total of180 adult and pediatric neuromuscular disease patients and healthy participants evaluated at Ohio State University Wexner Medical Center (adults) and Boston Children’s Hospital (children). During this data collection period, the Ohio State and Boston Children’s researchers will have real- time access to Myolex staff to provide feedback and have questions/problems answered and addressed. The user interface will continue to be refined and classification algorithms improved. At the conclusion of this work, a new diagnostic tool will be developed for potential 510(k) FDA approval. It will serve as the basis for a continuously self-refining system as additional data sets are collected by end-users employing them in regular clinical use. Project Narrative  Electrical impedance myography (EIM) is a valuable technique to assist with the evaluation of a variety of conditions affecting nerve and muscle. However, to date, only simplistic EIM outcomes have been utilized to assess muscle condition. In this proposed work, we will develop a software platform using machine learning to be incorporated into current EIM technology to allow for automated diseased classification and characterization using the entire large EIM data set collected with each muscle measurement. This will serve as the basis for a new, powerful and convenient tool for neuromuscular diagnosis that will continue to advance over time.",An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis,9846955,R44NS113756,"['Address', 'Adult', 'Affect', 'Algorithmic Software', 'Amyotrophic Lateral Sclerosis', 'Area', 'Back Pain', 'Boston', 'Businesses', 'Caliber', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Duchenne muscular dystrophy', 'Effectiveness', 'Electrodes', 'Ensure', 'Evaluation', 'Fatty acid glycerol esters', 'Feedback', 'Fiber', 'Frequencies', 'Functional disorder', 'Health', 'Inclusion Body Myositis', 'Individual', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Medical Technology', 'Medical center', 'Methods', 'Microscopic', 'Morphologic artifacts', 'Muscle', 'Muscular Dystrophies', 'Myography', 'Myopathy', 'Myositis', 'Nerve', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Ohio', 'Outcome', 'Participant', 'Pathologic', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phase', 'Physicians', 'Play', 'Positioning Attribute', 'Provider', 'Radiculopathy', 'Research Personnel', 'Role', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'Work', 'base', 'classification algorithm', 'cloud based', 'cloud platform', 'commercialization', 'data acquisition', 'design', 'diagnosis evaluation', 'disease classification', 'disease diagnosis', 'electric impedance', 'improved', 'indexing', 'interest', 'machine learning algorithm', 'method development', 'nerve injury', 'neuromuscular', 'novel diagnostics', 'pediatric patients', 'physical therapist', 'prototype', 'sarcopenia', 'software development', 'success', 'theories', 'tool', 'usability', 'user friendly software', 'user-friendly', 'voltage']",NINDS,"MYOLEX, INC.",R44,2019,621318,-0.010246351354311838
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9737676,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Quality', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,804907,-0.007082797919691069
"Evaluation of multiple medication exposures concurrently using a novel algorithm PROJECT SUMMARY The development of large observational health databases (OHD) has expanded the data available for analysis by pharmacoepidemiology research. The efficiency of these studies may be improved by simultaneously studying the association of multiple medications with a disease of interest. Unfortunately, prior research has demonstrated that it is difficult to distinguish true-positive from false-positive results when studying multiple exposures simultaneously, thus limiting the conclusions drawn from these types of studies and representing a major gap in the field. The objective of this proposal, which is the first step in achieving the applicant's long- term goal of improving the diagnosis and treatment of gastrointestinal diseases using insights derived from OHD, is to evaluate and validate medication class enrichment analysis (MCEA), a novel set-based signal-to- noise enrichment algorithm developed by the applicant to analyze multiple exposures from OHD with high sensitivity and specificity. The central hypothesis of this proposal is that MCEA has equal sensitivity and greater specificity compared to logistic regression, the most widely used analytic method for OHD, for identifying true associations between medications and clinical outcomes. The applicant will complete the following two interrelated specific aims to test the hypothesis: Aim 1 – to calculate the sensitivity and specificity of medication class enrichment analysis (MCEA) and logistic regression (LR) for identifying medication associations with Clostridium difficile infection (CDI) and Aim 2 – to calculate the sensitivity and specificity of MCEA and LR for identifying medication associations with gastrointestinal hemorrhage (GIH). The rationale for these aims is that by reproducing known medication-disease associations without false positives, MCEA can be used to identify novel pharmacologic associations with gastrointestinal diseases in future studies. The expected outcome for the proposed research is that it will demonstrate MCEA as a valid method for pharmacoepidemiology research, opening new research opportunities for the study of multi-exposure OHD. These new research opportunities may lead to more rapid identification of potential pharmacologic causes of emerging diseases and discovery of unanticipated beneficial medication effects, allowing such medications to be repurposed for new indications. To attain the expected outcome, the applicant will complete additional coursework that builds on his Master of Science in Clinical Epidemiology to learn computational biology, machine learning, and econometrics techniques. With the support of this grant and his institution, he will also directly apply these techniques to pharmacoepidemiology applications under the close mentorship of a carefully selected team of faculty with extensive experience in gastroenterology, pharmacoepidemiology, medical informatics, and mentoring prior K-award grant recipients. Through these activities, the applicant will develop the skills necessary to obtain NIH R01-level funding and become a leader in developing novel techniques for application to the epidemiologic study of gastrointestinal diseases. PROJECT NARRATIVE Traditionally, research studying medications associated with diseases are limited to analyzing one medication at a time. This novel proposal will validate medication class enrichment analysis, a recently developed algorithm to study multiple medications simultaneously for association with a disease of interest. Validation of this method will allow researchers to use existing medical databases to more rapidly identify potential medication causes of emerging diseases and identify medications with unanticipated beneficial effects, allowing such medications to be repurposed for new indications.",Evaluation of multiple medication exposures concurrently using a novel algorithm,9645854,K08DK119475,"['Address', 'Algorithms', 'Aminoglycosides', 'Antibiotics', 'Anticoagulants', 'Antiplatelet Drugs', 'Big Data to Knowledge', 'Biological', 'Carbapenems', 'Case-Control Studies', 'Cephalosporins', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Computational Biology', 'Computer software', 'Data', 'Databases', 'Development', 'Development Plans', 'Diagnosis', 'Digestive System Disorders', 'Disease', 'Electronic Health Record', 'Epidemiologic Methods', 'Evaluation', 'Faculty', 'Fluoroquinolones', 'Funding', 'Future', 'Gastroenterology', 'Gastrointestinal Diseases', 'Gastrointestinal Hemorrhage', 'Generations', 'Genomics', 'Goals', 'Grant', 'Health', 'Infection', 'Informatics', 'Inpatients', 'Institution', 'K-Series Research Career Programs', 'Lead', 'Learning', 'Logistic Regressions', 'Machine Learning', 'Master of Science', 'Medical', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Noise', 'Non-Steroidal Anti-Inflammatory Agents', 'Outcome', 'Penicillins', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacology', 'Research', 'Research Design', 'Research Personnel', 'Sensitivity and Specificity', 'Signal Transduction', 'Specificity', 'Techniques', 'Testing', 'Time', 'United Kingdom', 'United States National Institutes of Health', 'Validation', 'Veterans', 'analytical method', 'base', 'beta-Lactams', 'career development', 'clinical epidemiology', 'econometrics', 'epidemiology study', 'experience', 'improved', 'inhibitor/antagonist', 'insight', 'interest', 'novel', 'research study', 'simulation', 'skills', 'usability']",NIDDK,UNIVERSITY OF PENNSYLVANIA,K08,2019,169440,0.00029899459731194694
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9700030,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric infection', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2019,26919,0.0001644448701526404
"Systems Level Causal Discovery in Heterogeneous TOPMed Data SYSTEMS LEVEL CAUSAL DISCOVERY IN HETEROGENEOUS TOPMED DATA ABSTRACT The advent of new technologies for collecting and analyzing multiple heterogeneous data streams from the same individual makes possible the detailed phenotypic characterization of diseases and paves the way for the development of individualized precision therapies. A major bottleneck in this process is the lack of robust, efficient and truly integrative analytic methods for such multi-modal data. This proposal builds on the ongoing efforts of our group in the area of causal learning in biomedicine. The objective of this application is to extend, modify and tailor our causal probabilistic graphical models to data typically collected by TOPMed projects, such as –omics data (SNPs, metabolomics, RNA-seq, etc), imaging, patients' history, and clinical data. COPDGene® is one of the TOPMed projects and has generated datasets with those modalities for 10,000 patients with chronic obstructive pulmonary disease (COPD), the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with different characteristics. There is currently no satisfactory method for COPD subtyping or prediction of disease progression. In this project we will apply, test and validate our approaches on COPDGene® and another large independent COPD cohort. The extension and application of our methods to cross-sectional and longitudinal data will also allow us to investigate a number of important questions and aspects related to COPD. Mechanistically, we will investigate how SNPs, genes and their networks are causally linked to disease phenotypes. In pathology, we will identify conditional biomarkers, which will lead to disease sub-classification and identification of causal components in each subtype. In pathophysiology, we will identify features that are directly linked to lung function decline and outcome. We will make all our algorithms and results available to the community through web and public cloud interfaces. The deliverables will be (1) new probabilistic approaches for integration and analysis of multi-modal cross-sectional and longitudinal data, including SNPs, blood biomarkers, CT scans and clinical data; (2) new cloud-based server to make these approaches available to the research community; (3) results on the mechanism, pathology and pathophysiology of COPD facilitation and progression. To guarantee the success of the project we have assembled a team of experts in genomics, machine learning, cloud computing and COPD. This cross- disciplinary team project will have a positive impact beyond the above deliverables, since the generality of our approaches makes them applicable to any disease. We expect that during this U01 we will have the opportunity to collaborate with other teams in the TOPMed consortium to help them investigate the causes of their corresponding disease phenotypes. We do believe that data integration in a single probabilistic framework will be in the heart of precision medicine strategies in the future, when massive high-throughput data collection will become a routine diagnostic and prognostic procedure in all hospitals. PROJECT NARRATIVE Current technologies for high-throughput biomedical data collection allow the interrogation of multiple modalities from a single patient. New promising analytical methods started emerging, which can analyze those multi-modal data in a holistic way. Chronic obstructive pulmonary disease (COPD) constitutes the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with their own characteristics. There is currently no satisfactory method for COPD subtyping. We will apply, test and validate new probabilistic approaches on two cohorts of COPD patients. We will investigate the mechanisms of disease facilitation; we will identify patient cohorts with specific characteristics (disease subtypes); and investigate risk factors and causal variants for the disease progression in each subtype.  ",Systems Level Causal Discovery in Heterogeneous TOPMed Data,9678366,U01HL137159,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Biological Models', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Data', 'Data Collection', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Functional Imaging', 'Functional disorder', 'Funding', 'Future', 'Genes', 'Genetic Determinism', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Health Care Costs', 'Heart', 'Hospitals', 'Image', 'Individual', 'Internet', 'Learning', 'Lifting', 'Link', 'Machine Learning', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Morphology', 'Outcome', 'Outcome Assessment', 'Pathology', 'Patients', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Physiological', 'Precision therapeutics', 'Procedures', 'Process', 'Pulmonology', 'Recording of previous events', 'Research', 'Research Personnel', 'Respiratory physiology', 'Risk', 'Risk Factors', 'Science', 'Stream', 'Syndrome', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Tissues', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Universities', 'Visit', 'X-Ray Computed Tomography', 'analytical method', 'base', 'causal variant', 'clinical imaging', 'clinically relevant', 'cloud based', 'cohort', 'computer science', 'cost effective', 'data integration', 'disability', 'disease phenotype', 'disorder subtype', 'graphical user interface', 'high throughput technology', 'innovation', 'longitudinal dataset', 'medical schools', 'metabolomics', 'mortality', 'multimodal data', 'multimodality', 'new technology', 'novel', 'outcome forecast', 'patient subsets', 'precision genomic medicine', 'precision medicine', 'prognostic', 'repository', 'success', 'tool', 'transcriptome sequencing', 'user-friendly']",NHLBI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2019,602448,0.011405729458092371
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer’s disease, Parkinson’s disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these challenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymorphisms, whole brain neuroimaging, biofluids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mechanisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Specifically, in Aim 1, we develop efficient methods for multi-level semiparametric transformation models to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efficiency. Our methods do not rely on full pedigree genotyping and provide family-specific substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic systems through ordinary differential equations with random inflections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker profiles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efficient machine learning methods to identify predictive markers, estimate optimal individualized therapies, and identify subgroups who may receive the greatest benefit from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art analytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research community. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9694287,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Neurodegenerative Disorders', 'Non-linear Models', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'individualized medicine', 'learning strategy', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,340259,0.01593159219368432
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9817000,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2019,334891,-0.0010415836612067647
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9693291,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2019,397125,-0.019001882930572683
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9688116,R01AR068456,"['3-Dimensional', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2019,275934,0.0025080198162745434
"Statistical Methods for Analyzing Complex, Multi-dimensional Data from Cross-sectional and Longitudinal Mental Health Studies Project Summary  To address the burden of mental illness, National institute of Mental Health encourages development of computational approaches that provide novel ways to understand relationships among complex, large datasets to further the understanding of the underlying pathophysiology of mental diseases. These datasets are multi- dimensional, including clinical assessments, behavioral symptoms, biological measurements such as neu- roimaging and psychophysiological data. The overall objective of this grant is to advance methodology for analyzing such data to more effectively extract relevant information that are predictive of disease, to improve the understanding of individual variability in clinical and neurobiological phenotypes, and to provide the capac- ity to handle both cross-sectional and longitudinal data.  Our proposal will leverage two civilian trauma cohorts recruited through the Grady Trauma Project and the Grady Emergency Department Study, and an external validation cohort from the Hill Center study with a similar distribution of trauma exposure. We propose to develop statistically principled, computationally efﬁ- cient statistical learning methods for addressing key challenges in analyzing these large datasets. Challenges include multi-type outcomes, high dimensional data with sparse signals and high noise levels, spatial and tem- poral dependence of neuroimaging data, and heterogeneous effects across patient population. The scientiﬁc premise of this computational psychiatry research is that analytical methods integrating information from brain, behavior, and symptoms will provide much-needed data driven platforms for improving diagnosis and prediction of PTSD and other mental disorders.  In this application, we propose: (1) to develop partial generalized tensor regression methods and partial tensor quantile regression methods that can simultaneously achieve accurate prediction of clinical outcomes and efﬁcient feature extraction from high dimensional neuroimaging biomarkers; (2) to develop tensor response quantile regression methods and global inference that can achieve comprehensive and robust understanding of the heterogeneity in high-dimensional neuroimaging phenotypes in terms of environmental factors such as trauma exposure; and (3) to develop and extend methods in Aims 1 and 2 for longitudinal multi-dimensional data that will enable prediction of future post-trauma symptom severity trajectories in terms of neuroimaging biomarkers and robustify the evaluation of the impact of psychophysiological factors on neuroimaging phe- notypes. The proposed methods will be applied to the two Grady studies to address scientiﬁc hypotheses relevant to PTSD research. We will use the Hill Center study as an independent validation cohort to evaluate the reproducibility and generalizability of the ﬁndings. User-friendly software will be developed. The proposed methodology is generally applicable to many other mental health studies with complex multi-dimensional data. Narrative We propose to develop statistical methods for analyzing large-scale and multi-dimensional data in mental health studies to more effectively extract relevant information that are predictive of disease and to help understand individual variability in clinical and neurobiological phenotypes. The applications of the proposed methods will generate new knowledge to further the understanding of the mechanism and progression of the PTSD that will lead to improved disease management strategies.","Statistical Methods for Analyzing Complex, Multi-dimensional Data from Cross-sectional and Longitudinal Mental Health Studies",9839143,R01MH118771,"['Accident and Emergency department', 'Address', 'Amygdaloid structure', 'Behavioral Symptoms', 'Biological', 'Biological Markers', 'Child Sexual Abuse', 'Clinical', 'Clinical assessments', 'Complex', 'Data', 'Data Set', 'Demographic Factors', 'Dependence', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Management', 'Environmental Risk Factor', 'Face', 'Fright', 'Functional disorder', 'Future', 'Grant', 'Heterogeneity', 'Image', 'Impact evaluation', 'Individual', 'Knowledge', 'Machine Learning', 'Maps', 'Measurement', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Neurobiology', 'Noise', 'Outcome', 'Pattern', 'Phenotype', 'Post-Traumatic Stress Disorders', 'Procedures', 'Psyche structure', 'Psychiatry', 'Psychophysiology', 'Public Health', 'Reproducibility', 'Research', 'Severities', 'Signal Transduction', 'Statistical Methods', 'Stimulus', 'Strategic Planning', 'Structure', 'Symptoms', 'Trauma', 'United States', 'Validation', 'analytical method', 'base', 'brain behavior', 'burden of illness', 'clinical predictors', 'cohort', 'flexibility', 'high dimensionality', 'high risk', 'improved', 'individual variation', 'insight', 'learning strategy', 'multidimensional data', 'neural circuit', 'neurobiological mechanism', 'neuroimaging', 'neuroimaging marker', 'novel', 'patient population', 'predict clinical outcome', 'recruit', 'relating to nervous system', 'response', 'trauma exposure', 'trauma symptom', 'user friendly software', 'vector']",NIMH,EMORY UNIVERSITY,R01,2019,614061,0.020968009770038636
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9724344,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Streptococcus vaccine', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2019,872121,-0.02201862869038289
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9658524,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,305167,0.018509227725715983
"Statistical Methods for Multilevel Multivariate Functional Studies Abstract  While imaging studies are widely used in clinical practice and research, the number of neuroimaging- based biomarkers is small. For example, in clinical trials of immunomodulatory therapies for MS, the only commonly used imaging biomarkers are the total lesion volume and the number of new and en- hancing lesions. These biomarkers are essential, but do not capture the recovery process of lesions, which is thought to decline in more severe, progressive disease. The partial or complete recovery of lesions may depend both on the ability of the brain to heal and on external factors, such as treat- ment or environmental and behavioral exposures. In this proposal we take the natural next step of proposing imaging biomarkers for MS based on the formation and change of lesions as observed on multi-sequence structural MRIs. To solve this problem we propose to address several general method- ological problems: 1) develop models and methods for the longitudinal analysis of several images of the same brain; 2) identify and estimate the length of history that is necessary to estimate recovery; 3) study the association with known biomarkers of the disease (in this case total volume and number of new and enhancing lesions); 4) develop methods that are robust to changes in imaging protocols that inevitably arise in longitudinal neuroimaging studies; and 5) develop the computational tools that allow for sophisticated methods to be implemented seamlessly in practice. While our scientiﬁc problem is focused, the proposed statistical methods are general and can be applied to a wide variety of longitu- dinal neuroimaging studies. For example, there are many ongoing longitudinal neuroimaging studies, including the ADNI, AIBL, HBC, and MISTIE, where our methods could be used to study subtle or large changes in lesions or in white and gray matter intensities. Project narrative. The project provides statistical analysis methods for quantiﬁcation of the evolution in the intensity of brain lesions on multi-sequence Magnetic Resonance Imaging (MRI). Methods are motivated by the need to develop new neuroimaging-based biomarkers for multiple sclerosis (MS), but can be applied to other types of brain diseases including stroke, Alzheimer disease, and cancer.",Statistical Methods for Multilevel Multivariate Functional Studies,9635802,R01NS060910,"['Accounting', 'Address', 'Alzheimer&apos', 's Disease', 'Behavioral', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical Research', 'Clinical Trials', 'Computer software', 'Data', 'Databases', 'Disease', 'Enhancing Lesion', 'Event', 'Evolution', 'Funding', 'Grant', 'Graph', 'Image', 'Incidence', 'Length', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mediation', 'Mediator of activation protein', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Multiple Sclerosis Lesions', 'Names', 'Natural History', 'Nature', 'Online Systems', 'Pattern', 'Population Heterogeneity', 'Problem Solving', 'Process', 'Progressive Disease', 'Protocols documentation', 'Randomized', 'Recording of previous events', 'Recovery', 'Research', 'Sampling', 'Statistical Data Interpretation', 'Statistical Methods', 'Stroke', 'Structure', 'Supervision', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'biomarker validation', 'clinical practice', 'computerized tools', 'design', 'gray matter', 'healing', 'high dimensionality', 'imaging biomarker', 'imaging study', 'immunomodulatory therapies', 'improved', 'insight', 'longitudinal analysis', 'longitudinal database', 'multidimensional data', 'neuroimaging', 'non-Gaussian model', 'personalized approach', 'repaired', 'serial imaging', 'software development', 'treatment response', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R01,2019,633289,-0.025533456273581518
"Dynamic imaging-genomic models for characterizing and predicting psychosis and mood disorders Project Summary/Abstract  Disorders of mood and psychosis such as schizophrenia, bipolar disorder, and unipolar depression are  incredibly complex, influenced by both genetic and environmental factors, and the clinical characterizations are primarily based on symptoms rather than biological information. Current diagnostic approaches are based on symptoms, which overlap extensively in some cases, and there is growing consensus that we should approach mental illness as a continuum, rather than as a categorical entity. Since both genetic and environmental factors play a large role in mental illness, the combination of brain imaging and genomic data are poised to play an important role is clarifying our understanding of mental illness. However, both imaging and genomic data are high dimensional and include complex relationships that are poorly understood. To characterize the available information, we are in need of approaches that can deal with high-dimensional data exhibiting interactions at multiple levels (i.e., data fusion), while providing interpretable solutions (i.e., a focus on brain and genomic  networks). An additional challenge exists because the available data has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. To address these challenges, we introduce a new unified framework called flexible subspace analysis (FSA) that can automatically identify subspaces (groupings of unimodal or multimodal  components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus  leveraging the power of deep learning. We will apply the developed models to a large (N>60,000) dataset of  individuals along the mood and psychosis spectrum to evaluate the important question of disease categorization. We will compute fully cross-validated genomic-neuro-behavioral profiles of individuals including a comparison of the predictive accuracy of 1) standard categories from the diagnostic and statistical manual of mental disorders (DSM), 2) data-driven subgroups, and 3) dimensional relationships. We will also evaluate the single subject predictive power of these profiles in independent data to maximize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data  promises to advance our understanding of the nosology of mood and psychosis disorders in addition to providing new tools that can be widely applied to other studies of complex disease. Project Narrative  It is clear that mood and psychosis disorders, largely diagnosed without biological criteria, include a multitude of inter-related genetic and environmental factors. We propose to develop new flexible models to capture  multiscale (dynamic) brain imaging and genomics data, which we will use to study individuals along the mood and psychosis spectrum using a large aggregated dataset including a comparison of the predictive accuracy of two dichotomous approaches (standard diagnostic categories and unsupervised/data-driven) as well as a  dimensional approach to diagnosis.",Dynamic imaging-genomic models for characterizing and predicting psychosis and mood disorders,9935464,R01MH118695,"['3-Dimensional', 'Address', 'Algorithms', 'Behavior', 'Behavioral', 'Benchmarking', 'Biological', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Brain region', 'Categories', 'Clinical', 'Communities', 'Complex', 'Consensus', 'Data', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Environmental Risk Factor', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genetic Risk', 'Genomics', 'Goals', 'Grouping', 'Image', 'Individual', 'Joints', 'Lead', 'Link', 'Major Depressive Disorder', 'Maps', 'Mental disorders', 'Methods', 'Modeling', 'Mood Disorders', 'Moods', 'Noise', 'Pathway interactions', 'Patients', 'Pattern', 'Play', 'Property', 'Psychotic Disorders', 'Research Personnel', 'Role', 'Sampling', 'Schizoaffective Disorders', 'Schizophrenia', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Source', 'Structure', 'Subgroup', 'Supervision', 'Symptoms', 'Syndrome', 'Time', 'Unipolar Depression', 'Work', 'base', 'bipolar patients', 'blind', 'connectome', 'data anonymization', 'data warehouse', 'deep learning', 'disease classification', 'flexibility', 'genomic data', 'independent component analysis', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'profiles in patients', 'psychotic symptoms', 'statistics', 'tool', 'user friendly software']",NIMH,GEORGIA STATE UNIVERSITY,R01,2019,823331,-0.015045206189287364
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9785367,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2019,247413,0.02640136259307316
"Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders Project Summary/Abstract  In the field of Alzheimer’s and related disorder, there has been very little work focusing on imaging genomics biomarker approaches, despite considerable promise. In part this is due to the fact that most studies have fo- cused on candidate gene approaches or those that do not capitalize on capturing (and amplifying) small effects spread across many sites. Even for genome wide studies, the vast majority of imaging genomic studies still rely on massive univariate analyses. The use of multivariate approaches provides a powerful tool for analyzing the data in the context of genomic and connectomic networks (i.e. weighted combinations of voxels and genetic variables). It is clear that imaging and genomic data are high dimensional and include complex relationships that are poorly understood. Multivariate data fusion models that have been proposed to date typically suffer from two key limitations: 1) they require the data dimensionality to match (i.e. 4D fMRI data has to be reduced to 1D to match with the 1D genomic data, and 2) models typically assume linear relationships despite evidence of non- linearity in brain imaging and genomic data. New methods are needed that can handle data that has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. Secondly, methods that can handle complex relationships, such as groups of networks that are tightly coupled or nonlinear relationships in the data. To ad- dress these challenges, we introduce a new framework called flexible subspace analysis (FSA) that can auto- matically identify subspaces (groupings of unimodal or multimodal components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus leveraging the power of deep learning. We will apply the developed models to a large longitudinal dataset of individuals at various stages of cognitive impair- ment and dementia. Using follow-up outcomes data we will evaluate the predictive accuracy of a joint analysis compared to a unimodal analysis, as well as its ability to characterize various clinical subtypes including those driven by vascular effects including subcortical ischemic vascular dementia versus those that are more neuro- degenerative. We will evaluate the single subject predictive power of these profiles in independent data to max- imize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data promises to advance our understanding of Alzheimer’s and related disorders in addition to providing new tools that can be widely applied to other studies of complex disease. 3 Project Narrative  It is clear that multimodal data fusion provides benefits over unimodal analysis, however existing approaches typically require the data to have matched dimensionality, leading to a loss of information. In addition, most models assume linear relationships, despite strong evidence of nonlinear relationships in the data. We propose to develop new flexible models to capture multi-scale brain imaging and genomics data which we will use to study a large data set of individuals with Alzheimer’s disease and Alzheimer’s disease related disorders. 2",Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders,9826772,RF1AG063153,"['3-Dimensional', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Behavior', 'Benchmarking', 'Biological', 'Blood Vessels', 'Brain', 'Brain imaging', 'Brain region', 'Candidate Disease Gene', 'Categories', 'Classification', 'Communities', 'Complex', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Dementia', 'Diagnostic', 'Dimensions', 'Disease', 'Evaluation', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Image', 'Impaired cognition', 'Individual', 'Joints', 'Lead', 'Linear Models', 'Link', 'Magnetic Resonance Imaging', 'Meta-Analysis', 'Methods', 'Modality', 'Modeling', 'Motivation', 'Nerve Degeneration', 'Neurobiology', 'Noise', 'Outcome', 'Pathway interactions', 'Pattern', 'Research Personnel', 'Rest', 'Sampling', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Structure', 'Subgroup', 'Time', 'Vascular Dementia', 'Work', 'base', 'blind', 'clinical subtypes', 'connectome', 'data anonymization', 'data warehouse', 'deep learning', 'flexibility', 'follow-up', 'functional genomics', 'genome-wide analysis', 'genomic biomarker', 'genomic data', 'longitudinal dataset', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'patient subsets', 'statistics', 'structural genomics', 'subcortical ischemic vascular disease', 'tool', 'user friendly software', 'white matter damage']",NIA,GEORGIA STATE UNIVERSITY,RF1,2019,3319889,-0.04553567737384503
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9752596,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,323659,-0.003158192652285
"Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents PROJECT ABSTRACT  Antimicrobial resistance is a critical public health issue. Infections with drug resistant pathogens are estimated to cause an additional eight million hospitalization days annually over the hospitalizations that would be seen for infections with susceptible agents. The use of antibiotics (in both clinical and agricultural settings) is being viewed as precursor for these infections and thus, is a major public health concern—particularly as outbreaks become more frequent and severe. However, scientiﬁc evidence describing the hazards associated with antibiotic use is lacking due to inability to quantify the risk of these practices. One promising avenue to elucidate this risk is to use shotgun metagenomics to identify the AMR genes in samples taken through systematic spatiotemporal surveillance. The goal of this proposed work is to develop algorithms that will provide such a means for analysis. The algorithms need to be scalable to very large datasets and thus, will require the development and use succinct data structures.  In order to achieve this goal, the investigative team will develop the theoretical foundations and applied meth- ods needed to study AMR through the use of shotgun metagenomics. A major focus of the proposed work is developing algorithms that can handle very large datasets. To achieve this scalability, we will create novel means to create, compress, reconstruct and update very large de Bruijn graphs that metagenomics data in a manner needed to study AMR. In addition, we will pioneer the study of AMR through long read data by proposing new algorithmic problems and solutions that use data. For example, identifying the location of speciﬁc genes in a metagenomics sample using long read data has not been proposed or studied. Thus, the algorithmic ideas and techniques developed in this project will not only advance the study of AMR, but contribute to the growing domain of big data analysis and pan-genomics.  Lastly, we plan to apply our methods to samples collected from both agricultural and clinical settings in Florida. Analysis of preliminary and new data will allow us to conclude about (1) the public risk associated with antimicro- bial use in agriculture; (2) the effectiveness of interventions used to reduce resistant bacteria, and lastly, (3) the factors that allow resistant bacteria to grow, thrive and evolve. A–1 PROJECT NARRATIVE  Antibiotic use in agriculture is a major public health concern that is receiving a lot of media attention, par- ticularly as antibiotic-resistant infections in become more frequent and severe. This research will build a novel bioinformatics framework for determining how antimicrobial resistant genes evolve, grow, and persist in a system that has been affected by antibiotic use. This will, in turn, facilitate the development of effective intervention methods that reduce resistant pathogens in clinical and agricultural settings. N–1",Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents,9641899,R01AI141810,"['Affect', 'Agriculture', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Attention', 'Bacteria', 'Base Pairing', 'Big Data', 'Bioinformatics', 'Clinical', 'Collaborations', 'Combating Antibiotic Resistant Bacteria', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Compression', 'Data Set', 'Development', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Florida', 'Food production', 'Foundations', 'Genes', 'Genomics', 'Goals', 'Graph', 'Hospitalization', 'Infection', 'International', 'Investigation', 'Length', 'Location', 'Measures', 'Memory', 'Metagenomics', 'Methods', 'Monitor', 'Noise', 'Organism', 'Pathogenicity', 'Plasmids', 'Prevention', 'Public Health', 'Research', 'Resistance', 'Risk', 'Sampling', 'Shotguns', 'Structure', 'Surveillance Methods', 'System', 'Techniques', 'Time', 'Translating', 'Update', 'Work', 'bacterial resistance', 'base', 'combinatorial', 'drug resistant pathogen', 'effective intervention', 'foodborne outbreak', 'genetic variant', 'hazard', 'improved', 'machine learning algorithm', 'method development', 'microbial', 'microbiome analysis', 'microbiome research', 'novel', 'pathogen', 'petabyte', 'reconstruction', 'research and development', 'resistance gene', 'spatiotemporal', 'standard care']",NIAID,UNIVERSITY OF FLORIDA,R01,2019,450459,-0.01059400044206242
"Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study PROJECT SUMMARY ABSTRACT The overarching goals of this R01 proposal are to improve scientific understanding of potential mechanisms by which ophthalmic diseases lead to the risk of Alzheimer’s disease. The investigators will leverage modern ophthalmic data with state-of-the-art imaging and extensive archived clinical data from a well-characterized cohort of older adults. The investigators propose to examine the effect of structural and functional changes in retina and longitudinal severity of ophthalmic diseases on Alzheimer’s disease and related neuropathology. The proposal builds on the resources of the Adult Changes in Thought (ACT) study, a prospective longitudinal, population-based, dementia-free cohort of over 5,500 people to date established in 1994 which has detected >1,014 research quality diagnoses of Alzheimer’s disease and >1,254 dementia to date. ACT follows consenting participants to autopsy and has performed state-of-the arts autopsy on >781 decedents to date. In this extremely well-characterized cohort, the investigators found that several ophthalmic diseases (diabetic retinopathy, glaucoma, age-related macular degeneration) are significantly associated with the risk of developing Alzheimer’s disease. The investigators will use three advanced ophthalmic imaging modalities at both home and clinical research study visits: fundus photography, optical coherence tomography (OCT), and OCT angiography (OCTA), to obtain quantitative data relevant to these ophthalmic diseases. The study team will establish the distribution (Aim 1a) and 2- and 4-year evolution of ophthalmic imaging characteristics found in older adults in the community and determine associations with change in cognition (Aim 1 b, c). Additionally, magnetic resonance imaging (MRI) and MRI angiography (MRA) will be obtained in a subset of participants to investigate the contribution of small (retinal) and large (cerebral) vascular disease towards cognitive changes (Aim 1d). The study team will continue ACT study’s strong commitment for meaningful data sharing. In collaboration with the Laboratory of Neuro Imaging at University of Southern California, the study team will promulgate these ophthalmic data in addition to neuroimaging data to the research community (Aim 1e). In Aim 2, the investigators will use extensive clinical ophthalmology data captured over many decades and incorporate them in novel longitudinal models of eye disease severity. The investigators will analyze eye disease severity along with extensive neuropathology data from the ACT study, including both standard (Aim 2a) and novel quantitative (Aim 2b) neuropathology data, to further scientific understanding of neuropathological mechanisms underlying associations between eye conditions and Alzheimer’s disease risk. The brain is not amenable to direct observations during life. In contrast, the eye is an anterior extension of the central nervous system and may provide a valuable window to illuminate neurodegenerative processes in the aging brain. Proposed investigations will substantially enhance scientific understanding of the role of modern ophthalmic evaluations in delineating risk of Alzheimer's disease and other forms of neuropathology. PROJECT NARRATIVE Using a large, well-characterized, longitudinal, prospective, cohort study, the study team previously found that diabetic retinopathy, glaucoma, and age-related macular degeneration were significantly associated with Alzheimer’s disease risk. The team proposes to use three cutting edge ophthalmic imaging modalities to obtain quantitative data at both home and clinic research study visits in addition to MRI and MRA in a subset of participants to evaluate their associations with change in cognition over time (Aim 1). The team will leverage extensive ophthalmic clinical and neuropathological data already available for 781 study participants to date as well as new state-of-the-art quantitative measures of beta amyloid (A1-42) and phosphorylated tau to elucidate mechanisms underlying associations between ophthalmic conditions and Alzheimer’s disease (Aim 2).",Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study,9816310,R01AG060942,"['Abbreviations', 'Adult', 'Age related macular degeneration', 'Age-associated memory impairment', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Amyloid beta-Protein', 'Angiography', 'Anterior', 'Archives', 'Autopsy', 'Bayesian Modeling', 'Biological Markers', 'Blood Vessels', 'Brain', 'California', 'Cerebrovascular Disorders', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cognition', 'Collaborations', 'Communities', 'Consent', 'Data', 'Data Collection', 'Dementia', 'Diabetic Retinopathy', 'Diagnosis', 'Disease', 'Disease model', 'Drusen', 'Elderly', 'Evaluation', 'Evolution', 'Eye', 'Eye diseases', 'Fundus', 'Fundus photography', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Home environment', 'Image', 'Impaired cognition', 'Investigation', 'Laboratories', 'Lead', 'Life', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Microvascular Dysfunction', 'Modeling', 'Modernization', 'Nerve Degeneration', 'Nerve Fibers', 'Neuraxis', 'Occipital lobe', 'Ophthalmology', 'Optical Coherence Tomography', 'Participant', 'Pathology', 'Perfusion', 'Persons', 'Predisposition', 'Process', 'Prospective cohort study', 'Provider', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Retina', 'Retinal', 'Risk', 'Role', 'Scanning', 'Selection Bias', 'Series', 'Severities', 'Severity of illness', 'Structure', 'Technology', 'Testing', 'Time', 'Universities', 'Vascular Diseases', 'Visit', 'aging brain', 'area striata', 'base', 'cognitive change', 'cohort', 'data sharing', 'deep learning', 'diagnosis quality', 'epidemiology study', 'fiber cell', 'follow-up', 'high risk', 'imaging biomarker', 'imaging modality', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuropathology', 'novel', 'paired helical filament', 'population based', 'prospective', 'repository', 'research study', 'resilience', 'spelling', 'tau Proteins', 'tau-1', 'vascular contributions']",NIA,UNIVERSITY OF WASHINGTON,R01,2019,3869686,-0.07313002067785726
"Machine Learning and Deep Learning Solutions Supplement: Matching Methods for Causal Inference with Complex Data NARRATIVE SUMMARY The landscape of data formats is rapidly expanding, with image, text and other complex formats becoming available for health related outcomes. By considering such data within the context of observational causal inference, they can be leveraged to improve clinical decisions, help evaluate treatment efficacy by estimating individualized treatment effects and help develop intelligent therapeutic systems where individualized treatments can be deployed. In R01EB025021, we concentrate on understanding how nearly exact matching can be achieved in the presence of a large number of categorical covariates. The proposed approach (called FLAME - Fast Large Almost Matching Exactly) is able to quickly learn which categorical covariates are important and to produce high quality matches \citep{wang2017flame,dieng2018collapsing}. The main shortfall in the proposed work for R01EB025021 is that it does not naturally extend to more complex data types, it only works for categorical data in which each feature is meaningful. {\bf This proposal will develop new statistical and computational tools for causal analysis of complex data structures.} Our new approach is called {\emph Matching After Learning to Stretch (MALTS)}. For each unit (e.g. patient), we propose learn a latent representation of their covariate information and a distance metric on the latent space such that units that are matched tend to provide accurate estimates of treatment effect. MALTS can use deep learning to encode the latent representations for the units, or it can learn basis transformations in linear space (stretching and rotation matrices) for simpler continuous data types. We will develop the MALTS algorithm, and apply it in a medical context. Our goal is to construct high quality matches for the following types of data: (i) medical images, such as x-rays and CT scans, (ii) medical record data, (iii) time series data (continuous EEG data), (iv) a combination of any of the first three types of data. We aim to leverage the newly developed tools to continue our evaluation of the efficacy of isolation for flu-like ailments as well as to apply them more broadly to publicly available modern datasets such as the MIMIC III database. Reliable and consistent causal analysis of public health interventions requires the use of massive previously unavailable datastreams. For example, evaluation of the efficacy of isolation interventions on flu-like-illness spread must include information on friendships and interactions between individuals, biometric information, imaging, longitudinal health record data as well as standard demographic data. The proposed research provides machine learning and deep learning tools for properly employing this data for the identification and quantification of causal effects of such treatments that can lead to the development of better public health interventions.",Machine Learning and Deep Learning Solutions Supplement: Matching Methods for Causal Inference with Complex Data,9750434,R01EB025021,"['Algorithms', 'Biometry', 'Categories', 'Clinical', 'Complex', 'Data', 'Data Set', 'Databases', 'Development', 'Electroencephalography', 'Friendships', 'Goals', 'Health', 'Image', 'Individual', 'Intervention', 'Lead', 'Learning', 'Machine Learning', 'Medical', 'Medical Imaging', 'Medical Records', 'Methods', 'Modernization', 'Outcome', 'Patients', 'Research', 'Roentgen Rays', 'Rotation', 'Series', 'Stretching', 'Structure', 'System', 'Text', 'Therapeutic', 'Time', 'Treatment Efficacy', 'Work', 'X-Ray Computed Tomography', 'computerized tools', 'data format', 'deep learning', 'efficacy evaluation', 'flu', 'health record', 'improved', 'individualized medicine', 'novel strategies', 'public health intervention', 'tool', 'treatment effect']",NIBIB,DUKE UNIVERSITY,R01,2018,98714,-0.009472657329934438
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9520706,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2018,591130,0.002923785511543789
"Machine Learning for Generalized Multiscale Modeling Project Summary/Abstract  This project develops machine learning approaches that describe statistical systems in biology. By combining analytic results calculated from the exact probabilistic description of the system with machine learning inference, our new methods present exciting opportunities to model previously inaccessible complex dynamics. The resulting Boltzmann machine-like learning algorithms present a new class of modeling techniques based on the powerful in- ference of arti cial neural networks. Further development of this approach will bring the groundbreaking advances from the surge of recent interest in machine learning into the biological modeling eld. The mathematical methods we develop will be used to derive e cient algorithms for multiscale simulation, directly applicable to large scale biological modeling. In particular, the algorithms will be used to study the dynamics of stochastic biochemistry at synapses, with direct relevance to learning and memory formation in the brain. Current studies of these processes are limited by the long timescales involved and the highly spatially organized structures featured. In addition to leveraging the machine learning expertise we are developing, we also employ new electron microscopy datasets to produce 3D reconstructions of neural tissue with unprecedented accuracy. Consequentially, we will be able to study the fundamental mechanisms underlying synaptic plasticity, as well as the biochemical basis of oscillatory behavior in networks of neurons that occurs during sleep. Furthermore, the interactions of these highly stochastic ion channels with electrical in neurons will be explored through groundbreaking hybrid simulation environments. The software that we will develop combines existing popular simulation tools into multiscale approaches, and will be distributed as a powerful tool to the broader biological modeling community. Its usage in further computational experiments can present a key advancement in the development of pharmaceuticals, allowing the direct study of the interactions of biochemistry and whole neuron electrophysiology without making limiting assumptions to sim- plify the simulations. This has promising implications for intervening in age-related learning de cits, as well as in neurological disorders such as Alzheimers. Finally, this proposal will bring together our existing multiscale modeling community, the National Center for Multi-scale Modeling of Biological Systems (MMBioS), with the MSM consortium. The interactions of these organizations and their communities of expert researchers will foster new collaborative work on exciting multiscale problems in biology, including applications of the machine learning frameworks and software we are developing. 1 Project Narrative  A wide variety of biological systems can be described statistically, from molecular biochemistry up to the network level activity of neurons. This work develops machine learning approaches to approximate these systems, enabling new simulation methods that bridge di erent levels of description. The resulting computational studies aim to shed light on the basis of learning and computation in the brain, and will enable the development of pharmaceutical targets for learning de cits associated with aging and neurological disorders such as Alzheimers. 1",Machine Learning for Generalized Multiscale Modeling,9791802,R56AG059602,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Area', 'Behavior', 'Biochemical', 'Biochemistry', 'Biological Models', 'Biological Neural Networks', 'Biology', 'Brain', 'Calcium', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consequentialism', 'Coupling', 'Data Set', 'Development', 'Dimensions', 'Electron Microscopy', 'Electrophysiology (science)', 'Environment', 'Equation', 'Equilibrium', 'Evolution', 'Fostering', 'Hybrids', 'Image', 'Investigation', 'Ion Channel', 'Learning', 'Libraries', 'Light', 'Machine Learning', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Morphology', 'National Institute of General Medical Sciences', 'Neurons', 'Neuropil', 'Neurosciences', 'Pharmacologic Substance', 'Physics', 'Population', 'Potassium Channel', 'Process', 'Pythons', 'Reaction', 'Research Personnel', 'Sleep', 'Structure', 'Synapses', 'Synaptic plasticity', 'System', 'Techniques', 'Time', 'Tissues', 'United States National Institutes of Health', 'Vertebral column', 'Work', 'age related', 'base', 'biological systems', 'calmodulin-dependent protein kinase II', 'computer studies', 'experimental study', 'information processing', 'insight', 'interest', 'mathematical methods', 'men who have sex with men', 'microscopic imaging', 'multi-scale modeling', 'nervous system disorder', 'particle', 'postsynaptic', 'reconstruction', 'relating to nervous system', 'simulation', 'software development', 'success', 'tool', 'working group']",NIA,UNIVERSITY OF CALIFORNIA-IRVINE,R56,2018,619053,-0.015334648622850607
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9480874,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoke', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'improved', 'learning strategy', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2018,187699,-0.05127002175938055
"Machine learning for data-driven subtyping of major depression Major depressive disorder is highly prevalent, and represents a major driver of disability as well as health care cost. Progress in improving diagnosis and treatment of this disorder has been hindered by its heterogeneity in clinical presentation and course. Such heterogeneity makes the underlying neurobiology difficult to characterize, and has led to efforts to identify more homogeneous subgroups. These efforts date back to the dawn of the modern psychopharmacologic era - initially focused on atypical and melancholic depression, and more recently on subtypes such as anxious and irritable depression.  Subtyping efforts are complicated by a paucity of large clinical cohorts with similar ascertainment and phenotyping. In particular, the available data often focuses on a very narrow range of depressive symptoms, along with a restricted set of comorbidities, and typically encompasses only the acute phase of treatment. As a result, despite intriguing findings in one or occasionally two cohorts, subtyping has not been widely deployed in clinical practice, nor used to meaningfully improve translational investigation.  The utility of electronic health records and registries to create in silico cohort studies has been demonstrated in numerous settings, including psychiatry. Beyond sample size and efficiency of ascertainment, these data types often have advantages in the range of non-depressive phenotypes captured and availability of longitudinal data.  The present study therefore proposes to create a very large cohort of individuals with MDD, defined by a validated algorithm, spanning two health systems, and to apply novel machine learning methods to identify MDD subtypes. These subtypes will be validated by comparison with standard phenotypic definitions, annotation by trained raters using a standard 'intruder' paradigm, and correlation with medication prescribing Then, as proof of concept the biological basis of these subtypes will be characterized by examining heritability and polygenic risk using a large genetic biobank. Beyond determining convergent validity, this last step will provide proof-of-concept for broader application of data-driven subtypes for translational investigation in biobanks and registries.  The study builds on existing collaborations between a team experienced in mood disorder phenotypic and genomic study as well as application of electronic health records, and a team active in developing and applying emerging methods in machine learning. It will lay the groundwork for further validation and application of data-driven disease subtyping across medicine. Public health significance The wide variation in symptoms of major depressive disorder complicates efforts to understand the underlying causes of this illness. Applying machine learning methods to electronic health records should enable the identification of more specific disease subgroups. These subgroups will facilitate efforts to understand the causes of depression, and to begin to develop more targeted treatments.",Machine learning for data-driven subtyping of major depression,9717620,R56MH115187,"['Acute', 'Algorithms', 'Anhedonia', 'Anxiety', 'Atypical depressive disorder', 'Back', 'Biological', 'Bipolar Disorder', 'Cessation of life', 'Characteristics', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Cohort Studies', 'Collaborations', 'Comorbidity', 'Computer Simulation', 'Data', 'Data Set', 'Desire for food', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Endocrine System Diseases', 'European', 'Face', 'Genetic', 'Genomics', 'Health Care Costs', 'Health system', 'Heart Diseases', 'Heritability', 'Heterogeneity', 'Hospitals', 'Individual', 'Investigation', 'Laboratories', 'Link', 'Longitudinal cohort', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Medical', 'Medicine', 'Melancholic Depression', 'Mental Depression', 'Meta-Analysis', 'Methods', 'Modernization', 'Mood Disorders', 'Moods', 'Neurobiology', 'Outcome', 'Outpatients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Prevalence', 'Procedures', 'Psychiatry', 'Public Health', 'Quality of life', 'Reactive depression', 'Registries', 'Risk', 'Sample Size', 'Schizophrenia', 'Series', 'Severities', 'Sleep', 'Subgroup', 'Suicide', 'Symptoms', 'System', 'Therapeutic', 'Training', 'Validation', 'Variant', 'Work', 'anxious', 'base', 'biobank', 'clinical practice', 'cohort', 'cost', 'depressive symptoms', 'disability', 'disorder subtype', 'experience', 'genomic data', 'high dimensionality', 'hospital readmission', 'improved', 'innovation', 'interest', 'learning strategy', 'novel', 'novel therapeutics', 'patient population', 'portability', 'precision medicine', 'psychopharmacologic', 'suicidal risk', 'tool', 'treatment response']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R56,2018,595755,-0.004287842710742523
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9674585,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Machine Learning', 'Marines', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'dark matter', 'deep learning', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,289700,-0.0022233768356027055
"Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications Abstract Atrial fibrillation (AF) is a pervasive disease which affects over 30 million individuals worldwide, in whom it is associated with morbidity and mortality, yet for which therapeutic outcomes are suboptimal. One major limitation to mechanistic and clinical advances in AF is its taxonomy, which is based on number of days of detected AF rather than increasingly reported functional and personalized mechanisms. I reasoned that a digital and scalable AF taxonomy, based on interactions of anatomic and functional factors and clinical features, may better guide existing therapy and catalyze future mechanistic and therapeutic advances. I set out to create a predictive tool to guide therapy in AF patients using machine learning of rich mechanistic data from a large multicenter registry of patients undergoing ablation. I hypothesized that clinically actionable AF phenotypes can be defined by statistical clustering between electrophysiologic features, anatomic regions and clinical indices, that can be uncovered by physiological and statistical quantification and machine learning. I have two Specific Aims: 1) To construct a multimodal digital atlas of atrial fibrillation which registers functional indices at absolute and relative spatial locations in both atria from a multicenter registry, and make this atlas available as an open-source software resource. This deliverable will uniquely map the probability that specific mechanisms will be relevant to AF in a specific patient of given clinical characteristics. Novel pathophysiological phenotypes will be defined via probabilistic interactions in these individual components. 2) To develop a predictive tool using machine learning to estimate the likelihood that ablation at any site(s) will contribute to success tailored to individual characteristics, by learning clusters of electrophysiologic features, clinical indices, and anatomic regions in a training population and applying it to a validation cohort from a large multicenter registry. This project uses state-of-the-art computational tools and statistical methods that may reconcile divergent AF mechanistic hypotheses to define novel functional AF phenotypes and guide therapy. In the process, I will be mentored by world leading mentors, in an extraordinary training environment to facilitate this development into an independent physician-scientist in bioengineering-heart rhythm medicine. Project Narrative This research provides an avenue to define atrial fibrillation in an actionable classification rooted in pathophysiologic and mechanistic observations. Such a classification scheme would further our understanding and refine our conversation about complex arrhythmia in cardiac tissue. Only an understanding at this level is will provide truly effective and safe treatments of each individual patient’s arrhythmic condition.","Novel Atrial Fibrillation Phenotypes Defined by Functional-Anatomical, Machine-Learned Classifications",9611012,F32HL144101,"['Ablation', 'Affect', 'Anatomy', 'Anti-Arrhythmia Agents', 'Applications Grants', 'Arrhythmia', 'Atlases', 'Atrial Fibrillation', 'Biological Neural Networks', 'Biomedical Engineering', 'Cardiac', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Communities', 'Comorbidity', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Electrophysiology (science)', 'Enrollment', 'Environment', 'Faculty', 'Foundations', 'Freedom', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Heart Atrium', 'Individual', 'Injury', 'Language', 'Learning', 'Location', 'Machine Learning', 'Maps', 'Measurable', 'Measures', 'Medicine', 'Mentors', 'Mentorship', 'Mission', 'Morbidity - disease rate', 'Obstructive Sleep Apnea', 'Patients', 'Pharmacotherapy', 'Phenotype', 'Physicians', 'Physiological', 'Plant Roots', 'Population', 'Probability', 'Procedures', 'Process', 'Pulmonary veins', 'Randomized Clinical Trials', 'Registries', 'Reporting', 'Research', 'Resources', 'Scientist', 'Site', 'Statistical Methods', 'Structure', 'Supervision', 'Taxonomy', 'Testing', 'Therapeutic', 'Therapy trial', 'Tissues', 'Training', 'Translations', 'United States National Institutes of Health', 'Validation', 'base', 'clinically actionable', 'cohort', 'computer science', 'computerized tools', 'deep learning', 'digital', 'disease classification', 'health care service utilization', 'heart rhythm', 'improved outcome', 'indexing', 'individual patient', 'mortality', 'multimodality', 'novel', 'open source', 'patient registry', 'patient response', 'patient stratification', 'predictive tools', 'success', 'therapy outcome', 'tool', 'trial design']",NHLBI,STANFORD UNIVERSITY,F32,2018,63034,-0.0004350312120058127
"Assess and Model the Health Effects, Population, and Infrastructural Vulnerabilities of Power Outage PROJECT SUMMARY / ABSTRACT Climate change will lead to more intense and longer-lasting extreme weather events, such as floods, hurricanes, and severe storms, which will lead to more frequent power outage (PO). Significant gaps remain in our understanding of the impact of PO on human health: most previous studies were based on self-reported survey data, which is subject to reporting bias. In addition to weather factors that directly affect human health, other concurrent factors such as PO may also mediate with extreme weather on health, for which research is also lacking. Few studies assessed both power infrastructures and population vulnerabilities, and compared the effects of different PO causes. Furthermore, no health prediction models have been developed for PO. To fill these knowledge gaps, the proposed study will build upon multiple ongoing/ completed studies to: 1) assess the effects and mediating pathways of PO on electricity-dependent health outcomes, co-morbidities, and nursing home transfers; 2) identify infrastructure, environmental, and population vulnerabilities (individual and community); and 3) develop a vulnerability index and prediction model. We will effectively link the accessible New York statewide hospital admission and emergency department (ED) visit data with the existing data on PO, weather, air pollution, census, and nursing home transfer data. The associations between PO causes, frequency, duration, or area coverage of PO and electricity-dependent hospitalizations or ED visits due to asthma, chronic obstructive pulmonary disease (COPD), dialysis, water-/food-borne diseases, injury, and carbon monoxide poisoning will be assessed through Bayesian spatial-temporal model. This advanced technique will be able to control for both socio-demographic differences by regions and multiple temporal variables simultaneously. To improve scientific rigor, we will use control days (without PO and extreme weather) to separate PO from weather effects, and control diseases (e.g. appendicitis) to examine temporal changes of disease reporting. To understand PO’s natural direct and indirect effects, causal mediation analysis will be used. Furthermore, new variable selection methods, including Sure Independence Screening and Generalized Additive Model Selection will be used to screen and select predictors highly associated with outcomes. A composite vulnerability index weighed by risk factors identified and vulnerability maps will be developed. We will establish PO and PO-related health predictive models using the state-of-the-art data mining techniques, including Random Forest, Gradient Boosted Tree, and Ensemble Learning Decision Tree model. The excellent team with multidisciplinary and experienced investigators, numerous already collected and geocoded datasets, innovative data mining and analysis methods, continuation of student training, and successful prior partnerships with government agencies will maximize the probability of our success and feasibility. This project will also significantly enhance our institute’s environment and students’ involvement in research, and our findings will identify evidence-based strategies for emergency management and public health preparedness. NARRATIVE The proposed study will evaluate whether frequency, duration, coverage, and certain causes of power outage (PO) are associated with increased risks of electricity-dependent diseases, including asthma, chronic obstructive pulmonary disease (COPD), dialysis, water-/food-borne diseases, injury, and carbon monoxide poisoning. This may be the first study to assess whether people with certain demographic characteristics (e.g. elderly or different sex), living in a neighborhood with certain vulnerabilities (e.g. high land coverage or low hospital density), in certain seasons, degree of urbanicity, or specific causes of PO are more vulnerable to the impact of PO. The findings, vulnerability index and maps, and the forecast models derived from this study will help guide the state or federal environmental and health agencies to plan interventions and climate adaptation programs.","Assess and Model the Health Effects, Population, and Infrastructural Vulnerabilities of Power Outage",9440510,R15ES028000,"['Accident and Emergency department', 'Address', 'Admission activity', 'Adverse effects', 'Affect', 'Age', 'Air Pollution', 'Appendicitis', 'Area', 'Asthma', 'Carbon Monoxide Poisoning', 'Censuses', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Climate', 'Collaborations', 'Communities', 'Comorbidity', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Decision Trees', 'Dialysis procedure', 'Disasters', 'Disease', 'Elderly', 'Electricity', 'Emergency Situation', 'Emergency department visit', 'Engineering', 'Ensure', 'Environment', 'Environmental Health', 'Equipment Failure', 'Ethnic Origin', 'Event', 'Evidence based intervention', 'Floods', 'Frequencies', 'Gender', 'Geographic Factor', 'Government Agencies', 'Grant', 'Health', 'Hospitalization', 'Hospitals', 'Human', 'Hurricane', 'Individual', 'Injury', 'Institutes', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mediating', 'Mediation', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Natural Disasters', 'Neighborhoods', 'New York', 'Nursing Homes', 'Outcome', 'Pathway interactions', 'Patient Self-Report', 'Pattern', 'Population', 'Population Characteristics', 'Positioning Attribute', 'Predisposition', 'Probability', 'Public Health', 'Readiness', 'Recording of previous events', 'Recovery', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Risk Factors', 'Seasons', 'Socioeconomic Status', 'Statistical Methods', 'Students', 'Surveys', 'Techniques', 'Trees', 'Vulnerable Populations', 'Water', 'Weather', 'Work', 'base', 'climate change', 'climate impact', 'data mining', 'density', 'disorder control', 'evidence base', 'experience', 'extreme heat', 'foodborne', 'forest', 'health data', 'improved', 'indexing', 'innovation', 'learning strategy', 'low socioeconomic status', 'member', 'modifiable risk', 'multidisciplinary', 'predictive modeling', 'programs', 'public health priorities', 'screening', 'sex', 'student training', 'success']",NIEHS,STATE UNIVERSITY OF NEW YORK AT ALBANY,R15,2018,443750,-0.05348890302305495
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9519804,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'mild cognitive impairment', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2018,410000,-0.059292188103816375
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9553816,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2018,368020,-0.017342924907988427
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9547454,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2018,332952,-0.008220780968050408
"A computational approach to early sepsis detection Abstract Significance: In this SBIR project, we propose to improve the performance of InSight, a machine-learning- based sepsis screening system, in situations of limited training data from the target clinical site. The proposed work will make possible prospective clinical deployments to sites which are smaller or lack clinical data repositories, by significantly reducing the amount of training data necessary down to a few weeks of clinical observation. Classically, a machine-learning-based system like InSight requires complete retraining for each new clinical setting, in turn requiring a new and large collection of data from each target deployment site. We will circumvent this requirement via transfer learning techniques, which transfer knowledge acquired previously in a source clinical setting to a new, target setting. Research Questions: Which transfer learning methods and paired classification algorithms are most suitable for use with InSight, requiring minimal target-site training data while maintaining strong performance? Are these methods and algorithms robust across the several common sepsis-spectrum definitions? Prior Work: We have developed InSight using the MIMIC-III retrospective data set, on which it attains an area under the receiver operating characteristic curve (AUROC) of 0.88 for sepsis detection, and 0.74 for 4-hour early sepsis prediction. We have also conducted pilot transfer learning  ≥ experiments in a different clinical task, mortality forecasting, in which transfer learning yields a 10-fold reduction in the amount of target-site training data required to achieve AUROC 0.80. Specific Aims: Aim 1 - to implement and assess side-by-side four diverse transfer learning methods for a retrospective clinical sepsis prediction task, where the source data set is MIMIC-III and the simulated clinical target is a data set drawn from UCSF. Aim 2 - to determine which among the best methods from Aim 1 also provide robust performance when applied to two additional sepsis-spectrum gold standards. Methods: We will prepare implementations of transfer learning methods which use instance transfer, residual learning and/or feature augmentation, kernel length scale transfer, and feature transfer. We will test these methods with applicable classifiers on subsets of the UCSF set, using cross-validation and quantifying discrimination performance in terms of AUROC. The best method/classifier pairs will require no more than 30 examples of septic patients from the target set and attain AUROC superiorities of 0.05 in 0- and 4-hour pre-onset sepsis prediction/detection, relative to the best tested alternative screening systems (Aim 1). The top three pairs will then be tested for robustness to gold standard choice, using septic shock (0- and 4-hour) and SIRS-based sepsis (0-hour) gold standards; in these tests, at least one pair must again attain 0.05 margin of superiority in AUROC versus the alternative screening systems (Aim 2). Future Directions: The results of these experiments will enable InSight to be robustly deployed to diverse clinical sites, yielding high performance without the need for extensive target-site data acquisition. Narrative Clinical decision support (CDS) systems present critical information to medical professionals by examining patient data and providing relevant information. Machine learning is a powerful method for creating CDS tools, but accessing its full strength requires re-training with retrospective data from each target clinical site. We will use transfer learning techniques to dramatically reduce the amount of target-site training data required by InSight, our machine-learning-based CDS tool for sepsis prediction, and empirically evaluate several such methods on a patient data set, using three different sepsis-related gold standards.",A computational approach to early sepsis detection,9557664,R43TR002221,"['Address', 'Age', 'Algorithms', 'Area', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Collection', 'Custom', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Discrimination', 'Drops', 'Early Diagnosis', 'Early Intervention', 'Future', 'Gold', 'Healthcare', 'Healthcare Systems', 'Hour', 'Image', 'Immune response', 'Institution', 'Knowledge', 'Learning', 'Length', 'Machine Learning', 'Medical', 'Methods', 'Multicenter Studies', 'Nature', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Psychological Transfer', 'Receiver Operating Characteristics', 'Research', 'Residual state', 'Risk', 'SCAP2 gene', 'Sensitivity and Specificity', 'Sepsis', 'Septic Shock', 'Severities', 'Side', 'Site', 'Small Business Innovation Research Grant', 'Source', 'Survival Rate', 'System', 'Techniques', 'Testing', 'Training', 'Validation', 'Work', 'base', 'clinical data warehouse', 'clinical decision support', 'clinical research site', 'cost', 'data acquisition', 'experimental study', 'improved', 'insight', 'learning strategy', 'mortality', 'performance site', 'portability', 'prospective', 'screening', 'septic', 'septic patients', 'success', 'support tools']",NCATS,"DASCENA, INC.",R43,2018,310782,-0.00042754675577189944
"Bioinformatics for post-traumatic stress Project Summary/Abstract Maladaptive complications following trauma, including post-traumatic stress (PTS), are highly prevalent in both veterans and civilians, and have been difficult to accurately diagnose, manage and treat. Debate regarding diagnostic criteria and the need to represent the full spectrum of inter-connected features contributing to psychopathology has spawned the development of the Research Domain Criteria (RDoC) by the National Institute of Mental Health (NIMH). RDoC is a developing framework to help guide the discovery and validation of new dimensions of mental health disorders and their relationships to underlying biological mechanisms. NIMH now has a rich federated database that currently houses raw data from RDoC-sponsored clinical research, and clinical trial data from the National Database of Clinical Trials (NDCT) with information that may help to unlock the complex and overlapping relationships between symptoms of PTS and the underlying biomarkers to fuel improvements on diagnostic and therapeutic frameworks for trauma recovery. The proposed project will apply bioinformatics and machine learning analytical tools to these large, heterogeneous datasets to identify and validate new research dimensions of trauma-related psychopathology and treatment response trajectories and their predictors. Aim 1 will develop an in silico trauma patient population by integrating data from diverse sources, including cross-sectional and observational longitudinal clinical studies housed within available data repositories for trauma and other related mental health research. Data will include medical history, demographics, diagnostic tests, clinical outcomes, psychological assessments, genomics, imaging, and other relevant study and meta-data. Aim 2 will identify multiple dimensions of PTS diagnostic criteria, using a combination of unsupervised dimension-reduction statistical methods, internal and external cross-validation, and supervised hypothesis testing of predictive models to understand the heterogeneous subtypes of PTS. Aim 3 will deploy unsupervised machine learning methods, such as topological data analysis and hierarchical clustering, to identify unique clusters of patients based on symptomatology to develop clustering methods for precision mapping of PTS patients based on disease severity. Aim 4 will use supervised machine learning techniques for targeted predictive analytics focused on identifying treatment responders from the NDCT, and identification of latent variables that predict treatment response. The results of the proposed research project will greatly enrich the field of computational psychiatry research to identify conserved dimensions associated with the complex relationships of psychopathology and precision treatment planning following exposure to traumatic events. Project Narrative A recent restructuring of diagnostic and research criteria for psychiatric disorders has been implemented to promote greater understanding of the biological mechanisms involved in the development of complex mental health disorders. The proposed project aims to apply bioinformatics and machine learning analytics to large datasets from trauma-exposed patients to identify and validate dimensions of post-traumatic stress (PTS), relevant biological predictors, and precision treatment response trajectories.",Bioinformatics for post-traumatic stress,9612862,R01MH116156,"['Bioinformatics', 'Biological', 'Biological Markers', 'Categories', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Database', 'Complex', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Dimensions', 'Disease', 'Exposure to', 'Genomics', 'Growth', 'Image', 'Laboratories', 'Linear Models', 'Linear Regressions', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Medical History', 'Mental Health', 'Mental disorders', 'Metadata', 'Methods', 'Modality', 'Modeling', 'National Institute of Mental Health', 'Nervous System Trauma', 'Neurocognitive', 'Observational Study', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Principal Component Analysis', 'Psychiatry', 'Psychopathology', 'Recovery', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Research Project Grants', 'Severity of illness', 'Source', 'Statistical Methods', 'Supervision', 'Symptoms', 'Syndrome', 'Techniques', 'Testing', 'Therapeutic', 'Trauma', 'Trauma Research', 'Trauma patient', 'Trauma recovery', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Work', 'accurate diagnosis', 'analytical tool', 'base', 'biobehavior', 'combat', 'data archive', 'data mining', 'data sharing', 'data warehouse', 'demographics', 'federated computing', 'guided inquiry', 'hands-on learning', 'indexing', 'innovation', 'insight', 'interest', 'learning strategy', 'patient population', 'patient subsets', 'post-traumatic stress', 'precision medicine', 'predictive modeling', 'predictive test', 'psychologic', 'research and development', 'research study', 'response', 'statistics', 'stress related disorder', 'symptomatology', 'tool', 'trauma exposure', 'traumatic event', 'treatment planning', 'treatment responders', 'treatment response', 'unsupervised learning', 'vector']",NIMH,UNIVERSITY OF MINNESOTA,R01,2018,547853,0.007822155860175834
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9479280,K08HL136928,"['Affect', 'Bioinformatics', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Diseases', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'learning strategy', 'miRNA expression profiling', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2018,172800,-0.0034766174080266355
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9448918,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'big biomedical data', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2018,308503,0.017837106152089062
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9572992,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,297237,0.012572489144159316
"Towards automated phenotyping in epilepsy Over 5 million children and adults in the United States have had a diagnosis of epilepsy or a seizure disorder. However, treatment options for the epilepsies remain inadequate, because many patients suffer from uncontrolled seizures and from the negative side effects of treatment. A major obstacle to the faster development of new anti-convulsant therapies is the fact that rigorous preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). We propose to test if it is possible to perform objective, inexpensive and automated phenotyping of mice in various mouse models of acquired and genetic epilepsies. The approach rests on the recent recognition that mouse behaviors are structured in stereotyped modules at sub-second timescales that are arranged according to specific rules. These characteristic behavioral modules, and the transitions between them, can be identified without observer bias by combined 3D imaging and machine learning (ML) -assisted analytic methods. We propose to adopt this novel ML-assisted 3D video analysis technology to epilepsy research, in order to test if it can be used to identify mice with chronic temporal lobe epilepsy (TLE) during inter-ictal and ictal periods in two distinct experimental TLE models, and under various experimental conditions. In addition, we will also test whether the approach is able to automatically detect not only the overtly epileptic mice in a genetic model of severe childhood epilepsy (homozygous voltage-gated sodium channel β-subunit SCN1B-/- knock-out mice), but also distinguish the seemingly normal, non-epileptic, SCN1B+/- heterozygous mice from the wild-type controls. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user-independent, inexpensive analysis of acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will test if it is possible to objectively characterize epileptic phenotypes in mice using a breakthrough technology involving machine learning-assisted analysis of 3-dimensional video data of behavior. If successful, this innovative approach is expected to dramatically accelerate epilepsy research by enabling the objective, automated, inexpensive phenotyping of experimental animals to aid the testing of novel anticonvulsant therapies.",Towards automated phenotyping in epilepsy,9503816,R21NS102908,"['Adopted', 'Adult', 'Adverse effects', 'Animal Behavior', 'Animal Model', 'Animals', 'Anticonvulsants', 'Behavior', 'Behavioral', 'Characteristics', 'Child', 'Childhood', 'Chronic', 'Complex', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Epilepsy', 'Exhibits', 'Frequencies', 'Genetic', 'Genetic Models', 'Hippocampus (Brain)', 'Human', 'Human immunodeficiency virus test', 'Image', 'Knockout Mice', 'Machine Learning', 'Modeling', 'Monitor', 'Mus', 'Neurons', 'Observer Variation', 'Patients', 'Phenotype', 'Pilocarpine', 'Probability', 'Recurrence', 'Research', 'Rest', 'Seizures', 'Sodium Channel', 'Stereotyping', 'Structure', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Time', 'Translational Research', 'United States', 'Wild Type Mouse', 'analytical method', 'base', 'cost', 'dravet syndrome', 'evidence base', 'high throughput analysis', 'innovation', 'kainate', 'learning strategy', 'mouse model', 'novel', 'novel therapeutics', 'pre-clinical', 'voltage']",NINDS,STANFORD UNIVERSITY,R21,2018,237423,-0.015738692751631006
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis. PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.",Developing Classification Criteria for the Uveitides,9472335,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'age group', 'age related', 'aging population', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2018,418408,0.009569577569556075
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9639469,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Research Infrastructure', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computer infrastructure', 'digital', 'disease transmission', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'predictive modeling', 'social', 'social media', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector', 'vector control']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2018,407175,0.012015233652750595
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9515964,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2018,341899,-0.02994355284113031
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9567622,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Supervision', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'learning strategy', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'patient subsets', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2018,659674,0.001391543287281894
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9570304,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Simulation', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Supervision', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in vivo', 'insight', 'intracranial artery', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'single photon emission computed tomography', 'spatiotemporal', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2018,528639,0.008390115177288274
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9517061,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'data warehouse', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2018,306284,-0.018228613296468903
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9487840,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2018,141048,0.0001644448701526404
"Systems Level Causal Discovery in Heterogeneous TOPMed Data SYSTEMS LEVEL CAUSAL DISCOVERY IN HETEROGENEOUS TOPMED DATA ABSTRACT The advent of new technologies for collecting and analyzing multiple heterogeneous data streams from the same individual makes possible the detailed phenotypic characterization of diseases and paves the way for the development of individualized precision therapies. A major bottleneck in this process is the lack of robust, efficient and truly integrative analytic methods for such multi-modal data. This proposal builds on the ongoing efforts of our group in the area of causal learning in biomedicine. The objective of this application is to extend, modify and tailor our causal probabilistic graphical models to data typically collected by TOPMed projects, such as –omics data (SNPs, metabolomics, RNA-seq, etc), imaging, patients' history, and clinical data. COPDGene® is one of the TOPMed projects and has generated datasets with those modalities for 10,000 patients with chronic obstructive pulmonary disease (COPD), the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with different characteristics. There is currently no satisfactory method for COPD subtyping or prediction of disease progression. In this project we will apply, test and validate our approaches on COPDGene® and another large independent COPD cohort. The extension and application of our methods to cross-sectional and longitudinal data will also allow us to investigate a number of important questions and aspects related to COPD. Mechanistically, we will investigate how SNPs, genes and their networks are causally linked to disease phenotypes. In pathology, we will identify conditional biomarkers, which will lead to disease sub-classification and identification of causal components in each subtype. In pathophysiology, we will identify features that are directly linked to lung function decline and outcome. We will make all our algorithms and results available to the community through web and public cloud interfaces. The deliverables will be (1) new probabilistic approaches for integration and analysis of multi-modal cross-sectional and longitudinal data, including SNPs, blood biomarkers, CT scans and clinical data; (2) new cloud-based server to make these approaches available to the research community; (3) results on the mechanism, pathology and pathophysiology of COPD facilitation and progression. To guarantee the success of the project we have assembled a team of experts in genomics, machine learning, cloud computing and COPD. This cross- disciplinary team project will have a positive impact beyond the above deliverables, since the generality of our approaches makes them applicable to any disease. We expect that during this U01 we will have the opportunity to collaborate with other teams in the TOPMed consortium to help them investigate the causes of their corresponding disease phenotypes. We do believe that data integration in a single probabilistic framework will be in the heart of precision medicine strategies in the future, when massive high-throughput data collection will become a routine diagnostic and prognostic procedure in all hospitals. PROJECT NARRATIVE Current technologies for high-throughput biomedical data collection allow the interrogation of multiple modalities from a single patient. New promising analytical methods started emerging, which can analyze those multi-modal data in a holistic way. Chronic obstructive pulmonary disease (COPD) constitutes the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with their own characteristics. There is currently no satisfactory method for COPD subtyping. We will apply, test and validate new probabilistic approaches on two cohorts of COPD patients. We will investigate the mechanisms of disease facilitation; we will identify patient cohorts with specific characteristics (disease subtypes); and investigate risk factors and causal variants for the disease progression in each subtype.  ",Systems Level Causal Discovery in Heterogeneous TOPMed Data,9473087,U01HL137159,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Biological Models', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Data', 'Data Collection', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Functional Imaging', 'Functional disorder', 'Funding', 'Future', 'Genes', 'Genetic Determinism', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Health Care Costs', 'Heart', 'Hospitals', 'Image', 'Individual', 'Internet', 'Learning', 'Lifting', 'Link', 'Machine Learning', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Morphology', 'Outcome', 'Outcome Assessment', 'Pathology', 'Patients', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Physiological', 'Precision therapeutics', 'Procedures', 'Process', 'Pulmonology', 'Recording of previous events', 'Research', 'Research Personnel', 'Respiratory physiology', 'Risk', 'Risk Factors', 'Science', 'Stream', 'Syndrome', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Tissues', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'X-Ray Computed Tomography', 'analytical method', 'base', 'clinical imaging', 'clinically relevant', 'cloud based', 'cohort', 'computer science', 'cost effective', 'data integration', 'disability', 'disease phenotype', 'disorder subtype', 'graphical user interface', 'high throughput technology', 'innovation', 'longitudinal dataset', 'medical schools', 'metabolomics', 'mortality', 'multimodality', 'new technology', 'novel', 'outcome forecast', 'patient subsets', 'precision genomic medicine', 'precision medicine', 'prognostic', 'repository', 'success', 'tool', 'transcriptome sequencing', 'user-friendly']",NHLBI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2018,601486,0.011405729458092371
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9495704,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2018,396544,0.023867217146273573
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Project Summary:  Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer's disease, Parkinson's disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these chal- lenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymor- phisms, whole brain neuroimaging, bioﬂuids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mecha- nisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Speciﬁcally, in Aim 1, we develop efﬁcient methods for multi-level semiparametric transformation mod- els to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efﬁciency. Our methods do not rely on full pedigree genotyping and provide family-speciﬁc substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic sys- tems through ordinary differential equations with random inﬂections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker proﬁles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efﬁcient machine learning methods to identify predictive markers, estimate optimal individu- alized therapies, and identify subgroups who may receive the greatest beneﬁt from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art ana- lytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research com- munity. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9502388,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'individualized medicine', 'learning strategy', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2018,340491,0.01447974297471314
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9498252,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'cancer therapy', 'computerized data processing', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",R01,2018,158992,-0.019001882930572683
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9502903,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2018,879004,-0.02201862869038289
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9548457,R01AR068456,"['Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2018,273031,0.0025080198162745434
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9445086,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,305167,0.018509227725715983
"Statistical Methods for Multilevel Multivariate Functional Studies Abstract  While imaging studies are widely used in clinical practice and research, the number of neuroimaging- based biomarkers is small. For example, in clinical trials of immunomodulatory therapies for MS, the only commonly used imaging biomarkers are the total lesion volume and the number of new and en- hancing lesions. These biomarkers are essential, but do not capture the recovery process of lesions, which is thought to decline in more severe, progressive disease. The partial or complete recovery of lesions may depend both on the ability of the brain to heal and on external factors, such as treat- ment or environmental and behavioral exposures. In this proposal we take the natural next step of proposing imaging biomarkers for MS based on the formation and change of lesions as observed on multi-sequence structural MRIs. To solve this problem we propose to address several general method- ological problems: 1) develop models and methods for the longitudinal analysis of several images of the same brain; 2) identify and estimate the length of history that is necessary to estimate recovery; 3) study the association with known biomarkers of the disease (in this case total volume and number of new and enhancing lesions); 4) develop methods that are robust to changes in imaging protocols that inevitably arise in longitudinal neuroimaging studies; and 5) develop the computational tools that allow for sophisticated methods to be implemented seamlessly in practice. While our scientiﬁc problem is focused, the proposed statistical methods are general and can be applied to a wide variety of longitu- dinal neuroimaging studies. For example, there are many ongoing longitudinal neuroimaging studies, including the ADNI, AIBL, HBC, and MISTIE, where our methods could be used to study subtle or large changes in lesions or in white and gray matter intensities. Project narrative. The project provides statistical analysis methods for quantiﬁcation of the evolution in the intensity of brain lesions on multi-sequence Magnetic Resonance Imaging (MRI). Methods are motivated by the need to develop new neuroimaging-based biomarkers for multiple sclerosis (MS), but can be applied to other types of brain diseases including stroke, Alzheimer disease, and cancer.",Statistical Methods for Multilevel Multivariate Functional Studies,9492705,R01NS060910,"['Accounting', 'Address', 'Alzheimer&apos', 's Disease', 'Behavioral', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical Research', 'Clinical Trials', 'Computer software', 'Data', 'Databases', 'Disease', 'Enhancing Lesion', 'Event', 'Evolution', 'Funding', 'Grant', 'Graph', 'Image', 'Incidence', 'Length', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mediation', 'Mediator of activation protein', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Multiple Sclerosis Lesions', 'Names', 'Natural History', 'Nature', 'Online Systems', 'Pattern', 'Population Heterogeneity', 'Problem Solving', 'Process', 'Progressive Disease', 'Protocols documentation', 'Randomized', 'Recording of previous events', 'Recovery', 'Research', 'Sampling', 'Statistical Data Interpretation', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'biomarker validation', 'clinical practice', 'computerized tools', 'design', 'gray matter', 'healing', 'high dimensionality', 'imaging biomarker', 'imaging study', 'immunomodulatory therapies', 'improved', 'insight', 'longitudinal analysis', 'longitudinal database', 'neuroimaging', 'non-Gaussian model', 'personalized approach', 'repaired', 'software development', 'treatment response', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R01,2018,637414,-0.025533456273581518
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9559432,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2018,325325,0.01673781722097007
"Statistical Methods for Selection and Evaluation of Biomarkers DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particular, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pancreatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public. PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.",Statistical Methods for Selection and Evaluation of Biomarkers,9410515,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Cohort Studies', 'Cross-Sectional Studies', 'Custom', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of pancreas', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'biomarker evaluation', 'burden of illness', 'candidate marker', 'case control', 'clinical practice', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'flexibility', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'optimal treatments', 'outcome prediction', 'patient biomarkers', 'patient population', 'patient response', 'predictive marker', 'programs', 'public health relevance', 'randomized trial', 'screening', 'tool', 'treatment effect', 'treatment response']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2018,323434,-0.04414121807199599
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9661636,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2018,263913,0.02640136259307316
"Next Generation Testing Strategies for Assessment of Genotoxicity Project Summary  It is well recognized that current batteries of genetic toxicology assays exhibit two critical deficiencies. First, the throughput capacity of in vitro mammalian cell genotoxicity tests is low, and does not meet current needs. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no consideration for potency, and virtually no information is provided about molecular targets and mechanisms. These deficiencies in hazard characterization prevent genotoxicity data from optimally contributing to modern risk assessments, where this information is essential. We will address these major problems with current in vitro mammalian cell genetic toxicity assays by developing methods and associated commercial assay kits that dramatically enhance throughput capacity, and delineate genotoxicants' primary molecular targets, while simultaneously providing information about potency. Once biomarkers and a family of multiplexed assays have been developed for these purposes, an interlaboratory trial will be performed with prototype assay kits to assess the transferability of the methods. Project Narrative  DNA damage that cannot be faithfully repaired results in gene mutation and/or chromosomal aberrations, and these effects are known to contribute to cancer and other severe diseases. Thus, there is an important need for sensitive assays to evaluate chemicals for genotoxic and other deleterious effects. The work proposed herein will address issues that have plagued genotoxicity assessments for the last several decades: low throughput, lack of potency metrics, and little to no information about molecular targets. We will address these major problems with current genetic toxicity assays by developing new methods and associated commercial assay kits.",Next Generation Testing Strategies for Assessment of Genotoxicity,9465735,R44ES029014,"['Address', 'Affect', 'Aneugens', 'Antioxidants', 'Appearance', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Biological Response Modifiers', 'Bleomycin', 'Caspase', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Chemicals', 'Chromosome abnormality', 'Chromosomes', 'Classification', 'Cleaved cell', 'Colcemid', 'Companions', 'Complex', 'Computer Simulation', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'DNA-PKcs', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Dose', 'Epitopes', 'Etoposide', 'Exhibits', 'Family', 'GADD45A gene', 'Gamma-H2AX', 'Gene Mutation', 'Genetic', 'Goals', 'Harvest', 'Histone H3', 'Human', 'In Vitro', 'Intercalating Agents', 'Investigation', 'Kinetics', 'Label', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Modification', 'Molecular Target', 'Mutagenicity Tests', 'NF-kappa B', 'Nuclear', 'Pathway interactions', 'Phase', 'Physiologic pulse', 'Procedures', 'Protocols documentation', 'Reagent', 'Reference Values', 'Risk Assessment', 'Schedule', 'Scheme', 'Series', 'Stains', 'TP53 gene', 'Testing', 'Time', 'Toxic effect', 'Toxicogenetics', 'Training', 'Validation', 'Work', 'aurora kinase', 'base', 'clastogen', 'computerized tools', 'design', 'experimental study', 'forest', 'genotoxicity', 'hazard', 'inhibitor/antagonist', 'next generation', 'prediction algorithm', 'prevent', 'prototype', 'repaired', 'response', 'targeted agent', 'tool', 'treatment optimization', 'virtual']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2018,178854,-0.011311930286514673
"Sterol and Isoprenoid Diseases Consortium Supplement 2018 Sjögren-Larsson syndrome (SLS) is a rare inherited neurocutaneous disease characterized by ichthyosis, spastic diplegia or tetraplegia, intellectual disability, and a distinctive retinopathy. It is caused by mutations in ALDH3A2, which codes for fatty aldehyde dehydrogenase (FALDH) and results in abnormal lipid metabolism. Despite knowing the gene defect and enzyme abnormality, the pathogenic mechanisms are still unclear and no single biomarker exists that correlates with disease severity. FALDH deficiency results in several lipid abnormalities including accumulation of fatty aldehydes, which have potential toxic effects via formation of covalent adducts with proteins and lipids. This unusual lipid abnormality has the potential to affect multiple unrelated cellular pathways that are critical for disease pathogenesis. Our recent metabolomic studies in SLS have identified a unique biochemical profile of at least 30 metabolites in plasma that suggests disruption of several previously unsuspected pathways. We now propose to mine our STAIR 7004 clinical database of 20 SLS patients together with their associated metabolomic data to develop a minimal “metabolomic profile” that will correlate with severity of clinical symptoms and function as a reliable biomarker. These studies will utilize various statistical analytical methods, including principal component analysis, hierarchical clustering and random forest analysis, to define a minimal group of clinically informative metabolites, representing multiple biochemical pathways, for construction of a SLS metabolomic profile. When completed, this research will provide an objective biomarker for SLS disease description and therapeutic monitoring. Sjögren-Larsson syndrome is a rare genetic disease that is characterized by congenital ichthyosis, spasticity, intellectual disability and a distinct retinopathy. It is caused by mutations in a gene called ALDH3A2 and results in abnormal lipid metabolism. The pathogenic mechanisms that are responsible for the symptoms are still not defined. The proposed research will investigate whether a distinctly abnormal group of biochemicals in the blood, which constitute a “metabolomics profile”, will correlate with the severity of symptoms in patients and thereby act as an objective biomarker for disease severity.",Sterol and Isoprenoid Diseases Consortium Supplement 2018,9719054,U54HD061939,"['Affect', 'Area', 'Biochemical', 'Biochemical Pathway', 'Biological Markers', 'Blood', 'Clinical', 'Code', 'Congenital ichthyosis', 'Critical Pathways', 'Data', 'Data Set', 'Databases', 'Defect', 'Disease', 'Enrollment', 'Enzymes', 'Exhibits', 'Fatty Acids', 'Genes', 'Genetic Diseases', 'Individual', 'Inherited', 'Intellectual functioning disability', 'Lipids', 'Little&apos', 's Disease', 'Metabolic', 'Metabolism', 'Monitor', 'Mutation', 'Natural History', 'Neurocutaneous Syndromes', 'Oxides', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Plasma', 'Principal Component Analysis', 'Process', 'Proteins', 'Research', 'Retinal Diseases', 'Severities', 'Severity of illness', 'Sjogren-Larsson Syndrome', 'Spastic Tetraplegia', 'Specimen', 'Sterols', 'Symptoms', 'Therapeutic', 'Toxic effect', 'adduct', 'analytical method', 'biomarker identification', 'fatty aldehyde', 'forest', 'isoprenoid', 'lipid metabolism', 'long-chain-aldehyde dehydrogenase', 'metabolomics', 'novel', 'response', 'spasticity']",NICHD,UNIVERSITY OF NEBRASKA MEDICAL CENTER,U54,2018,58479,-0.01066154913982296
"Phamarcogenomics of Statin Therapy (SUPPLEMENT) PARENT ABSTRACT The overall objective of the Center ""Pharmacogenomics of Statin Therapy"" (POST) is to apply genomic, transcriptomic, and metabolomic analyses, together with studies in cellular and animal models, and innovative informatic tools, to identify and validate biomarkers for efficacy of statin drugs in reducing risk of cardiovascular disease (CVD), and for adverse effects of statins, specifically myopathy and type 2 diabetes. This multidisciplinary approach is enabled by a team of investigators with expertise in genomics (human, mouse, and molecular), statistics and informatics, and clinical medicine and pharmacology. The Center is comprised of three Projects, two Research Cores, and an Administrative Core. A major aim of Project 1 is the identification of cellular transcriptomic and metabolomic markers for clinical efficacy and adverse effects of statins. This will be accomplished by analyses in statin-exposed lymphoblast cell lines derived from patients with major adverse coronary events, or onset of myopathy or type 2 diabetes on statin treatment, compared with unaffected statin-treated controls. In addition, using genome wide genotypes from these patients, DNA variants will be identified that are associated with statin-induced changes in the transcripts and metabolites that most strongly discriminate affected patients and controls. Project 2 will use a unique, well- characterized panel of 100 inbred mouse strains to discover genetic variation associated with statin- induced myopathy and dysglycemia. Mechanisms underlying these effects will be investigated, with emphasis on the role of dysregulation of autophagy by statin treatment. Projects 1 and 2 will also use relevant cellular and mouse models, respectively, to perform functional studies to validate effects of genes identified in all POST projects as strong candidates for modulating statin efficacy or adverse effects. In Project 3, information derived from genome-wide genotypes, electronic health records, and pharmacy data in a very large and diverse population-based patient cohort will be leveraged to identify and replicate genetic associations with statin efficacy (lipid lowering and CVD event reduction) and adverse effects (myopathy and type 2 diabetes), as well as to assess the overall heritability of these responses. The Clinical Core, based in Kaiser Permanente of Northern California, will provide the clinical information and biologic materials for both Projects 1 and 3. Investigators in the Informatics Core will optimize data analysis and integration of results across all projects. The Administrative Core will provide scientific leadership and management of the Center, and foster scientific interactions and training opportunities. Overall, the research program of this Center provides an innovative model for a ""systems"" approach to pharmacogenomics that incorporates complementary investigative tools to discover and validate genetically influenced determinants of drug response. Moreover, the findings have the potential for guiding more effective use of statins for reducing CVD risk and minimizing adverse effects, and identifying biomarkers of pathways that modulate the multiple actions of this widely used class of drugs. ADMINISTRATIVE SUPPLEMENT ABSTRACT In response to NOT-AG-18-008, our goal is to extend the validation and application of our data integration methodologies into Alzheimer’s disease research. This administrative supplement is designed to extend the work of the existing subaward to the University of Pennsylvania subcontract for the POST Informatics Core. The PGRN POST Informatics Core serves as the central hub for data sharing and coordination across the three POST projects in the PGRN P50 award. One of our jobs is annotating the extensive information that will be collected and providing analysis expertise to the projects as needed. However, to make great strides in scientific progress and ensure that the collective whole of the Center is greater than the sum of the parts, a key function of the Informatics Core is to serve as ‘The Integrator’ to combine these data and information. We and others have shown that integration of complementary omics-based data can provide emergent insights into biological processes compared to what can be learned through any single approach alone. The methods that we are developing to integrate data for statin pharmacogenomic phenotypes will be equally applicable in the area of Alzheimer’s disease. Additionally, recent emphasis on open data science by Alzheimer’s disease researchers provides ample data for us to interrogate our method. We have developed novel statistical analysis tools such as the Analysis Tool for Heritable and Environmental Network Associations (ATHENA), and data visualization tools, such as PhenoGram, both of which are designed to collect and combine information from diverse data sources. With these tools, we will leverage publicly available Alzheimer’s disease datasets to maximize the knowledge gleaned about disease risk for Alzheimer’s diseases. The methodologies that we have been developing as part of the PGRN POST award for the past 2.5 years are clearly applicable to the study of Alzheimer’s disease risk. An important validation step of the application of our methodologies is to apply them to different types of datasets and in different phenotypic areas. This administrative supplement focused on extended research into having an Alzheimer’s disease focus is a great mechanism to simultaneously allow us to validate our methodologies with different types of data and potentially identify important risk factors and pathways toward a better understanding of the etiology of Alzheimer’s disease. Finally, we may have the opportunity to identify cross biological implications due to the known pleiotropic relationships between Alzheimer’s disease and cardiovascular disease. PROJECT NARRATIVE This administrative supplement is focused on extended our data integration research into Alzheimer’s disease. This funding mechanism will simultaneously allow us to validate our methodologies with different types of data and also potentially identify important risk factors and pathways toward a better understanding of the etiology of Alzheimer’s disease.",Phamarcogenomics of Statin Therapy (SUPPLEMENT),9719267,P50GM115318,"['Administrative Supplement', 'Adverse effects', 'Affect', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Animal Model', 'Area', 'Autophagocytosis', 'Award', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'California', 'Cardiovascular Diseases', 'Cell Line', 'Cell model', 'Clinical', 'Clinical Markers', 'Clinical Medicine', 'Clinical Pharmacology', 'DNA', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Electronic Health Record', 'Ensure', 'Etiology', 'Event', 'Fostering', 'Funding Mechanisms', 'Genes', 'Genetic Variation', 'Genomics', 'Genotype', 'Glean', 'Goals', 'Heritability', 'Human', 'Inbred Strains Mice', 'Informatics', 'Knowledge', 'Leadership', 'Learning', 'Lipids', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Myopathy', 'Non-Insulin-Dependent Diabetes Mellitus', 'Occupations', 'Parents', 'Pathway interactions', 'Patients', 'Pennsylvania', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacy facility', 'Phenotype', 'Population Heterogeneity', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Risk Factors', 'Role', 'Sampling', 'Source', 'Statistical Data Interpretation', 'Sum', 'System', 'Transcript', 'Universities', 'Validation', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'cardiovascular disorder risk', 'clinical efficacy', 'cohort', 'coronary event', 'data access', 'data hub', 'data integration', 'data sharing', 'data visualization', 'design', 'experimental study', 'genetic association', 'genome wide association study', 'genome-wide', 'genotyped patients', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'knowledge integration', 'lymphoblast', 'metabolomics', 'mouse model', 'multiple omics', 'novel', 'novel marker', 'open data', 'population based', 'predictive marker', 'programs', 'response', 'risk minimization', 'statistics', 'tool', 'training opportunity', 'transcriptomics']",NIGMS,CHILDREN'S HOSPITAL & RES CTR AT OAKLAND,P50,2018,395986,-0.05024066090836567
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9535429,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,315184,-0.003158192652285
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9316700,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoking', 'clinical imaging', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'improved', 'learning strategy', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2017,187400,-0.05127002175938055
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9307096,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pathology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genome-wide', 'genomic data', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'learning strategy', 'mild cognitive impairment', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2017,407500,-0.059292188103816375
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9335405,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modeling', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2017,372122,-0.017342924907988427
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9382936,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Architecture', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2017,225087,-0.008220780968050408
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9295208,K08HL136928,"['Affect', 'Bioinformatics', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Hereditary Disease', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'learning strategy', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2017,172800,-0.0034766174080266355
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9394127,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Behavior Therapy', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Health Care Research', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health care quality', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'symptomatic improvement', 'tool']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2017,338637,0.012572489144159316
"Towards automated phenotyping in epilepsy Over 5 million children and adults in the United States have had a diagnosis of epilepsy or a seizure disorder. However, treatment options for the epilepsies remain inadequate, because many patients suffer from uncontrolled seizures and from the negative side effects of treatment. A major obstacle to the faster development of new anti-convulsant therapies is the fact that rigorous preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). We propose to test if it is possible to perform objective, inexpensive and automated phenotyping of mice in various mouse models of acquired and genetic epilepsies. The approach rests on the recent recognition that mouse behaviors are structured in stereotyped modules at sub-second timescales that are arranged according to specific rules. These characteristic behavioral modules, and the transitions between them, can be identified without observer bias by combined 3D imaging and machine learning (ML) -assisted analytic methods. We propose to adopt this novel ML-assisted 3D video analysis technology to epilepsy research, in order to test if it can be used to identify mice with chronic temporal lobe epilepsy (TLE) during inter-ictal and ictal periods in two distinct experimental TLE models, and under various experimental conditions. In addition, we will also test whether the approach is able to automatically detect not only the overtly epileptic mice in a genetic model of severe childhood epilepsy (homozygous voltage-gated sodium channel β-subunit SCN1B-/- knock-out mice), but also distinguish the seemingly normal, non-epileptic, SCN1B+/- heterozygous mice from the wild-type controls. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user-independent, inexpensive analysis of acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will test if it is possible to objectively characterize epileptic phenotypes in mice using a breakthrough technology involving machine learning-assisted analysis of 3-dimensional video data of behavior. If successful, this innovative approach is expected to dramatically accelerate epilepsy research by enabling the objective, automated, inexpensive phenotyping of experimental animals to aid the testing of novel anticonvulsant therapies.",Towards automated phenotyping in epilepsy,9369284,R21NS102908,"['Adopted', 'Adult', 'Adverse effects', 'Animal Behavior', 'Animal Model', 'Animals', 'Anticonvulsants', 'Behavior', 'Behavioral', 'Characteristics', 'Child', 'Childhood', 'Chronic', 'Complex', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Epilepsy', 'Exhibits', 'Frequencies', 'Genetic', 'Genetic Models', 'Hippocampus (Brain)', 'Human', 'Human immunodeficiency virus test', 'Image', 'Knockout Mice', 'Machine Learning', 'Modeling', 'Monitor', 'Mus', 'Neurons', 'Observer Variation', 'Patients', 'Phenotype', 'Pilocarpine', 'Probability', 'Recurrence', 'Research', 'Rest', 'Seizures', 'Sodium Channel', 'Stereotyping', 'Structure', 'Syndrome', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Time', 'Translational Research', 'United States', 'Wild Type Mouse', 'analytical method', 'base', 'cost', 'evidence base', 'high throughput analysis', 'innovation', 'kainate', 'learning strategy', 'mouse model', 'novel', 'novel therapeutics', 'pre-clinical', 'voltage']",NINDS,STANFORD UNIVERSITY,R21,2017,197528,-0.015738692751631006
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis. PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.",Developing Classification Criteria for the Uveitides,9250167,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'age group', 'age related', 'aging population', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,400107,0.009569577569556075
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9287487,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multimodality', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2017,367055,-0.02994355284113031
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9382777,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Subgroup', 'Supervision', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'learning strategy', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2017,711253,0.001391543287281894
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9306122,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2017,306527,-0.018228613296468903
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9275314,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2017,142177,0.0001644448701526404
"Systems Level Causal Discovery in Heterogeneous TOPMed Data SYSTEMS LEVEL CAUSAL DISCOVERY IN HETEROGENEOUS TOPMED DATA ABSTRACT The advent of new technologies for collecting and analyzing multiple heterogeneous data streams from the same individual makes possible the detailed phenotypic characterization of diseases and paves the way for the development of individualized precision therapies. A major bottleneck in this process is the lack of robust, efficient and truly integrative analytic methods for such multi-modal data. This proposal builds on the ongoing efforts of our group in the area of causal learning in biomedicine. The objective of this application is to extend, modify and tailor our causal probabilistic graphical models to data typically collected by TOPMed projects, such as –omics data (SNPs, metabolomics, RNA-seq, etc), imaging, patients' history, and clinical data. COPDGene® is one of the TOPMed projects and has generated datasets with those modalities for 10,000 patients with chronic obstructive pulmonary disease (COPD), the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with different characteristics. There is currently no satisfactory method for COPD subtyping or prediction of disease progression. In this project we will apply, test and validate our approaches on COPDGene® and another large independent COPD cohort. The extension and application of our methods to cross-sectional and longitudinal data will also allow us to investigate a number of important questions and aspects related to COPD. Mechanistically, we will investigate how SNPs, genes and their networks are causally linked to disease phenotypes. In pathology, we will identify conditional biomarkers, which will lead to disease sub-classification and identification of causal components in each subtype. In pathophysiology, we will identify features that are directly linked to lung function decline and outcome. We will make all our algorithms and results available to the community through web and public cloud interfaces. The deliverables will be (1) new probabilistic approaches for integration and analysis of multi-modal cross-sectional and longitudinal data, including SNPs, blood biomarkers, CT scans and clinical data; (2) new cloud-based server to make these approaches available to the research community; (3) results on the mechanism, pathology and pathophysiology of COPD facilitation and progression. To guarantee the success of the project we have assembled a team of experts in genomics, machine learning, cloud computing and COPD. This cross- disciplinary team project will have a positive impact beyond the above deliverables, since the generality of our approaches makes them applicable to any disease. We expect that during this U01 we will have the opportunity to collaborate with other teams in the TOPMed consortium to help them investigate the causes of their corresponding disease phenotypes. We do believe that data integration in a single probabilistic framework will be in the heart of precision medicine strategies in the future, when massive high-throughput data collection will become a routine diagnostic and prognostic procedure in all hospitals. PROJECT NARRATIVE Current technologies for high-throughput biomedical data collection allow the interrogation of multiple modalities from a single patient. New promising analytical methods started emerging, which can analyze those multi-modal data in a holistic way. Chronic obstructive pulmonary disease (COPD) constitutes the third leading cause of death and a major cause of disability and health care costs in the US. The prevailing view is that COPD is a syndrome, consisting of multiple diseases with their own characteristics. There is currently no satisfactory method for COPD subtyping. We will apply, test and validate new probabilistic approaches on two cohorts of COPD patients. We will investigate the mechanisms of disease facilitation; we will identify patient cohorts with specific characteristics (disease subtypes); and investigate risk factors and causal variants for the disease progression in each subtype.  ",Systems Level Causal Discovery in Heterogeneous TOPMed Data,9310591,U01HL137159,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Biological Models', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Data', 'Data Collection', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Functional Imaging', 'Functional disorder', 'Funding', 'Future', 'Genes', 'Genetic Determinism', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Health Care Costs', 'Heart', 'Hospitals', 'Image', 'Individual', 'Internet', 'Learning', 'Lifting', 'Link', 'Machine Learning', 'Methods', 'Modality', 'Modeling', 'Molecular', 'Morphology', 'Outcome', 'Outcome Assessment', 'Pathology', 'Patients', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Physiological', 'Precision therapeutics', 'Procedures', 'Process', 'Pulmonology', 'Recording of previous events', 'Research', 'Research Personnel', 'Respiratory physiology', 'Risk', 'Risk Factors', 'Science', 'Stream', 'Subgroup', 'Syndrome', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Tissues', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'X-Ray Computed Tomography', 'analytical method', 'base', 'clinical imaging', 'clinically relevant', 'cloud based', 'cohort', 'computer science', 'cost effective', 'data integration', 'disability', 'disease phenotype', 'disorder subtype', 'graphical user interface', 'high throughput technology', 'innovation', 'longitudinal dataset', 'medical schools', 'metabolomics', 'mortality', 'multimodality', 'new technology', 'novel', 'outcome forecast', 'precision genomic medicine', 'precision medicine', 'prognostic', 'repository', 'success', 'tool', 'transcriptome sequencing', 'user-friendly']",NHLBI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2017,607934,0.011405729458092371
"Repurposing pyronaridine as a treatment for the Ebola virus Summary In 2014, the outbreak of the Ebola virus (EBOV) in West Africa highlighted the need for broad-spectrum antiviral drugs for this and other emerging viruses. Several groups had previously performed high throughput screens in 2013 and identified FDA approved drugs (amodiaquine, chloroquine, clomiphene and toremifene) with in vitro growth inhibitory activities against EBOV. We used these compounds to create a computational pharmacophore to identify additional compounds to test in vitro. In addition, data from a published large scale high throughput screen performed by SRI International and Texas Biomedical Research Institute was used to create machine learning models and then subsequently used to score clinical compounds for testing. We have published on how these combined methods identified 3 compounds for testing which were ultimately found to be nM in vitro. One of these compounds is an antimalarial approved in Europe called pyronaridine. We propose to characterize the ADME and PK properties of this compound prior to determining its efficacy in a mouse model of the Ebola virus infection. Therefore the Aims of this R21 proposal will fill some of the gaps inherent in the published data on pyronaridine so far: Aim 1. Perform preclinical in vitro characterization of pyronaridine. Aim 2. Formulate pyronaridine and perform PK studies in mouse. Aim 3. In vitro characterization of pyronaridine against multiple EBOV strains and in vivo efficacy in the mouse model of Ebola virus infection. The results of these aims will determine go/no go criteria for pursuing larger animal studies in non-human primates prior to clinical studies. In the light of a recent paper in the New England Journal of Medicine showing a clinical observation that EBOV patients treated with artesunate-amodiaquine had a 31% higher survival rate than those treated with artemether- lumefantrine 2, there will be considerable interest in evaluating antimalarials against Ebola. Our proposal to consider testing the efficacy in the mouse EBOV model using pyronaridine (which is used as artesunate- pyronaridine (Pyramax) and would be readily accessible in the clinic), presents a rapid approach to leverage the aforementioned clinical observations with a more potent compound. Pyronaridine also has additional benefits of tolerability which may be important in this patient population. Narrative Preliminary clinical data showed that Ebola virus (EBOV) patients treated with the antimalarials artesunate- amodiaquine had a higher survival rate than those treated with artemether-lumefantrine, in agreement with the in vitro EC50 for amodiaquine EC50 of 2.6µM. The antimalarial pyronaridine, a structural analog of amodiaquine, was identified by a computational repurposing strategy and further shown to have an EC50 of 420 nM against EBOV in vitro. We now propose to fully characterize this compound using standard preclinical ADME assays prior to mouse pharmacokinetic analysis, determine broad-spectrum applicability against multiple EBOV strains and ultimately in vivo efficacy testing in the mouse Ebola virus model prior to testing in a non-human primate model. Our aim is to show whether Pyronaridine is a viable clinical candidate to treat patients infected with EBOV.",Repurposing pyronaridine as a treatment for the Ebola virus,9357736,R21TR001718,"['Africa', 'Agreement', 'Amodiaquine', 'Animal Model', 'Animals', 'Antimalarials', 'Antiviral Agents', 'Area', 'Babesia', 'Behavioral', 'Binding Proteins', 'Biological Assay', 'Biological Availability', 'Biomedical Research', 'Blood specimen', 'Body Weight decreased', 'Bolus Infusion', 'Cessation of life', 'China', 'Chloroquine', 'Chloroquine resistance', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clomiphene', 'Control Groups', 'Data', 'Data Set', 'Disease', 'Disease Outbreaks', 'Dose', 'Drug Kinetics', 'Ebola virus', 'Ensure', 'Enzymes', 'Erythrocytes', 'Europe', 'European', 'FDA approved', 'Family', 'Female', 'Filoviridae', 'Filovirus', 'Formulation', 'Growth', 'Half-Life', 'Hour', 'In Vitro', 'Inbred BALB C Mice', 'Infection', 'International', 'Intestines', 'Journals', 'Knowledge', 'Lethal Dose 50', 'Libraries', 'Liver', 'Machine Learning', 'Malaria', 'Mannich Bases', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Mus', 'Natural Products', 'New England', 'Oral', 'Paper', 'Patients', 'Permeability', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Plasma', 'Plasma Proteins', 'Plasmodium falciparum', 'Plasmodium vivax', 'Property', 'PubChem', 'Publishing', 'Research Institute', 'Route', 'Solubility', 'Survival Rate', 'Techniques', 'Testing', 'Texas', 'Time', 'Toremifene', 'Toxic effect', 'Trypanosoma cruzi', 'Virus', 'Virus Diseases', 'Virus Inhibitors', 'Vivax Malaria', 'Whole Blood', 'analog', 'artemether', 'artesunate', 'base', 'benflumetol', 'clinical candidate', 'design', 'efficacy testing', 'high throughput screening', 'in vitro testing', 'in vivo', 'inhibitor/antagonist', 'interest', 'intraperitoneal', 'male', 'mouse model', 'neurotoxicity', 'nonhuman primate', 'patient population', 'pharmacophore', 'pre-clinical', 'preclinical study', 'prevent', 'pyronaridine', 'research clinical testing', 'response', 'treatment duration']",NCATS,"COLLABORATIONS PHARMACEUTICALS, INC.",R21,2017,312390,-0.0022008814423956076
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9279143,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronaviridae', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2017,202814,0.023867217146273573
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Project Summary:  Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer's disease, Parkinson's disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these chal- lenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymor- phisms, whole brain neuroimaging, bioﬂuids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mecha- nisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Speciﬁcally, in Aim 1, we develop efﬁcient methods for multi-level semiparametric transformation mod- els to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efﬁciency. Our methods do not rely on full pedigree genotyping and provide family-speciﬁc substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic sys- tems through ordinary differential equations with random inﬂections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker proﬁles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efﬁcient machine learning methods to identify predictive markers, estimate optimal individu- alized therapies, and identify subgroups who may receive the greatest beneﬁt from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art ana- lytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research com- munity. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9308279,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Neurodegenerative Disorders', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'learning strategy', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2017,366940,0.01447974297471314
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9250803,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'experimental study', 'genomic data', 'hazard', 'high dimensionality', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'predictive modeling', 'prevent', 'public health relevance', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2017,66026,-0.006110679075191399
"Statistical methods for biosignals with varying domains DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2 PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4",Statistical methods for biosignals with varying domains,9297324,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Multimodal Imaging', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical method', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'indexing', 'kinematics', 'member', 'new technology', 'public health relevance', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2017,404000,-0.025409315879153616
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9357870,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Protein Hybridization', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2017,720717,-0.02201862869038289
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9274155,R01AR068456,"['Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'screening', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'tool', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2017,269514,0.0025080198162745434
"Statistical Methods for Selection and Evaluation of Biomarkers DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particular, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pancreatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public. PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.",Statistical Methods for Selection and Evaluation of Biomarkers,8996183,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Custom', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of pancreas', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Screening for cancer', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'biomarker evaluation', 'burden of illness', 'candidate marker', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'flexibility', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'outcome prediction', 'patient biomarkers', 'patient population', 'predictive marker', 'programs', 'public health relevance', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect', 'treatment response']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2017,323753,-0.04414121807199599
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9349367,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2017,326784,0.01673781722097007
"Statistical Methods for Multilevel Multivariate Functional Studies Abstract  While imaging studies are widely used in clinical practice and research, the number of neuroimaging- based biomarkers is small. For example, in clinical trials of immunomodulatory therapies for MS, the only commonly used imaging biomarkers are the total lesion volume and the number of new and en- hancing lesions. These biomarkers are essential, but do not capture the recovery process of lesions, which is thought to decline in more severe, progressive disease. The partial or complete recovery of lesions may depend both on the ability of the brain to heal and on external factors, such as treat- ment or environmental and behavioral exposures. In this proposal we take the natural next step of proposing imaging biomarkers for MS based on the formation and change of lesions as observed on multi-sequence structural MRIs. To solve this problem we propose to address several general method- ological problems: 1) develop models and methods for the longitudinal analysis of several images of the same brain; 2) identify and estimate the length of history that is necessary to estimate recovery; 3) study the association with known biomarkers of the disease (in this case total volume and number of new and enhancing lesions); 4) develop methods that are robust to changes in imaging protocols that inevitably arise in longitudinal neuroimaging studies; and 5) develop the computational tools that allow for sophisticated methods to be implemented seamlessly in practice. While our scientiﬁc problem is focused, the proposed statistical methods are general and can be applied to a wide variety of longitu- dinal neuroimaging studies. For example, there are many ongoing longitudinal neuroimaging studies, including the ADNI, AIBL, HBC, and MISTIE, where our methods could be used to study subtle or large changes in lesions or in white and gray matter intensities. Project narrative. The project provides statistical analysis methods for quantiﬁcation of the evolution in the intensity of brain lesions on multi-sequence Magnetic Resonance Imaging (MRI). Methods are motivated by the need to develop new neuroimaging-based biomarkers for multiple sclerosis (MS), but can be applied to other types of brain diseases including stroke, Alzheimer disease, and cancer.",Statistical Methods for Multilevel Multivariate Functional Studies,9378514,R01NS060910,"['Accounting', 'Address', 'Alzheimer&apos', 's Disease', 'Behavioral', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical Research', 'Clinical Trials', 'Computer software', 'Data', 'Databases', 'Disease', 'Enhancing Lesion', 'Event', 'Evolution', 'Funding', 'Grant', 'Graph', 'Image', 'Incidence', 'Length', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mediation', 'Mediator of activation protein', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'Multiple Sclerosis Lesions', 'Names', 'Natural History', 'Nature', 'Online Systems', 'Pattern', 'Population Heterogeneity', 'Problem Solving', 'Process', 'Progressive Disease', 'Protocols documentation', 'Randomized', 'Recording of previous events', 'Recovery', 'Research', 'Sampling', 'Statistical Data Interpretation', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'clinical practice', 'computerized tools', 'design', 'gray matter', 'healing', 'high dimensionality', 'imaging biomarker', 'imaging study', 'immunoregulation', 'improved', 'insight', 'longitudinal analysis', 'longitudinal database', 'neuroimaging', 'non-Gaussian model', 'personalized approach', 'repaired', 'software development', 'treatment response', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R01,2017,659178,-0.025533456273581518
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9320865,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multimodal Imaging', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'analytical tool', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'high dimensionality', 'imaging Segmentation', 'imaging modality', 'imaging study', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'spatiotemporal', 'study population', 'terabyte', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2017,347156,0.018858061844145315
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS. PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,9265960,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Lateral', 'Lead', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinical diagnostics', 'clinical predictors', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'individual patient', 'insight', 'interest', 'multimodality', 'neuroimaging', 'neuroimaging marker', 'neurotransmission', 'novel', 'outcome forecast', 'predictive modeling', 'prevent', 'prognostic', 'public health relevance', 'rapid diagnosis', 'response', 'screening', 'spinal tract', 'treatment response', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2017,577180,-0.003840596965418296
"Optimizing electrical impedance myography outcomes through data mining Project summary  Electrical impedance myography (EIM) is a non-invasive technology for the assessment of muscle that is based on the application of a weak, high frequency electrical current to a muscle and the measurement of the resulting surface voltages. The further development and application of EIM remains the main business focus of Skulpt, Inc, a small business concern based in Boston and San Francisco (Specific Aims just say San Francisco). Alterations to the condition of the muscle, including myocyte atrophy, fat and connective tissue deposition, and inflammation all alter the EIM data in predictable and consistent ways. To date, through Skulpt, EIM has been applied as a potential biomarker for assessing disease progression and response to therapy in a wide variety of neuromuscular disorders, including amyotrophic lateral sclerosis, Duchenne muscular dystrophy, and spinal muscular atrophy, as well as other disorders that impact muscle condition, such as disuse atrophy and sarcopenia (age related muscle loss); over 1000 people have been studied with Skulpt’s EIM technology. Whereas the results of these applications are promising, the analytic approaches taken to the data sets have been fairly basic, utilizing only simple single frequency or simplistic multifrequency values. However, with every single muscle measurement, over 240 individual data points are acquired at different frequencies, different depths of muscle penetration, and at different angles to the major muscle fiber direction. Moreover, each of the above studies has been done in isolation, and thus how results differ between diseases is unknown. Given the plethora of data, applying more sophisticated analytic approaches has the potential of yielding improved EIM measures. Moreover, collaborators have already collected an associated wealth of animal EIM data that will help further inform this analysis. Thus, in this proposed Phase 1 SBIR, we plan to apply a variety of data mining techniques to the vast set of data already accumulated at Skulpt, Inc such that improved EIM outcomes can be developed and implemented. In Specific Aim 1, we will study human data across all disease types evaluated to determine which data sets are most effective at discriminating diseased from healthy muscle as well as distinguishing between diseases. In Specific Aim 2, we will focus on finding the metrics that are most sensitive to the degree of muscle pathology in a specific disease. In both of these aims, we will evaluate how these new metrics are mirrored in already obtained animal data. In Specific Aim 3, we will study these metrics in a new set of data (a test set) that was not used to develop the analytical paradigms so as to ensure their robustness. With the conclusion of this work, we will plan to pursue a Phase 2 SBIR that will focus on the development of a software suite to assist in EIM data interpretation based upon these results followed by a prospective observational clinical study to evaluate the efficacy of these newly developed metrics for disease diagnosis and tracking of progression/response to therapy. Project Narrative  Electrical impedance myography (EIM) is a non-invasive technology for the assessment of muscle that remains the main focus of Skulpt, Inc. Considerable EIM data has already been collected in a variety of neuromuscular diseases. In this study, the investigators plan to perform a more detailed analysis of all data collected to date (so-called “data mining”), such that improved EIM outcomes can be developed that will be applied to future studies.",Optimizing electrical impedance myography outcomes through data mining,9466075,R43AR073114,"['Age', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Animals', 'Area', 'Atrophic', 'Back Pain', 'Boston', 'Businesses', 'Categories', 'Characteristics', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Set', 'Deposition', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Disease model', 'Disease remission', 'Disuse Atrophy', 'Duchenne muscular dystrophy', 'Electrodes', 'Electrophysiology (science)', 'Ensure', 'Fatty acid glycerol esters', 'Fiber', 'Frequencies', 'Functional disorder', 'Funding', 'Future', 'Glycogen storage disease type II', 'Health', 'Inclusion Bodies', 'Individual', 'Inflammation', 'Laboratories', 'Machine Learning', 'Measurement', 'Measures', 'Medical Technology', 'Methods', 'Mining', 'Muscle', 'Muscle Cells', 'Muscle Fibers', 'Muscular Dystrophies', 'Musculoskeletal', 'Myography', 'Myopathy', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Penetration', 'Phase', 'Play', 'Positioning Attribute', 'Radiculopathy', 'Research Personnel', 'Role', 'San Francisco', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Technology Assessment', 'Testing', 'Tissues', 'Validation', 'Work', 'animal data', 'base', 'commercialization', 'data mining', 'disease classification', 'disease diagnosis', 'electric impedance', 'human data', 'improved', 'indexing', 'neuromuscular', 'potential biomarker', 'prospective', 'response', 'sarcopenia', 'voltage']",NIAMS,"MYOLEX, INC.",R43,2017,149998,0.005736281518941201
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9180344,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'base', 'career development', 'cigarette smoking', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'improved', 'learning strategy', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2016,188040,-0.05127002175938055
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9142240,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modeling', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2016,380459,-0.017342924907988427
"Identifying Huntington's disease markers by modern statistical learning methods. DESCRIPTION (provided by applicant): Designing an efficient Huntington's disease (HD) early intervention clinical trial for individuals who have an expanded CAG repeats in the huntingtin gene requires identifying and combining clinical, biological, cognitive, and brain imaging markers to accurately distinguish among subjects who will have a diagnosis during a given intervention period and those who will not, and to track early changes in the disease course. The goal of this project is to identify sensitive biomarkers for HD risk stratification, indexing disease progression, and developing clinical trial endpoints. The proposal directly adheres to ""2P's"" of the NIH New Strategic Vision of the ""4P's"" of Medicine: they will offer promising ways to predict when the disease will develop; and increase the capacity to personalize early intervention based on the informative patient-specific markers our models identify. Combining biomarkers to predict HD onset and progression is an essential step in a continuum of research for development of disease-modifying therapies. Composite markers and their risk profiles created from our model will offer quantitative way to monitor and compare potential interventions. Evidence collected from these comparisons will advance the development of efficacy studies in premanifest HD, where neuroprotective treatments would be most beneficial. We develop and apply a series of cutting-edge statistical learning methods based on support vector machine (SVM), variable selection, and dimension reduction to achieve these goals. These modern statistical methods designed for correlated big data have quickly emerged as among the most successful tools for hypothesis generation, classification and prediction in biomedical studies. However, they have not been introduced to HD biomarker research. In aim 1, using counting process, we propose SVM to handle time-to-event outcomes (e.g., time-to-HD-diagnosis) to combine markers into risk scores to discriminate subjects who will experience HD onset in the immediate future from those who will not,  based on their personalized features. Although SVM is well studied for binary outcomes, it is far less explored for time-to-event outcomes. We fill this gap in knowledge. In aim 2, we propose new learning methods for longitudinal outcomes to combine markers that modify the course of HD signs to monitor disease process and distinguish subjects with rapid progression from those with slower progression. In aim 3, we propose to use novel and robust performance measures to compare derived combined markers with existing disease indices and key markers. These aims will fundamentally advance our understanding of markers linked to HD onset and progression. The creation of statistical models for composite markers and risk profiles is especially useful in: (1) offering quantitative ways to monitor and compare potential interventions, and (2) improving power of efficacy studies targeted at premanifest individuals by narrowing the predictive interval which leads to future clinical trials that can be made shorter with fewer subjects. Finally, our improved predictions of HD onset and progression will provide more informative genetic counseling sessions for pre-symptomatic subjects at risk of HD. PUBLIC HEALTH RELEVANCE:  The goal of Huntington's disease (HD) research is to develop experimental therapeutics to delay onset or slow disease progression, and to provide different treatment regimens at each disease stage. To meet this goal, this proposal develops and applies a series of advanced statistical approaches to rank and combine clinical, behavioral, and brain imaging markers to predict HD diagnosis in premanifest subjects during a given time period and to measure disease progression. The creation of model for composite markers and risk profiles is useful in offering quantitative ways to monitor and compare interventions and powering clinical trials for premanifest HD individuals.",Identifying Huntington's disease markers by modern statistical learning methods.,9119862,U01NS082062,"['Accounting', 'Address', 'Advanced Development', 'Age', 'Algorithms', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain imaging', 'CAG repeat', 'Classification', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Intervention', 'Early intervention trials', 'Event', 'Future', 'Generations', 'Genes', 'Genetic Counseling', 'Genetic screening method', 'Goals', 'Health', 'Huntington Disease', 'Huntington gene', 'Image', 'Individual', 'Intervention', 'Investigational Therapies', 'Knowledge', 'Link', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Modeling', 'Monitor', 'Motor', 'Motor Manifestations', 'Mutation', 'Odds Ratio', 'Onset of illness', 'Outcome', 'Patients', 'Penetrance', 'Performance', 'Population', 'Predictive Value', 'Prevention', 'Process', 'ROC Curve', 'Research', 'Risk', 'Risk Marker', 'Series', 'Staging', 'Statistical Methods', 'Statistical Models', 'Stratification', 'Techniques', 'Testing', 'Time', 'Treatment Protocols', 'United States National Institutes of Health', 'Vision', 'Work', 'affection', 'base', 'burden of illness', 'cognitive testing', 'design', 'disease diagnosis', 'disorder risk', 'experience', 'functional outcomes', 'hazard', 'high risk', 'imaging biomarker', 'improved', 'indexing', 'interest', 'learning strategy', 'meetings', 'nervous system disorder', 'novel', 'research and development', 'tool']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2016,356244,-0.00017457230021442763
"Developing Classification Criteria for the Uveitides ﻿    DESCRIPTION (provided by applicant): The uveitides are a collection of ~30 distinct diseases characterized by intraocular infection. Each disease has its own features, course, treatment, and prognosis. Traditionally, the uveitides have been grouped by the primary anatomic site of inflammation as anterior uveitis, intermediate uveitis, posterior uveitis, and panuveitis. However, there are substantial limitations to this ""lumping"" of diseases. For example, among the posterior uveitides, some (e.g. toxoplasmic retinitis and cytomegalovirus retinitis) are infectious and require treatment with antimicrobial/antiviral agents, some are chronic, presumed immune-mediated diseases that require immunosuppression (e.g. birdshot chorioretinitis, multifocal choroiditis, serpiginous choroiditis), and a few are self-limited, spontaneously-remitting diseases with a good prognosis (e.g. acute posterior multifocal placoid pigment epitheliopathy and multiple evanescent white dot syndrome). As such precise diagnosis is critical for research, including epidemiology, translational pathogenesis research, outcomes research, and disease specific clinical trials. Classification criteria are a type of ""diagnostic"" criteria used for reserch purposes. Although classification criteria seek to optimize sensitivity and specificity, when a trade-off is required, they emphasize specificity in order to ensure that a homogeneous group of patients is being studied. A precise phenotype is required particularly for genomic risk factor studies of complex disorders and translational pathogenesis research, as inclusion of other diseases with different risk factors and disease mechanisms would confound the results. Currently there are no widely-accepted and validated classification criteria for any of the uveitides. Preliminary data indicate ""fair to moderate"" agreement at best on the independent diagnosis of any one case by uveitis experts (κ's 0.27-0.40), but the ability of committees to reach agreement on the diagnosis of >98% of cases. The goal of the ""Developing Classification Criteria for the Uveitides"" project is for the Standardization of Uveitis Nomenclature (SUN) Working Group to develop classification criteria for the 25 leading uveitides using a formal, rigorous approach. There are 4 phases to the project: 1) informatics, to develop a standardized terminology; 2) case collection, to develop a preliminary database of ~250 cases of each disease; 3) case selection, to select at least 150-200 cases of each disease that are generally accepted to be the disease (using formal consensus techniques) from the preliminary database into a final database; and 4) data analysis, using machine learning approaches, of the final database to develop a parsimonious set of criteria for each disease that minimizes misclassification. The informatics and case collection phases of the Project are complete. The case selection phase is well underway and uses online voting and consensus conference calls to achieve supermajority acceptance on all cases included in the final database. The goals of this application are to complete case selection and data analysis and develop classification criteria for the 25 of the major uveitides. These results are crucial to future clinical research i the field of uveitis.         PUBLIC HEALTH RELEVANCE:  Collectively, the uveitides are the 5th leading cause of blindness in the U.S., and the cost of treating them is estimated to be similar to that of treating diabetic retinopathy. Because uveitis occurs in all age groups, including children and working-age adults, there is a greater potential for years of vision lost than with age- related diseases. Clinical research in the field of uveitis has been hampered by diagnostic imprecision and a lack of widely-accepted and validated classification criteria, the development of which is the goal of this application; these criteria are needed urgently to advance epidemiology, genomic research, translational pathogenesis research, outcomes research, and disease-specific clinical trials.            ",Developing Classification Criteria for the Uveitides,9081760,R01EY026593,"['Acute', 'Adult', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Anterior uveitis', 'Antiviral Agents', 'Blindness', 'Child', 'Choroiditis', 'Chronic', 'Classification', 'Clinical Research', 'Clinical Trials', 'Collection', 'Complex', 'Consensus', 'Cytomegalovirus Retinitis', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Enrollment', 'Ensure', 'Epidemiology', 'Future', 'Genomics', 'Goals', 'Immune', 'Immunosuppression', 'Infection', 'Inflammation', 'Informatics', 'Intermediate Uveitis', 'Machine Learning', 'Mediating', 'Nomenclature', 'Outcomes Research', 'Panuveitis', 'Pathogenesis', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Pigments', 'Population', 'Posterior Uveitis', 'Publications', 'Research', 'Retinitis', 'Risk Factors', 'Sensitivity and Specificity', 'Specificity', 'Standardization', 'Syndrome', 'Techniques', 'Terminology', 'Translational Research', 'United States', 'Uveitis', 'Vision', 'Visual impairment', 'Voting', 'Work', 'age group', 'age related', 'antimicrobial', 'birdshot chorioretinitis', 'cost', 'outcome forecast', 'public health relevance', 'symposium', 'tool', 'web page', 'working group']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428590,0.009569577569556075
"Statistical Modeling of Complex Traits in Genetic Reference Super-Populations DESCRIPTION (provided by applicant):     Genetic crosses in model organisms play an essential role in understanding the heritable architecture of medically relevant phenotypes. Traditionally, such crosses have tended to be on a small scale with either limited power to detect genetic effects or limited resolution to localize causal variants. Recently, however, the emergence of larger-scale interdisciplinary research, cheaper genotyping and parallel advances in human genetics, have spurred the development of more sophisticated and powerful experimental designs. Genetic Resource Populations (GRPs) use economies of scale to provide cost-effective and replicable platforms for genetic studies. This project concerns the largest, most ambitious GRP in mouse genetics to date, the Collaborative Cross (CC), and a series of crosses and designs related to or derived from it: the Diversity Outbred (DO) cross, the CC Recombinant Inbred Cross (CC-RIX) and the diallel. Experiments on each separate cross provide distinct information about the heritable architecture of a target complex disease. In combination, this Genetic Reference Super-Population (GRSP) potentially provides an unparalleled basis for cross-study replication and integration in mouse genetics. This project aims to develop statistical methods that advance the current state of complex trait analysis of these populations separately, and, by exploiting the unique structure that connects them, proposes to develop a statistical framework that allows for their joint use.  Aim 1 develops a Bayesian probabilistic framework for haplotype-based analysis of quantitative trait loci (QTL). Aim 1a develops a statistical software module for flexible haplotype-based analysis, which can be ex- tended by the researcher to model a rich variety of designs and disease types. Aim 1b will adapt machine learning techniques to provide posterior inference of the allelic series of a QTL. Aim 1c will incorporate Bayesian modeling of polygenic effects.  Aim 2 and 3 concern joint analysis, building on the foundation set by Aim 1. Aim 2 develops methods to optimize experimental design of follow-up studies in one population given results from another. Aim 2a uses the diallel to inform design of CC/CC-RIX/DO experiments. Aim 2b uses partial data on CC/CC-RIX/DO to guide collection of additional data. Aim 3 explores models for jointly analyzing multiple populations in the GRSP, using complementary datasets to stabilize analysis at single QTL (Aim 3a) and across multiple QTL (Aim 3b).  These aims address specific and persistent challenges in the cost-effective design and efficient analysis of multiparent genetic data, in particular the CC, DO, CC-RIX and diallel. The project will generate tools useful for a wide range of model organism crosses and can be applied to the genetic study of any complex disease. The proposed research will lead to improvements in the analysis and design of genetic studies on animal models of human disease. Because the project focuses on statistical methodology applied to experimental mouse populations, the scientific output of the project is expected to be applicable to basic research focusing on any medical condition that can be studied in the mouse.",Statistical Modeling of Complex Traits in Genetic Reference Super-Populations,9126587,R01GM104125,"['Accounting', 'Address', 'Affect', 'Animal Model', 'Anxiety', 'Architecture', 'Asthma', 'Basic Science', 'Bayesian Modeling', 'Biomedical Research', 'Collection', 'Complex', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Environmental Risk Factor', 'Equilibrium', 'Etiology', 'Experimental Designs', 'Foundations', 'Funding', 'Generations', 'Genetic', 'Genetic Crosses', 'Genetic Programming', 'Genetic study', 'Genotype', 'Haplotypes', 'Heart Diseases', 'Human Genetics', 'Hybrids', 'Inbreeding', 'Influentials', 'Interdisciplinary Study', 'Joints', 'Lead', 'Machine Learning', 'Maps', 'Medical', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Output', 'Pattern', 'Phenotype', 'Play', 'Plug-in', 'Population', 'Population Analysis', 'Quantitative Trait Loci', 'Randomized', 'Recombinants', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Variant', 'Weight', 'base', 'cost', 'cost effective', 'design', 'disease phenotype', 'experience', 'flexibility', 'follow-up', 'genetic resource', 'human disease', 'insight', 'interest', 'population based', 'prospective', 'research study', 'response', 'simulation', 'success', 'tool', 'trait']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2016,241086,-0.023139230964232756
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9099858,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2016,306754,-0.018228613296468903
"Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores ﻿    DESCRIPTION (provided by applicant): Structural analysis of large polysaccharides remains challenging in glycobiology. The problem is especially acute when polysaccharides in question are glycosaminoglycans (GAGs). GAGs are large, linear, sulfated polysaccharides ubiquitous to all mammals. Interests in GAG structures stem from GAGs' diverse biological activities that govern phenomena such as tissue development/regeneration, inflammation, blood coagulation and amyloid plaque formation. Abnormal GAG structures have also been associated with the development of a number of diseases, notably cancer and inflammation. As a result, there has been a desire to understand how GAG structures correlate with their biological activities, especially how the distribution of sulfate groups along the chain influence their interactions with GAG-binding proteins. However, GAGs' large size and complex sulfation patterns make analysis of intact GAG chains by conventional ensemble analytical techniques difficult, if not impossible. Here we propose to develop a single molecule sequencer for analysis of polysaccharides using the recognition tunneling nanopore (RTP) device currently under development for ""$1000 genome"" project as a template. With the R21 grant, we will demonstrate the feasibility by carrying out pre-requisite work needed to achieve single molecule sequencing of intact GAG chains using RTP. A RTP device incorporates a nanopore with a tunneling nanogap that contains two electrodes functionalized with recognition molecules capable of forming transient complexes with functional groups on a polymeric chain as it translocates the nanopore, thus generating electrical signals. Single molecule sequencing of GAG chains proposed here circumvents the need to obtain homogeneous samples of GAGs, greatly reducing complexity of sample preparation. GAG analysis by RT devices also does not have the size limitations of most of the existing analytical techniques, and the solid state device planned here are economical to manufacturer and operate. In this application, we aim to carry out pilot studies needed to make GAG sequencing by RTPs feasible: (1) we will investigate the translocation of size defined sulfated GAG fragments through nanopores to optimize the translocation efficiency of GAG ligands as well as to understand the influence of GAG sulfation density and GAG size on their translocation efficiency and speed; (2) we will carry out recognition tunneling experiments on sulfated GAG disaccharides as well as trisaccharides so these signals of GAGs can be analyzed using machine learning algorithms to identify unique signatures needed to detect the presence of these sulfation motifs in longer GAG chains. Completion of these aims will provide all the knowledge required for correct interpretations of RT signals produced by GAG translocation and sets the stage for sequencing of intact GAG chains by RT devices. PUBLIC HEALTH RELEVANCE:     Work proposed here will allow single molecule sequencing of glycosaminoglycan polysaccharides using an electronic chip with a high speed and low cost for the first time. Glycosaminoglycans have important pharmacological properties and are modulators of critical biological phenomena such as tissue development/regeneration and inflammation. Determination of their sequence structures will allow better understanding of how organisms control these physiological events through glycosaminoglycans.",Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores,9109642,R21GM118339,"['Acute', 'Algorithms', 'Amino Acids', 'Architecture', 'Binding Proteins', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Blood coagulation', 'Cells', 'Charge', 'Chemistry', 'Complex', 'Coupled', 'DNA', 'DNA Sequence', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Disaccharides', 'Disease', 'Electrodes', 'Electronics', 'Electrons', 'Environment', 'Enzymes', 'Event', 'Genome', 'Glycobiology', 'Glycosaminoglycans', 'Goals', 'Grant', 'Health', 'Imidazole', 'Individual', 'Inflammation', 'Inorganic Sulfates', 'Ions', 'Isomerism', 'Knowledge', 'Leukocyte Trafficking', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Mammals', 'Manufacturer Name', 'Mediating', 'Methods', 'Microbe', 'Natural regeneration', 'Neoplasm Metastasis', 'Oligosaccharides', 'Organism', 'Pattern', 'Physiological', 'Pilot Projects', 'Play', 'Polysaccharides', 'Preparation', 'Process', 'Property', 'Proteins', 'Publishing', 'Reader', 'Reading', 'Research', 'Role', 'Sampling', 'Senile Plaques', 'Side', 'Signal Transduction', 'Signaling Protein', 'Site', 'Speed', 'Staging', 'Structure', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Tissues', 'Trisaccharides', 'Unspecified or Sulfate Ion Sulfates', 'Work', 'amyloid formation', 'analytical method', 'base', 'cancer cell', 'cost', 'density', 'design', 'extracellular', 'functional group', 'interest', 'nanopore', 'polysulfated glycosaminoglycan', 'programs', 'research study', 'single molecule', 'solid state', 'stem', 'sugar', 'sulfation', 'therapeutic biomarker', 'tool']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2016,271743,-0.00879429186910205
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,9103177,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'learning strategy', 'meetings', 'population based', 'programs', 'whole genome']",NHGRI,STANFORD UNIVERSITY,R01,2016,300000,-0.003590735935668945
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9094553,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Effectiveness of Interventions', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Qualifying', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Staging', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'research study', 'response', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2016,141588,0.0001644448701526404
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9099895,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2016,418572,0.023867217146273573
"Repurposing pyronaridine as a treatment for the Ebola virus Summary In 2014, the outbreak of the Ebola virus (EBOV) in West Africa highlighted the need for broad-spectrum antiviral drugs for this and other emerging viruses. Several groups had previously performed high throughput screens in 2013 and identified FDA approved drugs (amodiaquine, chloroquine, clomiphene and toremifene) with in vitro growth inhibitory activities against EBOV. We used these compounds to create a computational pharmacophore to identify additional compounds to test in vitro. In addition, data from a published large scale high throughput screen performed by SRI International and Texas Biomedical Research Institute was used to create machine learning models and then subsequently used to score clinical compounds for testing. We have published on how these combined methods identified 3 compounds for testing which were ultimately found to be nM in vitro. One of these compounds is an antimalarial approved in Europe called pyronaridine. We propose to characterize the ADME and PK properties of this compound prior to determining its efficacy in a mouse model of the Ebola virus infection. Therefore the Aims of this R21 proposal will fill some of the gaps inherent in the published data on pyronaridine so far: Aim 1. Perform preclinical in vitro characterization of pyronaridine. Aim 2. Formulate pyronaridine and perform PK studies in mouse. Aim 3. In vitro characterization of pyronaridine against multiple EBOV strains and in vivo efficacy in the mouse model of Ebola virus infection. The results of these aims will determine go/no go criteria for pursuing larger animal studies in non-human primates prior to clinical studies. In the light of a recent paper in the New England Journal of Medicine showing a clinical observation that EBOV patients treated with artesunate-amodiaquine had a 31% higher survival rate than those treated with artemether- lumefantrine 2, there will be considerable interest in evaluating antimalarials against Ebola. Our proposal to consider testing the efficacy in the mouse EBOV model using pyronaridine (which is used as artesunate- pyronaridine (Pyramax) and would be readily accessible in the clinic), presents a rapid approach to leverage the aforementioned clinical observations with a more potent compound. Pyronaridine also has additional benefits of tolerability which may be important in this patient population. Narrative Preliminary clinical data showed that Ebola virus (EBOV) patients treated with the antimalarials artesunate- amodiaquine had a higher survival rate than those treated with artemether-lumefantrine, in agreement with the in vitro EC50 for amodiaquine EC50 of 2.6µM. The antimalarial pyronaridine, a structural analog of amodiaquine, was identified by a computational repurposing strategy and further shown to have an EC50 of 420 nM against EBOV in vitro. We now propose to fully characterize this compound using standard preclinical ADME assays prior to mouse pharmacokinetic analysis, determine broad-spectrum applicability against multiple EBOV strains and ultimately in vivo efficacy testing in the mouse Ebola virus model prior to testing in a non-human primate model. Our aim is to show whether Pyronaridine is a viable clinical candidate to treat patients infected with EBOV.",Repurposing pyronaridine as a treatment for the Ebola virus,9204673,R21TR001718,"['Africa', 'Agreement', 'Amodiaquine', 'Animal Model', 'Animals', 'Antimalarials', 'Antiviral Agents', 'Area', 'Babesia', 'Behavioral', 'Binding Proteins', 'Biological Assay', 'Biological Availability', 'Biomedical Research', 'Blood specimen', 'Body Weight decreased', 'Bolus Infusion', 'Cessation of life', 'China', 'Chloroquine', 'Chloroquine resistance', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clomiphene', 'Control Groups', 'Data', 'Data Set', 'Disease', 'Disease Outbreaks', 'Dose', 'Drug Kinetics', 'Ebola virus', 'Ensure', 'Enzymes', 'Erythrocytes', 'Europe', 'European', 'FDA approved', 'Family', 'Female', 'Filovirus', 'Formulation', 'Growth', 'Half-Life', 'Hour', 'In Vitro', 'Infection', 'International', 'Intestines', 'Journals', 'Knowledge', 'Lethal Dose 50', 'Libraries', 'Liver', 'Machine Learning', 'Malaria', 'Mannich Bases', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Mus', 'Natural Product Drug', 'New England', 'Oral', 'Paper', 'Patients', 'Permeability', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Plasma', 'Plasma Proteins', 'Plasmodium falciparum', 'Plasmodium vivax', 'Property', 'PubChem', 'Publishing', 'Research Institute', 'Route', 'Solubility', 'Staging', 'Survival Rate', 'Techniques', 'Testing', 'Texas', 'Time', 'Toremifene', 'Toxic effect', 'Trypanosoma cruzi', 'Virus', 'Virus Diseases', 'Virus Inhibitors', 'Vivax Malaria', 'Whole Blood', 'analog', 'artemether', 'artesunate', 'base', 'benflumetol', 'bioactive natural products', 'design', 'efficacy testing', 'high throughput screening', 'in vitro testing', 'in vivo', 'inhibitor/antagonist', 'interest', 'male', 'mouse model', 'neurotoxicity', 'nonhuman primate', 'novel therapeutics', 'patient population', 'pharmacophore', 'pre-clinical', 'preclinical study', 'prevent', 'pyronaridine', 'research clinical testing', 'response', 'treatment duration']",NCATS,"COLLABORATIONS PHARMACEUTICALS, INC.",R21,2016,288801,-0.0022008814423956076
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9041640,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'genomic data', 'hazard', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2016,255295,-0.006110679075191399
"Statistical methods for biosignals with varying domains DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2 PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4",Statistical methods for biosignals with varying domains,9081248,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'imaging modality', 'indexing', 'kinematics', 'member', 'new technology', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2016,404000,-0.025409315879153616
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9106828,R01AR068456,"['Accounting', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cost', 'density', 'experience', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'screening', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'tool', 'treatment response']",NIAMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2016,31678,0.0025080198162745434
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9115248,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'study population', 'terabyte', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2016,347156,0.018858061844145315
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9135552,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2016,324169,0.01673781722097007
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS. PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,9052846,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Health', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Lateral', 'Lead', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Process', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'individual patient', 'insight', 'interest', 'meetings', 'neuroimaging', 'neurotransmission', 'novel', 'outcome forecast', 'predictive modeling', 'prevent', 'response', 'screening', 'spinal tract', 'treatment response', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2016,1,-0.003840596965418296
"A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans ﻿    DESCRIPTION (provided by applicant): Exercise is arguably the most potent approach we can take to defer physical decline associated with aging and to protect against late onset diseases such as diabetes, cancer, and Alzheimer's disease. Molecular understanding of how exercise benefits translate into healthy aging is thus of definitive medical interest. We study fundamental processes relevant to healthy aging in the 959-celled nematode C. elegans. Recently we made a fascinating discovery-C. elegans can exercise (swim) to exhibit training benefits, and appear to gain benefits by molecular pathways conserved in humans. Our initial model development opens up a new research area for understanding how tissue-specific and organism-wide health benefits are induced by exercise, and creates a novel paradigm for identifying exercise mimetic drugs that might promote healthy aging. To really harvest the potential of this model, we need to measure the strength of the tiny C. elegans. We collaborated to develop a strength test in which trained animals thread through a matrix of deformable pillars, and the extent of pillar deflection is used to calculate force. Our ""NemaFlex"" force detection device is the quantitative foundation with which we expect to break new ground in understanding exercise impact on healthy aging. Here we propose required development to enhance assay throughput and pursue applications that will not only anchor this technology as an essential component of C. elegans exercise evaluation but also accelerate studies on exercise biology and healthy aging in this powerful model. Aim 1 is to develop a novel high throughput tool for direct strength evaluation in C. elegans.  This aim will generate an essential tool for analysis of C. elegans strength at multiple life stages, define the exercise regimen that will become the anchor protocol in the field, and reveal features of training in this model. Aim 2 is to use NemaFlex to evaluate exercise mimetic drugs & to facilitate focused pilot genetic screens. This aim will establish critical proof-of-principle for genetic and drug discovery using the NemaFlex. Aim 3 is to initiate dissection of the functional and molecular relationship between exercise and healthy aging, grounded in NemaFlex force measures of training benefits.  To begin, we will test how optimized strength training tracks with a broad spectrum of healthspan indicators that decline with age, we will investigate impact of cessation of training on aging quality, and we will ask if exercise mimetic drugs extend healthspan in the absence of training. Our goals will create novel technology that for the first time permits facile quantitativ analysis of exercise adaptations in the powerful C. elegans genetic model. Accomplishment of our tractable aims will anchor a new subfield of genetic investigation of exercise and healthy aging that may influence design of interventions that broadly promote health and defer aging. PUBLIC HEALTH RELEVANCE: Exercise has a profound positive impact on health of the aging population in that it protects against age-associated diseases including cancer, diabetes, and cardiovascular disease, at the same time it maintains muscle, immune system, and nervous system function in aging. We are developing the first exercise model in the simple animal C. elegans, in which training benefits appear mediated by conserved mechanisms and exercise promotes healthy aging. We will optimize a novel tool for direct strength measurement of these tiny 959-celled animals and show how our device can facilitate searches for exercise mimetic drugs and genes that are associated with training adaptations, and can also help define exercise impact on a broad range of healthy aging measures. The experimental advantages of C. elegans may yield unexpected insights that inspire development of novel interventions that protect against age-associated disease and age-associated decline.",A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans,9116734,R21AG050503,"['Address', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Animals', 'Area', 'Automation', 'Biological Assay', 'Biology', 'Biology of Aging', 'Caenorhabditis elegans', 'Cardiac', 'Cardiovascular Diseases', 'Cells', 'Collaborations', 'Computer Vision Systems', 'Detection', 'Development', 'Device Designs', 'Devices', 'Diabetes Mellitus', 'Disease', 'Dissection', 'Elderly', 'Engineering', 'Evaluation', 'Exercise', 'Exhibits', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Engineering', 'Genetic Models', 'Genetic Screening', 'Goals', 'Harvest', 'Health', 'Health Benefit', 'Human', 'Immune system', 'Intervention', 'Investigation', 'Late-Onset Disorder', 'Life', 'Longevity', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediating', 'Mediator of activation protein', 'Medical', 'Modeling', 'Molecular', 'Molecular Genetics', 'Muscle', 'Muscle function', 'Nematoda', 'Nervous System Physiology', 'Organism', 'Outcome', 'Pathway interactions', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Protocols documentation', 'Pump', 'Regimen', 'Reporting', 'Research', 'Staging', 'Swimming', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translating', 'Work', 'age-related muscle loss', 'aging population', 'anti aging', 'base', 'cognitive function', 'design', 'drug discovery', 'exercise regimen', 'exercise training', 'experience', 'fascinate', 'healthy aging', 'immune function', 'improved', 'insight', 'interest', 'mimetics', 'model development', 'new technology', 'novel', 'programs', 'strength training', 'therapy design', 'tool']",NIA,TEXAS TECH UNIVERSITY,R21,2016,184503,-0.02842254275690845
"Automatically Creating and Updating Meta-Studies of Randomized Controlled Trials ﻿    DESCRIPTION (provided by applicant):  A ""meta-study"" (or ""meta-analysis"") collects and analyzes many studies on the same topic to understand if there is a meaningful, overall result. Meta-studies can support (or refute) interventions, spur new investigations, and lead to novel clinical guidelines. However, constructing meta-studies is a time intensive process of searching the literature, compiling the results, and performing the statistical analysis. Due to the time commitment that is required, many topics are unexplored, and many meta-studies are not kept up-to-date with the latest published results. Finally, a number of (unknown) biases, via subjective choices during the meta-study, may influence the results. Our long-term goal is to automate, as much as possible, the meta-study process. This should decrease subjective bias; increase the dissemination of evidence, especially for diseases and interventions that receive less attention; and allow for the automatic updating of meta-studies as new results are published. We propose a computer system that uses statistical machine learning to gather and group studies focused on similar interventions and outcomes; extract the necessary results from the text; and analyze the results using standard meta-analysis techniques. The final output will be presented in a spreadsheet-like Web-interface where users can explore and even change the data and meta-analyses. Our team uniquely blends technical expertise in machine learning with leadership in publishing meta-studies about Inflammatory Bowel Disease (IBD), our disease of focus for our Phase I feasibility study. We are therefore qualified technically and able to ensure that the techniques generate valid and accurate meta-studies. Our Phase I results will define the current state-of-the-art for this novel task. Further, although we will initially focus n IBD, our Phase I results will demonstrate that our approach can generalize to other diseases, eventually applying to any intervention and any disease. The feasibility shown by our Phase I results will motivate our Phase II effort where we will focus on dramatically improving the approach, yielding broad coverage of all medical literature and generating human-quality meta-studies. We note that by the end of Phase I we should have a viable end-to-end prototype, focused on IBD, which we can begin taking to market. The final product should significantly benefit our target markets given the Phase II emphasis to improve the technology, user experience, and scope of covered diseases. PUBLIC HEALTH RELEVANCE:  A meta-analysis collects and analyzes the results from multiple studies that are all focused on the same topic, and it can confirm (or refute) the overall effect across the studies, lead to changes in clinical guidelines, or spur new directions for research. However, generating a meta-analysis is an extremely time-consuming process, so many diseases are not covered, and most meta-analyses are not updated to reflect the latest published studies. This work begins to automate the process of creating meta-analyses, overcoming these difficulties in order to make the results published in the medical literature more accessible.",Automatically Creating and Updating Meta-Studies of Randomized Controlled Trials,8977531,R43LM012210,"['Adverse event', 'Algorithms', 'Attention', 'Clinical', 'Computer Systems', 'Computer software', 'Data', 'Data Aggregation', 'Diabetes Mellitus', 'Disease', 'Disease remission', 'Dourine', 'Ensure', 'Evidence Based Medicine', 'Feasibility Studies', 'Goals', 'Grouping', 'Guidelines', 'Hand', 'Health', 'Human', 'Inflammatory Bowel Diseases', 'Intervention', 'Intervention Studies', 'Investigation', 'Lead', 'Leadership', 'Literature', 'Lupus', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Meta-Analysis', 'Modeling', 'Odds Ratio', 'Outcome', 'Outcome Study', 'Output', 'Pattern', 'Performance', 'Phase', 'Placebos', 'Population Sizes', 'Process', 'Publishing', 'Qualifying', 'Randomized Controlled Trials', 'Research', 'Research Personnel', 'System', 'Technical Expertise', 'Techniques', 'Technology', 'Text', 'Time', 'Update', 'Work', 'abstracting', 'base', 'experience', 'falls', 'improved', 'novel', 'primary outcome', 'programs', 'prototype', 'software development', 'text searching', 'web interface']",NLM,INFERLINK CORPORATION,R43,2015,150000,0.005319102125422468
"Reproducibility Assessment for Multivariate Assays DESCRIPTION (provided by applicant): This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of features, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combination of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penalization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools availble for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.",Reproducibility Assessment for Multivariate Assays,8828718,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'Health', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'research study']",NIGMS,INSILICOS,R43,2015,64005,0.0007396342998982267
"Identifying Huntington's disease markers by modern statistical learning methods. DESCRIPTION (provided by applicant): Designing an efficient Huntington's disease (HD) early intervention clinical trial for individuals who have an expanded CAG repeats in the huntingtin gene requires identifying and combining clinical, biological, cognitive, and brain imaging markers to accurately distinguish among subjects who will have a diagnosis during a given intervention period and those who will not, and to track early changes in the disease course. The goal of this project is to identify sensitive biomarkers for HD risk stratification, indexing disease progression, and developing clinical trial endpoints. The proposal directly adheres to ""2P's"" of the NIH New Strategic Vision of the ""4P's"" of Medicine: they will offer promising ways to predict when the disease will develop; and increase the capacity to personalize early intervention based on the informative patient-specific markers our models identify. Combining biomarkers to predict HD onset and progression is an essential step in a continuum of research for development of disease-modifying therapies. Composite markers and their risk profiles created from our model will offer quantitative way to monitor and compare potential interventions. Evidence collected from these comparisons will advance the development of efficacy studies in premanifest HD, where neuroprotective treatments would be most beneficial. We develop and apply a series of cutting-edge statistical learning methods based on support vector machine (SVM), variable selection, and dimension reduction to achieve these goals. These modern statistical methods designed for correlated big data have quickly emerged as among the most successful tools for hypothesis generation, classification and prediction in biomedical studies. However, they have not been introduced to HD biomarker research. In aim 1, using counting process, we propose SVM to handle time-to-event outcomes (e.g., time-to-HD-diagnosis) to combine markers into risk scores to discriminate subjects who will experience HD onset in the immediate future from those who will not,  based on their personalized features. Although SVM is well studied for binary outcomes, it is far less explored for time-to-event outcomes. We fill this gap in knowledge. In aim 2, we propose new learning methods for longitudinal outcomes to combine markers that modify the course of HD signs to monitor disease process and distinguish subjects with rapid progression from those with slower progression. In aim 3, we propose to use novel and robust performance measures to compare derived combined markers with existing disease indices and key markers. These aims will fundamentally advance our understanding of markers linked to HD onset and progression. The creation of statistical models for composite markers and risk profiles is especially useful in: (1) offering quantitative ways to monitor and compare potential interventions, and (2) improving power of efficacy studies targeted at premanifest individuals by narrowing the predictive interval which leads to future clinical trials that can be made shorter with fewer subjects. Finally, our improved predictions of HD onset and progression will provide more informative genetic counseling sessions for pre-symptomatic subjects at risk of HD. PUBLIC HEALTH RELEVANCE:  The goal of Huntington's disease (HD) research is to develop experimental therapeutics to delay onset or slow disease progression, and to provide different treatment regimens at each disease stage. To meet this goal, this proposal develops and applies a series of advanced statistical approaches to rank and combine clinical, behavioral, and brain imaging markers to predict HD diagnosis in premanifest subjects during a given time period and to measure disease progression. The creation of model for composite markers and risk profiles is useful in offering quantitative ways to monitor and compare interventions and powering clinical trials for premanifest HD individuals.",Identifying Huntington's disease markers by modern statistical learning methods.,8896079,U01NS082062,"['Accounting', 'Address', 'Advanced Development', 'Age', 'Algorithms', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain imaging', 'CAG repeat', 'Classification', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Intervention', 'Early intervention trials', 'Event', 'Future', 'Generations', 'Genes', 'Genetic Counseling', 'Genetic screening method', 'Goals', 'Health', 'Huntington Disease', 'Image', 'Individual', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Motor Manifestations', 'Mutation', 'Odds Ratio', 'Onset of illness', 'Outcome', 'Patients', 'Penetrance', 'Performance', 'Population', 'Predictive Value', 'Prevention', 'Process', 'ROC Curve', 'Relative (related person)', 'Research', 'Risk', 'Risk Marker', 'Series', 'Staging', 'Statistical Methods', 'Statistical Models', 'Stratification', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Treatment Protocols', 'United States National Institutes of Health', 'Vision', 'Work', 'affection', 'base', 'burden of illness', 'cognitive testing', 'design', 'disease diagnosis', 'disorder risk', 'experience', 'functional outcomes', 'hazard', 'high risk', 'human Huntingtin protein', 'improved', 'indexing', 'interest', 'meetings', 'nervous system disorder', 'novel', 'research and development', 'tool']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2015,339691,-0.00017457230021442763
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages.         PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.            ",Multi-Resolution Docking Methods for Electron Microscopy,8964685,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Image', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Relative (related person)', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2015,307928,-0.018228613296468903
"Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores ﻿    DESCRIPTION (provided by applicant): Structural analysis of large polysaccharides remains challenging in glycobiology. The problem is especially acute when polysaccharides in question are glycosaminoglycans (GAGs). GAGs are large, linear, sulfated polysaccharides ubiquitous to all mammals. Interests in GAG structures stem from GAGs' diverse biological activities that govern phenomena such as tissue development/regeneration, inflammation, blood coagulation and amyloid plaque formation. Abnormal GAG structures have also been associated with the development of a number of diseases, notably cancer and inflammation. As a result, there has been a desire to understand how GAG structures correlate with their biological activities, especially how the distribution of sulfate groups along the chain influence their interactions with GAG-binding proteins. However, GAGs' large size and complex sulfation patterns make analysis of intact GAG chains by conventional ensemble analytical techniques difficult, if not impossible. Here we propose to develop a single molecule sequencer for analysis of polysaccharides using the recognition tunneling nanopore (RTP) device currently under development for ""$1000 genome"" project as a template. With the R21 grant, we will demonstrate the feasibility by carrying out pre-requisite work needed to achieve single molecule sequencing of intact GAG chains using RTP. A RTP device incorporates a nanopore with a tunneling nanogap that contains two electrodes functionalized with recognition molecules capable of forming transient complexes with functional groups on a polymeric chain as it translocates the nanopore, thus generating electrical signals. Single molecule sequencing of GAG chains proposed here circumvents the need to obtain homogeneous samples of GAGs, greatly reducing complexity of sample preparation. GAG analysis by RT devices also does not have the size limitations of most of the existing analytical techniques, and the solid state device planned here are economical to manufacturer and operate. In this application, we aim to carry out pilot studies needed to make GAG sequencing by RTPs feasible: (1) we will investigate the translocation of size defined sulfated GAG fragments through nanopores to optimize the translocation efficiency of GAG ligands as well as to understand the influence of GAG sulfation density and GAG size on their translocation efficiency and speed; (2) we will carry out recognition tunneling experiments on sulfated GAG disaccharides as well as trisaccharides so these signals of GAGs can be analyzed using machine learning algorithms to identify unique signatures needed to detect the presence of these sulfation motifs in longer GAG chains. Completion of these aims will provide all the knowledge required for correct interpretations of RT signals produced by GAG translocation and sets the stage for sequencing of intact GAG chains by RT devices.         PUBLIC HEALTH RELEVANCE:     Work proposed here will allow single molecule sequencing of glycosaminoglycan polysaccharides using an electronic chip with a high speed and low cost for the first time. Glycosaminoglycans have important pharmacological properties and are modulators of critical biological phenomena such as tissue development/regeneration and inflammation. Determination of their sequence structures will allow better understanding of how organisms control these physiological events through glycosaminoglycans.            ",Single Molecule Sequencing of Glycosaminoglycans using Recognition Tunneling Nanopores,8984813,R21GM118339,"['Acute', 'Algorithms', 'Amino Acids', 'Architecture', 'Binding Proteins', 'Biological', 'Biological Markers', 'Biological Phenomena', 'Blood coagulation', 'Cells', 'Charge', 'Chemistry', 'Complex', 'Coupled', 'DNA', 'DNA Sequence', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Disaccharides', 'Disease', 'Electrodes', 'Electronics', 'Electrons', 'Environment', 'Enzymes', 'Event', 'Genome', 'Glycobiology', 'Glycosaminoglycans', 'Goals', 'Grant', 'Imidazole', 'Individual', 'Inflammation', 'Inorganic Sulfates', 'Ions', 'Isomerism', 'Knowledge', 'Leukocyte Trafficking', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Mammals', 'Manufacturer Name', 'Mediating', 'Methods', 'Microbe', 'Natural regeneration', 'Neoplasm Metastasis', 'Oligosaccharides', 'Organism', 'Pattern', 'Physiological', 'Pilot Projects', 'Play', 'Polysaccharides', 'Preparation', 'Process', 'Property', 'Proteins', 'Publishing', 'Reader', 'Reading', 'Research', 'Role', 'Sampling', 'Senile Plaques', 'Side', 'Signal Transduction', 'Signaling Protein', 'Site', 'Speed', 'Staging', 'Structure', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Tissues', 'Trisaccharides', 'Unspecified or Sulfate Ion Sulfates', 'Work', 'amyloid formation', 'analytical method', 'base', 'cancer cell', 'cost', 'density', 'design', 'extracellular', 'functional group', 'interest', 'nanopore', 'polysulfated glycosaminoglycan', 'programs', 'public health relevance', 'research study', 'single molecule', 'solid state', 'stem', 'sugar', 'sulfation', 'tool']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2015,273816,-0.00879429186910205
"Statistical Modeling of Complex Traits in Genetic Reference Super-Populations DESCRIPTION (provided by applicant):     Genetic crosses in model organisms play an essential role in understanding the heritable architecture of medically relevant phenotypes. Traditionally, such crosses have tended to be on a small scale with either limited power to detect genetic effects or limited resolution to localize causal variants. Recently, however, the emergence of larger-scale interdisciplinary research, cheaper genotyping and parallel advances in human genetics, have spurred the development of more sophisticated and powerful experimental designs. Genetic Resource Populations (GRPs) use economies of scale to provide cost-effective and replicable platforms for genetic studies. This project concerns the largest, most ambitious GRP in mouse genetics to date, the Collaborative Cross (CC), and a series of crosses and designs related to or derived from it: the Diversity Outbred (DO) cross, the CC Recombinant Inbred Cross (CC-RIX) and the diallel. Experiments on each separate cross provide distinct information about the heritable architecture of a target complex disease. In combination, this Genetic Reference Super-Population (GRSP) potentially provides an unparalleled basis for cross-study replication and integration in mouse genetics. This project aims to develop statistical methods that advance the current state of complex trait analysis of these populations separately, and, by exploiting the unique structure that connects them, proposes to develop a statistical framework that allows for their joint use.  Aim 1 develops a Bayesian probabilistic framework for haplotype-based analysis of quantitative trait loci (QTL). Aim 1a develops a statistical software module for flexible haplotype-based analysis, which can be ex- tended by the researcher to model a rich variety of designs and disease types. Aim 1b will adapt machine learning techniques to provide posterior inference of the allelic series of a QTL. Aim 1c will incorporate Bayesian modeling of polygenic effects.  Aim 2 and 3 concern joint analysis, building on the foundation set by Aim 1. Aim 2 develops methods to optimize experimental design of follow-up studies in one population given results from another. Aim 2a uses the diallel to inform design of CC/CC-RIX/DO experiments. Aim 2b uses partial data on CC/CC-RIX/DO to guide collection of additional data. Aim 3 explores models for jointly analyzing multiple populations in the GRSP, using complementary datasets to stabilize analysis at single QTL (Aim 3a) and across multiple QTL (Aim 3b).  These aims address specific and persistent challenges in the cost-effective design and efficient analysis of multiparent genetic data, in particular the CC, DO, CC-RIX and diallel. The project will generate tools useful for a wide range of model organism crosses and can be applied to the genetic study of any complex disease. The proposed research will lead to improvements in the analysis and design of genetic studies on animal models of human disease. Because the project focuses on statistical methodology applied to experimental mouse populations, the scientific output of the project is expected to be applicable to basic research focusing on any medical condition that can be studied in the mouse.",Statistical Modeling of Complex Traits in Genetic Reference Super-Populations,8919917,R01GM104125,"['Accounting', 'Address', 'Affect', 'Animal Model', 'Anxiety', 'Architecture', 'Asthma', 'Basic Science', 'Bayesian Modeling', 'Biomedical Research', 'Collection', 'Complex', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Environmental Risk Factor', 'Equilibrium', 'Etiology', 'Experimental Designs', 'Foundations', 'Funding', 'Generations', 'Genetic', 'Genetic Crosses', 'Genetic Programming', 'Genetic study', 'Genotype', 'Haplotypes', 'Heart Diseases', 'Human Genetics', 'Hybrids', 'Inbreeding', 'Influentials', 'Interdisciplinary Study', 'Joints', 'Lead', 'Machine Learning', 'Maps', 'Medical', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Output', 'Pattern', 'Phenotype', 'Play', 'Plug-in', 'Population', 'Population Analysis', 'Quantitative Trait Loci', 'Randomized', 'Recombinants', 'Relative (related person)', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Variant', 'Weight', 'base', 'cost', 'cost effective', 'design', 'disease phenotype', 'experience', 'flexibility', 'follow-up', 'genetic resource', 'human disease', 'insight', 'interest', 'population based', 'prospective', 'research study', 'response', 'simulation', 'success', 'tool', 'trait']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2015,241086,-0.023139230964232756
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,8930750,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs']",NHGRI,STANFORD UNIVERSITY,R01,2015,292499,-0.003590735935668945
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress. Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8913771,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'imaging system', 'improved', 'interest', 'mild cognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2015,330386,-0.004922362535102985
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs.         PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.            ",New Serological Measures of Infectious Disease Transmission Intensity,8947064,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Effectiveness of Interventions', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Low income', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Qualifying', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Staging', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk', 'improved', 'intervention effect', 'intervention program', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2015,142069,0.0001644448701526404
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",8927659,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2015,434391,0.023867217146273573
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,8858662,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2015,248912,-0.006110679075191399
"Improving the Detection of Activation in High Resolution fMRI using Multivariate DESCRIPTION (provided by applicant): The overall goal of this project is to develop a local multivariate analysis software package for fMRI data analysis. It will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. This project will lead to better brain activation maps and thus promote the discovery of currently unknown aspects of brain function. Mass-univariate analysis, such as the general linear model (GLM), is the prevailing fMRI data analysis method. However, it suffers from blurring of edges of activation and potential elimination of the detection of weak activated regions due to routinely applied fixed isotropic spatial Gaussian smoothing. Local multivariate methods such as canonical correlation analysis (CCA) and its variants have been shown to significantly increase the detection power of fMRI activations and improve activation maps. As an advantage, CCA uses adaptive spatial filtering kernels to accurately extract the signal better in a noisy environment. However, there are several drawbacks, particularly low spatial specificity, long computational time, and single-factor experimental design limitation. Furthermore, a parametric estimation method does not exist to determine the family-wise error rate, no extension to group analysis has been investigated, and no studies extending local CCA to nonlinear CCA for fMRI data using kernel methods have been systematically carried out. All these drawbacks prevent local CCA methods from being widely accepted in neuroscience research in fMRI. In this proposal, our goals are to eliminate these drawbacks using novel local multivariate analysis methods (based on CCA) and to develop a software tool to widen its broader application in the neuroscience research community. We expect this software tool to be particularly valuable for neuroscience research where detections of weak activations or spatially localized patterns of activations are desired. As high resolution imaging and computer power advance, we expect an increase in demand for this software tool, thus advancing new discoveries of brain function and more precise spatial localization of activations. As a particular application, we will focus on studying memory actions using a novel event-related recognition paradigm to investigate the effects of familiarity and recollection in subregions of the medial temporal lobes (MTL) for high resolution fMRI. This research will advance our understanding of hippocampal/MTL contributions to memory, which can substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer¿s disease, schizophrenia, and major depression. More generally, it will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8841351,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'mathematical methods', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,CLEVELAND CLINIC LERNER COM-CWRU,R01,2015,275802,-0.028432403967556585
"Statistical methods for biosignals with varying domains DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2 PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4",Statistical methods for biosignals with varying domains,8915740,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'imaging modality', 'indexing', 'kinematics', 'member', 'new technology', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2015,397940,-0.025409315879153616
"Statistical Methods for Selection and Evaluation of Biomarkers DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particular, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pancreatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public. PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.",Statistical Methods for Selection and Evaluation of Biomarkers,8808769,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Health', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of pancreas', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'burden of illness', 'cancer genome', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'patient population', 'programs', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2015,324063,-0.04414121807199599
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,8929328,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'genetic makeup', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'personalized medicine', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2015,329422,0.01673781722097007
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,8890255,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2015,347156,0.018858061844145315
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS. PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,8839318,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Health', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Individual', 'Lateral', 'Lead', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Process', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'insight', 'interest', 'meetings', 'neuroimaging', 'neurotransmission', 'novel', 'outcome forecast', 'predictive modeling', 'prevent', 'response', 'screening', 'spinal tract', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2015,577490,-0.003840596965418296
"Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies DESCRIPTION (provided by applicant): Common mental disorders such as Alzheimer's disease and schizophrenia are largely heritable with complex genetic underpinnings. Large-scale genome-wide association studies that contrast DNA sequence data from patients and controls have recently identified novel genetic risk variants for these disorders. Nevertheless, the processes through which genotype increases risk are yet to be fully characterized.  Neuroimaging offers a richer picture of the underlying disease processes than a clinical diagnosis. Thus the joint analysis of neuroimaging and genetics data promises to advance our understanding of these processes. Today, neuroimaging genetics studies however face important challenges that obstruct progress: small sample sizes, modest effect sizes, and the extreme dimensionality of the data limit statistical power and thus our ability to explore the complex and subtle associations between genes, neuroanatomy and clinical decline. Currently, the prevalent approach in neuroimaging genetics is to concentrate the analysis on a small number of anatomic regions of interest and/or candidate genes and often ignore a large portion of the data. The core goal of the proposed project is to develop computational tools that will take full advantage of the richness in the datasets and facilitate the exploration of the multifaceted associations between genotype, neuroimaging measurements and clinical phenotype. The proposed project will use advanced multivariate pattern analysis methods such as support vector machines to compute image-based and genetic scores that reflect pathology. We will validate the tools based on their association with classical biomarkers of disease. Finally, we will develop a model that uses both imaging and genotype data to predict future clinical outcome. We expect these tools will enable progress along three directions relevant to complex mental disorders, e.g. late-onset Alzheimer's disease (AD): (1) confirming and characterizing risk genes, (2) identifying disease-specific anatomical alterations in healthy individuals, and (3) early diagnosis and prognosis. The project will (1) use three already-collected large-scale datasets to apply the developed tools to AD, (2) build on cutting-edge image processing algorithms that we have been developing, and (3) allow the candidate to receive further training in neuroanatomy, mental disorders and genetics, forming the foundation for his future career as an independent researcher. n/a",Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies,8916113,K25EB013649,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Anatomy', 'Biological Markers', 'Brain', 'Candidate Disease Gene', 'Clinical', 'Clinical Trials', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Dementia', 'Development', 'Disease', 'Early Diagnosis', 'Event', 'Exhibits', 'Face', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Research', 'Genetic Risk', 'Genetic screening method', 'Genetic study', 'Genotype', 'Goals', 'Hereditary Disease', 'Hippocampus (Brain)', 'Image', 'Individual', 'Joints', 'Late Onset Alzheimer Disease', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Measurement', 'Mental disorders', 'Methods', 'Mining', 'Modeling', 'Motivation', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Probability', 'Process', 'Recruitment Activity', 'Research Personnel', 'Risk', 'Sample Size', 'Schizophrenia', 'Testing', 'Thick', 'Training', 'base', 'career', 'clinical Diagnosis', 'clinical phenotype', 'cognitive performance', 'computerized tools', 'data modeling', 'disorder risk', 'entorhinal cortex', 'genetic risk factor', 'genetic variant', 'genome wide association study', 'high risk', 'image processing', 'improved', 'in vivo', 'interest', 'mild cognitive impairment', 'molecular pathology', 'neuroimaging', 'novel', 'outcome forecast', 'pre-clinical', 'programs', 'risk variant', 'tool']",NIBIB,MASSACHUSETTS GENERAL HOSPITAL,K25,2015,175392,-0.04542495527494405
"Informatic tools for predicting an ordinal response for high-dimensional data DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale. Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8900334,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Breast Cancer Patient', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2015,121902,0.006366413453524833
"A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans ﻿    DESCRIPTION (provided by applicant): Exercise is arguably the most potent approach we can take to defer physical decline associated with aging and to protect against late onset diseases such as diabetes, cancer, and Alzheimer's disease. Molecular understanding of how exercise benefits translate into healthy aging is thus of definitive medical interest. We study fundamental processes relevant to healthy aging in the 959-celled nematode C. elegans. Recently we made a fascinating discovery-C. elegans can exercise (swim) to exhibit training benefits, and appear to gain benefits by molecular pathways conserved in humans. Our initial model development opens up a new research area for understanding how tissue-specific and organism-wide health benefits are induced by exercise, and creates a novel paradigm for identifying exercise mimetic drugs that might promote healthy aging. To really harvest the potential of this model, we need to measure the strength of the tiny C. elegans. We collaborated to develop a strength test in which trained animals thread through a matrix of deformable pillars, and the extent of pillar deflection is used to calculate force. Our ""NemaFlex"" force detection device is the quantitative foundation with which we expect to break new ground in understanding exercise impact on healthy aging. Here we propose required development to enhance assay throughput and pursue applications that will not only anchor this technology as an essential component of C. elegans exercise evaluation but also accelerate studies on exercise biology and healthy aging in this powerful model. Aim 1 is to develop a novel high throughput tool for direct strength evaluation in C. elegans.  This aim will generate an essential tool for analysis of C. elegans strength at multiple life stages, define the exercise regimen that will become the anchor protocol in the field, and reveal features of training in this model. Aim 2 is to use NemaFlex to evaluate exercise mimetic drugs & to facilitate focused pilot genetic screens. This aim will establish critical proof-of-principle for genetic and drug discovery using the NemaFlex. Aim 3 is to initiate dissection of the functional and molecular relationship between exercise and healthy aging, grounded in NemaFlex force measures of training benefits.  To begin, we will test how optimized strength training tracks with a broad spectrum of healthspan indicators that decline with age, we will investigate impact of cessation of training on aging quality, and we will ask if exercise mimetic drugs extend healthspan in the absence of training. Our goals will create novel technology that for the first time permits facile quantitativ analysis of exercise adaptations in the powerful C. elegans genetic model. Accomplishment of our tractable aims will anchor a new subfield of genetic investigation of exercise and healthy aging that may influence design of interventions that broadly promote health and defer aging.         PUBLIC HEALTH RELEVANCE: Exercise has a profound positive impact on health of the aging population in that it protects against age-associated diseases including cancer, diabetes, and cardiovascular disease, at the same time it maintains muscle, immune system, and nervous system function in aging. We are developing the first exercise model in the simple animal C. elegans, in which training benefits appear mediated by conserved mechanisms and exercise promotes healthy aging. We will optimize a novel tool for direct strength measurement of these tiny 959-celled animals and show how our device can facilitate searches for exercise mimetic drugs and genes that are associated with training adaptations, and can also help define exercise impact on a broad range of healthy aging measures. The experimental advantages of C. elegans may yield unexpected insights that inspire development of novel interventions that protect against age-associated disease and age-associated decline.              ",A Strength Analysis Tool for Studying Healthy Aging via Exercise in C. elegans,8936078,R21AG050503,"['Address', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Animals', 'Area', 'Automation', 'Biological Assay', 'Biology', 'Biology of Aging', 'Caenorhabditis elegans', 'Cardiac', 'Cardiovascular Diseases', 'Cells', 'Collaborations', 'Computer Vision Systems', 'Detection', 'Development', 'Device Designs', 'Devices', 'Diabetes Mellitus', 'Disease', 'Dissection', 'Elderly', 'Engineering', 'Evaluation', 'Exercise', 'Exhibits', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Engineering', 'Genetic Models', 'Genetic Screening', 'Goals', 'Harvest', 'Health', 'Health Benefit', 'Human', 'Immune system', 'Intervention', 'Investigation', 'Late-Onset Disorder', 'Life', 'Longevity', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediating', 'Mediator of activation protein', 'Medical', 'Modeling', 'Molecular', 'Molecular Genetics', 'Muscle', 'Muscle function', 'Nematoda', 'Nervous System Physiology', 'Organism', 'Outcome', 'Pathway interactions', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Protocols documentation', 'Pump', 'Regimen', 'Reporting', 'Research', 'Staging', 'Swimming', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translating', 'Work', 'aging population', 'anti aging', 'base', 'cognitive function', 'design', 'drug discovery', 'experience', 'fascinate', 'healthy aging', 'immune function', 'improved', 'insight', 'interest', 'mimetics', 'model development', 'new technology', 'novel', 'programs', 'public health relevance', 'strength training', 'therapy design', 'tool']",NIA,TEXAS TECH UNIVERSITY,R21,2015,235630,-0.02842254275690845
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8657051,R01GM053163,"['Address', 'Algorithms', 'Biochemical', 'Budgets', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Development', 'Drug Design', 'Evaluation', 'Funding', 'Geometry', 'Goals', 'Image', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Output', 'Pattern', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Relative (related person)', 'Research', 'Shapes', 'Signal Transduction', 'Solutions', 'Specimen', 'Spottings', 'Structure', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2014,320096,-0.005255698471903612
"Reproducibility Assessment for Multivariate Assays  Project Summary. This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of fea- tures, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combina- tion of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penal- ization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools available for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.            ",Reproducibility Assessment for Multivariate Assays,8647816,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Simulate', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'public health relevance', 'research study']",NIGMS,INSILICOS,R43,2014,131071,0.00039742849973208163
"Identifying Huntington's disease markers by modern statistical learning methods.     DESCRIPTION (provided by applicant): Designing an efficient Huntington's disease (HD) early intervention clinical trial for individuals who have an expanded CAG repeats in the huntingtin gene requires identifying and combining clinical, biological, cognitive, and brain imaging markers to accurately distinguish among subjects who will have a diagnosis during a given intervention period and those who will not, and to track early changes in the disease course. The goal of this project is to identify sensitive biomarkers for HD risk stratification, indexing disease progression, and developing clinical trial endpoints. The proposal directly adheres to ""2P's"" of the NIH New Strategic Vision of the ""4P's"" of Medicine: they will offer promising ways to predict when the disease will develop; and increase the capacity to personalize early intervention based on the informative patient-specific markers our models identify. Combining biomarkers to predict HD onset and progression is an essential step in a continuum of research for development of disease-modifying therapies. Composite markers and their risk profiles created from our model will offer quantitative way to monitor and compare potential interventions. Evidence collected from these comparisons will advance the development of efficacy studies in premanifest HD, where neuroprotective treatments would be most beneficial. We develop and apply a series of cutting-edge statistical learning methods based on support vector machine (SVM), variable selection, and dimension reduction to achieve these goals. These modern statistical methods designed for correlated big data have quickly emerged as among the most successful tools for hypothesis generation, classification and prediction in biomedical studies. However, they have not been introduced to HD biomarker research. In aim 1, using counting process, we propose SVM to handle time-to-event outcomes (e.g., time-to-HD-diagnosis) to combine markers into risk scores to discriminate subjects who will experience HD onset in the immediate future from those who will not,  based on their personalized features. Although SVM is well studied for binary outcomes, it is far less explored for time-to-event outcomes. We fill this gap in knowledge. In aim 2, we propose new learning methods for longitudinal outcomes to combine markers that modify the course of HD signs to monitor disease process and distinguish subjects with rapid progression from those with slower progression. In aim 3, we propose to use novel and robust performance measures to compare derived combined markers with existing disease indices and key markers. These aims will fundamentally advance our understanding of markers linked to HD onset and progression. The creation of statistical models for composite markers and risk profiles is especially useful in: (1) offering quantitative ways to monitor and compare potential interventions, and (2) improving power of efficacy studies targeted at premanifest individuals by narrowing the predictive interval which leads to future clinical trials that can be made shorter with fewer subjects. Finally, our improved predictions of HD onset and progression will provide more informative genetic counseling sessions for pre-symptomatic subjects at risk of HD.         PUBLIC HEALTH RELEVANCE:  The goal of Huntington's disease (HD) research is to develop experimental therapeutics to delay onset or slow disease progression, and to provide different treatment regimens at each disease stage. To meet this goal, this proposal develops and applies a series of advanced statistical approaches to rank and combine clinical, behavioral, and brain imaging markers to predict HD diagnosis in premanifest subjects during a given time period and to measure disease progression. The creation of model for composite markers and risk profiles is useful in offering quantitative ways to monitor and compare interventions and powering clinical trials for premanifest HD individuals.            ",Identifying Huntington's disease markers by modern statistical learning methods.,8721037,U01NS082062,"['Accounting', 'Address', 'Advanced Development', 'Age', 'Algorithms', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain imaging', 'CAG repeat', 'Classification', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Intervention', 'Early intervention trials', 'Event', 'Future', 'Generations', 'Genes', 'Genetic Counseling', 'Genetic screening method', 'Goals', 'Huntington Disease', 'Image', 'Individual', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Motor Manifestations', 'Mutation', 'Odds Ratio', 'Onset of illness', 'Outcome', 'Patients', 'Penetrance', 'Performance', 'Population', 'Predictive Value', 'Prevention', 'Process', 'ROC Curve', 'Relative (related person)', 'Research', 'Risk', 'Risk Marker', 'Series', 'Staging', 'Statistical Methods', 'Statistical Models', 'Stratification', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Treatment Protocols', 'United States National Institutes of Health', 'Vision', 'Work', 'affection', 'base', 'burden of illness', 'design', 'disease diagnosis', 'disorder risk', 'experience', 'functional outcomes', 'hazard', 'high risk', 'human Huntingtin protein', 'improved', 'indexing', 'interest', 'meetings', 'nervous system disorder', 'novel', 'public health relevance', 'research and development', 'tool']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,U01,2014,359539,-0.00017457230021442763
"Statistical Modeling of Complex Traits in Genetic Reference Super-Populations     DESCRIPTION (provided by applicant):     Genetic crosses in model organisms play an essential role in understanding the heritable architecture of medically relevant phenotypes. Traditionally, such crosses have tended to be on a small scale with either limited power to detect genetic effects or limited resolution to localize causal variants. Recently, however, the emergence of larger-scale interdisciplinary research, cheaper genotyping and parallel advances in human genetics, have spurred the development of more sophisticated and powerful experimental designs. Genetic Resource Populations (GRPs) use economies of scale to provide cost-effective and replicable platforms for genetic studies. This project concerns the largest, most ambitious GRP in mouse genetics to date, the Collaborative Cross (CC), and a series of crosses and designs related to or derived from it: the Diversity Outbred (DO) cross, the CC Recombinant Inbred Cross (CC-RIX) and the diallel. Experiments on each separate cross provide distinct information about the heritable architecture of a target complex disease. In combination, this Genetic Reference Super-Population (GRSP) potentially provides an unparalleled basis for cross-study replication and integration in mouse genetics. This project aims to develop statistical methods that advance the current state of complex trait analysis of these populations separately, and, by exploiting the unique structure that connects them, proposes to develop a statistical framework that allows for their joint use.  Aim 1 develops a Bayesian probabilistic framework for haplotype-based analysis of quantitative trait loci (QTL). Aim 1a develops a statistical software module for flexible haplotype-based analysis, which can be ex- tended by the researcher to model a rich variety of designs and disease types. Aim 1b will adapt machine learning techniques to provide posterior inference of the allelic series of a QTL. Aim 1c will incorporate Bayesian modeling of polygenic effects.  Aim 2 and 3 concern joint analysis, building on the foundation set by Aim 1. Aim 2 develops methods to optimize experimental design of follow-up studies in one population given results from another. Aim 2a uses the diallel to inform design of CC/CC-RIX/DO experiments. Aim 2b uses partial data on CC/CC-RIX/DO to guide collection of additional data. Aim 3 explores models for jointly analyzing multiple populations in the GRSP, using complementary datasets to stabilize analysis at single QTL (Aim 3a) and across multiple QTL (Aim 3b).  These aims address specific and persistent challenges in the cost-effective design and efficient analysis of multiparent genetic data, in particular the CC, DO, CC-RIX and diallel. The project will generate tools useful for a wide range of model organism crosses and can be applied to the genetic study of any complex disease.              The proposed research will lead to improvements in the analysis and design of genetic studies on animal models of human disease. Because the project focuses on statistical methodology applied to experimental mouse populations, the scientific output of the project is expected to be applicable to basic research focusing on any medical condition that can be studied in the mouse.            ",Statistical Modeling of Complex Traits in Genetic Reference Super-Populations,8725211,R01GM104125,"['Accounting', 'Address', 'Affect', 'Animal Model', 'Anxiety', 'Architecture', 'Asthma', 'Basic Science', 'Bayesian Modeling', 'Biomedical Research', 'Collection', 'Complex', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Environmental Risk Factor', 'Equilibrium', 'Etiology', 'Experimental Designs', 'Foundations', 'Funding', 'Generations', 'Genetic', 'Genetic Crosses', 'Genetic Programming', 'Genotype', 'Haplotypes', 'Heart Diseases', 'Human Genetics', 'Hybrids', 'Inbreeding', 'Influentials', 'Interdisciplinary Study', 'Joints', 'Lead', 'Machine Learning', 'Maps', 'Medical', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Output', 'Pattern', 'Phenotype', 'Play', 'Plug-in', 'Population', 'Population Analysis', 'Quantitative Trait Loci', 'Randomized', 'Recombinants', 'Relative (related person)', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Variant', 'Weight', 'base', 'cost', 'cost effective', 'design', 'disease phenotype', 'experience', 'flexibility', 'follow-up', 'genetic resource', 'human disease', 'insight', 'interest', 'population based', 'prospective', 'research study', 'response', 'simulation', 'success', 'tool', 'trait']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2014,241086,-0.023139230964232756
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics     DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress.             Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8714056,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Simulate', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'interest', 'mild cognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2014,330386,-0.004922362535102985
"Statistical and computational analysis in whole genome sequencing studies.     DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges.         PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.                ",Statistical and computational analysis in whole genome sequencing studies.,8750827,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs', 'public health relevance']",NHGRI,STANFORD UNIVERSITY,R01,2014,300000,-0.003590735935668945
"Models for synthesising molecular, clinical and epidemiological data, and transla     DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)?         PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.            ","Models for synthesising molecular, clinical and epidemiological data, and transla",8703195,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2014,427668,0.023867217146273573
"Heterogeneous and Robust Survival Analysis in Genomic Studies     DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed.         PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.            ",Heterogeneous and Robust Survival Analysis in Genomic Studies,8696520,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,255295,-0.006110679075191399
"Improving the Detection of Activation in High Resolution fMRI using Multivariate     DESCRIPTION (provided by applicant): The overall goal of this project is to develop a local multivariate analysis software package for fMRI data analysis. It will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. This project will lead to better brain activation maps and thus promote the discovery of currently unknown aspects of brain function. Mass-univariate analysis, such as the general linear model (GLM), is the prevailing fMRI data analysis method. However, it suffers from blurring of edges of activation and potential elimination of the detection of weak activated regions due to routinely applied fixed isotropic spatial Gaussian smoothing. Local multivariate methods such as canonical correlation analysis (CCA) and its variants have been shown to significantly increase the detection power of fMRI activations and improve activation maps. As an advantage, CCA uses adaptive spatial filtering kernels to accurately extract the signal better in a noisy environment. However, there are several drawbacks, particularly low spatial specificity, long computational time, and single-factor experimental design limitation. Furthermore, a parametric estimation method does not exist to determine the family-wise error rate, no extension to group analysis has been investigated, and no studies extending local CCA to nonlinear CCA for fMRI data using kernel methods have been systematically carried out. All these drawbacks prevent local CCA methods from being widely accepted in neuroscience research in fMRI. In this proposal, our goals are to eliminate these drawbacks using novel local multivariate analysis methods (based on CCA) and to develop a software tool to widen its broader application in the neuroscience research community. We expect this software tool to be particularly valuable for neuroscience research where detections of weak activations or spatially localized patterns of activations are desired. As high resolution imaging and computer power advance, we expect an increase in demand for this software tool, thus advancing new discoveries of brain function and more precise spatial localization of activations. As a particular application, we will focus on studying memory actions using a novel event-related recognition paradigm to investigate the effects of familiarity and recollection in subregions of the medial temporal lobes (MTL) for high resolution fMRI. This research will advance our understanding of hippocampal/MTL contributions to memory, which can substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer¿s disease, schizophrenia, and major depression. More generally, it will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods.         PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.                ",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8656325,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'mathematical methods', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'public health relevance', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,RYERSON  UNIVERSITY,R01,2014,65141,-0.028432403967556585
"Statistical methods for biosignals with varying domains     DESCRIPTION (provided by applicant): Clinical care and large observational studies are characterized by periods of intense health monitoring during hospital visits followed by long periods of low-intensity or no-monitoring between visits. Data obtained during in-hospital visits come from a host of new technologies, such as very densely sampled biosignal recordings (EEG, ECG, health scores) and high resolution multi-modality imaging (MRI, CT, PET). A major characteristic of this type of data is that it is collected for a period of time that is subject-spcific. Indeed, the in-hospital length and amount of monitoring varies between subjects, and is highly informative both for studying health outcomes in the hospital and after discharge. One among many examples is a recent study of subjects admitted to the Intensive Care Unit (ICU) with Acute Respiratory Distress Syndrome (ARDS). For each subject the Sequential Organ Failure Assessment (SOFA) score, a commonly- used scoring system to measure organ dysfunction in the ICU, was collected daily for each subject for the duration of their ICU stay. The ICU length of stay is different by subject and likely to be highly informative of current and future health outcomes. In this application, a set of relevant problems are conceptualized and distilled to statistical aims to address specific complexities associated with this type of data sampling. Specifically, the proposal addresses the following fundamental unsolved problems in studies that collect high density biosignals: 1) introducing statistical models for the association between high density biosignals with uneven support and health outcomes; 2) developing functional registration-by-prediction models that transform the support of biosignals to provide best prediction of health outcomes; and 3) developing models for describing the cross-sectional and longitudinal variability of biosignals obtained in studies with rare -but intense- health monitorin. While focus lies on research studies that collect quasi- continuous ultra-high resolution biosignals for subject-specific lengths of time, methods will be generalizable to many other studies with similar data sampling structures. 2         PUBLIC HEALTH RELEVANCE: This project provides analytic methods for biological and health signals that are measured often for unequal periods of time (e.g. disease severity scores during hospital stays, EEG data during sleep, reaching hand movement after stroke). Special emphasis is given to the study of the association between these biosignals and health outcomes. 4                ",Statistical methods for biosignals with varying domains,8742367,R01HL123407,"['Address', 'Adult Respiratory Distress Syndrome', 'Applications Grants', 'Biological', 'Characteristics', 'Complex', 'Data', 'Data Analyses', 'Development', 'Electrocardiogram', 'Electroencephalography', 'Event', 'Functional disorder', 'Future', 'Hand', 'Health', 'Heterogeneity', 'Hospitals', 'Hour', 'Intensive Care Units', 'Length', 'Length of Stay', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Observational Study', 'Organ', 'Organ failure', 'Outcome', 'Participant', 'Patients', 'Population', 'Positron-Emission Tomography', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Severity of illness', 'Shapes', 'Signal Transduction', 'Sleep', 'Statistical Methods', 'Statistical Models', 'Stroke', 'Structure', 'Study Subject', 'Survival Analysis', 'System', 'Techniques', 'Time', 'Visit', 'Width', 'analytical tool', 'base', 'clinical care', 'density', 'experience', 'hazard', 'imaging modality', 'indexing', 'kinematics', 'member', 'new technology', 'public health relevance', 'research study', 'statistics', 'ultra high resolution']",NHLBI,JOHNS HOPKINS UNIVERSITY,R01,2014,419500,-0.025409315879153616
"Data-Driven Statistical Learning with Applications to Genomics     DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software.         PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.            ",Data-Driven Statistical Learning with Applications to Genomics,8796068,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Simulate', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'high throughput analysis', 'interest', 'novel', 'patient population', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2014,361063,0.01673781722097007
"Statistical Methods for Selection and Evaluation of Biomarkers     DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particulr, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pan- creatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public.         PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.            ",Statistical Methods for Selection and Evaluation of Biomarkers,8660307,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Outcome', 'Pancreas', 'Patients', 'Performance', 'Population', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'burden of illness', 'cancer genome', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'patient population', 'programs', 'public health relevance', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2014,324364,-0.04450424255662555
"Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler     DESCRIPTION (provided by applicant): Amyotrophic lateral sclerosis (ALS) is a progressive degenerative motor neuron disease involving the motor cortex, corpus callosum, cortical spinal tract and spinal anterior horn neurons. The disease has a uniformly fatal outcome, although the clinical presentation and course is quite heterogeneous, with median survival times between 2 - 4 years. Approximately 30,000 people in the United States are living with ALS. There is no definitive diagnostic test for ALS. Confident diagnosis is primarily based on clinical assessment and relies on the detection of upper motor neuron (UMN) and lower motor neuron (LMN) signs in multiple body segments, together with a history of progression of symptoms. Evaluation of LMN pathology may be supplemented by electromyography, but UMN pathology can remain occult as it is only assessed using clinical examination which can lead to diagnostic uncertainty. Unfortunately, there is on average a one- year delay between the onset of symptoms and diagnosis for this rapidly progressive disease; this delay prevents early treatment with emerging disease-modifying drugs. Thus, reliable biomarkers for the early diagnosis and disease prognostication are needed.  Conventional magnetic resonance imaging techniques provide limited and inconsistent information in ALS patients. Therefore, there has been and continues to be great interest in using advanced neuroimaging techniques to establish improved markers of the disease. Although advanced neuroimaging techniques such as magnetic resonance spectroscopy (MRS), diffusion tensor imaging (DTI) and resting state functional connectivity (fcMRI) have identified differences between ALS patients and healthy controls, they lack sufficient accuracy to reliably classify individual patients. To meet this important unmet need, the proposed study will use novel advanced neuroimaging techniques to develop a multimodal biomarker of ALS, and validate a discrimination and prediction model to refine the diagnostic clinical workup for ALS.         PUBLIC HEALTH RELEVANCE: There are no definitive tests for amyotrophic lateral sclerosis and many of these patients have a delayed diagnosis preventing early intervention with new emerging treatments. Furthermore, disease prognosis is challenging due to the variability of the natural history of amyotrophic lateral sclerosis. This study will use multiple advanced neuroimaging methods to build a robust diagnostic test and prognostic model of amyotrophic lateral sclerosis. We will use a novel statistical approach to develop and validate the models.            ",Development of a Multi-Modal Neuroimaging Biomarker for Amyotrophic Lateral Scler,8695570,R01NS082304,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Anterior', 'Anterior Horn Cells', 'Biological Markers', 'Brain', 'Clinical', 'Clinical Treatment', 'Clinical assessments', 'Corpus Callosum', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Diffusion Magnetic Resonance Imaging', 'Discrimination', 'Disease', 'Disease Marker', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Early treatment', 'Electromyography', 'Evaluation', 'Fatal Outcome', 'Functional disorder', 'Future', 'Gold', 'Heterogeneity', 'Horns', 'Image', 'Imaging Techniques', 'Individual', 'Lateral', 'Lead', 'Life', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Measures', 'Methodology', 'Methods', 'Metric', 'Modality', 'Modeling', 'Monitor', 'Motor Cortex', 'Motor Neuron Disease', 'Motor Neurons', 'Natural History', 'Neuraxis', 'Newly Diagnosed', 'Outcome', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Process', 'Progressive Disease', 'Recording of previous events', 'Research', 'Rest', 'Riluzole', 'Spinal', 'Statistical Methods', 'Statistical Models', 'Symptoms', 'Techniques', 'Testing', 'Thick', 'Time', 'Transcend', 'Uncertainty', 'United States', 'base', 'clinically relevant', 'diagnosis evaluation', 'improved', 'in vivo', 'insight', 'interest', 'meetings', 'neuroimaging', 'neurotransmission', 'novel', 'outcome forecast', 'prevent', 'prognostic', 'public health relevance', 'response', 'screening', 'spinal tract', 'treatment trial']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2014,616625,-0.003840596965418296
"Statistical methods for large and complex databases of ultra-high-dimensional     DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences.         PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8738735,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2014,343683,0.018858061844145315
"Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies    DESCRIPTION (provided by applicant): Common mental disorders such as Alzheimer's disease and schizophrenia are largely heritable with complex genetic underpinnings. Large-scale genome-wide association studies that contrast DNA sequence data from patients and controls have recently identified novel genetic risk variants for these disorders. Nevertheless, the processes through which genotype increases risk are yet to be fully characterized.  Neuroimaging offers a richer picture of the underlying disease processes than a clinical diagnosis. Thus the joint analysis of neuroimaging and genetics data promises to advance our understanding of these processes. Today, neuroimaging genetics studies however face important challenges that obstruct progress: small sample sizes, modest effect sizes, and the extreme dimensionality of the data limit statistical power and thus our ability to explore the complex and subtle associations between genes, neuroanatomy and clinical decline. Currently, the prevalent approach in neuroimaging genetics is to concentrate the analysis on a small number of anatomic regions of interest and/or candidate genes and often ignore a large portion of the data. The core goal of the proposed project is to develop computational tools that will take full advantage of the richness in the datasets and facilitate the exploration of the multifaceted associations between genotype, neuroimaging measurements and clinical phenotype. The proposed project will use advanced multivariate pattern analysis methods such as support vector machines to compute image-based and genetic scores that reflect pathology. We will validate the tools based on their association with classical biomarkers of disease. Finally, we will develop a model that uses both imaging and genotype data to predict future clinical outcome. We expect these tools will enable progress along three directions relevant to complex mental disorders, e.g. late-onset Alzheimer's disease (AD): (1) confirming and characterizing risk genes, (2) identifying disease-specific anatomical alterations in healthy individuals, and (3) early diagnosis and prognosis. The project will (1) use three already-collected large-scale datasets to apply the developed tools to AD, (2) build on cutting-edge image processing algorithms that we have been developing, and (3) allow the candidate to receive further training in neuroanatomy, mental disorders and genetics, forming the foundation for his future career as an independent researcher.       n/a",Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies,8726983,K25EB013649,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Anatomy', 'Biological Markers', 'Brain', 'Candidate Disease Gene', 'Clinical', 'Clinical Trials', 'Cognitive', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Dementia', 'Development', 'Disease', 'Early Diagnosis', 'Event', 'Exhibits', 'Face', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Research', 'Genetic Risk', 'Genotype', 'Goals', 'Hereditary Disease', 'Hippocampus (Brain)', 'Image', 'Individual', 'Joints', 'Late Onset Alzheimer Disease', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Measurement', 'Mental disorders', 'Methods', 'Mining', 'Modeling', 'Motivation', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phenotype', 'Probability', 'Process', 'Recruitment Activity', 'Research Personnel', 'Risk', 'Sample Size', 'Schizophrenia', 'Testing', 'Thick', 'Training', 'base', 'career', 'clinical Diagnosis', 'clinical phenotype', 'computerized tools', 'data modeling', 'disorder risk', 'entorhinal cortex', 'genetic risk factor', 'genetic variant', 'genome wide association study', 'high risk', 'image processing', 'improved', 'in vivo', 'interest', 'mild cognitive impairment', 'molecular pathology', 'neuroimaging', 'novel', 'outcome forecast', 'pre-clinical', 'programs', 'risk variant', 'tool']",NIBIB,MASSACHUSETTS GENERAL HOSPITAL,K25,2014,175392,-0.04542495527494405
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8714054,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2014,227026,0.006366413453524833
"Improving the Detection of Activation in High Resolution fMRI using Multivariate No abstract available PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8920855,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'mathematical methods', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,CLEVELAND CLINIC LERNER COM-CWRU,R01,2014,130281,-0.009601590888536718
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8470172,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2013,309127,-0.005255698471903612
"Statistical Modeling of Complex Traits in Genetic Reference Super-Populations     DESCRIPTION (provided by applicant):     Genetic crosses in model organisms play an essential role in understanding the heritable architecture of medically relevant phenotypes. Traditionally, such crosses have tended to be on a small scale with either limited power to detect genetic effects or limited resolution to localize causal variants. Recently, however, the emergence of larger-scale interdisciplinary research, cheaper genotyping and parallel advances in human genetics, have spurred the development of more sophisticated and powerful experimental designs. Genetic Resource Populations (GRPs) use economies of scale to provide cost-effective and replicable platforms for genetic studies. This project concerns the largest, most ambitious GRP in mouse genetics to date, the Collaborative Cross (CC), and a series of crosses and designs related to or derived from it: the Diversity Outbred (DO) cross, the CC Recombinant Inbred Cross (CC-RIX) and the diallel. Experiments on each separate cross provide distinct information about the heritable architecture of a target complex disease. In combination, this Genetic Reference Super-Population (GRSP) potentially provides an unparalleled basis for cross-study replication and integration in mouse genetics. This project aims to develop statistical methods that advance the current state of complex trait analysis of these populations separately, and, by exploiting the unique structure that connects them, proposes to develop a statistical framework that allows for their joint use.  Aim 1 develops a Bayesian probabilistic framework for haplotype-based analysis of quantitative trait loci (QTL). Aim 1a develops a statistical software module for flexible haplotype-based analysis, which can be ex- tended by the researcher to model a rich variety of designs and disease types. Aim 1b will adapt machine learning techniques to provide posterior inference of the allelic series of a QTL. Aim 1c will incorporate Bayesian modeling of polygenic effects.  Aim 2 and 3 concern joint analysis, building on the foundation set by Aim 1. Aim 2 develops methods to optimize experimental design of follow-up studies in one population given results from another. Aim 2a uses the diallel to inform design of CC/CC-RIX/DO experiments. Aim 2b uses partial data on CC/CC-RIX/DO to guide collection of additional data. Aim 3 explores models for jointly analyzing multiple populations in the GRSP, using complementary datasets to stabilize analysis at single QTL (Aim 3a) and across multiple QTL (Aim 3b).  These aims address specific and persistent challenges in the cost-effective design and efficient analysis of multiparent genetic data, in particular the CC, DO, CC-RIX and diallel. The project will generate tools useful for a wide range of model organism crosses and can be applied to the genetic study of any complex disease.              The proposed research will lead to improvements in the analysis and design of genetic studies on animal models of human disease. Because the project focuses on statistical methodology applied to experimental mouse populations, the scientific output of the project is expected to be applicable to basic research focusing on any medical condition that can be studied in the mouse.            ",Statistical Modeling of Complex Traits in Genetic Reference Super-Populations,8550119,R01GM104125,"['Accounting', 'Address', 'Affect', 'Animal Model', 'Anxiety', 'Architecture', 'Asthma', 'Basic Science', 'Biomedical Research', 'Collection', 'Complex', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Environmental Risk Factor', 'Equilibrium', 'Etiology', 'Experimental Designs', 'Foundations', 'Funding', 'Generations', 'Genetic', 'Genetic Crosses', 'Genetic Programming', 'Genotype', 'Haplotypes', 'Heart Diseases', 'Human Genetics', 'Hybrids', 'Inbreeding', 'Influentials', 'Interdisciplinary Study', 'Joints', 'Lead', 'Machine Learning', 'Maps', 'Medical', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Output', 'Pattern', 'Phenotype', 'Play', 'Plug-in', 'Population', 'Population Analysis', 'Quantitative Trait Loci', 'Randomized', 'Recombinants', 'Relative (related person)', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Variant', 'Weight', 'base', 'cost', 'cost effective', 'design', 'disease phenotype', 'experience', 'flexibility', 'follow-up', 'genetic resource', 'human disease', 'insight', 'interest', 'population based', 'prospective', 'research study', 'response', 'simulation', 'success', 'tool', 'trait']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2013,232648,-0.023139230964232756
"Automatic 3D Quantification of Synapse Distribution in Complex Dendritic Arbor     DESCRIPTION (provided by applicant): The subcellular distribution of synapses is critical for the assembly, function, and plasticity of the nervous system and plays a role in its disorders. Underlying molecular mechanisms, however, remain largely unknown. While advanced multidimensional images, in conjunction with single-cell genetic techniques, have afforded an unprecedented opportunity to understand synapse development at a new level, there is a knowledge gap in our capacity to effectively quantify subcellular synapses from large quantities of three-dimensional images. This is a significant problem and has hampered large-scale studies of the molecular mechanisms of synapse development, especially in neurons with complex arbor-such as Purkinje cells in mammals and lobula plate tangential cells (LPTC) in Drosophila-where existing approaches do not yield complete or robust synapse quantification for the entire dendritic tree and do not scale to efficient genetic screening. The objective of thi project is to bridge this gap by providing tools for quantitative investigation of subcellular synapse distribution and its molecular mechanisms using three-dimensional microscopy images. Specifically, our highly cross- disciplinary team will pursue two aims: (1) Develop automatic algorithms to analyze and quantify synapse distribution in the entire dendritic tree of neurons with complex arbor. Holistic and objective description of synapse density will enable automatic detection of mutant patterns. (2) Develop automatic algorithms to analyze and quantify synapse distribution in different parts of the entire dendritic tree of neurons with complex arbor. Efficient quantification at distinct subcellular locations will assist discovery of novel regulators for different subcellular parts. As a test case, we will use synapse distribution n Drosophila LPTC neurons, which are amenable to both genome-wide genetic screens and genetic manipulations with single-neuron resolution. We will develop reliable methods to characterize the density of inhibitory GABAergic and excitatory cholinergic synapses from three-dimensional fluorescence confocal images. Our algorithms will lead to the next level of mechanistic understanding that controls the subcellular distribution of inhibitory and excitatory synapses, and enable a wide range of quantitative analyses for other types of neurons with similar complexity. Powerful multichannel co-analysis and machine learning approaches will be used to improve synapse detection and subcellular compartment extraction for overcoming challenges in 3D confocal image, including staining artifacts and anisotropic resolution. Algorithms will be developed using a model-guided methodology that emphasizes efficiency for large volume 3D images during genetic screening. Pattern-recognition methods will be used to speed up proofreading of the synapse quantification results. A novel ordering strategy will be adapted for neurons of complex dendritic arbor to quantify subcellular synapses in a functionally meaningful way. The project will produce a set of open-source, extensible tools for automatic synapse quantification and proofreading, with friendly graphical-user interfaces, to serve the neuroscience community.         PUBLIC HEALTH RELEVANCE: The underlying molecular mechanisms for the subcellular distribution of synapses remain largely unknown, which hinders the discovery of novel therapies for many neurological disorders. By developing new, efficient automatic algorithms and open-source tools for quantifying synapses in neurons, this research intends to advance the capacity to effectively analyze large quantities of three-dimensional neuronal images, especially those of complex dendritic arbor. The work will impact public health by enabling a better understanding of disease mechanisms, which is the critical first step toward new treatments, and supports NIH's goal to advance understanding of fundamental biology to uncover the causes of specific diseases.            ",Automatic 3D Quantification of Synapse Distribution in Complex Dendritic Arbor,8574710,R15MH099569,"['Academic Research Enhancement Awards', 'Algorithms', 'Area', 'Biology', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Data', 'Dendrites', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Excitatory Synapse', 'Fluorescence', 'Generations', 'Genetic', 'Genetic Screening', 'Genetic Techniques', 'Goals', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'Inhibitory Synapse', 'Investigation', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Mammals', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Morphology', 'Nervous system structure', 'Neurons', 'Neurosciences', 'Pattern', 'Pattern Recognition', 'Play', 'Public Health', 'Purkinje Cells', 'Pyramidal Cells', 'Research', 'Resolution', 'Role', 'Speed', 'Staging', 'Staining method', 'Stains', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Trees', 'Variant', 'Work', 'base', 'cholinergic synapse', 'density', 'falls', 'genetic manipulation', 'genome-wide', 'graduate student', 'graphical user interface', 'high throughput technology', 'improved', 'in vivo', 'innovation', 'interdisciplinary approach', 'mutant', 'nervous system disorder', 'novel', 'open source', 'public health relevance', 'tool', 'undergraduate student', 'user-friendly']",NIMH,NORTHERN ILLINOIS UNIVERSITY,R15,2013,461165,-0.018623777475591265
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics     DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress.              Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8538499,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Simulate', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'interest', 'mild cognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2013,313357,-0.004922362535102985
"Improving the Detection of Activation in High Resolution fMRI using Multivariate     DESCRIPTION (provided by applicant): The overall goal of this project is to develop a local multivariate analysis software package for fMRI data analysis. It will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. This project will lead to better brain activation maps and thus promote the discovery of currently unknown aspects of brain function. Mass-univariate analysis, such as the general linear model (GLM), is the prevailing fMRI data analysis method. However, it suffers from blurring of edges of activation and potential elimination of the detection of weak activated regions due to routinely applied fixed isotropic spatial Gaussian smoothing. Local multivariate methods such as canonical correlation analysis (CCA) and its variants have been shown to significantly increase the detection power of fMRI activations and improve activation maps. As an advantage, CCA uses adaptive spatial filtering kernels to accurately extract the signal better in a noisy environment. However, there are several drawbacks, particularly low spatial specificity, long computational time, and single-factor experimental design limitation. Furthermore, a parametric estimation method does not exist to determine the family-wise error rate, no extension to group analysis has been investigated, and no studies extending local CCA to nonlinear CCA for fMRI data using kernel methods have been systematically carried out. All these drawbacks prevent local CCA methods from being widely accepted in neuroscience research in fMRI. In this proposal, our goals are to eliminate these drawbacks using novel local multivariate analysis methods (based on CCA) and to develop a software tool to widen its broader application in the neuroscience research community. We expect this software tool to be particularly valuable for neuroscience research where detections of weak activations or spatially localized patterns of activations are desired. As high resolution imaging and computer power advance, we expect an increase in demand for this software tool, thus advancing new discoveries of brain function and more precise spatial localization of activations. As a particular application, we will focus on studying memory actions using a novel event-related recognition paradigm to investigate the effects of familiarity and recollection in subregions of the medial temporal lobes (MTL) for high resolution fMRI. This research will advance our understanding of hippocampal/MTL contributions to memory, which can substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer¿s disease, schizophrenia, and major depression. More generally, it will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods.         PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.                ",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8438968,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'public health relevance', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,RYERSON  UNIVERSITY,R01,2013,279190,-0.028432403967556585
"Statistical Methods for Selection and Evaluation of Biomarkers     DESCRIPTION (provided by applicant): Recent advances in the laboratory sciences have led to the discovery of a large number of candidate biomarkers, which hold great potential for disease diagnosis and treatment. At this time, an important research bottleneck is the lack of well-developed statistical methods for effectively using these candidate biomarkers to enhance clinical practice. It is our goal to develop new tools to select, combine, and evaluate biomarkers for disease classification and treatment selection. Classification markers predict an individual's disease outcome and are useful for the detection of diseases at an early stage when a treatment is most effective. Research proposed in Aim 1 seeks to select and combine markers to improve the classification performance in disease screening and diagnosis. Treatment selection markers predict a patient's response to different therapies and allow for the selection of a therapy that has the best predicted outcome. Aim 2 seeks to develop marker-based treatment selection rules to maximize the benefit to the patient population. A biomarker that is useful for guiding treatment decision to the general population will have different values to different patients due to individual differences in their response to treatment and in their tolerance of the disease harm and treatment cost. Aim 3 seeks to develop a new graphical tool to customize the evaluation of a biomarker for aiding treatment decision based on personal characteristics.  Our statistical methods will apply broadly to general medical fields. In particulr, we will apply these methods to analyze several cancer studies including (1) biomarker studies for prostate cancer and pan- creatic cancer from the Early Detection and Research Network; (2) the Women's Health Initiative breast cancer genome-wide association study; and (3) the Oncotype-Dx breast cancer study from the Southwest Oncology Group. Programs and algorithms developed in this proposal will be made available to public.         PUBLIC HEALTH RELEVANCE: The focus of this proposal is to develop novel statistical methods for the design and analysis of biomarker studies. In particular, the proposed methods will develop marker combinations to improve disease diagnosis, develop treatment selection rules to cost-effectively reduce population disease burden, and help patients and clinicians make informed decisions about the use of medical tests in clinical practices.            ",Statistical Methods for Selection and Evaluation of Biomarkers,8483561,R01GM106177,"['Address', 'Algorithms', 'Area', 'Biological Markers', 'Case-Control Studies', 'Characteristics', 'Classification', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Early Detection Research Network', 'Evaluation', 'General Population', 'Goals', 'Individual', 'Individual Differences', 'Laboratories', 'Linear Models', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Measures', 'Medical', 'Methods', 'Modeling', 'Outcome', 'Pancreas', 'Patients', 'Performance', 'Population', 'Probability', 'ROC Curve', 'Research', 'Research Design', 'Risk Factors', 'Sampling', 'Scheme', 'Science', 'Selection for Treatments', 'Sensitivity and Specificity', 'Southwest Oncology Group', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Testing', 'Time', 'Treatment Cost', 'Women&apos', 's Health', 'base', 'burden of illness', 'cancer genome', 'case control', 'clinical practice', 'cohort', 'cost', 'design', 'disease classification', 'disease diagnosis', 'disorder risk', 'genome wide association study', 'improved', 'interest', 'malignant breast neoplasm', 'novel', 'patient population', 'programs', 'public health relevance', 'randomized trial', 'response', 'screening', 'tool', 'treatment effect']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2013,324656,-0.04450424255662555
"Statistical methods for large and complex databases of ultra-high-dimensional  Abstract Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8614974,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2013,373406,0.018358594368344916
"Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies    DESCRIPTION (provided by applicant): Common mental disorders such as Alzheimer's disease and schizophrenia are largely heritable with complex genetic underpinnings. Large-scale genome-wide association studies that contrast DNA sequence data from patients and controls have recently identified novel genetic risk variants for these disorders. Nevertheless, the processes through which genotype increases risk are yet to be fully characterized.  Neuroimaging offers a richer picture of the underlying disease processes than a clinical diagnosis. Thus the joint analysis of neuroimaging and genetics data promises to advance our understanding of these processes. Today, neuroimaging genetics studies however face important challenges that obstruct progress: small sample sizes, modest effect sizes, and the extreme dimensionality of the data limit statistical power and thus our ability to explore the complex and subtle associations between genes, neuroanatomy and clinical decline. Currently, the prevalent approach in neuroimaging genetics is to concentrate the analysis on a small number of anatomic regions of interest and/or candidate genes and often ignore a large portion of the data. The core goal of the proposed project is to develop computational tools that will take full advantage of the richness in the datasets and facilitate the exploration of the multifaceted associations between genotype, neuroimaging measurements and clinical phenotype. The proposed project will use advanced multivariate pattern analysis methods such as support vector machines to compute image-based and genetic scores that reflect pathology. We will validate the tools based on their association with classical biomarkers of disease. Finally, we will develop a model that uses both imaging and genotype data to predict future clinical outcome. We expect these tools will enable progress along three directions relevant to complex mental disorders, e.g. late-onset Alzheimer's disease (AD): (1) confirming and characterizing risk genes, (2) identifying disease-specific anatomical alterations in healthy individuals, and (3) early diagnosis and prognosis. The project will (1) use three already-collected large-scale datasets to apply the developed tools to AD, (2) build on cutting-edge image processing algorithms that we have been developing, and (3) allow the candidate to receive further training in neuroanatomy, mental disorders and genetics, forming the foundation for his future career as an independent researcher.       n/a",Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies,8535152,K25EB013649,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Anatomy', 'Biological Markers', 'Brain', 'Candidate Disease Gene', 'Clinical', 'Clinical Trials', 'Cognitive', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Dementia', 'Development', 'Disease', 'Early Diagnosis', 'Event', 'Exhibits', 'Face', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Research', 'Genetic Risk', 'Genotype', 'Goals', 'Hereditary Disease', 'Hippocampus (Brain)', 'Image', 'Individual', 'Joints', 'Late Onset Alzheimer Disease', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Measurement', 'Mental disorders', 'Methods', 'Mining', 'Modeling', 'Motivation', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phenotype', 'Probability', 'Process', 'Recruitment Activity', 'Research Personnel', 'Risk', 'Sample Size', 'Schizophrenia', 'Testing', 'Thick', 'Training', 'base', 'career', 'clinical Diagnosis', 'clinical phenotype', 'computerized tools', 'data modeling', 'disorder risk', 'entorhinal cortex', 'genetic risk factor', 'genetic variant', 'genome wide association study', 'high risk', 'image processing', 'improved', 'in vivo', 'interest', 'mild cognitive impairment', 'molecular pathology', 'neuroimaging', 'novel', 'outcome forecast', 'pre-clinical', 'programs', 'risk variant', 'tool']",NIBIB,MASSACHUSETTS GENERAL HOSPITAL,K25,2013,175392,-0.04542495527494405
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8538496,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2013,234332,0.006366413453524833
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data.  PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.          ",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,8520329,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Health', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'NMR Spectroscopy', 'Negative Staining', 'Neighborhoods', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2013,301072,-0.0189085372795729
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8269876,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2012,323305,-0.005255698471903612
"Statistical Modeling of Complex Traits in Genetic Reference Super-Populations     DESCRIPTION (provided by applicant):     Genetic crosses in model organisms play an essential role in understanding the heritable architecture of medically relevant phenotypes. Traditionally, such crosses have tended to be on a small scale with either limited power to detect genetic effects or limited resolution to localize causal variants. Recently, however, the emergence of larger-scale interdisciplinary research, cheaper genotyping and parallel advances in human genetics, have spurred the development of more sophisticated and powerful experimental designs. Genetic Resource Populations (GRPs) use economies of scale to provide cost-effective and replicable platforms for genetic studies. This project concerns the largest, most ambitious GRP in mouse genetics to date, the Collaborative Cross (CC), and a series of crosses and designs related to or derived from it: the Diversity Outbred (DO) cross, the CC Recombinant Inbred Cross (CC-RIX) and the diallel. Experiments on each separate cross provide distinct information about the heritable architecture of a target complex disease. In combination, this Genetic Reference Super-Population (GRSP) potentially provides an unparalleled basis for cross-study replication and integration in mouse genetics. This project aims to develop statistical methods that advance the current state of complex trait analysis of these populations separately, and, by exploiting the unique structure that connects them, proposes to develop a statistical framework that allows for their joint use.  Aim 1 develops a Bayesian probabilistic framework for haplotype-based analysis of quantitative trait loci (QTL). Aim 1a develops a statistical software module for flexible haplotype-based analysis, which can be ex- tended by the researcher to model a rich variety of designs and disease types. Aim 1b will adapt machine learning techniques to provide posterior inference of the allelic series of a QTL. Aim 1c will incorporate Bayesian modeling of polygenic effects.  Aim 2 and 3 concern joint analysis, building on the foundation set by Aim 1. Aim 2 develops methods to optimize experimental design of follow-up studies in one population given results from another. Aim 2a uses the diallel to inform design of CC/CC-RIX/DO experiments. Aim 2b uses partial data on CC/CC-RIX/DO to guide collection of additional data. Aim 3 explores models for jointly analyzing multiple populations in the GRSP, using complementary datasets to stabilize analysis at single QTL (Aim 3a) and across multiple QTL (Aim 3b).  These aims address specific and persistent challenges in the cost-effective design and efficient analysis of multiparent genetic data, in particular the CC, DO, CC-RIX and diallel. The project will generate tools useful for a wide range of model organism crosses and can be applied to the genetic study of any complex disease.        PUBLIC HEALTH RELEVANCE:     The proposed research will lead to improvements in the analysis and design of genetic studies on animal models of human disease. Because the project focuses on statistical methodology applied to experimental mouse populations, the scientific output of the project is expected to be applicable to basic research focusing on any medical condition that can be studied in the mouse.                  The proposed research will lead to improvements in the analysis and design of genetic studies on animal models of human disease. Because the project focuses on statistical methodology applied to experimental mouse populations, the scientific output of the project is expected to be applicable to basic research focusing on any medical condition that can be studied in the mouse.            ",Statistical Modeling of Complex Traits in Genetic Reference Super-Populations,8420828,R01GM104125,"['Accounting', 'Address', 'Affect', 'Animal Model', 'Anxiety', 'Architecture', 'Asthma', 'Basic Science', 'Biomedical Research', 'Collection', 'Complex', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Environmental Risk Factor', 'Equilibrium', 'Etiology', 'Experimental Designs', 'Foundations', 'Funding', 'Generations', 'Genetic', 'Genetic Crosses', 'Genetic Programming', 'Genotype', 'Haplotypes', 'Heart Diseases', 'Human Genetics', 'Hybrids', 'Inbreeding', 'Influentials', 'Interdisciplinary Study', 'Joints', 'Lead', 'Machine Learning', 'Maps', 'Medical', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Output', 'Pattern', 'Phenotype', 'Play', 'Plug-in', 'Population', 'Population Analysis', 'Quantitative Trait Loci', 'Randomized', 'Recombinants', 'Relative (related person)', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Variant', 'Weight', 'base', 'cost', 'cost effective', 'design', 'disease phenotype', 'experience', 'flexibility', 'follow-up', 'genetic resource', 'human disease', 'insight', 'interest', 'population based', 'prospective', 'research study', 'response', 'simulation', 'success', 'tool', 'trait']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2012,241086,-0.020896776640201488
"Bioinformatics Strategies for Multidimensional Brain Imaging Genetics     DESCRIPTION (provided by applicant):         Today's generation of multi-modal imaging systems produces massive high dimensional data sets, which when coupled with high throughput genotyping data such as single nucleotide polymorphisms (SNPs), provide exciting opportunities to enhance our understanding of phenotypic characteristics and the genetic architecture of human diseases. However, the unprecedented scale and complexity of these data sets have presented critical computational bottlenecks requiring new concepts and enabling tools. To address these challenges, using the study of Alzheimer's disease (AD) as a test bed, this project will develop and validate novel bioinformatics strategies for multidimensional brain imaging genetics. Aim 1 is to develop a novel bi- multivariate analysis strategy, S3K-CCA, for studying imaging genetic associations. Existing imaging genetics methods are typically designed to discover single-SNP-single-QT, single-SNP-multi-QT or multi-SNP-single- QT associations, and have limited power in revealing complex relationships between interlinked genetic markers and correlated brain phenotypes. To overcome this limitation, S3K-CCA is designed to be a sparse bi- multivariate learning model that simultaneously uses multiple response variables with multiple predictors for analyzing large-scale multi-modal neurogenomic data. Aim 2 is to develop HD-BIG, a visualization and systems biology framework for integrative analysis of High-Dimensional Brain Imaging Genetics data. Machine learning strategies to seamlessly incorporate valuable domain knowledge to produce biologically meaningful results is still an under-explored area in imaging genetics. In this aim, we will develop a user-friendly heat map interface to visualize high-dimensional results, adjust learning parameters and strategies, interact with existing bioinformatics resources and tools, and facilitate visual exploratory and systems biology analysis. A novel imaging genetic enrichment analysis (IGEA) method will be developed to identify relevant genetic pathways and associated brain circuits, and to reveal complex relationships among them. Aim 3 is to evaluate the proposed S3K-CCA and IGEA methods and the HD-BIG framework using both simulated and real imaging genetics data. This project is expected to produce novel bioinformatics algorithms and tools for comprehensive joint analysis of large scale heterogeneous imaging genetics data. The availability of these powerful methods is critical to the success of many imaging genetics initiatives. In addition, they can also help enable new computational applications in other areas of biomedical research where systematic and integrative analysis of large-scale multi-modal data is critical. Using AD as an exemplar, the proposed methods will demonstrate the potential for enhancing mechanistic understanding of complex disorders, which can benefit public health outcomes by facilitating diagnostic and therapeutic progress.              Public Health Relevance (Narrative) Recent advances in multi-modal imaging and high throughput genotyping techniques provide exciting opportunities to enhance our understanding of phenotypic characteristics and underlying genetic mechanisms associated with human diseases. This proposal seeks to develop new bi-multivariate machine learning models and novel enrichment analysis methods, coupled with a visualization and systems biology framework, for integrative analysis of high-dimensional brain imaging genetics data. The methods and tools are developed and evaluated in an imaging genetic study of Alzheimer's disease, and can also be applied to many other disorders to improve public health outcomes by facilitating diagnostic and therapeutic progress.",Bioinformatics Strategies for Multidimensional Brain Imaging Genetics,8342777,R01LM011360,"['Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Architecture', 'Area', 'Atlases', 'Beds', 'Biochemical Pathway', 'Bioinformatics', 'Biological Markers', 'Biomedical Research', 'Brain', 'Brain imaging', 'Characteristics', 'Clinical', 'Complex', 'Coupled', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Epidemiology', 'Evaluation', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Variation', 'Genomics', 'Genotype', 'Heart', 'Heating', 'Human', 'Image', 'Imagery', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Multivariate Analysis', 'Ontology', 'Outcome', 'Participant', 'Pathway interactions', 'Phenotype', 'Positron-Emission Tomography', 'Public Health', 'Research', 'Resources', 'Simulate', 'Single Nucleotide Polymorphism', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual', 'base', 'cohort', 'density', 'design', 'genetic association', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'interest', 'mild neurocognitive impairment', 'neuroimaging', 'neuropsychological', 'novel', 'novel strategies', 'public health relevance', 'public-private partnership', 'response', 'simulation', 'success', 'tool', 'trait', 'user-friendly']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2012,405500,-0.004922362535102985
"Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies    DESCRIPTION (provided by applicant): Common mental disorders such as Alzheimer's disease and schizophrenia are largely heritable with complex genetic underpinnings. Large-scale genome-wide association studies that contrast DNA sequence data from patients and controls have recently identified novel genetic risk variants for these disorders. Nevertheless, the processes through which genotype increases risk are yet to be fully characterized.  Neuroimaging offers a richer picture of the underlying disease processes than a clinical diagnosis. Thus the joint analysis of neuroimaging and genetics data promises to advance our understanding of these processes. Today, neuroimaging genetics studies however face important challenges that obstruct progress: small sample sizes, modest effect sizes, and the extreme dimensionality of the data limit statistical power and thus our ability to explore the complex and subtle associations between genes, neuroanatomy and clinical decline. Currently, the prevalent approach in neuroimaging genetics is to concentrate the analysis on a small number of anatomic regions of interest and/or candidate genes and often ignore a large portion of the data. The core goal of the proposed project is to develop computational tools that will take full advantage of the richness in the datasets and facilitate the exploration of the multifaceted associations between genotype, neuroimaging measurements and clinical phenotype. The proposed project will use advanced multivariate pattern analysis methods such as support vector machines to compute image-based and genetic scores that reflect pathology. We will validate the tools based on their association with classical biomarkers of disease. Finally, we will develop a model that uses both imaging and genotype data to predict future clinical outcome. We expect these tools will enable progress along three directions relevant to complex mental disorders, e.g. late-onset Alzheimer's disease (AD): (1) confirming and characterizing risk genes, (2) identifying disease-specific anatomical alterations in healthy individuals, and (3) early diagnosis and prognosis. The project will (1) use three already-collected large-scale datasets to apply the developed tools to AD, (2) build on cutting-edge image processing algorithms that we have been developing, and (3) allow the candidate to receive further training in neuroanatomy, mental disorders and genetics, forming the foundation for his future career as an independent researcher.       n/a",Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies,8308347,K25EB013649,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Anatomy', 'Biological Markers', 'Brain', 'Candidate Disease Gene', 'Clinical', 'Clinical Trials', 'Cognitive', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Dementia', 'Development', 'Disease', 'Early Diagnosis', 'Event', 'Exhibits', 'Face', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Research', 'Genetic Risk', 'Genotype', 'Goals', 'Hereditary Disease', 'Hippocampus (Brain)', 'Image', 'Individual', 'Joints', 'Late Onset Alzheimer Disease', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Measurement', 'Mental disorders', 'Methods', 'Mining', 'Modeling', 'Motivation', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phenotype', 'Probability', 'Process', 'Recruitment Activity', 'Research Personnel', 'Risk', 'Sample Size', 'Schizophrenia', 'Testing', 'Thick', 'Training', 'Variant', 'base', 'career', 'clinical Diagnosis', 'clinical phenotype', 'computerized tools', 'data modeling', 'disorder risk', 'entorhinal cortex', 'genetic risk factor', 'genetic variant', 'genome wide association study', 'high risk', 'image processing', 'improved', 'in vivo', 'interest', 'mild neurocognitive impairment', 'molecular pathology', 'neuroimaging', 'novel', 'outcome forecast', 'pre-clinical', 'programs', 'tool']",NIBIB,MASSACHUSETTS GENERAL HOSPITAL,K25,2012,175392,-0.04542495527494405
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8216289,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2012,255679,0.006366413453524833
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,8281471,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Health', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'NMR Spectroscopy', 'Negative Staining', 'Neighborhoods', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2012,272755,-0.0189085372795729
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8062031,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Health', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2011,359403,0.012826744588444572
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.      PUBLIC HEALTH RELEVANCE: The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.             The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8108523,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2011,341852,-0.01611605058489367
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,8133946,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computational algorithm', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2011,297497,0.007543730944699584
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,8055907,R01LM009731,"['Address', 'Algorithms', 'Area', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'International', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Population', 'Prevention', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'disorder prevention', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2011,325956,0.01997328132733593
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,8079474,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,250488,-0.03669265170162069
"Statistical Methods for Correlated and High-Dimensional Biomedical Data We propose in the renewal of this MERIT award application to continue developing advanced statistical and computational methods for analysis of correlated and high-dimensional data, which arise frequently in health science research, especially in cancer research. Correlated data are often observed in observational studies and clinical trials, such as longitudinal studies and familial studies. High-dimensional data have emerged rapidly in recent years due to the advance of high-throughput 'omics technologies, e.g, in Genome- Wide Association Studies (GWAS), and genome-wide epigenetic (DNA methylation) studies. Massive next generation sequencing data are soon available. There is an urgent need to develop advanced stati stical and computational methods for analyzing such high throughput 'omics data in observational studies and clinical trials. We propose to develop statistical and computational methods for analysis of (1) genome-wide association studies; (2) sequencing data for studying rare variant effects; (3) genome-wide DNA methylation studies; (4) gene-gene and gene-environment interactions. We will develop methods for both case-control studies and cohort studies, such as longitudinal studies and survival studies. We will study the theoretical properties of the proposed methods and evaluate the finite s ample performance using simulation studies. We will develop efficient numerical algorithms and user-friendly statistical software, and disseminate these tools to health sciences researchers. In collaboration with biomedical investigators, we will apply the proposed models methods to data from several genome-wide epidemiological studies in cancer and other chronic diseases. RELEVANCE (See instructions):  Development of new statistical methods for analysis of correlated and high-dimensional data will provide  powerful analytic tools to advance 'omics research in observational studies and clinical trials and to help  understand the roles of genes, gene products, and the environment in causing human diseases.",Statistical Methods for Correlated and High-Dimensional Biomedical Data,8117858,R37CA076404,"['Advanced Development', 'Aging', 'Algorithms', 'Award', 'Biomedical Research', 'Case-Control Studies', 'Childhood', 'Chronic Disease', 'Clinical Treatment', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Computer software', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Development', 'Dimensions', 'Disease', 'Environment', 'Epidemiologic Studies', 'Epigenetic Process', 'Etiology', 'Explosion', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health Sciences', 'Human Genome', 'Instruction', 'Intervention', 'Investigation', 'Knowledge', 'Longitudinal Studies', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measures', 'Messenger RNA', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Prevention strategy', 'Property', 'Proteins', 'Public Health Schools', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Single Nucleotide Polymorphism', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Technology', 'Testing', 'Theoretical Studies', 'Variant', 'anticancer research', 'cancer risk', 'case control', 'computer science', 'computerized data processing', 'epigenomics', 'gene environment interaction', 'gene interaction', 'genome wide association study', 'genome-wide', 'health science research', 'high throughput analysis', 'human disease', 'malignant breast neoplasm', 'new technology', 'next generation', 'novel', 'programs', 'simulation', 'tool', 'user-friendly']",NCI,HARVARD SCHOOL OF PUBLIC HEALTH,R37,2011,308344,-0.03287245027730207
"Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies    DESCRIPTION (provided by applicant): Common mental disorders such as Alzheimer's disease and schizophrenia are largely heritable with complex genetic underpinnings. Large-scale genome-wide association studies that contrast DNA sequence data from patients and controls have recently identified novel genetic risk variants for these disorders. Nevertheless, the processes through which genotype increases risk are yet to be fully characterized.  Neuroimaging offers a richer picture of the underlying disease processes than a clinical diagnosis. Thus the joint analysis of neuroimaging and genetics data promises to advance our understanding of these processes. Today, neuroimaging genetics studies however face important challenges that obstruct progress: small sample sizes, modest effect sizes, and the extreme dimensionality of the data limit statistical power and thus our ability to explore the complex and subtle associations between genes, neuroanatomy and clinical decline. Currently, the prevalent approach in neuroimaging genetics is to concentrate the analysis on a small number of anatomic regions of interest and/or candidate genes and often ignore a large portion of the data. The core goal of the proposed project is to develop computational tools that will take full advantage of the richness in the datasets and facilitate the exploration of the multifaceted associations between genotype, neuroimaging measurements and clinical phenotype. The proposed project will use advanced multivariate pattern analysis methods such as support vector machines to compute image-based and genetic scores that reflect pathology. We will validate the tools based on their association with classical biomarkers of disease. Finally, we will develop a model that uses both imaging and genotype data to predict future clinical outcome. We expect these tools will enable progress along three directions relevant to complex mental disorders, e.g. late-onset Alzheimer's disease (AD): (1) confirming and characterizing risk genes, (2) identifying disease-specific anatomical alterations in healthy individuals, and (3) early diagnosis and prognosis. The project will (1) use three already-collected large-scale datasets to apply the developed tools to AD, (2) build on cutting-edge image processing algorithms that we have been developing, and (3) allow the candidate to receive further training in neuroanatomy, mental disorders and genetics, forming the foundation for his future career as an independent researcher.      PUBLIC HEALTH RELEVANCE: Project Narrative/Relevance We will develop computational tools for analyzing complex associations between images, genotype and clinical phenotype. The tools will be user-friendly and freely available, and will potentially facilitate accurate early diagnosis and prognosis of mental disorders such as Alzheimer's.             n/a",Multivariate Pattern Analysis Methods for Neuroimaging Genetics Studies,8165447,K25EB013649,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Anatomy', 'Biological Markers', 'Brain', 'Candidate Disease Gene', 'Clinical', 'Clinical Trials', 'Cognitive', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Dementia', 'Development', 'Disease', 'Early Diagnosis', 'Event', 'Exhibits', 'Face', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Research', 'Genetic Risk', 'Genotype', 'Goals', 'Hereditary Disease', 'Hippocampus (Brain)', 'Image', 'Individual', 'Joints', 'Late Onset Alzheimer Disease', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Measurement', 'Mental disorders', 'Methods', 'Mining', 'Modeling', 'Motivation', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phenotype', 'Probability', 'Process', 'Recruitment Activity', 'Research Personnel', 'Risk', 'Sample Size', 'Schizophrenia', 'Testing', 'Thick', 'Training', 'Variant', 'base', 'career', 'clinical Diagnosis', 'clinical phenotype', 'computerized tools', 'data modeling', 'disorder risk', 'entorhinal cortex', 'genetic risk factor', 'genetic variant', 'genome wide association study', 'high risk', 'image processing', 'improved', 'in vivo', 'interest', 'mild neurocognitive impairment', 'molecular pathology', 'neuroimaging', 'novel', 'outcome forecast', 'pre-clinical', 'programs', 'tool']",NIBIB,MASSACHUSETTS GENERAL HOSPITAL,K25,2011,175392,-0.044061423828302436
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,8098196,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Health', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'NMR Spectroscopy', 'Negative Staining', 'Neighborhoods', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2011,311172,-0.0189085372795729
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,8049892,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2010,5742,0.021690985144954378
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8068069,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,51400,0.012826744588444572
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7828142,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,338802,0.012826744588444572
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7912919,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,304151,0.007543730944699584
"Prediction of influenza antigenic variants using a novel sparse multitask learnin    DESCRIPTION (provided by applicant): Influenza and influenza related complication lead to more than 200,000 hospitalizations and approximately 36,000 deaths in the United States each year, and vaccination is the primary option for reducing influenza effect. A large amount of global efforts has to be made each year to identify antigenic variants and decide whether new vaccine strains are needed. Current laboratory based antigenic characterization processes are labor intensive and time consuming, and it has been the bottleneck for generating an effective influenza vaccination program. A robust method without such a laboratory characterization is demanding for rapid identification of influenza antigenic variants. This project proposes to develop a novel sparse multitask learning method in predicting influenza antigenic variants solely based on the input of protein sequences, and further to apply this method in mapping antigenic drift pathway of A/H3N2 influenza viruses and studying antigenic drift patterns leading to influenza outbreaks. This method is based on the assumption that influenza antigenicity would be determined by certain features in hemagglutinin (HA) protein sequence and tertiary structure. This assumption was well evidenced that the viruses with conserved HAs generated cross-reactions in serological reactions and also provided cross- protection in both laboratory experiments and field practices. The proposed method is novel since it combines multitask learning and sparse learning. Therefore not only this project will develop significant technology for antigenic variant screen, but also new machine learning methods. This project will facilitate vaccine strain selection since the proposed method can potentially reduce and even eliminate serological assay, one of the most labor intensive procedures, in influenza surveillance. In addition, the antigenicity specific features and the drift patterns causing influenza outbreaks to be identified in this study will enhance our understanding about antigen-antibody interaction thus enhance our knowledge in influenza immunology and serology. Furthermore, the proposed method is potentially applicable in characterizing antigenic properties of other pathogens with significant antigenic variations, for example, rotavirus. The specific aims are the following: (1) Development of a novel sparse multitask learning method in generating antigenic distance matrix using hemagglutinin inhibition (HI) data; (2) Development of a quantitative method for predicting antigenic variants in silicon; (3) Application of this method in studying seasonal influenza antigenic drift pathway and antigenic drift patterns leading to influenza outbreaks. This nature of this study is to address a novel predictive method for measuring antigenic divergence between influenza viruses, which is critical in influenza vaccine strain selection. Thus, we are submitting this project to the broad challenge area (06) Enabling Technologies and fit for the Specific Challenge 06-GM-103: development of predictive method for molecular structure, recognition, and ligand interaction.       PUBLIC HEALTH RELEVANCE: This study is to develop a novel computational method for influenza antigenic variant prediction, which is very useful in influenza vaccine strain selection. This method will also be applied in studying antigenic drift patterns leading to influenza outbreak and epidemics.               Project Narrative This study is to develop a novel computational method for influenza antigenic variant prediction, which is very useful in influenza vaccine strain selection. This method will also be applied in studying antigenic drift patterns leading to influenza outbreak and epidemics.",Prediction of influenza antigenic variants using a novel sparse multitask learnin,7835340,RC1AI086830,"['Address', 'Amino Acid Sequence', 'Antibodies', 'Antigenic Variation', 'Antigens', 'Area', 'Biological Assay', 'Cessation of life', 'Communities', 'Complication', 'Computing Methodologies', 'Cross Reactions', 'Data', 'Development', 'Epidemic', 'Hemagglutinin', 'Hospitalization', 'Immunology', 'Influenza', 'Influenza A Virus, H3N2 Subtype', 'Influenza vaccination', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Ligands', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Membrane Glycoproteins', 'Methods', 'Molecular Structure', 'Mutation', 'Nature', 'Online Systems', 'Pathway interactions', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Procedures', 'Process', 'Property', 'Reaction', 'Research', 'Rotavirus', 'Seasons', 'Serologic tests', 'Serological', 'Silicon', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Trees', 'United States', 'Vaccination', 'Vaccines', 'Variant', 'Viral', 'Virus', 'base', 'genetic analysis', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'multitask', 'novel', 'novel vaccines', 'pathogen', 'programs', 'public health relevance', 'research study', 'seasonal influenza', 'tool', 'vector']",NIAID,MISSISSIPPI STATE UNIVERSITY,RC1,2010,412913,-0.021374342771219187
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7805478,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2010,339537,0.01997328132733593
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7825424,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2010,185505,0.021325707571986603
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7798186,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'allograft rejection', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2010,289814,0.0038096474850894567
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7902293,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's Disease patient registry', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronics', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'National Human Genome Research Institute', 'National Institute on Aging', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Statistical Methods', 'System', 'Testing', 'Text', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'biobank', 'case control', 'cohort', 'cost', 'data sharing', 'development policy', 'economic cost', 'gene environment interaction', 'genome wide association study', 'genome-wide', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'population based', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2010,889184,-0.03142966860222747
"Analysis Tool for Heritable and Envirnonmental Network Associations    DESCRIPTION (provided by applicant):       The efforts of the human genome project are beginning to provide important findings for human health. Technological advances in the laboratory, particularly in characterizing human genomic variation, have created new approaches for studying the human genome. However, current statistical and computational strategies are taking only partial advantage of this wealth of information. In the quest for disease susceptibility genes for common, complex disease, we are faced with many challenges. Selecting genetic, clinical, and environmental factors important for the trait of interest is increasingly more difficult as high throughput data generation technologies are developed. We know that genes do not act in isolation, thus numerous other factors are likely important in complex disease phenotypes. However, techniques for robust statistical modeling of important variables to predict clinical outcomes are limited in their capability for interaction effects. Ultimately, we want to know what factors are important to provide superior prevention, diagnosis, and treatment of human disease. Unfortunately, interpretation of statistical models in a meaningful way for biomedical research has been lacking due to the inherent difficulty in making such connections. Thus, a technology that embraces the complexity of human disease and integrates multiple data sources including biological knowledge from the public domain, through a powerful analytical framework is essential for dissecting the architecture of common diseases. ATHENA: the Analysis Tool for Heritable and Environmental Network Associations is a novel framework that incorporates variable selection, modeling, and interpretation to learn more about diseases of public health interest. As the field gains experience in analyzing large scale genomic data, it is crucial that we learn from each other and develop and codify the best strategies.            Many common, complex diseases are likely due to a combination of genetic and environmental risk factors. Out ability to extract all of the meaningful information from very large genomic and phenotypic datasets has been limited by our analytic strategies. The methodology described in this proposal is a powerful new approach to maximize the information learned from large datasets to improve prevention, diagnosis, and treatment of diseases of public health interest.",Analysis Tool for Heritable and Envirnonmental Network Associations,7860712,R01LM010040,"['Architecture', 'Arts', 'Base Pairing', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Biology', 'Biomedical Research', 'Candidate Disease Gene', 'Clinical', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Diagnosis', 'Disease', 'Disease susceptibility', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genome', 'Human Genome Project', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Methodology', 'Modeling', 'Noise', 'Outcome', 'Prevention', 'Proteomics', 'Public Domains', 'Public Health', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Simulate', 'Single Nucleotide Polymorphism', 'Solutions', 'Statistical Models', 'Susceptibility Gene', 'Techniques', 'Technology', 'Time', 'Variant', 'Vision', 'base', 'computerized tools', 'disease phenotype', 'disorder risk', 'experience', 'flexibility', 'follow-up', 'gene environment interaction', 'gene interaction', 'genetic analysis', 'genome wide association study', 'human disease', 'improved', 'interest', 'novel', 'novel strategies', 'simulation', 'success', 'tool', 'tool development', 'trait']",NLM,VANDERBILT UNIVERSITY,R01,2010,297123,-0.032695557855295765
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7858165,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,253269,-0.03669265170162069
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,7901378,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'Muscle Rigidity', 'NMR Spectroscopy', 'Negative Staining', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'public health relevance', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2010,273363,-0.0189085372795729
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7670456,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,74750,0.021690985144954378
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant): This proposal is submitted in response to NOT-OD-09-058 NIH Announces the Availability of Recovery Act Funds for Competitive Revision Applications. Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Our currently funded R03 grant, ""Recursive partitioning and ensemble methods for classifying an ordinal response,"" consists of the following three specific aims (SA.1) extend the recursive partitioning and random forest classification methodologies for predicting an ordinal response by developing computational tools for the R programming environment including implementing our ordinal impurity criteria in rpart and implementing the ordinal impurity criteria in randomForest; (SA.2) evaluate the proposed ordinal classification methods in comparison to existing nominal and continuous response methods using simulated, benchmark, and gene expression datasets; and (SA.3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been described. Herein we propose to extend the L1 penalized method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing a model-based ordinal classification methodology applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies considered in the parent grant. The specific aims of this competitive revision application are to: Aim 1) Extend the L1 penalized methodology to enable predicting an ordinal response by developing computational tools for the R programming environment; Aim 2) Using simulated, benchmark, and gene expression datasets, evaluate L1 penalized ordinal response models by comparing error rates from our L1 fitting algorithm to those obtained when using a forward variable selection modeling strategy and our ordinal random forest approach; and Aim 3) Evaluate methods for assessing important covariates from L1 penalized ordinal response models.           This project will develop L1 penalized ordinal response models and implement them in the R programming environment. By conducting extensive comparisons of various ordinal response modeling methods using simulated, benchmark, and gene expression datasets, we will be able to make a recommendation regarding ordinal response modeling to the scientific community. This research is significant since the ordinal response modeling methods developed during the project period will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.",Recursive partitioning and ensemble methods for classifying an ordinal response,7805045,R03LM009347,"['Advocate', 'Algorithms', 'Applications Grants', 'Area', 'Behavioral Research', 'Benchmarking', 'Bioconductor', 'Biopsy Specimen', 'Cancer Patient', 'Chronic Hepatitis', 'Classification', 'Clinical', 'Communities', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Drug toxicity', 'Economics', 'Education', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Gene Expression', 'Genomics', 'Grant', 'Health', 'Health Status', 'Health Surveys', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Literature', 'Location', 'Logistics', 'Machine Learning', 'Mathematics', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Neoplasm Metastasis', 'Occupations', 'Outcome', 'Patients', 'Performance', 'Positioning Attribute', 'Progressive Disease', 'Recommendation', 'Recovery', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Sample Size', 'Sampling', 'Science', 'Scoring Method', 'Simulate', 'Solid Neoplasm', 'Stable Disease', 'Staging', 'Technology', 'Translational Research', 'Travel', 'Trees', 'United States National Institutes of Health', 'base', 'computerized tools', 'cost', 'forest', 'heuristics', 'improved', 'indexing', 'interest', 'liver biopsy', 'meetings', 'neglect', 'novel', 'parent grant', 'partial response', 'preference', 'programs', 'research study', 'response', 'simulation', 'social', 'software development', 'symposium', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,75000,0.00534762420372902
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7577491,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2009,342223,0.012826744588444572
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7670408,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,311541,0.007543730944699584
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7848604,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170861,0.01997328132733593
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7901729,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170789,0.01997328132733593
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7612766,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,342967,0.01997328132733593
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7620994,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2009,263148,0.021325707571986603
"Novel Analytic Techniques to Assess Physical Activity Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship. n/a",Novel Analytic Techniques to Assess Physical Activity,7809191,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Data', 'Diet', 'Discriminant Analysis', 'Dose', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'intervention effect', 'markov model', 'meetings', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2009,140804,0.02090604763436637
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7599555,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2009,290671,0.0038096474850894567
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7684273,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's Disease patient registry', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronics', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'National Human Genome Research Institute', 'National Institute on Aging', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Statistical Methods', 'System', 'Testing', 'Text', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'biobank', 'case control', 'cohort', 'cost', 'data sharing', 'development policy', 'economic cost', 'gene environment interaction', 'genome wide association study', 'genome-wide', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'population based', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2009,1039667,-0.03142966860222747
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7921317,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's Disease patient registry', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronics', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'National Human Genome Research Institute', 'National Institute on Aging', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Statistical Methods', 'System', 'Testing', 'Text', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'biobank', 'case control', 'cohort', 'cost', 'data sharing', 'development policy', 'economic cost', 'gene environment interaction', 'genome wide association study', 'genome-wide', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'population based', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2009,118469,-0.03142966860222747
"Analysis Tool for Heritable and Envirnonmental Network Associations    DESCRIPTION (provided by applicant):       The efforts of the human genome project are beginning to provide important findings for human health. Technological advances in the laboratory, particularly in characterizing human genomic variation, have created new approaches for studying the human genome. However, current statistical and computational strategies are taking only partial advantage of this wealth of information. In the quest for disease susceptibility genes for common, complex disease, we are faced with many challenges. Selecting genetic, clinical, and environmental factors important for the trait of interest is increasingly more difficult as high throughput data generation technologies are developed. We know that genes do not act in isolation, thus numerous other factors are likely important in complex disease phenotypes. However, techniques for robust statistical modeling of important variables to predict clinical outcomes are limited in their capability for interaction effects. Ultimately, we want to know what factors are important to provide superior prevention, diagnosis, and treatment of human disease. Unfortunately, interpretation of statistical models in a meaningful way for biomedical research has been lacking due to the inherent difficulty in making such connections. Thus, a technology that embraces the complexity of human disease and integrates multiple data sources including biological knowledge from the public domain, through a powerful analytical framework is essential for dissecting the architecture of common diseases. ATHENA: the Analysis Tool for Heritable and Environmental Network Associations is a novel framework that incorporates variable selection, modeling, and interpretation to learn more about diseases of public health interest. As the field gains experience in analyzing large scale genomic data, it is crucial that we learn from each other and develop and codify the best strategies.            Many common, complex diseases are likely due to a combination of genetic and environmental risk factors. Out ability to extract all of the meaningful information from very large genomic and phenotypic datasets has been limited by our analytic strategies. The methodology described in this proposal is a powerful new approach to maximize the information learned from large datasets to improve prevention, diagnosis, and treatment of diseases of public health interest.",Analysis Tool for Heritable and Envirnonmental Network Associations,7642231,R01LM010040,"['Architecture', 'Arts', 'Base Pairing', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Biology', 'Biomedical Research', 'Candidate Disease Gene', 'Clinical', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Diagnosis', 'Disease', 'Disease susceptibility', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genome', 'Human Genome Project', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Methodology', 'Modeling', 'Noise', 'Outcome', 'Prevention', 'Proteomics', 'Public Domains', 'Public Health', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Simulate', 'Single Nucleotide Polymorphism', 'Solutions', 'Statistical Models', 'Susceptibility Gene', 'Techniques', 'Technology', 'Time', 'Variant', 'Vision', 'base', 'computerized tools', 'disease phenotype', 'disorder risk', 'experience', 'flexibility', 'follow-up', 'gene environment interaction', 'gene interaction', 'genetic analysis', 'genome wide association study', 'human disease', 'improved', 'interest', 'novel', 'novel strategies', 'simulation', 'success', 'tool', 'tool development', 'trait']",NLM,VANDERBILT UNIVERSITY,R01,2009,922959,-0.032695557855295765
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7666186,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,256073,-0.03669265170162069
"Improved algorithms for macromolecular structure determination by cryo-EM and NMR    DESCRIPTION (provided by applicant): Single-particle electron cryomicroscopy (cryo-EM) and 2D NMR spectroscopy are methods for observing the three-dimensional structures of large and small macromolecules. respectively. We propose to develop and apply novel algorithms for solving the difficult mathematical problems posed by these techniques of structural biology. In cryo-EM the experimental data consist of noisy, random projection images of macromolecular ""particles"", and the problem is finding the 3D structure which is consistent with these images. Present reconstruction techniques rely on user input or ad hoc models to initiate a refinement cycle. We propose a new algorithm, ""globally consistent angular reconstitution"" (GCAR) that provides an unbiased and direct solution to the reconstruction problem. We further propose an extension to GCAR to handle heterogeneous particle populations. We also will pursue a powerful new approach to determining class averages, ""triplet class averaging"". This should allow GCAR to be used with data having very low signal-to-noise ratios, as is commonly obtained. The experimental data from NMR consist of estimates of local distances between atoms, and the goal is to find a globally consistent coordinate system. The same theory behind GCAR, involving the properties of sparse linear operators, can be applied to obtain a fast and direct solution to the distance geometry problem. We will develop and implement all of these algorithms and test them with experimental cryo-EM and NMR data. PUBLIC HEALTH RELEVANCE:  Determining the structures of proteins and other large molecules is an essential step in the basic understanding of biological processes, as well as the first step in rational drug design. We propose to develop new, faster and more reliable computer algorithms to increase the power of two structure-determination methods, cryo-EM and NMR.           n/a",Improved algorithms for macromolecular structure determination by cryo-EM and NMR,7787325,R01GM090200,"['Affinity', 'Algorithms', 'Area', 'Biological Process', 'Chemicals', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Databases', 'Discipline', 'Drug Design', 'Failure', 'Filtration', 'Goals', 'Heterogeneity', 'Hydrogen Bonding', 'Image', 'Individual', 'Knowledge', 'Least-Squares Analysis', 'Link', 'Maps', 'Methods', 'Microscope', 'Modeling', 'Molecular', 'Molecular Structure', 'Muscle Rigidity', 'NMR Spectroscopy', 'Negative Staining', 'Noise', 'Performance', 'Population', 'Potassium Channel', 'Procedures', 'Property', 'Proteins', 'Radial', 'Recovery', 'Relative (related person)', 'Research', 'Risk', 'Signal Transduction', 'Simulate', 'Solutions', 'Spiders', 'Structure', 'System', 'Techniques', 'Testing', 'Torsion', 'Triplet Multiple Birth', 'Variant', 'base', 'data mining', 'high risk', 'image processing', 'improved', 'macromolecule', 'mathematical theory', 'novel', 'novel strategies', 'particle', 'performance tests', 'programs', 'protein structure', 'public health relevance', 'receptor', 'reconstitution', 'reconstruction', 'structural biology', 'success', 'theories', 'three dimensional structure']",NIGMS,PRINCETON UNIVERSITY,R01,2009,293039,-0.0189085372795729
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7470967,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Class', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Numbers', 'Outcome', 'Performance', 'Polymerase Chain Reaction', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Standards of Weights and Measures', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2008,74521,0.021690985144954378
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7431959,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Condition', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Numbers', 'Patients', 'Play', 'Population', 'Process', 'Public Health', 'Rate', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'particle', 'size', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2008,376423,0.012826744588444572
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7596501,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Chromosome Pairing', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Numbers', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Purpose', 'Rate', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'concept', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,319129,0.007543730944699584
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7354450,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Class', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Epidemiology, Other', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Infectious Disease Epidemiology', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'success', 'theories', 'tool', 'transmission process', 'transposon/insertion element', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2008,342967,0.01997328132733593
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7417618,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2008,263507,0.021325707571986603
"Novel Analytic Techniques to Assess Physical Activity Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship. n/a",Novel Analytic Techniques to Assess Physical Activity,7611584,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2008,142424,0.02090604763436637
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7407451,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2008,291451,0.0038096474850894567
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7688756,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Economics', 'Electronics', 'Elevation', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genome', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Institutes', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Numbers', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Policy Developments', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Testing', 'Text', 'Thinking', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'case control', 'cohort', 'cost', 'development policy', 'gene environment interaction', 'genome wide association study', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'patient registry', 'prescription document', 'prescription procedure', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2008,219307,-0.03142966860222747
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7502172,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Economics', 'Electronics', 'Elevation', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genome', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Institutes', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Numbers', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Policy Developments', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Testing', 'Text', 'Thinking', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'case control', 'cohort', 'cost', 'development policy', 'gene environment interaction', 'genome wide association study', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'patient registry', 'prescription document', 'prescription procedure', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2008,987473,-0.03142966860222747
"Genotype/Phenotype Correlations in Lysosomal Storage Diseases    DESCRIPTION (provided by applicant): Our long-term goal is to understand the detailed molecular mechanisms that connect genotype, phenotype and response to therapy in lysosomal storage diseases (LSDs). LSDs are a family of genetic metabolic diseases caused by lysosomal enzyme deficiencies. In this project we use Fabry disease as a model system to develop a bioinformatics-based paradigm to address two fundamental issues: 1) The relationship between genotype and phenotype in LSDs. This task is challenging because in LSDs different mutations in the same enzyme often lead to different disease phenotypes. 2) The relationship between genotype and response to ""pharmacological chaperone"" therapy. Pharmacological chaperones are small-molecule ligands that are used to rescue mutants, resulting in increased enzymatic activity; several Fabry mutations have been shown to be rescueable in this way. The same therapy is likely to be useful for other LSDs, particularly those with neurological involvement, for which enzyme replacement therapy is not viable. The two aims of this application address, at different levels, both issues described above. The first aim, tests the hypothesis that knowing the change that occurs in the protein sequence, together with the structural environment in which it occurs, is sufficient to predict the resulting disease phenotype and response to pharmacological chaperone therapy. This is tested through the rigorous training of classification methods using sequence and structure-derived descriptors for a large set of Fabry mutants of known phenotype. The resulting classification provides a large- scale quantitative description of the correlation between genotype and phenotype. The accuracy of predictions based on this approach is a measure of how much information about the genotype the descriptors contain. The same approach will be used to establish a quantitative correlation between genotype and response to pharmacological chaperone therapy. Finally, applying the classification methods to mutations in other LSDs will test the generality of the approach. The second aim of this application addresses the issue of genotype/phenotype correlation from a biophysical point of view. We test the hypothesis that a combination of factors, mainly folding free energy, ligand binding affinity, and relative pH stability of the mutants determines the disease phenotype and response to pharmacological chaperone therapy. This is done analyzing selected mutants using molecular modeling and molecular dynamics simulations of the enzyme/ligand and enzyme/receptor interactions, as well as, pH stability, and other calculations. The methods used in the second aim are very detailed, but are not applicable at a large scale. Thus, both aims provide complementary views of genotype/phenotype correlation in LSDs. The successful completion of this project will, for the first time, provide a quantitative connection between genotype and phenotype in LSDs and a detailed biophysical description of the molecular mechanisms underlying genotype/phenotype correlations and response to pharmacological chaperone therapy in Fabry disease. Relevance of this research to public health. Lysosomal storage diseases (LSDs) are a group of more than 40 genetic metabolic disorders. Worldwide, the incidence of patients with LSDs is estimated to be ~ 1 in 8,000 live births. Understanding the correlation between genotype, phenotype, and response to treatment in these diseases will help in their diagnosis and treatment, particularly for LSDs that affect the brain, for which no effective treatment is available to date.          n/a",Genotype/Phenotype Correlations in Lysosomal Storage Diseases,7388981,R21DK078345,"['Address', 'Affect', 'Affinity', 'Amino Acid Sequence', 'Amino Acids', 'Binding', 'Bioinformatics', 'Biological Models', 'Brain', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Data', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Endoplasmic Reticulum', 'Environment', 'Enzymes', 'Fabry Disease', 'Feedback', 'Free Energy', 'Galactosidase', 'Genetic', 'Genotype', 'Goals', 'Incidence', 'Lead', 'Ligand Binding', 'Ligands', 'Live Birth', 'Lysosomal Storage Diseases', 'Lysosomes', 'Machine Learning', 'Measures', 'Metabolic Diseases', 'Methods', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Models', 'Mutate', 'Mutation', 'Neurologic', 'None or Not Applicable', 'Numbers', 'Output', 'Patients', 'Peptide Sequence Determination', 'Pharmacogenomics', 'Phenotype', 'Principal Investigator', 'Public Health', 'Relative (related person)', 'Research', 'Residual state', 'Structure', 'Testing', 'Time', 'Training', 'base', 'design', 'disease phenotype', 'enzyme deficiency', 'enzyme replacement therapy', 'enzyme structure', 'enzyme substrate', 'family genetics', 'improved', 'insight', 'molecular dynamics', 'molecular modeling', 'mutant', 'prevent', 'programs', 'protein degradation', 'receptor', 'receptor binding', 'response', 'simulation', 'small molecule', 'three dimensional structure']",NIDDK,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R21,2008,124583,-0.004205203575125002
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7386333,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'None or Not Applicable', 'Numbers', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Purpose', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'cancer microarray', 'cancer type', 'design', 'desire', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,255036,-0.03669265170162069
"Causal Discovery Algorithms for Translational Research with High-Throughput Data Project Summary Causal Discovery Algorithms for Translational Research with High-Throughput Data The long-term goal of this project is to provide to the biomedical community next-generation causal algorithms to facilitate discovery of disease molecular pathways and causative as well as predictive biomarkers and molecular signatures from high-throughput data. Such knowledge and methods are necessary toward earlier and more accurate diagnosis and prognosis, personalized medicine, and rational drug design. If successful, the proposed research will have significant and wide methodological and practical implications spanning several areas of biomedicine with a primary focus and immediate benefits in high-throughput diagnostics and personalized medicine. It will provide significantly improved computational methods and deeper theoretical understanding related to producing molecular signatures and understanding mechanisms of disease and concomitant leads for new drugs. It will provide evidence about applicability of novel causal methods in other types of data. It will generate insights in specific pathways of lung cancer in humans. It will deepen our understanding and solutions to the Rashomon effect in ¿omics¿ data. The proposed research will also shed light on the operational value of the stability heuristic. Finally the research will engage the international research community to address open computational causal discovery problems relevant to high-throughput and other biomedical data. ¿ Aim 1. Evaluate and characterize several novel causal algorithms for biomarker selection, molecular signature creation and reverse network engineering using real, simulated, resimulated, and experimental datasets. Study generality of the methods by means of applicability to non-¿omics¿ datasets. ¿ Aim 2. Evaluate and characterize, novel and state of the art causal algorithms against state-of-the-art non-causal and quasi-causal algorithms. ¿ Aim 3. Systematically investigate the Rashomon effect as it applies to biomarker and signature multiplicity. ¿ Aim 4. Systematically investigate the utility of applying the stability heuristic for causal discovery. ¿ Aim 5. Derive novel biomarkers, pathways and hypotheses for lung cancer. ¿ Aim 6. Induce novel solutions through an international causal discovery competition. ¿ Aim 7. Disseminate findings. n/a",Causal Discovery Algorithms for Translational Research with High-Throughput Data,7643514,R56LM007948,"['AKT1 gene', 'AKT2 gene', 'AKT3 gene', 'Address', 'Affect', 'Algorithms', 'Area', 'Arts', 'Benchmarking', 'Bioinformatics', 'Biologic Characteristic', 'Biological Markers', 'Biology', 'Biometry', 'Book Chapters', 'Books', 'Cancer cell line', 'Causations', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Communities', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Set', 'Depth', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discipline', 'Disease', 'Drug Design', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Ensure', 'Epidermal Growth Factor Receptor', 'European', 'Evaluation', 'Event', 'Excision', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Hereditary Disease', 'Home environment', 'Human', 'Human Cell Line', 'Inferior', 'Information Retrieval', 'Institution', 'International', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Malignant neoplasm of lung', 'Marker Discovery', 'Medicine', 'Methods', 'Modality', 'Molecular', 'Molecular Profiling', 'Neighborhoods', 'Noise', 'Numbers', 'Online Systems', 'Outcome', 'Output', 'Paper', 'Pathway interactions', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteomics', 'Protocols documentation', 'Public Domains', 'Publishing', 'Quality Control', 'Random Allocation', 'Randomized', 'Rate', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Sample Size', 'Sampling', 'Schedule', 'Score', 'Services', 'Simulate', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Text', 'Thinking', 'Tissues', 'Translational Research', 'Variant', 'Work', 'base', 'c-erbB-1 Proto-Oncogenes', 'clinically relevant', 'computer based statistical methods', 'computer science', 'contextual factors', 'coping', 'data mining', 'design', 'drug development', 'heuristics', 'human data', 'human tissue', 'improved', 'innovation', 'insight', 'journal article', 'member', 'new technology', 'next generation', 'novel', 'novel diagnostics', 'outcome forecast', 'reconstruction', 'research study', 'software systems', 'symposium', 'theories', 'tool']",NLM,VANDERBILT UNIVERSITY,R56,2008,4434,-0.04026466650728744
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7226297,R01HL065462,"['Accounting', 'Algorithms', 'Arts', 'Biological', 'Class', 'Classification', 'Communities', 'Computer software', 'Condition', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease regression', 'Documentation', 'Effectiveness', 'Employee Strikes', 'Environment', 'Etiology', 'Gene Expression', 'Gene Expression Profiling', 'Genes', 'Genomics', 'Goals', 'Heart failure', 'Knowledge', 'Least-Squares Analysis', 'Machine Learning', 'Mass Spectrum Analysis', 'Mechanics', 'Medical', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Profiling', 'Motivation', 'Pan Genus', 'Patients', 'Performance', 'Principal Investigator', 'Procedures', 'Property', 'Proteomics', 'Public Domains', 'Research Personnel', 'Sample Size', 'Sampling', 'Scheme', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Tissue-Specific Gene Expression', 'Weight', 'Work', 'base', 'forest', 'gene function', 'improved', 'novel', 'protein protein interaction', 'response', 'statistics', 'tool']",NHLBI,UNIVERSITY OF MINNESOTA,R01,2007,141753,0.006234223165319574
"A RuleFit Product for Classification and Regression Prediction and data exploration are important aspects of modern commercial and scientific life. Regression methods predict dependent variables (e.g., tumor growth, severity of disease), while classification methods predict class membership (e.g., tumor or disease type). Both use a vector of independent variables to make the predictions. Because they are often superior predictors, can handle large numbers observations and large numbers of variables, can often yield insight into the data not provided by other methods, and because they can adapt to arbitrarily complex relationships, modern machine learning methods based on tree ensembles such as RANDOM FORESTS and MART have become leading modern analytical methods. Here we propose to commercially implement RULEFIT, a recent innovative method extending the RANDOM FORESTS and MART approaches, that shows strong evidence of being consistently more accurate than either ensemble. RULEFIT also includes groundbreaking new methods for variable selection in the face of huge numbers of predictors, and for identifying interactions, and ranking their importance. Optionally, RULEFIT extracts ""rules"" of special interest: succinct statements of conditions under which an outcome is especially likely or unlikely, or especially large or small. The primary output of RULEFIT is a numeric value reecting a prediction of the value of the dependent variable or the probability of a class membership. RULEFIT is likely to become a leading technique in the machine learning and statistics. It builds on RANDOM FORESTS and MART and includes all their useful benefits such as variable selection, data exploration, data reduction, outlier detection, and missing value imputation, while enhancing and extending these benefits.  COMMERCIAL POTENTIAL The market for advanced analytical tools has been growing strongly over the last decade and the growth shows no signs of diminishing. Modelers and data analysts in both university- based and commercial settings are increasingly aware of the power and value of new analytical tools derived from modern statistics and machine learning research. The increased accuracy of the new methods and the acceleration they provide to the analysis of complex data are fueling demand for this new technology. The advances embedded in the proposed product represent substantial improvements to existing technology and include methods to solve vexing problems in contemporary data analysis, and thus should find a welcoming market.  There are further reasons to forecast robust commercial potential for this product. The applicant organization has a strong track record in the industry and is widely recognized as a developer of high quality software. We have been working with consultant Friedman since 1990 and have gained exclusive rights to the proprietary sourcecode for a number of his innovations. These include CART, MARS, MART and PRIM. With the addition of RULEFIT and its associated sub-components, these products represent a unique collection of pedigreed tools. We have also forged a similar relationship with the (late) Leo Breiman and have the exclusive rights to commercialization of Breiman's Random Forests sourcecode. Our proposed package thus occupies a distinctive position in machine learning software which cannot be replicated by other vendors. Keywords: machine learning; classi?cation; prediction; supervised learning; variable importance; inter- action detection; Justi?cation Dr. Steinberg has extensive experience in software development for advanced statistical and machine learning methods, particularly in the area of classi?cation and regression trees, sur- vival analysis, adaptive modeling, RANDOM FORESTS and MART. He will oversee all aspects of the project. He will will work with Dr. Cardell, Professor Friedman, Mr. Colla, and with the Salford Systems software development engineer in creating and studying the software and methods used in this proposal. He will also be responsible for the architecture of the Phase I software. Professor Friedman and Dr. Cardell will provide technical support as follows: Dr. Fried- man is an expert on machine learning methods and is one of the developers of the RULEFIT technique. Regular consultation with him will be in this area. Dr. Cardell is an expert in asymptotic theory, and in the design of Monte Carlo and other tests for the evaluation of ma- chine learning algorithms. He also has extensive experience in machine learning, including adaptive modeling, neural networks, logistic regression, and classi?cation methods. He will review core algorithms of RULEFIT for possible improvement and extension and design the Monte Carlo tests. Mr. Colla has extensive experience in software development and with machine learning methods, including work on the commercial implementations of CART, MARS, RANDOM FORESTS, and MART. Working with Dr. Cardell, he will be responsible for much of the new software coding. 5 Project Description Page 7 Principal Investigator/Program Director (Last, first, middle): Steinberg, Dan Prediction models based upon classification and regression tree ensembles have become important in medical and other research. There are currently no commercial products available that implement the proposed RuleFit methodology. These methods have significant advantages over existing techniques, and will aid researchers in obtaining the best possible predictions.   n/a",A RuleFit Product for Classification and Regression,7268612,R43CA124294,"['Acceleration', 'Agreement', 'Algorithms', 'Architecture', 'Area', 'Beds', 'Build-it', 'Cations', 'Class', 'Classification', 'Code', 'Collection', 'Comparative Study', 'Complex', 'Computer software', 'Condition', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Detection', 'Disease', 'Disease regression', 'Engineering', 'Evaluation', 'Face', 'Generations', 'Growth', 'Industry', 'Information Systems', 'Investigation', 'Learning', 'Left', 'Life', 'Linear Models', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Neural Network Simulation', 'Numbers', 'Outcome', 'Output', 'Painless', 'Pattern', 'Performance', 'Phase', 'Plant Leaves', 'Play', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Rate', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Rights', 'Role', 'Sampling', 'Severity of illness', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Trees', 'Universities', 'Variant', 'Vendor', 'Work', 'analytical method', 'analytical tool', 'base', 'commercialization', 'data mining', 'data structure', 'design', 'evaluation/testing', 'experience', 'forest', 'forging', 'graphical user interface', 'innovation', 'insight', 'interest', 'loss of function', 'man', 'new technology', 'novel', 'professor', 'programs', 'prototype', 'relating to nervous system', 'research study', 'software development', 'statistics', 'theories', 'tool', 'tumor', 'tumor growth', 'vector']",NCI,SALFORD SYSTEMS,R43,2007,91700,-0.02010833775746837
"Novel Analytic Techniques to Assess Physical Activity    DESCRIPTION (provided by applicant): Progress has been made in developing and using accelerometer-based motion sensors for physical activity research. However, traditional methods of processing activity monitor data do not provide sufficient accuracy to satisfy current trends in the use of objective physical activity data in the research arena. The aims of this proposal address this weakness in accelerometer- based PA assessment methodologies: The specific aims are: 1) To develop and validate novel methods to process Actigraph accelerometer data to improve estimates of PA using powerful modern classification methods (classification trees, discriminant analyses, hidden Markov models, neural networks, regression splines, and support vector machines); 2) To compare these classification methods and traditional approaches for assessing PA in a controlled setting; 3) To compare the classification methods and traditional approaches for quantifying PA in free living PA conditions and to select a recommended method; and 4) To correct for measurement error in summary estimates of habitual PA from the novel classification methods and traditional approaches for quantifying PA. Our uniquely qualified multidisciplinary research group will address these aims by first developing innovative classification methods to identify specific activities in a laboratory setting, and then validating the models using data collected from known activities performed in both controlled laboratory environments and free- living situations. Based on the results of these studies, the classification methods will be refined, and estimates of PA behavior will be adjusted using statistical measurement error methods to derive more accurate estimates of PA. We have chosen the classification methods to include publicly available ""off-the shelf"" classification methods that others can easily use. The resulting data processing programs will be implemented in popular commercial software packages and made freely available. The results of the proposed investigations will move the field of PA assessment forward by providing innovative approaches to derive more accurate and detailed estimates of PA using a popular accelerometer-based PA monitor. This systematic approach will provide information leading to a clearer understanding of the dose-response relationship between PA and health and the physiological basis of this relationship.           n/a",Novel Analytic Techniques to Assess Physical Activity,7262592,R01CA121005,"['Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Chronic Disease', 'Classification', 'Computer software', 'Condition', 'Daily', 'Data', 'Diet', 'Discriminant Analysis', 'Disease regression', 'Dose', 'Effectiveness of Interventions', 'Environment', 'Health', 'Interdisciplinary Study', 'Intervention', 'Investigation', 'Laboratories', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Motion', 'NIH Program Announcements', 'Nature', 'Outcome', 'Output', 'Participant', 'Pattern', 'Performance', 'Physical activity', 'Physiological', 'Population', 'Principal Investigator', 'Process', 'Qualifying', 'Recommendation', 'Research', 'Scientist', 'Series', 'Techniques', 'Time', 'Time Study', 'Trees', 'Validation', 'Walking', 'Work', 'base', 'computerized data processing', 'improved', 'innovation', 'markov model', 'novel', 'novel strategies', 'nutritional epidemiology', 'programs', 'response', 'sensor', 'trend']",NCI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2007,263847,0.021325707571986603
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7247404,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2007,292160,0.0038096474850894567
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7427364,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Economics', 'Electronics', 'Elevation', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genome', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Institutes', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Numbers', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Policy Developments', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Testing', 'Text', 'Thinking', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'case control', 'cohort', 'cost', 'development policy', 'gene environment interaction', 'genome wide association study', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'patient registry', 'prescription document', 'prescription procedure', 'prospective', 'success', 'trait', 'virtual']",NHGRI,GROUP HEALTH COOPERATIVE,U01,2007,970601,-0.03142966860222747
"Genotype/Phenotype Correlations in Lysosomal Storage Diseases    DESCRIPTION (provided by applicant): Our long-term goal is to understand the detailed molecular mechanisms that connect genotype, phenotype and response to therapy in lysosomal storage diseases (LSDs). LSDs are a family of genetic metabolic diseases caused by lysosomal enzyme deficiencies. In this project we use Fabry disease as a model system to develop a bioinformatics-based paradigm to address two fundamental issues: 1) The relationship between genotype and phenotype in LSDs. This task is challenging because in LSDs different mutations in the same enzyme often lead to different disease phenotypes. 2) The relationship between genotype and response to ""pharmacological chaperone"" therapy. Pharmacological chaperones are small-molecule ligands that are used to rescue mutants, resulting in increased enzymatic activity; several Fabry mutations have been shown to be rescueable in this way. The same therapy is likely to be useful for other LSDs, particularly those with neurological involvement, for which enzyme replacement therapy is not viable. The two aims of this application address, at different levels, both issues described above. The first aim, tests the hypothesis that knowing the change that occurs in the protein sequence, together with the structural environment in which it occurs, is sufficient to predict the resulting disease phenotype and response to pharmacological chaperone therapy. This is tested through the rigorous training of classification methods using sequence and structure-derived descriptors for a large set of Fabry mutants of known phenotype. The resulting classification provides a large- scale quantitative description of the correlation between genotype and phenotype. The accuracy of predictions based on this approach is a measure of how much information about the genotype the descriptors contain. The same approach will be used to establish a quantitative correlation between genotype and response to pharmacological chaperone therapy. Finally, applying the classification methods to mutations in other LSDs will test the generality of the approach. The second aim of this application addresses the issue of genotype/phenotype correlation from a biophysical point of view. We test the hypothesis that a combination of factors, mainly folding free energy, ligand binding affinity, and relative pH stability of the mutants determines the disease phenotype and response to pharmacological chaperone therapy. This is done analyzing selected mutants using molecular modeling and molecular dynamics simulations of the enzyme/ligand and enzyme/receptor interactions, as well as, pH stability, and other calculations. The methods used in the second aim are very detailed, but are not applicable at a large scale. Thus, both aims provide complementary views of genotype/phenotype correlation in LSDs. The successful completion of this project will, for the first time, provide a quantitative connection between genotype and phenotype in LSDs and a detailed biophysical description of the molecular mechanisms underlying genotype/phenotype correlations and response to pharmacological chaperone therapy in Fabry disease. Relevance of this research to public health. Lysosomal storage diseases (LSDs) are a group of more than 40 genetic metabolic disorders. Worldwide, the incidence of patients with LSDs is estimated to be ~ 1 in 8,000 live births. Understanding the correlation between genotype, phenotype, and response to treatment in these diseases will help in their diagnosis and treatment, particularly for LSDs that affect the brain, for which no effective treatment is available to date.          n/a",Genotype/Phenotype Correlations in Lysosomal Storage Diseases,7239204,R21DK078345,"['Address', 'Affect', 'Affinity', 'Amino Acid Sequence', 'Amino Acids', 'Binding', 'Bioinformatics', 'Biological Models', 'Brain', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Data', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Endoplasmic Reticulum', 'Environment', 'Enzymes', 'Fabry Disease', 'Feedback', 'Free Energy', 'Galactosidase', 'Genetic', 'Genotype', 'Goals', 'Incidence', 'Lead', 'Ligand Binding', 'Ligands', 'Live Birth', 'Lysosomal Storage Diseases', 'Lysosomes', 'Machine Learning', 'Measures', 'Metabolic Diseases', 'Methods', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Models', 'Mutate', 'Mutation', 'Neurologic', 'None or Not Applicable', 'Numbers', 'Output', 'Patients', 'Peptide Sequence Determination', 'Pharmacogenomics', 'Phenotype', 'Principal Investigator', 'Public Health', 'Relative (related person)', 'Research', 'Residual state', 'Structure', 'Testing', 'Time', 'Training', 'base', 'design', 'disease phenotype', 'enzyme deficiency', 'enzyme replacement therapy', 'enzyme structure', 'enzyme substrate', 'family genetics', 'improved', 'insight', 'molecular dynamics', 'molecular modeling', 'mutant', 'prevent', 'programs', 'protein degradation', 'receptor', 'receptor binding', 'response', 'simulation', 'small molecule', 'three dimensional structure']",NIDDK,MOUNT SINAI SCHOOL OF MEDICINE,R21,2007,127125,-0.004205203575125002
"Systems analysis of oxygen regulation in Halobacterium    DESCRIPTION (provided by applicant): To withstand environmental onslaught, biological systems mount global programs to coordinate the induction of protection and repair mechanisms. This proposal poses the hypothesis that the transcriptional networks underlying such responses to diverse stressors are interrelated. Halobacterium, a halophilic archaeon, has been chosen as a model for this study because it routinely negotiates an array of adverse conditions in its extreme environment, including anoxia, metal stress, and radiation damage. This proposal will investigate the inter-relationship of these responses using global approaches. Given that basal genetic information processing pathways in Halobacterium are mediated by eukaryotic-like proteins, findings from this study will have a direct impact on understanding how complex eukaryotic organisms elicit orthogonal responses in disease-perturbed or infection states. Specifically, I will (1) Characterize key transcriptional regulators responsible for mediating responses to fluctuating oxygen concentrations and identify regulons under their direct and indirect control; (2) Through statistical analysis of integrated datasets, evaluate the extent of cross-regulation of the anoxic response with other environmental perturbations; (3) Experimentally test new hypotheses generated by statistical analysis. These proposed experiments are expected to result in a transcriptional network model that addresses how organisms maintain homeostasis despite stress.           n/a",Systems analysis of oxygen regulation in Halobacterium,7261251,F32GM078980,"['Address', 'Aerobic', 'Algorithms', 'Anoxia', 'Archaea', 'Behavioral', 'Binding Sites', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Phenomena', 'Cells', 'Collection', 'Complex', 'Computer software', 'Condition', 'Couples', 'Data', 'Data Set', 'Defect', 'Disease', 'Electrophoretic Mobility Shift Assay', 'Environment', 'Equilibrium', 'Experimental Designs', 'Face', 'Facility Construction Funding Category', 'Fellowship', 'Gene Targeting', 'Genes', 'Genetic Information Processing Pathway', 'Genome', 'Goals', 'Growth', 'Halobacterium', 'Homeostasis', 'Hydrogen Peroxide', 'Individual', 'Infection', 'Information Systems', 'Knock-out', 'Laboratories', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Manuscripts', 'Maps', 'Mediating', 'Mediation', 'Metals', 'Modeling', 'Molecular Biology', 'Mutate', 'Names', 'Organism', 'Oxidation-Reduction', 'Oxidative Stress', 'Oxygen', 'Oxygen measurement, partial pressure, arterial', 'Play', 'Preparation', 'Property', 'Proteins', 'Proteomics', 'Protocols documentation', 'Radiation', 'Regulation', 'Regulator Genes', 'Regulon', 'Relative (related person)', 'Role', 'Stress', 'Study models', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Testing', 'Time', 'TimeLine', 'Training', 'Transcription Initiation Site', 'Transcriptional Regulation', 'Work', 'biological adaptation to stress', 'cell injury', 'chromatin immunoprecipitation', 'halobacteria', 'high throughput screening', 'in vivo', 'insight', 'metal poisoning', 'mutant', 'network models', 'novel', 'programs', 'repaired', 'research study', 'response', 'stressor', 'transcription factor']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,F32,2007,48796,-0.02179046901819246
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,7015648,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2006,500182,0.005731316410393685
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,7107885,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2006,385718,-0.00998017243652673
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7056185,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,145987,0.006234223165319574
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,6863029,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2005,498368,0.005731316410393685
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6949109,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2005,395000,-0.00998017243652673
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,6922406,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,174500,0.006234223165319574
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6797879,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2004,395000,-0.00998017243652673
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6721300,R37GM030998,"['DNA', 'artificial intelligence', 'biochemical evolution', 'computational neuroscience', 'computer assisted sequence analysis', 'computer simulation', 'gene frequency', 'genetic models', 'mathematical model', 'method development', 'model design /development', 'natural selections', 'nucleic acid sequence', 'species difference', 'statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2004,161792,-0.022111752439578164
"Tree Ensemble Regression and Classification Methods    DESCRIPTION (provided by applicant):    This SBIR aims to produce next generation classification and regression software based upon ensembles of decision trees: bagging, random forests, and boosting. The prediction accuracy of these methods has caused much excitement in the machine learning community, and both challenges and complements the data modeling culture prevalent among biostatisticians. Recent research extends the methodology to likelihood based methods used in biostatistics, leading to models for survival data and generalized forest models. Generalized forest models extend regression forests in the same way that generalized linear models extend linear models.      This software would apply broadly, including to medical diagnosis, prognostic modeling, and detecting cancer; and for modeling patient characteristics like blood pressure, discrete responses in clinical trials, and count data.      Phase I work will prototype software for survival data, and investigate the performance of ensemble methods on simulated and real data. For survival applications, we will assess out-of-bag estimates of performance, and investigate measures of variable importance and graphics that help clinicians understand the results. Experience writing prototypes and using them on data will lead to a preliminary software design that serves as the foundation of Phase II work.      Phase II will expand upon this work to create commercial software. We will research and implement algorithms for a wider range of applications including generalized forest models, classification, and least squares regression. We will also implement robust loss criteria that enable good performance on noisy data, and make adaptations to handle large data sets.      This proposed software will enable medical researchers to obtain high prediction accuracy, and complement traditional tools like discriminant analysis, linear and logistic regression models, and the Cox model.         n/a",Tree Ensemble Regression and Classification Methods,6832086,R43CA105724,"['clinical research', 'computer assisted medical decision making', 'computer graphics /printing', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'method development', 'model design /development', 'neoplasm /cancer classification /staging', 'neoplasm /cancer diagnosis', 'neoplasm /cancer remission /regression', 'prognosis', 'statistics /biometry']",NCI,INSIGHTFUL CORPORATION,R43,2004,99937,0.013388035369884756
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6617906,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,UNIVERSITY OF NORTH CAROLINA CHAPEL HILL,R01,2003,170109,0.030531321095489755
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6663283,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2003,395000,-0.00998017243652673
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6635877,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2003,161792,-0.022111752439578164
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6636516,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2003,374063,-0.004602307259296022
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6605420,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,UNIVERSITY OF NORTH CAROLINA CHAPEL HILL,R01,2002,174567,0.030531321095489755
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6513068,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,DANA-FARBER CANCER INSTITUTE,R01,2002,21871,0.030531321095489755
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6519073,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2002,161792,-0.022111752439578164
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6554738,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' method development', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2002,395000,-0.00998017243652673
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6520234,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2002,163400,0.03059718859321749
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6520333,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2002,363261,-0.004602307259296022
"Markov Chain Monte Carlo and Exact Logistic Regression   DESCRIPTION (provided by applicant): Logistic regression is a very popular           model for the analysis of binary data with widespread applicability in the           physical, behavioral and biomedical sciences. Parameter inference for this           model is usually based on maximizing the unconditional likelihood function.          However unconditional maximum likelihood inference can produce inconsistent          point estimates, inaccurate p-values and inaccurate confidence intervals for         small or unbalanced data sets and for data sets with a large number of               parameters relative to the number of observations. Sometimes the method fails        entirely as no estimates can be found that maximize the unconditional                likelihood function. A methodologically sound alternative approach that has          none of the aforementioned drawbacks is the exact conditional approach in which      one generates the permutation distributions of the sufficient statistics for         the parameters of interest conditional on fixing the sufficient statistics of        the remaining nuisance parameters at their observed values. The major stumbling      block to this approach is the heavy computational burden it imposes. Monte           Carlo methods attempt to overcome this problem by sampling from the reference        set of possible permutations instead of enumerating them all. Two competing          Monte Carlo methods are network based sampling and Markov Chain Monte Carlo          (MCMC) sampling. Network sampling suffers from memory limitations while MCMC         sampling can produce incorrect results if the Markov chain is not ergodic or if      the process is not in the steady state. We propose a novel approach which            combines the network and MCMC sampling, draws upon the strengths of each of          them and overcomes their individual limitations. We propose to implement this        hybrid network-MCMC method in our LogXact software and as an external procedure      in the SAS system.                                                                   PROPOSED COMMERCIAL APPLICATION:  There is great demand for logistic regression software that can handle small, sparse or  unbalanced data sets by exact methods.  Our LogXact package is the only software that  can provide exact inference for data sets which are not ""toy problems"".  Yet even  LogXact quickly breaks down on moderate sized problems.  The new generation of hybrid  network-MCMC algorithms will handle substantially larger problems that nevertheless need  exact inference.  The commercial potential is considerable since such data sets are common  in scientific studies.                                                                                      n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6404971,R43CA093112,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R43,2001,113111,0.00402231789647193
"Inference in Regression Models with Missing Covariates DESCRIPTION:  (Adapted from investigator's abstract) This project will examine new methodology for making inference about the regression parameters in the presence of missing covariate data for two commonly used classes of regression models.  In particular, we examine the class of generalized linear models for general types of response data and the Cox model for survival data.  The methodology addresses problems occurring frequently in clinical investigations for chronic disease, including cancer and AIDS.  The specific objectives of the project are to:  1) develop and study classical and Bayesian methods of inference for the class of generalized linear models (GLM's) in the presence of missing covariate data.  In particular, we will  i) examine methods for estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Also, parametric models for the covariate distribution will be examined.  The methods of estimation will focus on the Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990) along with the adaptive rejection algorithm of Gilks and Wild (1992) will be used to sample from the conditional distribution of the missing covariates given the observed data.  ii) examine estimating the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Models for the missing data mechanism will be studied.  iii) develop and study Bayesian methods of inference in the presence of missing covariate data when the missing covariates are either categorical or continuous and the missing data mechanism is ignorable.  Parametric prior distributions for the regression coefficients are proposed.  Properties of the posterior distributions of the regression coefficients will be studied.  The methodology will be implemented using Markov Chain Monte Carlo methods similar to those of Tanner and Wong (1987). iv) investigate Bayesian methods when the covariates are either categorical or continuous and the missing data mechanism is nonignorable.  Multinomial models for the missing data mechanism will be studied.  Dirichlet prior distributions for the multinomial parameters will be investigated.  2) develop and study classical and Bayesian methods of inference for the Cox model for survival outcomes in the presence of missing covariates.  Specifically, we will  i) develop and study estimation methods for the Cox model for survival outcomes in the presence of missing covariates. Methods for estimating the regression parameters when the missing covariates are either categorical or continuous will be studied.  The methods of estimation will focus on an EM type algorithm similar to that of Wei and Tanner (1990).  ii) study estimation of the regression parameters when the missing covariates are either categorical or continuous and the missing data mechanisms nonignorable.  Models for the missing data mechanism will be studied.  Bayesian methods similar to those of 1-iii) and -iv) will be investigated. Computational techniques using the Monte Carlo methods described in 1-iii) will be implemented.  n/a",Inference in Regression Models with Missing Covariates,6326240,R01CA074015,"['artificial intelligence', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' mathematical model', ' method development', ' model design /development', ' statistics /biometry']",NCI,DANA-FARBER CANCER INSTITUTE,R01,2001,183883,0.030531321095489755
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6385455,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2001,161792,-0.022111752439578164
"RULE DISCOVERY IN BODY CAVITY EFFUSIONS  DESCRIPTION (adapted from the Abstract):                                             Machine learning methods are innovative tools used to find patterns in medical       data.  Laboratory data is suited to computerized Interpretation because of its       objective, quantitative nature.  Body fluid analysis is a good model for             evaluating machine learning in the laboratory.  Pathologists spend a                 substantial amount of time analyzing and classifying body fluids, or                 effusions, which are abnormal accumulations of fluid within body cavities of         human beings and animals, caused by diseases such as congestive heart failure.       Fluid classification provides clinicians with important diagnostic information       about the underlying disease process.  Automation of body fluid analysis by a        machine learning system would substantially increase the efficiency and              profitability of a medical laboratory.  In a pilot study, RIPPER (Repeated           Incremental Pruning to Produce Error Reduction), a rule discovery tool,              accurately classified effusions from animals into five standard categories,          based on the physical, chemical, and cellular characteristics of the fluid.          The purposes of this study are: 1) to determine the accuracy of RIPPER on a          larger data set, to expand and strengthen the results of the pilot; 2) to test       the accuracy of RIPPER's fluid classifications prospectively in a large              veterinary teaching hospital laboratory, 3) to determine the acceptance rate         or reason for rejection of RIPPER's classification by clinical pathologists;         and (4) to use RIPPER to discover novel rules for classifying effusions by           underlying disease process.                                                                                                                                               The results of this study will validate and test the acceptance of a machine         learning system applicable to fluid analysis in both human and veterinary            clinical laboratories.  By discovering new patterns in quantitative data that        identify the specific underlying disease, RIPPER can greatly enhance the             diagnostic value of laboratory analysis.                                                                                                                                  n/a",RULE DISCOVERY IN BODY CAVITY EFFUSIONS,6467346,F32LM000095,"['body fluids', ' classification', ' computer assisted instruction', ' programmed instruction', ' veterinary science']",NLM,UNIVERSITY OF CALIFORNIA DAVIS,F32,2001,52501,-0.027850876760718648
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6387141,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2001,163400,0.03059718859321749
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6484619,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2001,337739,-0.004602307259296022
"SELECTING AMONG MATHEMATICAL MODELS OF COGNITION DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling       of cognition, it is important to have well-justified criteria for choosing       among differing explanations (i.e., models) of observed data.  This project      investigates those criteria as well as their instantiation in five model         selection methods.                                                                                                                                                Two lines of research will be undertaken.  In the first, a thorough              investigation of model complexity will be conducted.  Comprehensive              simulations re intended to determine complexity's contribution to model fit      and to model selection.  An analytical solution will also be sought with the     hope of quantifying model complexity.                                                                                                                             The second line of work examines the utility of each of the five selection       methods in choosing among models in three topic areas in cognitive               psychology (information integration, categorization, connectionist               modeling), the end goal being to identify their merits and shortcomings.                                                                                          Findings should provide a better understanding of model selection than           currently available and serve as a useful guide for researchers comparing        the suitability of quantitative models of cognition.                                                                                                               n/a",SELECTING AMONG MATHEMATICAL MODELS OF COGNITION,6185788,R01MH057472,"['artificial intelligence', ' choice', ' cognition', ' computer simulation', ' information dissemination', ' mathematical model', ' psychometrics']",NIMH,OHIO STATE UNIVERSITY,R01,2000,77332,0.005788334099741439
"NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating             Characteristic (ROC) analysis is recognized widely as the best way of            measuring and specifying the accuracies of diagnostic procedures, because it     is able to distinguish between actual differences in discrimination              capacity, on one hand, and apparent differences that are due only to             decision-threshold effects, on the other.  Key methodological needs remain       to be satisfied before ROC analysis can address all of the practically           important situations that arise in diagnostic applications, however.  This       project employs signal detection theory and computer simulation to address       several of those needs, by:  (1) refining and continuing distribution of         software developed previously by the applicants for fitting ROC curves and       for testing the statistical significance of differences between ROC curve        estimates; (2) developing and evaluating new algorithms for ROC                  curve-Fitting and statistical testing, based on their recently-developed         ""proper"" binormal model, that should provide more meaningful results in          experimental situations that involve small samples of cases; (3)                 investigating the usefulness of a form of ROC methodology that is based on       mixture distributions in order to rduce the need for diagnostic truth in ROC     experiments; (4) investigating the effect of case-saple difficulty on the        statistical power tests for differences between ROC curves, in order to          determine the optimal difficulty of cases that shouldbe studied on rank          diagnostic systems; and (5) developing methods for training artificial           neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC           analysis and signal detection theory.                                             n/a",NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY,6181168,R01GM057622,"['artificial intelligence', ' computer assisted diagnosis', ' computer system design /evaluation', ' method development', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R01,2000,218176,-0.007895846394127808
"PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution.  n/a",PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY,6182183,R21GM059398,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer simulation', ' computer system design /evaluation', ' electron crystallography', ' electron density', ' molecular biology information system', ' physical model', ' protein structure', ' structural biology']",NIGMS,TEXAS ENGINEERING EXPERIMENT STATION,R21,2000,101500,-0.0367947427357429
"ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS Predictive models that generate estimate probabilities for medical outcomes have become widely used in health services research, in health policy, and increasingly, for the assessment of health care and for real-time decision support.  Logistic regression models for medical events are central to most probabilistic predictive clinical decision aids and are fundamental to comparative analyses of medical care based on risk-adjusted events.  In such applications, inaccurate assessment of patient risk can have significant health care and health policy implications. New computer-based modeling techniques including generalized additive models, classification trees, and neural networks may potentially capture information that regression methods may miss or misrepresent.  However, these methods use very local information in model construction and may be overfit to the sample data and thus not transport well to new settings.  In years 1-3, we investigated the relative accuracy of predictions made by these modeling methods under a variety of data structures, including the presence of outliers and missing data. For many of these data structures we found that the more ""local"" procedures frequently did not generalize to new test data as well as traditional regression methods.  However, our results suggest that as sample size and data complexity increases the performance of these procedures may substantially improved. Thus, to test these findings under more general conditions, we now propose two additional years of research to 1) rigorously assess the relative predictive performance and transportability of other new innovative modeling methods and of original hybrid model construction methods; 2) systematically investigate the relative predictive performance and model transportability of modeling methods applied to large and complex data structures; and 3) explore and assess procedures for handling outliers and missing data for classification trees and neural networks. The completion of the proposed work will result in the first systematic exploration of the factors affecting the predictive performance of the major modeling methods used to predict medical outcomes, and the comparative performance of models constructed by these methods on the extremely large data sets of the type that are becoming increasing available to researchers.  n/a",ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS,6185210,R01LM005607,"['artificial intelligence', ' computational neuroscience', ' computer assisted medical decision making', ' computer simulation', ' health care facility information system', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' outcomes research', ' prognosis', ' statistics /biometry']",NLM,TUFTS MEDICAL CENTER,R01,2000,270618,0.0044223691544290926
"STATISTICAL STUDIES OF DNA EVOLUTION Our goals are to develop methods for statistical analyses of DNA sequence data and to understand the mechanisms of DNA evolution. The specific aims are: l. To examine current methods and develop new methods for estimating evolutionary dates, which is now a central issue in molecular evolution. We shall use the new methods to study divergence dates in mammals, which have recently become very controversial. 2. To develop methods for estimating selection intensities in different regions of a gene and to carry out statistical analyses of DNA sequence data from mammals. 3. To develop fast algorithms for finding optimal trees for the following methods: maximum likelihood, maximum parsimony, and minimum evolution. Such algorithms are much needed because these methods require a tremendous amount of computer time-and are not feasible for large trees. 4. An expert system for choosing the best tree reconstruction method for a data set according to the attributes of the data. 5. To introduce the neural network approach into phylogenetic study; this approach has proved extremely powerful in many branches of science and engineering.  n/a",STATISTICAL STUDIES OF DNA EVOLUTION,6131906,R37GM030998,"['DNA', ' artificial intelligence', ' biochemical evolution', ' computational neuroscience', ' computer assisted sequence analysis', ' computer simulation', ' gene frequency', ' genetic models', ' mathematical model', ' method development', ' model design /development', ' natural selections', ' nucleic acid sequence', ' species difference', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R37,2000,152928,-0.022111752439578164
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,6185220,R01LM006326,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,2000,117821,0.002043527943161976
"RULE DISCOVERY IN BODY CAVITY EFFUSIONS  DESCRIPTION (adapted from the Abstract):                                             Machine learning methods are innovative tools used to find patterns in medical       data.  Laboratory data is suited to computerized Interpretation because of its       objective, quantitative nature.  Body fluid analysis is a good model for             evaluating machine learning in the laboratory.  Pathologists spend a                 substantial amount of time analyzing and classifying body fluids, or                 effusions, which are abnormal accumulations of fluid within body cavities of         human beings and animals, caused by diseases such as congestive heart failure.       Fluid classification provides clinicians with important diagnostic information       about the underlying disease process.  Automation of body fluid analysis by a        machine learning system would substantially increase the efficiency and              profitability of a medical laboratory.  In a pilot study, RIPPER (Repeated           Incremental Pruning to Produce Error Reduction), a rule discovery tool,              accurately classified effusions from animals into five standard categories,          based on the physical, chemical, and cellular characteristics of the fluid.          The purposes of this study are: 1) to determine the accuracy of RIPPER on a          larger data set, to expand and strengthen the results of the pilot; 2) to test       the accuracy of RIPPER's fluid classifications prospectively in a large              veterinary teaching hospital laboratory, 3) to determine the acceptance rate         or reason for rejection of RIPPER's classification by clinical pathologists;         and (4) to use RIPPER to discover novel rules for classifying effusions by           underlying disease process.                                                                                                                                               The results of this study will validate and test the acceptance of a machine         learning system applicable to fluid analysis in both human and veterinary            clinical laboratories.  By discovering new patterns in quantitative data that        identify the specific underlying disease, RIPPER can greatly enhance the             diagnostic value of laboratory analysis.                                                                                                                                  n/a",RULE DISCOVERY IN BODY CAVITY EFFUSIONS,6144004,F32LM000095,"['body fluids', ' classification', ' computer assisted instruction', ' programmed instruction', ' veterinary science']",NLM,UNIVERSITY OF CALIFORNIA DAVIS,F32,2000,52420,-0.027850876760718648
"CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be       taken to advance our understanding of the genetic and associated co-             variates affecting the inheritance of complex diseases. To that end, this        proposal will develop improved quantitative methods to detect genetic            factors contributing to increased susceptibility to complex disorders and        implement these methods in software for distribution to the research             community.                                                                                                                                                        The methods will concentrate on the use of classification techniques             applied to allele sharing data and other risk factors which affect the           trait. Allele sharing methods for mapping genes will be extended to              include the classification methods known as latent class models, cluster         analysis, and artificial neural networks, as well as a novel use of              logistic regression Co-variates such as gender, parental diagnosis, or           other concomitant factors will be systematically studied through                 applications to both stimulated and existing data sets. An additional goal       is to determine the optimal distribution of relative pairs (e.g. siblings,       first cousins) for these methods. Of great importance to this proposal is        the development of well-documented, user-friendly software and                   documentation which will be distributed to the scientific community via          the Internet. Existing software developed by the PI will be extensively          expanded for latent class models. Existing cluster analysis software will        be modified and combined for ease of use.                                                                                                                         This proposal consists of theoretical exploration, computer simulation,          data analysis, and software development. First, solutions of theoretical         questions relating to classification techniques will be pursued; second,         adaptation of computer programs to implement the analytic methods, and           investigation into alternative research strategies will be accomplished.         The new strategies will be applied to stimulated data, and finally, to           existing data sets of pedigrees in which a complex trait has been                diagnosed. Findings from this research may contribute to the ability to          locate susceptibility loci in complex traits and to the clarification of         those etiological mechanisms responsible for susceptibility.                      n/a",CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI,6168495,R01AA012239,"['alleles', ' analytical method', ' artificial intelligence', ' biomedical resource', ' computer program /software', ' computer simulation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' family genetics', ' gene environment interaction', ' gene expression', ' genetic disorder', ' genetic disorder diagnosis', ' genetic mapping', ' genetic markers', ' genetic susceptibility', ' human data', ' mathematical model', ' model design /development', ' quantitative trait loci', ' statistics /biometry']",NIAAA,WASHINGTON UNIVERSITY,R01,2000,180260,0.0005247752903025231
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6090912,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2000,214602,0.03059718859321749
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6193019,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN,R01,2000,478050,-0.004602307259296022
"Deep learning for population genetics Project Summary The revolution in genome sequencing technologies over the past 15 years has created an explosion of population genomic data but has left in its wake a gap in our ability to make sense of data at this scale. In particular, whereas population genetics as a field has been traditionally data-limited, the massive volume of current sequencing means that previously unanswerable questions may now be within reach. To capitalize on this flood of information we need new methods and modes of analysis.  In the past 5 years the world of machine learning has been revolutionized by the rise of deep neural networks. These so-called deep learning methods offer incredible flexibility as well as astounding improvements in performance for a wide array of machine learning tasks, including computer vision, speech recognition, and natural language processing. This proposal aims to harness the great potential of deep learning for population genetic inference.  In recent years our group has made great strides in using supervised machine learning for population genomic analysis (reviewed in Schrider and Kern 2018). However, this work has focused primarily on using more traditional machine learning methods such as random forests. As we argue in this proposal, DNA sequence data are particularly well suited for modern deep learning techniques, and we demonstrate that the application of these methods can rapidly lead to state-of-the-art performance in very difficult population genetic tasks such as estimating rates of recombination. The power of these methods for handling genetic data stems in part from their ability to automatically learn to extract as much useful information as possible from an alignment of DNA sequences in order to solve the task at hand, rather than relying on one or more predefined summary statistics which are generally problem-specific and may omit information present in the raw data.  In this proposal we lay out a systematic approach for both empowering the field with these tools and understanding their shortcomings. In particular, we propose to design deep neural networks for solving population genetic problems, and incorporate successful networks into user-friendly software tools that will be shared with the community. We will also investigate a variety of methods for estimating the uncertainty of predictions produced by deep learning methods; this area is understudied in machine learning but of great importance to biological researchers who require an accurate measure of the degree of uncertainty surrounding an estimate. Finally, we will explore the impact of training data misspecification—wherein the data used to train a machine learning method differ systematically from the data to which it will be applied in practice. We will devise techniques to mitigate the impact of such misspecification in order to ensure that our tools will be robust to the complications inherent in analyzing real genomic data sets. Together, these advances have the potential to transform the methodological landscape of population genetic inference. Project Narrative Deep learning has revolutionized such disparate fields as computer vision, natural language processing, and speech recognition. In this proposal we aim to harness the great potential of deep learning for population genetic inference. We will design, implement, and apply novel deep learning methods and provide open source software for others to both use and build upon, thereby producing valuable tools for the genetics researchers at large.",Deep learning for population genetics,9976348,R01HG010774,"['Algorithms', 'Area', 'Biological', 'Biology', 'Classification', 'Code', 'Communities', 'Computer Vision Systems', 'Computer software', 'DNA Sequence', 'Data', 'Development', 'Ensure', 'Floods', 'Genetic', 'Genetic Recombination', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Image', 'Lead', 'Learning', 'Left', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Natural Language Processing', 'Natural Selections', 'Nature', 'Performance', 'Population', 'Population Explosions', 'Population Genetics', 'Process', 'Program Development', 'Publishing', 'Research Personnel', 'Sequence Alignment', 'Software Tools', 'Techniques', 'Technology', 'Training', 'Trees', 'Uncertainty', 'Ursidae Family', 'Work', 'base', 'computational chemistry', 'convolutional neural network', 'deep learning', 'deep neural network', 'design', 'empowered', 'flexibility', 'genetic information', 'genome sequencing', 'genomic data', 'infancy', 'innovation', 'learning classifier', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'network architecture', 'neural network', 'neural network architecture', 'next generation', 'novel', 'open source', 'random forest', 'recurrent neural network', 'research and development', 'speech recognition', 'statistics', 'stem', 'success', 'supervised learning', 'tool', 'tool development', 'user friendly software']",NHGRI,UNIVERSITY OF OREGON,R01,2020,529154,-0.022489273842691435
"Development of a joint machine learning/de novo assembly system for resolving viral quasispecies PROJECT SUMMARY Viral hepatitis from hepatitis B (HBV) establishes chronic infections in >250M people worldwide; chronicity is on the rise, and approximately one-third of the world’s population (2 billion) has serologic evidence of exposure. HBV coinfection with HCV and HIV is a hidden consequence of the substance use disorder epidemic. Viral populations have extremely high sequence diversity and rapidly evolve, which explains the vaccine failure rates and viral resistance to existing therapies and makes discovering lasting therapies extremely challenging. Next Generation Sequencing (NGS) is the method of choice to assess the intra-host virus population, termed a “quasispecies”. While a large set of short DNA sequencing reads are acquired that represent the virions in the quasispecies, computational technologies are limited in their analysis capabilities, resulting in particularly low resolution of complex HBV genomic structures. Another challenge is assembling NGS reads representing short fragment of the host genome into full strains (haplotypes) without knowledge of their true occurrence in the samples. To meet these challenges, GATACA is developing pathogen-specific bioinformatics software, GAT-ML (GATACA Assembly Tool – machine learning [ML]) to support treatment discovery and improve infection control. Its specifically designed algorithm utilizes novel ML methodologies adapted and modified for assisting genome assembly that will allow GAT-ML to reconstruct complete viral haplotypes and populations by learning the ‘language’ of the sequences. Tailored initially for HBV samples, GAT and its new ML system will be integrated for feasibility testing in this Phase I with the following Specific Aims: 1. Specific Aim 1. Build a joint learning system. Train and test natural language processing (NLP) methods on HBV genetic variation. 2. Specific Aim 2. Implement and test the machine learning methods in GAT (GAT-ML). We anticipate a working tool for characterizing HBV haplotypes, validated with multi-sourced datasets, and extensive testing and benchmarking of offline and integrated methods. The proposed project will develop and increase the capabilities of our novel computational tool, GAT, to help researchers identify the full spectrum of genetic features of a viral population—such as emergence and persistence of resistance or baseline polymorphisms regardless of their frequencies—and translate these findings to the development of new or improved antiviral drugs and other applications requiring high analytic sensitivity. GAT will particularly benefit researchers working in preclinical stages of drug development who require rapid, sensitive, and reliable results to inform decisions about which targets to advance to clinical trial testing.",Development of a joint machine learning/de novo assembly system for resolving viral quasispecies,10011686,R43AI152894,"['Adoption', 'Algorithm Design', 'Algorithms', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Chronic', 'Chronic Hepatitis', 'Classification', 'Clinical Trials', 'Complex', 'Computer software', 'DNA Structure', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Epidemic', 'Failure', 'Frequencies', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'HIV', 'HIV/HCV', 'Haplotypes', 'Healthcare', 'Hepatitis B', 'Hepatitis B Virus', 'Infection Control', 'Joints', 'Knowledge', 'Language', 'Language Development', 'Learning', 'Link', 'Liver diseases', 'Machine Learning', 'Metagenomics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Natural Language Processing', 'Outcome', 'Pattern', 'Performance', 'Phase', 'Population', 'Population Analysis', 'Privatization', 'Research Personnel', 'Resistance', 'Resolution', 'Sampling', 'Semantics', 'Serological', 'Serotyping', 'Source', 'Speed', 'Substance Use Disorder', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translating', 'Trust', 'Vaccines', 'Validation', 'Variant', 'Viral', 'Viral hepatitis', 'Virion', 'Virus', 'base', 'chronic infection', 'co-infection', 'commercialization', 'computerized tools', 'contig', 'design', 'drug development', 'improved', 'insertion/deletion mutation', 'machine learning algorithm', 'machine learning method', 'multiple data sources', 'neural network', 'next generation sequencing', 'novel', 'pathogen', 'pre-clinical', 'structural genomics', 'syntax', 'tool', 'viral resistance']",NIAID,"GATACA, LLC",R43,2020,267225,-0.018700496707727474
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy Project Summary/Abstract Artificial intelligence on genomic/healthcare data that is performed jointly between multiple collaborating institutions relies on a trust model but can accelerate genomic medicine research and facilitate quality improvement. To conduct such machine learning while protecting patient privacy and reducing security risks, we are developing blockchain-based privacy-preserving learning methods in a K99/R00 study supported by the National Human Genome Research Institute (NHGRI). However, our previous design of privacy-preserving learning on private blockchain assumed “semi-honesty” as the underlying adversary assumption. That is, we assume that each participating site is curious yet very careful and honest, such that it would only submit correct predictive models. Nevertheless, in real world this assumption may be too optimistic; the models submitted could be an old one due to network latency or malicious users may try to create fake models, which can in turn lead to bioethical concerns and reduce the incentives for genomic/clinical institutions to participate in the collaborative predictive modeling. Therefore, the capability to detect, assess and prevent “model misconducts” is critical to increase the integrity/reliability of machine learning. To address this issue, we consider the following 3 types of model misconducts: (1) model plagiarism, of which a site becomes a free-rider and just submits a copy of a model from the other sites, trying to hide their own information and inspect models from other sites; (2) model fabrication, of which a site mocks up a model, trying to hide information and disturb the machine learning process; and (3) model falsification, of which a site tweaks its model a bit, trying to just disturb the learning process. For each type of the model misconducts, we are interest in how to detect these misconducts of another site, how to assess the losses of machine learning results due to misconducts, and how to prevent these model misconducts. Our aims include (a) detecting model misconducts using model properties, (b) assessing model misconducts losses via model simulation, and (c) preventing model misconducts based on whole model history. The innovative components to our proposed project include (i) summarizing various types of model misconduct, (ii) developing a complete strategy to handle the model misconduct, and (iii) providing a generalizable approach to mitigate bioethical concerns for collaborative machine learning. Project Narrative Artificial intelligence performed jointly between multiple collaborating institutions can accelerate genomic medicine research and facilitate quality improvement, but relies on a trust model which may be too optimistic in real-world setting. In this project, we plan to develop a comprehensive detection, assessment and prevention mechanism to address the potential bioethical risks brought by misconducts of model plagiarism, fabrication, and falsification. The proposed study can supplement the considerations of model misconducts for our original project of privacy-preserving learning on blockchain.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,10130868,R00HG009680,"['Address', 'Artificial Intelligence', 'Bioethics', 'Budgets', 'Calibration', 'Clinical', 'Data', 'Data Set', 'Detection', 'Digit structure', 'Discrimination', 'Event', 'Genomic medicine', 'Genomics', 'Healthcare', 'Incentives', 'Institution', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pattern', 'Plagiarism', 'Prevention', 'Privacy', 'Privatization', 'Process', 'Property', 'Randomized', 'Recording of previous events', 'Research', 'Risk', 'Security', 'Site', 'Sum', 'Testing', 'Time', 'Trust', 'base', 'blockchain', 'design', 'distributed ledger', 'innovation', 'interest', 'learning strategy', 'models and simulation', 'patient privacy', 'predictive modeling', 'prevent', 'privacy preservation', 'statistics']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,102049,-0.019629385388992258
"Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models Project Summary Amyotrophic lateral sclerosis (ALS) and Frontotemporal Dementia FTD are devastating neurodegenerative disorders that lie on a genetic and mechanistic continuum. ALS is a disease of motor neurons that that is almost uniformly lethal within only 3-5 years of diagnosis. FTD is a heterogeneous, rapidly progressing syndrome that is among the top three causes of presenile dementia. About 10% of ALS cases are caused by dominantly transmitted gene defects. SOD1 and FUS mutations cause aggressive motor neuron pathology while TDP43 mutations cause ALS-FTD. Further, wild type FUS and TDP43 are components of abnormal inclusions in many FTD cases, suggesting a mechanistic link between these disorders. Early phenotypes are of particular interest because these could lead to targeted interventions aimed at the root cause of the disorder that could stem the currently inexorable disease progression. Elucidating such early, potentially shared characteristics of these disorders should be greatly aided by: 1) knock-in animal models expressing familial ALS-FTD genes; 2) sensitive, rigorous and objective behavioral phenotyping methods to analyze and compare models generated in different laboratories. In published work the co-PIs applied their first-generation, machine vision-based automated phenotyping method, ACBM ‘1.0’ (automated continuous behavioral monitoring) to detect and quantify the earliest-observed phenotypes in Tdp43Q331K knock-in mice. This method entails continuous video recording for 5 days to generate >14 million frames/mouse. These videos are then scored by a trained computer vision system. In addition to its sensitivity, objectivity and reproducibility, a major advantage of this method is the ability to acquire and archive video recordings and to analyze the data at sites, including the Cloud, remote from those of acquisition. We will use Google Cloud TPUs supercomputers that have been designed from the ground up to accelerate cutting-edge machine learning workloads, with a special focus on deep learning. We will analyze this data using Bayesian hierarchical spline models that describe the different mouse behaviors along the circadian rhythm. The current proposal has two main goals: 1) Use deep learning to refine and apply a Next Generation ACBM - ‘2.0’ - that will allow for more sensitive, expansive and robust automated behavioral phenotyping of four novel knock-in models along with the well characterized SOD1G93A transgenic mouse. 2) To establish and validate procedures to enable remote acquisition of video recording data with cloud-based analysis. Our vision is to establish sensitive, robust, objective, and open-source machine vision-based behavioral analysis tools that will be widely available to researchers in the field. Since all the computer-annotated video data is standardized in ACBM 2.0 and will be archived, we envision a searchable ‘behavioral database’, that can be freely mined and analyzed. Such tools are critical to accelerate the development of novel and effective therapeutics for ALS-FTD. Narrative ALS and Frontotemporal Dementia (FTD) are devastating, rapidly progressing diseases and current treatments are of limited value. In this proposal a neuroscientist and a computer scientist have teamed up to develop a new machine vision-based method for behavioral analysis novel mouse models of ALS-FTD. The ultimate goal is to reveal early phenotypes in ALS-FTD models that can be used in understanding disease pathology and in the development of new therapeutic targets.",Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models,9979408,R21NS112743,"['Amyotrophic Lateral Sclerosis', 'Animal Model', 'Archives', 'Behavior', 'Behavior monitoring', 'Behavioral', 'Characteristics', 'Circadian Rhythms', 'Computer Vision Systems', 'Computers', 'Data', 'Data Set', 'Databases', 'Defect', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Expression Profiling', 'Familial Amyotrophic Lateral Sclerosis', 'Frontotemporal Dementia', 'Gene Expression', 'Generations', 'Genes', 'Genetic', 'Goals', 'Hour', 'Human', 'Intervention', 'Knock-in', 'Knock-in Mouse', 'Laboratories', 'Lead', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Motor Neuron Disease', 'Motor Neurons', 'Mus', 'Mutation', 'Neurodegenerative Disorders', 'Paralysed', 'Pathology', 'Phenotype', 'Plant Roots', 'Presenile Dementia', 'Procedures', 'Publishing', 'Reproducibility', 'Research', 'Research Personnel', 'Respiratory Paralysis', 'Scientist', 'Site', 'Standardization', 'Syndrome', 'TensorFlow', 'Time', 'Training', 'Transgenic Mice', 'Transgenic Organisms', 'Treatment Efficacy', 'Video Recording', 'Vision', 'Work', 'Workload', 'base', 'behavioral phenotyping', 'cloud based', 'data archive', 'deep learning', 'design', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'interest', 'knockin animal', 'machine vision', 'mouse model', 'new therapeutic target', 'next generation', 'novel', 'open source', 'programs', 'protein TDP-43', 'stem', 'supercomputer', 'superoxide dismutase 1', 'tool']",NINDS,BROWN UNIVERSITY,R21,2020,446875,-0.02812652121033042
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9921505,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'data fusion', 'deep learning', 'design', 'feature extraction', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neural network classifier', 'neurophysiology', 'novel', 'post-trauma', 'predictive modeling', 'prevent', 'random forest', 'support vector machine', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,245552,0.0037814030761402235
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9878070,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'machine learning method', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,591130,0.002923785511543789
"A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER) Many promising peripheral neuromodulation techniques have been proposed to treat lower urinary tract (LUT) dysfunction, but our lack of predictive models has forced the community (including the PI’s lab) to explore the vast parameter space of nerve targets, stimulation parameterizations, and electrode designs empirically in animal experiments by trial and error. This type of exploratory experimentation is the only current method of optimizing, personalizing, or discovering novel LUT neuromodulation techniques. Motivated by this clinical need, our long-term goal for this work is to predict the effects of neuromodulation on the LUT. To move toward this goal, we propose to develop a new modeling framework that integrates disparate biophysics models through machine learning, thereby emulating an entire organ system through a process we call Biomechanistic Learning Augmentation of Deep Differential Equation Representations (BLADDER). We will develop and use the general BLADDER framework to create an organ-level model of the normal healthy LUT throughout its filling and voiding cycles, including non-volitional neural reflex control over the bladder and urethra. Our focus on neural reflex control and organ-level scales ensures that, if successful, the BLADDER LUT model will be poised to predict effects of neuromodulation using computational studies, which so far has been impossible due to the complexity of the LUT. The BLADDER framework unites multiple individual mechanistic models (each accounting for a component function of an organ system) by using deep recurrent neural networks (RNN) to learn the appropriate coupling dynamics linking each component model. The combination of mechanistic and machine learning models under a single framework allows us to harness the advantages of both: mechanistic models excel at interpretability but suffer from a lack of scalability (becoming intractable at the level of organ systems), while machine learning models are excellent at scale but lack generalizability and insights for hypothesis generation. The BLADDER framework will scale up mechanistic models to the level of systems physiology by linking tractable model components together using a supervisory RNN, allowing the BLADDER framework to deliver both interpretability and scale. We will draw on existing SPARC datasets in the cat (e.g., Bruns and Gaunt), existing publicly available data in rat, and generate new data in the rat to construct a training dataset for the supervisory RNN. We will further draw from already published small-scale mechanistic models, validated on human and animal data, for the mechanistic components of the BLADDER LUT model. The formal process of identifying these models and datasets, and checking their validity and robustness, will clearly reveal the deficits and strengths in our theoretical and experimental understanding of the LUT in a straightforward and rational way. We will use the 10 Simple Rules to vet mechanistic models for inclusion in the BLADDER LUT model and compile a public inventory for the neurourology community. Major task 1 (Q1-2): Identify available datasets and candidate mechanistic models from published literature. Major deliverables are a public database and a whitepaper detailing the state of the field and prospects for modeling and experimental work. Major Task 2 (Q1-3): Demonstrate proof of concept of BLADDER framework. Major deliverables are a publicly available code linking two LUT component models via supervisory RNN and a report on suitable RNN architectures based on fully described dynamical systems. Major Task 3 (Q3-6): Create a multi-component BLADDER model. Major deliverables are code used to link separate mechanistic LUT models via the supervisory RNN, and an in vivo rat dataset to fill in critical measurables for the machine learning training set. Major Task 4 (Q6-8): Deploy the fully operational BLADDER model of the LUT, including autonomously predicted neural reflex control. Major deliverables are publicly available codes and datasets, and a hypothesis-driven computational experiment to predict simple interventions. n/a",A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER),10206953,OT2OD030524,"['Accounting', 'Animal Experiments', 'Bladder', 'Clinical', 'Code', 'Communities', 'Coupling', 'Data', 'Data Set', 'Databases', 'Differential Equation', 'Electrodes', 'Ensure', 'Equipment and supply inventories', 'Felis catus', 'Functional disorder', 'Generations', 'Goals', 'Individual', 'Intervention', 'Learning', 'Link', 'Literature', 'Lower urinary tract', 'Machine Learning', 'Measurable', 'Methods', 'Modeling', 'Nerve', 'Organ', 'Peripheral', 'Physiology', 'Process', 'Publishing', 'Rattus', 'Reflex control', 'Reporting', 'System', 'Techniques', 'Training', 'Urethra', 'Work', 'animal data', 'base', 'biophysical model', 'body system', 'computer studies', 'design', 'dynamic system', 'experimental study', 'human data', 'in vivo', 'insight', 'neural network architecture', 'neuroregulation', 'novel', 'predictive modeling', 'recurrent neural network', 'relating to nervous system', 'scale up']",OD,FLORIDA INTERNATIONAL UNIVERSITY,OT2,2020,1025141,-0.01440429274017321
"The interaction of myosin and the thin filament: how mutations cause allosteric dysfunction and their connection to genetic cardiomyopathy Project Summary: The long-term goal of this research program is to develop a rigorously experimentally validated all-atom computational model of the cardiac thin filament (CTF) bound to myosin S1 which provides a unique and accessible platform to identify novel, high resolution disease mechanisms linked to Hypertrophic Cardiomyopathy (HCM). In the prior funding period, we refined and extended our existing CTF computational model and successfully employed it to identify unique and clinically relevant allosteric disease mechanisms including HCM mutation-induced changes in myofilament Ca2+ kinetics, mutation-specific molecular causes of differential cardiac remodeling and disease progression. This included an in vivo validation via the development of a novel transgenic mouse model of cTnT-linked dilated cardiomyopathy and a predictive algorithm to determine the pathogenicity of cTnT mutations that out-performed existing computational approaches in a preliminary test. The key to these advances has been the ability of the current model to precisely identify and locate allosteric changes caused by mutations throughout all components of the CTF followed by closely coupled experimental validation and eventual in vivo model correlation. We now propose to significantly expand the biological complexity of the model to include myosin S1, the molecular motor that drives contraction and the second most common genetic cause of HCM. This important and challenging advance will facilitate a deeper understanding of disease pathogenesis by, for the first time, incorporating the role of molecular allosteric mechanisms between myosin S1 and thin filament. This new computational – experimental platform will be used for both mechanistic insight (for example used for the identification of novel myofilament disease targets,) and the development of a comprehensive deep-learning predictive algorithm to assign pathogenicity to both myosin and thin filament HCM mutations. The latter represents the first use of high-resolution structure, dynamics and function to predict HCM disease allele pathogenicity, a central challenge in the clinical management of these complex patients. Both the training and testing components of the deep learning development will utilize data from the highly annotated and curated SHaRe HCM registry thus greatly improving translational power. Two Specific Aims will be pursued: Aim 1 will utilize state of the art rare event simulation methods developed in one of our groups and refinement of existing unstructured domains of the CTF via FRET to establish the new model. Aim 2 will employ an extensive program of computational analysis and subsequent in vitro validation using pathogenic, variants of unknown significance and non- pathogenic HCM alleles derived from SHaRe to provide inputs to the machine learning environment for algorithm development. Novel disease mechanisms for myosin and thin filament HCM that include crosstalk between the two components will also be explored. Elucidation of these mechanisms can be the basis for robust molecular approaches to disease. Precision medicine and “molecular” medicine are concepts that aim to employ a patient’s genetic structure to discern the best medical treatments for disease. Hypertrophic cardiomyopathy is a genetic disease that afflicts 1/500 people. This application translates our knowledge of the molecular level effects of cardiac tissue mutation to disease and will aim to lead to eventual treatment.",The interaction of myosin and the thin filament: how mutations cause allosteric dysfunction and their connection to genetic cardiomyopathy,10071638,R01HL107046,"['Address', 'Alleles', 'Anisotropy', 'Artificial Intelligence', 'Biological', 'Biological Assay', 'Biology', 'Biophysics', 'Breath Tests', 'C-terminal', 'Cardiac', 'Chemistry', 'Clinical Management', 'Complex', 'Computer Analysis', 'Computer Models', 'Contracts', 'Coupled', 'Data', 'Data Set', 'Descriptor', 'Development', 'Differential Scanning Calorimetry', 'Dilated Cardiomyopathy', 'Disease', 'Disease Progression', 'Distant', 'Engineering', 'Enzymes', 'Event', 'Fluorescence Anisotropy', 'Fluorescence Resonance Energy Transfer', 'Functional disorder', 'Funding', 'Generations', 'Genetic', 'Genetic Diseases', 'Genetic Structures', 'Goals', 'Grant', 'Hand', 'Human', 'Hypertrophic Cardiomyopathy', 'In Vitro', 'Individual', 'Induced Mutation', 'Kinetics', 'Knowledge', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Microfilaments', 'Modeling', 'Molecular', 'Molecular Conformation', 'Molecular Medicine', 'Molecular Motors', 'Motor', 'Mutation', 'Myosin ATPase', 'Pathogenesis', 'Pathogenicity', 'Patients', 'Perception', 'Physiological', 'Play', 'Protein Conformation', 'Protein Dynamics', 'Proteins', 'Registries', 'Research', 'Resolution', 'Resources', 'Role', 'Sampling', 'Sarcomeres', 'Site', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thick Filament', 'Thin Filament', 'Thinness', 'Time', 'Tissues', 'Training', 'Transgenic Mice', 'Translating', 'Validation', 'Variant', 'Work', 'algorithm development', 'automated analysis', 'base', 'cell motility', 'clinically relevant', 'deep learning', 'educational atmosphere', 'experimental study', 'improved', 'in vivo', 'in vivo Model', 'inherited cardiomyopathy', 'insight', 'machine learning algorithm', 'mouse model', 'neural network', 'next generation', 'novel', 'phosphorescence', 'precision medicine', 'prediction algorithm', 'programs', 'quantum chemistry', 'response', 'simulation', 'stopped-flow fluorescence', 'success', 'variant of unknown significance']",NHLBI,UNIVERSITY OF ARIZONA,R01,2020,585711,-0.027322300220431167
"Machine Learning Development for Subtyping COPD Project Summary Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. To date, researchers have attempted to use standard machine learning methodology to identify more meaningful subtypes of COPD, but these methods often make general assumptions about the data, limiting their ability to penetrate more complex patterns in some data sets. Thus, a meaningful reclassification of COPD subtypes that could lead to more targeted therapies and interventions has been elusive. The applicant introduces a new way of looking at the COPD subtyping problem by recasting it in terms of discovering associations of individuals to disease trajectories – i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables. The machine learning methods proposed build on the most recent advances in Bayesian nonparametrics, a collection of theoretical ideas and techniques that permit very flexible data representations. In this career development proposal, the applicant hypothesizes that these machine learning methods and extensions thereof – together with data sources not previously leveraged for COPD subtyping – will produce more biologically meaningful sub-groupings of patients, leading to a better understanding of the genetic and biological underpinnings of the disease and ultimately improved patient management. Aim 1 of this application involves evaluating the utility of CT-assessed lung mass – a potentially more discriminative measure of emphysema than conventionally used measures – for defining COPD subtypes using both K-means clustering and our disease trajectory algorithm. The goal of Aim 2 is to evaluate the utility of comorbidity data for defining COPD subtypes using our trajectory clustering algorithm. Novel computed tomography based measures of muscle wasting (cachexia) and pulmonary vascular pruning will be explored to determine their efficacy in subtype determination. Additionally, we will extend and test the trajectory algorithm in order to model discrete outputs (such as physician-diagnosed comorbidities), count data (e.g. exacerbations), and time-to-event data (death). In Aim 3, the applicant will extend our trajectory clustering algorithms to directly incorporate genetic and omics data for subtype discovery. Together, the research proposed in the aims of this award will take full advantage of the comprehensive data set available through the COPDGene study. Execution of the aims in this proposal will be possible through active collaboration with Dr. Ron Kikinis, M.D., a renowned leader in the field of medical image analysis, and Dr. Ed Silverman, an internationally recognized expert in the genetic epidemiology of COPD. Project Narrative Chronic obstructive pulmonary disease (COPD) is a heterogeneous lung condition characterized by progressive loss of lung function with subsequent increasing breathlessness and worsening quality of life. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. In carrying out the research outlined in the following proposal, we plan to develop and apply machine learning methods to better identify subpopulations of individuals who have similar forms of COPD, potentially enabling better, targeted therapies.",Machine Learning Development for Subtyping COPD,9948018,K25HL130637,"['Affect', 'Algorithms', 'Award', 'Bayesian Analysis', 'Biological', 'Biological Markers', 'Blood Vessels', 'Cachexia', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Chronic Obstructive Airway Disease', 'Clinical', 'Collaborations', 'Collection', 'Complex', 'Data', 'Data Reporting', 'Data Set', 'Data Sources', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Disease susceptibility', 'Doctor of Medicine', 'Dyspnea', 'Environment', 'Environmental Risk Factor', 'Event', 'Failure', 'Functional Imaging', 'Genetic', 'Goals', 'Grouping', 'Health', 'Heterogeneity', 'Image', 'Image Analysis', 'Individual', 'Inflammatory Response', 'International', 'Intervention', 'Lead', 'Lung', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Muscular Atrophy', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Physicians', 'Process', 'Publishing', 'Pulmonary Emphysema', 'Pulmonary Mass', 'Quality of life', 'Research', 'Research Personnel', 'Respiratory physiology', 'Scheme', 'Smoke', 'Statistical Models', 'Subgroup', 'Syndrome', 'Techniques', 'Testing', 'Time', 'X-Ray Computed Tomography', 'airway obstruction', 'base', 'career development', 'cigarette smoke', 'comorbidity', 'design', 'disorder subtype', 'flexibility', 'genetic association', 'genetic epidemiology', 'imaging biomarker', 'improved', 'machine learning algorithm', 'machine learning method', 'mortality', 'novel', 'particle', 'peripheral blood', 'predictive modeling', 'response', 'targeted treatment']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K25,2020,189000,-0.05127002175938055
"Deep Learning Algorithms for FreeSurfer Abstract FreeSurfer is a tool for the analysis of Magnetic Resonance Imaging (MRI) that has proven to be a flexible and powerful technology for quantifying the effects of many conditions, including numerous neurological disorders, on human brain anatomy, connectivity, vasculature, chemical composition, physiology and function. In the past 20 years, these open source tools have been developed to accurately and automatically segment an array of brain structures and have become the core analysis infrastructure for the Alzheimer’s Disease NeuroImaging Initiative (ADNI). In this project, we seek the resources to radically increase the speed, accuracy and flexibility of these tools, taking advantage of exciting new results in Deep Learning. This will enable us to more accurately quantify neuroanatomical changes that are critical to diagnosing, staging and assessing the efficacy of potential therapeutic interventions in diseases such as Alzheimer’s. This includes the generation of documentation, tutorials, unit tests, regression tests and system tests to harden the tools and make them usable by clinicians and neuroscientists, and finally the distribution and support of the data, manual labelings and tools to the more than 40,000 researchers that use FreeSurfer through our existing open source mechanism. In addition, we will analyze the entire Alzheimer’s Disease NeuroImaging Initiative dataset and return it for public release, including a set of manually labeled data that can be used to optimize Deep Learning tools for Alzheimer’s Disease over the next decade. Relevance Successful completion of the proposed project will increase the usability and accuracy of our publicly available segmentation tools, and open up new possibilities, such as integrating them into the MRI scanner and rapidly detecting Alzheimer’s-related changes. These new capabilities well enable other studies to significantly increase their ability to detect AD and other disease effects in research settings as well as phase II and phase III clinical trials due to the radical increase in speed of the new tools, enabling them to be applied to a diverse set of MRI contrasts and much larger datasets, rapidly and accurately. Further, they will allow rapid application of cutting-edge analyses to the ongoing Alzheimer’s Disease NeuroImaging Initiative dataset, improving the ability to extract early biomarkers of this devastating disease.",Deep Learning Algorithms for FreeSurfer,9970009,R01AG064027,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Chemicals', 'Code', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Engineering', 'Ensure', 'Excision', 'Functional Magnetic Resonance Imaging', 'Future', 'Generations', 'Hour', 'Human', 'Image', 'Infrastructure', 'Label', 'Licensing', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Memory', 'Modeling', 'Neurobiology', 'Pattern', 'Phase II Clinical Trials', 'Phase III Clinical Trials', 'Physiology', 'Population', 'Procedures', 'Publishing', 'Recording of previous events', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Rest', 'Sensitivity and Specificity', 'Speed', 'Staging', 'Stream', 'Structure', 'Surface', 'System', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Validation', 'Variant', 'Work', 'base', 'contrast imaging', 'convolutional neural network', 'cranium', 'deep learning', 'deep learning algorithm', 'early detection biomarkers', 'flexibility', 'high resolution imaging', 'human disease', 'improved', 'large datasets', 'morphometry', 'nervous system disorder', 'neuroimaging', 'novel', 'open source', 'prevent', 'prototype', 'skills', 'spatial relationship', 'support tools', 'tool', 'usability', 'web site', 'wiki']",NIA,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,649026,-0.07479436086432384
"Deep-learning based spatial multiple testing for Alzheimer's neuroimaging data Project Summary The broad objective of this research is to develop a powerful deep-learning based multiple testing approach for high-dimensional spatial data that arise commonly in biomedical imaging studies, in particular, brain imaging studies. The motivating problem is to detect the cerebral metabolic abnormalities in Alzheimer’s disease (AD) from Fluorine-18 fluorodeoxyglucose positron emission tomography (FDG-PET) data. Existing multiple testing approaches in solving this problem often ignore or inadequately capture the spatial dependence among the test statistics obtained from brain voxels and thus lose substantial power for the detection. We will develop a novel spatial multiple testing method that utilizes the deep convolutional neural network (DCNN), a key deep- learning technique, to well capture the spatial dependence among test statistics and thus to achieve the optimal power in the sense of minimizing the false nondiscovery rate (FNR) while correctly controlling the false discovery rate (FDR) at a given level. The proposed DCNN-based FDR controlling method has enhanced power to discover new AD-related brain regions that are missed by conventional methods, thereby leading to novel clinical and pathological studies. The specific aims of this proposal include: 1. To develop an optimal spatial FDR controlling approach by connecting the unsupervised local-significance-index based multiple testing with the supervised DCNN-based image segmentation; 2. To evaluate the proposed spatial FDR controlling approach via extensive simulations under various three-dimensional spatial dependence structures, in comparison with multiple classical and state-of-the-art methods; 3. To apply proposed spatial FDR controlling approach to detect AD-related brain regions using the FDG-PET datasets from the Alzheimer’s Disease Neuroimaging Initiative and the Weill Cornell Brain Health Imaging Institute; 4. To develop a user- friendly and publicly available software package with versions in both Python and R to implement the proposed spatial FDR controlling approach. The proposed DCNN-based approach will also be widely applicable to large- scale multiple testing problems in other fields of biomedical research that involve spatial dependence. Project Narrative This project will exploit recent advances in deep learning to efficiently solve the large-scale spatial multiple testing problems that arise commonly in biomedical imaging studies. The proposed powerful deep-learning based spatial multiple testing approach will be particularly useful in brain imaging studies on neurodegenerative disorders such as Alzheimer’s disease and age-related cognitive impairment.",Deep-learning based spatial multiple testing for Alzheimer's neuroimaging data,10107565,R21AG070303,"['3-Dimensional', 'Affect', 'Age-associated memory impairment', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease patient', 'Architecture', 'Biomedical Research', 'Brain', 'Brain imaging', 'Brain region', 'Cerebrum', 'Clinical', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Detection', 'Disease', 'Early Diagnosis', 'Family', 'Fluorine', 'Glucose', 'Goals', 'Health Sciences', 'Image', 'Institutes', 'Learning', 'Literature', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Monitor', 'Network-based', 'Neurodegenerative Disorders', 'Pathologic', 'Patients', 'Performance', 'Population Group', 'Positron-Emission Tomography', 'Problem Solving', 'Procedures', 'Pythons', 'Research', 'Research Personnel', 'Structural Models', 'Structure', 'Supervision', 'Techniques', 'Testing', 'Training', 'base', 'bioimaging', 'brain health', 'convolutional neural network', 'deep learning', 'fluorodeoxyglucose positron emission tomography', 'high dimensionality', 'imaging Segmentation', 'imaging study', 'indexing', 'metabolic rate', 'mild cognitive impairment', 'neuroimaging', 'novel', 'repository', 'simulation', 'statistics', 'success', 'theories', 'user friendly software', 'user-friendly']",NIA,NEW YORK UNIVERSITY,R21,2020,435875,-0.058248023962327033
"Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia ABSTRACT The “preclinical” phase of Alzheimer’s disease (AD) is characterized by abnormal levels of brain amyloid accumulation in the absence of major symptoms, can last decades, and potentially holds the key to successful therapeutic strategies. Today there is an urgent need for quantitative biomarkers and genetic tests that can predict clinical progression at the individual level. This project will develop cutting edge machine learning algorithms that will mine high dimensional, multi-modal, and longitudinal data to derive models that yield individual-level clinical predictions in the context of dementia. The developed prognostic models will specifically utilize ubiquitous and affordable data types: structural brain MRI scans, saliva or blood-derived genome-wide sequence data, and demographic variables (age, education, and sex). Prior research has demonstrated that all these variables are strongly associated with clinical decline to dementia, however to date we have no model that can harvest all the predictive information embedded in these high dimensional data. Machine learning (ML) algorithms are increasingly used to compute clinical predictions from high- dimensional biomedical data such as clinical scans. Yet, most prior ML methods were developed for applications where the ``prediction’’ task was about concurrent condition (e.g., discriminate cases and controls); and established risk factors (e.g., age), multiple modalities (e.g., genotype and images) and longitudinal data were not fully exploited. This application’s core innovation will be to develop rigorous, flexible, and practical ML methods that can fully exploit multi-modal, longitudinal, and high- dimensional biomedical data to compute prognostic clinical predictions. The proposed project will build on the PI’s strong background in computational modeling and analysis of large-scale biomedical data. We will employ an innovative Bayesian ML framework that offers the flexibility to handle and exploit real-life longitudinal and multi-modal data. We hypothesize that the developed models will be more useful than alternative benchmarks for identifying preclinical individuals who are at heightened risk of imminent clinical decline. We will use a statistically rigorous approach for discovery, cross-validation, and benchmarking the developed tools. This project will yield freely distributed, documented, and validated software and models for predicting future clinical progression based on whole-genome, longitudinal structural MRI and demographic data. We believe the algorithms and software we develop will yield invaluable tools for stratifying preclinical AD subjects in drug trials, optimizing future therapies, and minimizing the risk of adverse effects. NARRATIVE Emerging technologies allow us to identify clinically healthy subjects harboring Alzheimer’s pathology. While many of these preclinical individuals progress to dementia, sometimes quite quickly, others remain asymptomatic for decades. The proposed project will develop sophisticated data mining algorithms to derive models that can predict future clinical decline based on ubiquitous, easy- to-collect, and affordable data modalities: brain MRI scans, saliva or blood- derived whole-genome sequences, and clinical and demographic variables.","Advanced machine learning algorithms that integrate genomewide, longitudinal MRI and demographic data to predict future cognitive decline toward dementia",9963080,R01AG053949,"['Activities of Daily Living', 'Adverse effects', 'Age', 'Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Alzheimer&apos', 's disease pathology', 'Amyloid', 'Amyloid beta-Protein', 'Anatomy', 'Bayesian learning', 'Benchmarking', 'Biological Markers', 'Blood', 'Brain', 'Clinical', 'Clinical Data', 'Complex', 'Computer Analysis', 'Computer Models', 'Computer software', 'Data', 'Dementia', 'Education', 'Elderly', 'Emerging Technologies', 'Foundations', 'Funding', 'Future', 'Genetic', 'Genomics', 'Genotype', 'Harvest', 'Hippocampus (Brain)', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Laboratories', 'Life', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Methods', 'Mining', 'Modality', 'Modeling', 'Outcome', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Prevention approach', 'Research', 'Risk', 'Risk Factors', 'Saliva', 'Scanning', 'Secondary Prevention', 'Site', 'Structure', 'Study Subject', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'aging brain', 'base', 'big biomedical data', 'case control', 'clinical predictors', 'clinical risk', 'cognitive ability', 'cognitive testing', 'data mining', 'flexibility', 'functional disability', 'genetic testing', 'genome-wide', 'genomic data', 'genomic locus', 'high dimensionality', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'large scale data', 'machine learning algorithm', 'machine learning method', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neuroimaging', 'novel', 'pre-clinical', 'predictive modeling', 'prognostic', 'risk minimization', 'serial imaging', 'sex', 'software development', 'sound', 'tool', 'whole genome']",NIA,CORNELL UNIVERSITY,R01,2020,410000,-0.059292188103816375
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9923688,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'contig', 'dark matter', 'deep learning', 'deep learning algorithm', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'machine learning method', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'statistical and machine learning', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,276700,-0.0022233768356027055
"Improving Population Representativeness of the Inference from Non-Probability Sample Analysis SUMMARY The critical role of population-representativeness for estimating disease incidence and prevalence has been widely accepted in epidemiologic studies. Improving population representativeness of nonprobability samples, such as samples of volunteers in epidemiologic studies or electronic health records, however, has received little attention by biostatisticians or epidemiologists. In this project, we propose two innovative “pseudoweight” construction methods: 1) two-step matching, and 2) calibration, under an adapted exchangeability assumption, for unbiased estimation of disease incidence and prevalence in the target population. The proposed methods, combined with machine learning methods for propensity score estimation, will achieve significant bias reduction, especially when selection into nonprobability samples is driven by complex relationships between the covariates. We will quantify the bias reduced by the proposed “pseudoweights”, numerically and empirically, on the estimation of disease incidence and prevalence in the target population. Monte Carlo simulation studies are designed under varying degrees of departure from the adapted exchangeability assumption to evaluate the bias of the proposed estimates. The robustness of the proposed estimators against varying sample sizes, number of clusters in survey, and complexities of the true propensity score modeling will be investigated in scenarios that differ by levels of non-linearity, non-additivity and correlations between covariates in the true propensity model. Using data from National Institutes of Health and the American Association of Retired Persons (NIH-AARP, a nonprobability cohort sample) data and the US National Health Interview Survey (NHIS, a probability survey sample), the proposed methods will be applied to estimate the prevalence of self-reported diseases and all-cause or all-cancer mortality rates for people aged 50-71 in the US. To test our methods, we will purposely select outcome variables that are available in both the NIH-AARP and the NHIS. Thus, the amount of bias in NIH-AARP estimates corrected by the proposed pseudoweights can be quantified in practice, assuming the weighted NHIS estimate is true. The proposed methods, although motivated by the volunteer-based epidemiological studies, have wide applications outside of epidemiology, such as electronic health records or web surveys. The results from this project can be used by epidemiologists and health policy makers to improve the understanding of the health-related characteristics in the general population. Computer software that implements the proposed methods will be made available for public use. PROJECT NARRATIVE The project proposes innovative “pseudoweights” construction methods for nonprobability samples, such as samples of volunteers in epidemiologic studies or electronic health records, to improve their population representativeness. The project will quantify the amount of bias reduced by the proposed “pseudoweights,” numerically and empirically, on the estimation of population parameters such as disease incidence and prevalence. The result can be used by epidemiologists and health policy makers to improve the understanding of the health related characteristics in the general population.",Improving Population Representativeness of the Inference from Non-Probability Sample Analysis,10046869,R03CA252782,"['American', 'Attention', 'Calibration', 'Characteristics', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Disease', 'Effectiveness', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Equilibrium', 'General Population', 'Health', 'Health Policy', 'Incidence', 'Internet', 'Lead', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monte Carlo Method', 'National Health Interview Survey', 'Outcome', 'Patient Self-Report', 'Policy Maker', 'Population', 'Prevalence', 'Probability', 'Probability Samples', 'Research', 'Role', 'Sample Size', 'Sampling', 'Source', 'Surveys', 'Target Populations', 'Testing', 'Trees', 'United States National Institutes of Health', 'Weight', 'aged', 'base', 'cohort', 'complex data ', 'design', 'epidemiology study', 'flexibility', 'improved', 'innovation', 'machine learning method', 'mortality', 'random forest', 'retiree', 'software development', 'volunteer']",NCI,"UNIV OF MARYLAND, COLLEGE PARK",R03,2020,154500,0.010994769277831518
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,-0.0075210835275757466
"A Transfer Learning Framework for Creating Subject-Specific Musculoskeletal Models of the Hand PROJECT SUMMARY Restoring hand function remains an elusive goal for many clinical conditions, including stroke, osteoarthritis, tetraplegia, amputation, and traumatic injury. The hand’s anatomical complexity makes restoring hand function particularly challenging because altering any one parameter in the hand can have cascading effects that are difficult to predict, but essential to control. In this proposal, as a critical step toward informing personalized treatments for the hand, we will study how subject-specific differences influence hand function. Completion of this proposal will rely on collection of three datasets that are designed to provide varying levels of biomechanical detail and require varying levels of effort to collect. Briefly, these datasets include (1) a simulation dataset containing 500,000 simulations fully describing all musculoskeletal parameters involved in hand force production, (2) a dense, biomechanical datasets that describes the kinematics, kinetics, and muscle activity required for hand force production in 30 adults, and (3) a sparse, clinically-inspired dataset that describes demographics, anthropometrics, and clinical metrics of hand function in 1000 adults. In Aim 1, we will leverage the first two datasets to design a data-driven analysis framework that identifies the most important biomechanical parameter(s) and maps how those parameters influence hand force production. Completion of this aim will elucidate the biomechanical mechanisms that modulate hand force production and evaluate the ability to use simulation data, instead of experimental data, to identify these mechanisms. In Aim 2, we will leverage all three datasets to create a transfer learning framework capable of efficiently and accurately predicting subject-specific muscle force-generating parameters from easy to collect clinical data. We specifically focus on muscle force- generating parameters because these parameters remain challenging to quickly and accurately estimate, are known to vary across the population, and are highly related to functional metrics like strength. Completion of this aim will provide a new approach for rapidly estimating subject-specific musculoskeletal parameters, thereby enabling efficient creation of subject-specific models and potentially catalyzing use of such models in a clinical setting. Overall, the results from this study could enhance our ability to provide personalized diagnoses and prognoses for individuals suffering from hand impairments. PROJECT NARRATIVE The proposed project aims to understand the biomechanical mechanisms underlying force production in the hand. Specifically, we utilize machine learning methods to examine how subject-specific differences influence hand force production and create subject-specific computer models from easy to obtain clinical data. The results, which integrate modeling with an individual’s clinical data, could enhance our ability to provide personalized diagnoses and prognoses for individuals suffering from hand impairments.",A Transfer Learning Framework for Creating Subject-Specific Musculoskeletal Models of the Hand,10040078,R21EB030068,"['Address', 'Adult', 'Amputation', 'Anatomy', 'Biomechanics', 'Clinical', 'Clinical Data', 'Code', 'Collection', 'Complex', 'Computer Models', 'Computer Simulation', 'Data', 'Data Set', 'Degenerative polyarthritis', 'Diagnostic', 'Floor', 'Future', 'Goals', 'Hand', 'Hand Strength', 'Hand functions', 'Individual', 'Joints', 'Kinetics', 'Learning', 'Maps', 'Methods', 'Modeling', 'Muscle', 'Musculoskeletal', 'Musculoskeletal System', 'Outcome', 'Patients', 'Perception', 'Physics', 'Population', 'Production', 'Psychological Transfer', 'Quadriplegia', 'Research', 'Sensory', 'Stroke', 'Study Subject', 'System', 'Testing', 'Traumatic injury', 'Work', 'Wrist', 'base', 'bone', 'computational platform', 'computerized tools', 'deep neural network', 'demographics', 'design', 'experimental study', 'grasp', 'hand dysfunction', 'hand rehabilitation', 'individual patient', 'kinematics', 'machine learning method', 'motor control', 'neural network', 'neuromuscular', 'novel strategies', 'open-access repositories', 'personalized diagnostics', 'personalized medicine', 'prognostic', 'random forest', 'simulation', 'tool']",NIBIB,UNIVERSITY OF FLORIDA,R21,2020,560939,-0.005211949652974747
"Integrating Machine Learning and Genomic Approaches to Understand Cerebral Small Vessel Disease Pathogenesis from White Matter Hyperintensity Patterns PROJECT SUMMARY  As a neurointensivist and neurologist at Washington University School of Medicine in St. Louis (WUSM), my career goal is to develop an independent research program as a computational biologist capable of using advanced bioinformatics and statistical methods to integrate analysis of large-scale neuroimaging and genetic data, with the aim of deepening understanding of the biological mechanisms influencing cerebral small vessel disease (SVD) and identifying new targets for therapeutic development. As a first step towards this goal, I have designed an innovative proposal that combine machine-learning (ML) methods and integrated imaging genetic analyses of large-scale neuroimaging and genetic data to improve characterization of SVD disease mechanisms.  The clinical, imaging, and etiologic heterogeneity of SVD have impeded efforts to uncover the pathophysiology of this common and debilitating neurological disease. White matter hyperintensities (WMH), a major imaging endpoint of SVD, are comprised of multiple SVD pathologic processes. Growing evidence suggests location-specific vulnerability of brain parenchyma to different underlying SVD pathologic processes, in which spatially localized WMH patterns may reflect distinct SVD etiologies. Characterizing WMH spatial pattern variations in SVD will not only provide insights into underlying pathogenesis, such as vascular amyloid deposition, arteriolosclerosis, and other less well defined or as-yet unknown disease mechanisms, but also lead to creation of novel imaging biomarkers of these SVD pathologic processes. This proposal addresses a key inadequacy, as existing WMH pattern definitions are determined empirically and cannot distinguish overlapping SVD etiologies and risk factors. In this proposal, I aim to capture WMH spatial pattern variations that reflect distinct SVD etiologies in an unbiased manner, by applying clustering analysis/ML methods to structural MRI data to create novel etiology-specific SVD imaging phenotypes. Moreover, given that genetics influence the variance of WMH, I will integrate genetic analyses of these WMH patterns to uncover novel mechanisms that influence SVD pathogenesis. My preliminary data demonstrate the feasibility of identifying data-driven WMH spatial pattern variations, which are specific to distinct SVD etiologies, and allow detection of genetic risk variants that may help inform SVD pathologic processes.  My career plan leverages the extensive resources and exceptional environments at WUSM, under the guidance of a multidisciplinary mentorship team with expertise across diverse fields including cerebrovascular physiology, neuroimaging, informatics, genetics, and machine learning (Drs. Jin-Moo Lee, Daniel Marcus, Carlos Cruchaga and Yasheng Chen). In this Career Development Award, I propose to: 1) determine distinct WMH spatial patterns that can discriminate underlying SVD pathology and/or risk factors by applying pattern analysis ML methods to structural MRI data from three unique cohorts (n=2,710) enriched for different SVD pathologies (Aim 1a), and examining if the ML-defined WMH patterns segregate individuals by well-defined SVD risk factors as biologic validation (Aim 1b), and 2) identify genetic variants (Aim 2a) associated with WMH patterns that reflect diverse pathologic processes influencing SVD using genome wide association and gene-based analyses; replicate the top variants (Aim 2b) in an independent population-based cohort (n=21,708); and use advanced bioinformatics tools to uncover new biologic pathways associated with WMH spatial patterns (Aim 2c).  This research proposal and accompanying development plan with focused training in machine learning, neuroimaging, and multivariate methods for integrated imaging genetics analysis, will build on my background in genetics towards a career investigating cerebrovascular disorders using translational bioinformatics. This Award will provide me with the necessary training to evolve into an independent investigator with a computational research program that can integrate large imaging and genetics datasets to derive results that are highly relevant to the prevention and treatment of cerebrovascular disease in my clinical patient population. PROJECT NARRATIVE Cerebral small vessel disease (SVD) is one of the most prevalent neurological conditions in older adults, and a leading cause of stroke and cognitive impairment, for which existing treatment and preventative options have been limited or lack efficacy. This project’s goal is to enhance our knowledge of the complex disease processes underlying SVD using machine learning and integrating individual imaging and genetic data from over 20,000 adults. By understanding these disease processes, we can design novel methods to more effectively treat and prevent stroke and dementia.",Integrating Machine Learning and Genomic Approaches to Understand Cerebral Small Vessel Disease Pathogenesis from White Matter Hyperintensity Patterns,10022173,K23NS110927,"['3-Dimensional', 'Address', 'Adult', 'Affect', 'Alzheimer&apos', 's Disease', 'Amyloid deposition', 'Archives', 'Arteries', 'Award', 'Bioinformatics', 'Biological', 'Biology', 'Blood Vessels', 'Brain', 'Categories', 'Cerebral Amyloid Angiopathy', 'Cerebral hemisphere hemorrhage', 'Cerebral small vessel disease', 'Cerebrovascular Disorders', 'Cerebrovascular Physiology', 'Clinical', 'Cluster Analysis', 'Complex', 'Computational Technique', 'Data', 'Data Set', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Disease', 'Disease Progression', 'Distal', 'Elderly', 'Environment', 'Etiology', 'Failure', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Risk', 'Genetic Variation', 'Genomic approach', 'Genotype', 'Goals', 'Grant', 'Heritability', 'Heterogeneity', 'Hypertension', 'Image', 'Impaired cognition', 'Individual', 'Informatics', 'Intervention', 'Investigation', 'Ischemic Stroke', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Lobar', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mentorship', 'Methods', 'Microvascular Dysfunction', 'Neurologic', 'Neurologist', 'Pathogenesis', 'Pathogenicity', 'Pathologic', 'Pathologic Processes', 'Pathology', 'Pathway interactions', 'Pattern', 'Phenotype', 'Play', 'Population', 'Prevention', 'Process', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'Risk Factors', 'Role', 'Statistical Methods', 'Stroke', 'Stroke prevention', 'Structure', 'Testing', 'Training', 'Universities', 'Validation', 'Variant', 'Washington', 'White Matter Hyperintensity', 'arteriole', 'base', 'biobank', 'bioinformatics tool', 'brain parenchyma', 'career', 'cohort', 'design', 'disorder subtype', 'effective therapy', 'genetic analysis', 'genetic association', 'genetic variant', 'genome wide association study', 'genome-wide', 'hypertension control', 'imaging biomarker', 'imaging genetics', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'machine learning method', 'medical schools', 'multidisciplinary', 'nervous system disorder', 'neuroimaging', 'new therapeutic target', 'novel', 'patient population', 'population based', 'programs', 'racial and ethnic', 'risk variant', 'serial imaging', 'stroke therapy', 'success', 'therapeutic development', 'tool', 'treatment strategy', 'unsupervised learning']",NINDS,WASHINGTON UNIVERSITY,K23,2020,178809,-0.027538786478815906
"Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data Project Abstract/Summary Our interdisciplinary research team will develop algorithms to accelerate the detection of respiratory virus outbreaks at an unprecedented local scale in US cities. We propose to advance outbreak detection by combining machine learning data integration methods and spatial models of disease transmission. The dynamic models that will be developed will provide mechanistic engines for distinguishing typical from atypical disease trends and the optimization methods evaluate the informativeness of data sources to achieve specified public health goals through the rapid evaluation of diverse input data sources. Working with local healthcare and public health leaders, we will translate the algorithms into user-friendly online tools to support preparedness plans and decision-making. Our proposed research is organized around three major aims. In Aim 1, we will apply machine learning and signal processing methods to build systems that track the earliest indicators of emerging outbreaks within seven US cities. We will evaluate non-clinical data reflecting early and mild symptoms as well as clinical data covering underserved communities and geographic and demographic hotspots for viral emergence. In Aim 2, we will develop sub-city scale models reflecting the syndemics of co-circulating respiratory viruses and chronic respiratory diseases (CRD) that can exacerbate viral infections. We will infer viral transmission rates and socio-environmental risk cofactors by fitting the model to respiratory disease data extracted from millions of electronic health records (EHRs) for the last nine years. We will then partner with clinical and EHR experts to translate our models into the first outbreak detection system for severe respiratory viruses that incorporates EHR data on CRDs. Using machine learning techniques, we will further integrate other surveillance, environmental, behavioral and internet predictor data sources to maximize the accuracy, sensitivity, speed and population coverage of our algorithms. In Aim 3, we will develop an open-access Python toolkit to facilitate the integration of next generation data into outbreak surveillance models. This project will produce practical early warning algorithms for detecting emerging viral threats at high spatiotemporal resolution in several US cities, elucidate socio-geographic gaps in current surveillance systems and hotspots for viral emergence, and provide a robust design framework for extrapolating these algorithms to other US cities. Project Narrative We will develop innovative algorithms for detecting emerging respiratory viruses within US cities. To do so, we will model the syndemic dynamics of respiratory viruses and chronic respiratory diseases and apply machine learning to combine geospatial data that track early indicators of emerging threats. Working with local public health and healthcare collaborators, we will translate this research into practical tools for addressing socio- geographic gaps in surveillance and accelerating the detection, prevention and mitigation of severe outbreaks.","Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data",9946212,R01AI151176,"['Absenteeism', 'Address', 'African', 'Age', 'Algorithm Design', 'Algorithms', 'Area', 'Articulation', 'Bayesian Method', 'Behavioral', 'Caring', 'Chronic', 'Chronic Disease', 'Cities', 'Climate', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Communities', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Disease model', 'Ebola', 'Electronic Health Record', 'Ensure', 'Epidemic', 'Evaluation', 'Geography', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Location', 'Lung diseases', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mexico', 'Modeling', 'Neighborhoods', 'Pollution', 'Population', 'Prevention', 'Public Health', 'Pythons', 'Readiness', 'Reporting', 'Research', 'Resolution', 'Risk', 'Rural', 'Schools', 'Sentinel', 'Series', 'Signal Transduction', 'Social Environment', 'Specific qualifier value', 'Speed', 'Subgroup', 'Surveillance Modeling', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Translating', 'Uncertainty', 'Validation', 'Viral', 'Virus', 'Virus Diseases', 'Visualization', 'Work', 'World Health Organization', 'austin', 'base', 'cofactor', 'comorbidity', 'dashboard', 'data acquisition', 'data integration', 'design', 'digital', 'disease transmission', 'diverse data', 'epidemiologic data', 'epidemiological model', 'experimental study', 'flexibility', 'global health', 'health care availability', 'health goals', 'high risk', 'high risk population', 'influenza outbreak', 'influenzavirus', 'innovation', 'insight', 'metropolitan', 'next generation', 'novel', 'outcome prediction', 'pandemic disease', 'public health intervention', 'respiratory virus', 'school district', 'signal processing', 'simulation', 'social media', 'sociodemographic group', 'socioeconomics', 'sound', 'spatiotemporal', 'stem', 'tool', 'transmission process', 'trend', 'user-friendly', 'viral transmission']",NIAID,YALE UNIVERSITY,R01,2020,611043,-0.003736684355263636
"Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1 PROJECT SUMMARY Previous studies showed discrepancies of health and behavior prevalence between American Indians (AI) population and other racial or ethnic groups. Most health surveys have certain limitations when studying AI population due to the small sample sizes for AI population. Data collected by AI Tribal Epidemiology Centers (TECs) provides an excellent opportunity to conduct research for AI population due to sufficient sample size and extensive information. However, most surveys conducted by TECs used non-probability sampling design (e.g. convenient sample) due to its lower cost and increased time efficiency. Non-probability sample may suffer from sampling, coverage and nonresponse errors without further proper adjustments. Such difficulties greatly hampers the analysis of AI population in health and behavior research. Our general hypothesis is that data integration by combining information from non-probability and probability samples can reduce sampling, coverage and nonresponse errors in original non-probability sample. The Goal of this project is to develop an accurate and robust data integration methodology for AI population analysis specifically tailored to health and behavior research. During the past years, we have 1) studied data integration using calibration and parametric modeling approaches; 2) investigated machine learning and propensity score modeling methods in survey sampling and other fields; and 3) assembled an experienced team of multi-disciplinary team of experts. In this project, we propose to capitalize on our expertise and fulfill the following Specific Aims: Aim 1. Develop a data integration approach using machine learning and propensity score modeling We will develop machine learning and propensity score based data integration approaches to combine information from non-probability and probability samples. Compared to existing methods (i.e., Calibration, Parametric approach), our proposed approaches are more robust against the failure of underlying model assumptions. The inference is more general and multi-purpose (e.g. one can estimate most parameters such as means, totals and percentiles). Simulation studies will be performed to compare our proposed methods with other existing methods. A computing package will be built to implement the method in other settings. Aim 2. Evaluate the accuracy and robustness of the proposed method in AI health and behavior research We will use real data to validate the proposed methods in terms of accuracy and robustness to the various data types. The performance will also be assessed by comparing with results from existing data integration methods such as calibration and parametric modeling approaches. The planned study takes advantage of a unique data source and expands the impact of the Indian Health Service (IHS)-funded research. We expect this novel integration method will vertically advance the field by facilitating the analysis based on non-probability sample, which can provide in-depth understanding regarding the AI population health and behavior studies. Project Narrative The overall goal of this R21 project is to develop an accurate, robust and multi-purpose data integration methodology for AI population (non-probability sample) analysis specifically tailored to health and behavior research such as diabetes and smoking. The code implementing the proposed method will be released and is general enough to be applied to AI population studies of other fileds. The success of this study will vertically advance the field by facilitating the AI population analysis, which can provide a better guidance and new insights on the future precision personalized prevention and treatment of certain diseases.",Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1,10063407,R21MD014658,"['Adult', 'Age', 'American', 'American Indians', 'Behavioral', 'Behavioral Risk Factor Surveillance System', 'Calibration', 'Censuses', 'Code', 'Communities', 'Community Surveys', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Disease', 'Epidemiology', 'Ethnic group', 'Event', 'Failure', 'Funding', 'Future', 'General Population', 'Geographic state', 'Goals', 'Health', 'Health Fairs', 'Health Surveys', 'Health behavior', 'High Prevalence', 'Kansas', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Not Hispanic or Latino', 'Oklahoma', 'Performance', 'Population', 'Population Analysis', 'Population Study', 'Prevalence', 'Probability', 'Probability Samples', 'Publishing', 'Race', 'Research', 'Research Personnel', 'Respondent', 'Risk Factors', 'Sample Size', 'Sampling', 'Smoking', 'Surveys', 'Target Populations', 'Testing', 'Texas', 'Time', 'Tobacco', 'Training', 'United States Indian Health Service', 'Weight', 'Work', 'Youth', 'base', 'behavioral study', 'cigarette smoking', 'cluster computing', 'cost', 'data integration', 'data quality', 'design', 'experience', 'improved', 'individualized prevention', 'innovation', 'insight', 'multidisciplinary', 'novel', 'personalized medicine', 'population health', 'simulation', 'smoking prevalence', 'success', 'therapy development', 'tribal health']",NIMHD,UNIVERSITY OF OKLAHOMA HLTH SCIENCES CTR,R21,2020,115176,0.005615291270393658
"Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios PROJECT SUMMARY The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four months represents a nearly unprecedented challenge for the infectious disease modelling community. This pandemic has benefitted from huge volumes of data being generated, but the rate of dissemination of these data has often outpaced existing data pipelines. While the last decade has seen significant advances in real-time infectious disease forecasting — spurred by rapid growth in data and computational methods — these methods have primarily focused on seasonal endemic diseases based, are based on historical data, and so do not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leverage the wealth of surveillance data at fine spatial granularity, together with associated information about policy interventions and environmental conditions over space and time, to reason directly about the mechanisms to forecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must use sound statistical and epidemiological principles and be flexible and computationally efficient to provide real- time forecasts to guide public health decision-making and respond to changing aspects of this global crisis. The central research activities of this project are (1) to develop scalable, computationally efficient Bayesian hierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2) to design models and conduct analyses to draw robust inference about the effectiveness of interventions in impacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base for continued responses to COVID-19 and future pandemics. PUBLIC HEALTH NARRATIVE The SARS-CoV-2 pandemic is an emerging public health crisis. A fundamental challenge is how to turn data into evidence that can inform decision-making about managing resources, improving health outcomes, and controlling further spread of SARS-CoV-2. Real-time forecasting and flexible mechanistic models to understand the disease dynamics can provide policy-makers tools to manage public response. The goal of the proposed research is to adapt existing statistical modeling frameworks and develop new ones for making forecasts of COVID-19 in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios,10150377,R35GM119582,"['2019-nCoV', 'Award', 'Budgets', 'COVID-19', 'Calibration', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Contracts', 'Data', 'Data Collection', 'Data Reporting', 'Data Sources', 'Decision Making', 'Dengue', 'Dengue Fever', 'Development', 'Disease', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Endemic Diseases', 'Epidemic', 'Epidemiology', 'Evaluation Methodology', 'Future', 'Goals', 'Grant', 'Health', 'Hospitalization', 'Infection', 'Influenza', 'Influenza prevention', 'Intervention', 'Machine Learning', 'Methods', 'Modeling', 'Natural experiment', 'Outcome', 'Performance', 'Policies', 'Policy Maker', 'Programmed Learning', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Resources', 'Series', 'Social Distance', 'Standardization', 'Statistical Methods', 'Statistical Models', 'Structure', 'Testing', 'Thailand', 'Time', 'Work', 'base', 'data dissemination', 'data pipeline', 'deep learning', 'evidence base', 'flexibility', 'human pathogen', 'improved', 'infectious disease model', 'influenza epidemic', 'machine learning method', 'model design', 'novel', 'pandemic disease', 'pathogen', 'predictive modeling', 'rapid growth', 'real world application', 'reproductive', 'response', 'seasonal influenza', 'sound', 'statistical and machine learning', 'surveillance data', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2020,78507,-0.007357253321942546
"Application of advanced methodology to osteoarthritis phenotyping Osteoarthritis (OA) is highly prevalent, contributes to substantial morbidity in the population, and lacks effective interventions to prevent onset and progression. Importantly, and like many other chronic conditions, OA is not a single disease but rather a heterogeneous condition consisting of multiple subgroups, or phenotypes, with differing underlying pathophysiological mechanisms. It is becoming increasingly clear that consideration of specific OA phenotypes in clinical studies and trials is critically needed to move the field forward. The overall goal of this line of work is to identify and understand potential phenotypes of knee osteoarthritis (KOA) to better inform future research efforts and treatments; this exploratory R21 project using OA Initiative (OAI) data will investigate novel methodology to support phenotyping in KOA. Successful treatments for OA will need to be targeted to, and tested in, specifically chosen OA phenotypes. Our hypothesis is that an understanding of KOA phenotypes, a key step toward Precision Medicine in OA, will lead to more successful clinical studies in the long-term. To approach this important clinical problem, we propose a project in which we will apply innovative machine learning methods and validation strategies to data from the large, publicly available OAI cohort. We will leverage this large dataset, along with local expertise in statistics, biostatistics and machine learning methodology, to tackle the problem of phenotyping this heterogeneous disease. In Aim 1, we will utilize a data-driven, unsupervised learning approach, to cluster features that best define and discriminate among phenotypes of KOA in the OAI dataset, using biclustering and a novel significance test (SigClust) developed by co-I Marron. For Aim 2, we will test specific hypotheses of relevance to OA outcomes, such as differences between those with and without OA, or those who do or do not develop new or worsening disease, using another set of machine learning methods (Direction-projection-permutation [DiProPerm] hypothesis testing, and Distance-Weighted Discrimination [DWD]), also developed by co-I Marron, in the full cohort and in any identified clusters from Aim 1. In order to address these aims, this proposal involves interdisciplinary collaborations among experts in statistics, biostatistics, computer science, rheumatology, and epidemiology. This work will significantly impact the field by fulfilling a critical need to accurately define OA phenotypes, discover the key features associated with these phenotypes, link phenotype subgroups to underlying mechanisms and use this information to inform and focus future clinical studies. In the long term, we expect that this strategy will lead to more personalized and successful management of the millions of people affected by OA. Project narrative Osteoarthritis is an enormous and increasing public health problem, and like many other chronic conditions it is not a single disease but a heterogeneous condition consisting of multiple subgroups, or phenotypes, with differing underlying mechanisms. The lack of appreciation of this heterogeneity has contributed to the failure of all attempts to date to develop disease-modifying osteoarthritis drugs; future trials will need to target specific OA phenotypes. There is a critical need to define and understand phenotypes in OA and link these to outcomes, leading to more personalized and successful management of this common and debilitating disease.",Application of advanced methodology to osteoarthritis phenotyping,9889390,R21AR074685,"['Address', 'Affect', 'Age-Years', 'Arthritis', 'Biomechanics', 'Biometry', 'Cartilage', 'Chronic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Data', 'Data Set', 'Degenerative polyarthritis', 'Discrimination', 'Disease', 'Epidemiology', 'Etiology', 'Failure', 'Fibrinogen', 'Future', 'General Population', 'Goals', 'Heterogeneity', 'Individual', 'Inflammation', 'Injury', 'Intervention', 'Joints', 'Knee Injuries', 'Knee Osteoarthritis', 'Link', 'Machine Learning', 'Meniscus structure of joint', 'Methodology', 'Morbidity - disease rate', 'Non obese', 'Obesity', 'Outcome', 'Pain', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Progressive Disease', 'Public Health', 'Randomized', 'Research Methodology', 'Resources', 'Rheumatology', 'Risk Factors', 'Structure', 'Subgroup', 'Symptoms', 'Syndrome', 'Synovial Membrane', 'Techniques', 'Testing', 'Time', 'Tissues', 'Validation', 'Visit', 'Work', 'base', 'bone', 'cohort', 'common treatment', 'computer science', 'demographics', 'design', 'disability', 'drug development', 'effective intervention', 'experience', 'improved', 'injured', 'innovation', 'interdisciplinary collaboration', 'joint destruction', 'large datasets', 'loss of function', 'machine learning method', 'novel', 'precision medicine', 'prevent', 'statistics', 'unsupervised learning']",NIAMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2020,200007,0.0065162742238412
"Bioinformatics for post-traumatic stress Project Summary/Abstract Maladaptive complications following trauma, including post-traumatic stress (PTS), are highly prevalent in both veterans and civilians, and have been difficult to accurately diagnose, manage and treat. Debate regarding diagnostic criteria and the need to represent the full spectrum of inter-connected features contributing to psychopathology has spawned the development of the Research Domain Criteria (RDoC) by the National Institute of Mental Health (NIMH). RDoC is a developing framework to help guide the discovery and validation of new dimensions of mental health disorders and their relationships to underlying biological mechanisms. NIMH now has a rich federated database that currently houses raw data from RDoC-sponsored clinical research, and clinical trial data from the National Database of Clinical Trials (NDCT) with information that may help to unlock the complex and overlapping relationships between symptoms of PTS and the underlying biomarkers to fuel improvements on diagnostic and therapeutic frameworks for trauma recovery. The proposed project will apply bioinformatics and machine learning analytical tools to these large, heterogeneous datasets to identify and validate new research dimensions of trauma-related psychopathology and treatment response trajectories and their predictors. Aim 1 will develop an in silico trauma patient population by integrating data from diverse sources, including cross-sectional and observational longitudinal clinical studies housed within available data repositories for trauma and other related mental health research. Data will include medical history, demographics, diagnostic tests, clinical outcomes, psychological assessments, genomics, imaging, and other relevant study and meta-data. Aim 2 will identify multiple dimensions of PTS diagnostic criteria, using a combination of unsupervised dimension-reduction statistical methods, internal and external cross-validation, and supervised hypothesis testing of predictive models to understand the heterogeneous subtypes of PTS. Aim 3 will deploy unsupervised machine learning methods, such as topological data analysis and hierarchical clustering, to identify unique clusters of patients based on symptomatology to develop clustering methods for precision mapping of PTS patients based on disease severity. Aim 4 will use supervised machine learning techniques for targeted predictive analytics focused on identifying treatment responders from the NDCT, and identification of latent variables that predict treatment response. The results of the proposed research project will greatly enrich the field of computational psychiatry research to identify conserved dimensions associated with the complex relationships of psychopathology and precision treatment planning following exposure to traumatic events. Project Narrative A recent restructuring of diagnostic and research criteria for psychiatric disorders has been implemented to promote greater understanding of the biological mechanisms involved in the development of complex mental health disorders. The proposed project aims to apply bioinformatics and machine learning analytics to large datasets from trauma-exposed patients to identify and validate dimensions of post-traumatic stress (PTS), relevant biological predictors, and precision treatment response trajectories.",Bioinformatics for post-traumatic stress,9952129,R01MH116156,"['Bioinformatics', 'Biological', 'Biological Markers', 'Categories', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Database', 'Complex', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Diagnostics Research', 'Dimensions', 'Disease', 'Exposure to', 'Genomics', 'Growth', 'Image', 'Laboratories', 'Linear Models', 'Linear Regressions', 'Logistics', 'Machine Learning', 'Maps', 'Measures', 'Medical History', 'Mental Health', 'Mental disorders', 'Metadata', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nervous System Trauma', 'Neurocognitive', 'Observational Study', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Principal Component Analysis', 'Psychiatry', 'Psychopathology', 'Recovery', 'Reproducibility', 'Research', 'Research Domain Criteria', 'Research Project Grants', 'Severity of illness', 'Source', 'Statistical Methods', 'Supervision', 'Symptoms', 'Syndrome', 'Techniques', 'Testing', 'Therapeutic', 'Trauma', 'Trauma Research', 'Trauma patient', 'Trauma recovery', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Work', 'accurate diagnosis', 'analytical tool', 'base', 'biobehavior', 'combat', 'computational platform', 'data archive', 'data mining', 'data sharing', 'data warehouse', 'demographics', 'diverse data', 'feature selection', 'federated computing', 'guided inquiry', 'hands-on learning', 'heterogenous data', 'in silico', 'indexing', 'innovation', 'insight', 'interest', 'large datasets', 'machine learning method', 'multidimensional data', 'multimodality', 'patient population', 'patient subsets', 'post-traumatic stress', 'post-traumatic symptoms', 'precision medicine', 'predictive modeling', 'predictive test', 'psychologic', 'research and development', 'research study', 'response', 'statistics', 'stress related disorder', 'supervised learning', 'symptomatology', 'tool', 'trauma exposure', 'traumatic event', 'treatment planning', 'treatment responders', 'treatment response', 'unsupervised learning', 'vector']",NIMH,UNIVERSITY OF MINNESOTA,R01,2020,501996,0.007822155860175834
"Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease PROJECT SUMMARY/ABSTRACT Chronic obstructive pulmonary disease (COPD) is the leading cause of respiratory mortality in the United States. COPD is a highly heterogeneous disease and some COPD therapies are only applied to specific clinically defined subtypes. With the advent of multiple high-throughput biological assays and machine learning approaches, data-driven subtypes are increasingly being recognized. We hypothesize that such subtypes exist in COPD and that they can be identified using an integrative, multi-'omic approach. To accomplish this goal, we first propose to complement existing RNA and whole genome sequencing data in the well-phenotyped COPDGene study with peripheral blood microRNA sequencing. We will study the relationship of microRNA to genetic variation and gene expression in COPD. Next, we will apply a patient-based network similarity method to these three data types to identify COPD molecular subtypes. Finally, we will associate these subtypes with important clinical phenotypes and outcomes, and validate these subtypes in an independent subset of subjects. Our analysis targets a key clinical problem in COPD management, and will allow the mentee to become an independent investigator, applying bioinformatic and machine learning methods to genomic data in respiratory diseases. PROJECT NARRATIVE Chronic obstructive pulmonary disease (COPD) is a leading cause of death in the United States. Patients with COPD may have very similar lung function but differ in many other characteristics. We propose to use multiple types of biologic data to identify different COPD subtypes, which may be important for disease prognosis and treatment.",Multi-omic Subtyping of Chronic Obstructive Pulmonary Disease,9975882,K08HL136928,"['Affect', 'Bioinformatics', 'Biological', 'Biological Assay', 'Blood', 'Cause of Death', 'Characteristics', 'Chronic', 'Chronic Obstructive Airway Disease', 'Clinical', 'Complement', 'Complex', 'Data', 'Diagnostic radiologic examination', 'Disease', 'Disease Management', 'Expert Opinion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Diseases', 'Genetic Risk', 'Genetic Variation', 'Genomics', 'Goals', 'Impairment', 'Individual', 'Lung', 'Lung diseases', 'Lung volume reduction surgery', 'Machine Learning', 'Measures', 'Methods', 'MicroRNAs', 'Molecular', 'Molecular Profiling', 'Multiomic Data', 'Muscular Atrophy', 'Network-based', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Investigator', 'Pulmonary Emphysema', 'RNA', 'Research Personnel', 'Respiratory physiology', 'SNP array', 'Severities', 'Spirometry', 'Testing', 'Training', 'Trans-Omics for Precision Medicine', 'United States', 'base', 'clinical phenotype', 'clinically relevant', 'cohort', 'disorder subtype', 'exome', 'genome sequencing', 'genome wide association study', 'genomic data', 'machine learning method', 'miRNA expression profiling', 'molecular subtypes', 'mortality', 'multiple omics', 'next generation sequencing', 'novel', 'outcome forecast', 'patient subsets', 'peripheral blood', 'personalized approach', 'programs', 'quantitative imaging', 'respiratory', 'risk variant', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,K08,2020,172800,-0.0034766174080266355
"The Effects of Insurance Benefit Design Innovation on Patient Health Abstract My research in health economics has focused on how information and targeted consumer cost-sharing influences how patients choose providers and the financial savings of incentivizing patients to choose low-price providers. I have also examined the opposite side of the market, how patient use of information and targeted consumer incentives spurs provider price competition. These topics provided the framework for my research as a PhD student in Health Economics at the University of California, Berkeley and I continue to build on these topics while a policy researcher at the RAND Corporation. A natural next step for my career is to expand this line of research but in a more in-depth manner and using more advanced statistical methods. Performing mentored research in these areas will help me successfully make the transition from directed to independent research. The proposed study will help me to (1) contribute to a deeper understanding of patient health effects of an innovative insurance benefit design that is particularly relevant for the aging population; (2) continue to build capabilities working with large medical claims data sets and develop expertise in innovative statistical methods from different disciplines; (3) gain training in aging-related health-services research; (4) expand my exposure to the aging, health economics, and health services research communities; and (5) develop my abilities as an independent health services researcher and build the foundations to successfully compete for R01-level grants.  In this project, I propose to examine whether reference pricing for colonoscopies and pharmaceuticals decreases adherence to recommended colorectal cancer screening and medication therapies among the near- elderly population. I will also examine the impact of reference pricing on patient health outcomes and the aging process. To do so, I intend to apply novel machine-learning statistical methods that have been recently developed in the computer science and statistics fields. As part of this proposal, I have built a formal training plan to develop expertise in these methods. This project will provide me with the flexibility and support to develop a long-term research agenda that focuses on using innovative statistical methods to evaluate the comprehensive effects of consumer cost- sharing programs. Although this study focuses on a single cost- sharing program, reference pricing, the skills I gain through this award will allow me to independently lead evaluations of future benefit designs. The application of machine-learning methods to the setting of reference pricing will provide a framework that I or other researchers can use to evaluate other insurance benefit designs or alternative patient populations. ! Narrative An increasingly popular insurance benefit design, reference pricing, provides targeted financial incentives for consumers to receive care at low-cost providers. While the financial savings from reference pricing programs are well-known, the health impacts have yet to be studied. The proposed career grant will apply machine learning techniques to develop a long-term research agenda focused on understanding the patient health effects of reference pricing for colonoscopies and medication therapies, which are services that are especially relevant for the aging population. !",The Effects of Insurance Benefit Design Innovation on Patient Health,9891936,K01AG061274,"['Accident and Emergency department', 'Adherence', 'Admission activity', 'Adult', 'Advisory Committees', 'Age', 'Aging', 'Area', 'Award', 'Behavior', 'Big Data', 'California', 'Caring', 'Chronic', 'Chronic Disease', 'Colonoscopy', 'Communities', 'Cost Sharing', 'Data Set', 'Deductibles', 'Development', 'Diabetes Mellitus', 'Discipline', 'Elderly', 'Employee', 'Evaluation', 'Exposure to', 'Foundations', 'Future', 'Grant', 'Health', 'Health Benefit', 'Health Care Costs', 'Health Services', 'Health Services Research', 'Healthcare', 'Heart Diseases', 'Heart Rate', 'Heterogeneity', 'Hospitals', 'Incentives', 'Individual', 'Inpatients', 'Insurance', 'Insurance Benefits', 'Insurance Carriers', 'Internal Medicine', 'Journals', 'Lead', 'Link', 'Machine Learning', 'Medical', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'New England', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Policies', 'Population', 'Preventive service', 'Price', 'Process', 'Provider', 'Publications', 'Publishing', 'Quality of life', 'Research', 'Research Methodology', 'Research Personnel', 'Retirement', 'Savings', 'Screening for cancer', 'Services', 'Side', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Training', 'Universities', 'Work', 'aging population', 'asthmatic patient', 'career', 'colorectal cancer screening', 'comorbidity', 'compliance behavior', 'computer science', 'cost', 'design', 'doctoral student', 'financial incentive', 'flexibility', 'health data', 'health economics', 'health plan', 'improved', 'innovation', 'machine learning method', 'mortality', 'novel', 'patient population', 'programs', 'response', 'semiparametric', 'skills', 'statistical and machine learning', 'statistics', 'treatment effect']",NIA,RAND CORPORATION,K01,2020,130228,-0.0019945526997384457
"Multi-Study Integer Programming Methods for Human Voltammery Project Summary/Abstract  The development of treatments for addiction requires the characterization of neural mechanisms underlying reward. Studying reward in humans requires assays that can detect changes in neurotransmitter levels with high chemical specificity. Recently, fast-scan cyclic voltammetry (FSCV) has been implemented in humans to measure dopamine with high temporal and spatial resolution. This technological achievement was enabled in large part through the novel application of machine learning methods. FSCV relies on statistical tools since FSCV records an electrochemical response which must be converted into concentration estimates via a statistical model. The validity of the scientific conclusions from human FSCV studies therefore depends heavily on the reliability of these statistical models to generate accurate dopamine concentration estimates.  In human FSCV, models are fit on in vitro training sets as making in vivo training sets in humans is infeasible. Producing accurate estimates thus requires that models trained on in vitro training sets generalize to in vivo brain recordings. Combining data from multiple training sets is the standard approach human FSCV researchers have employed to improve model generalizability. This proposal extends work that shows that multi-study machine learning methods improve dopamine concentration estimates by combining training sets from different electrodes such that the resulting average signal (“cyclic voltammogram” or CV) is similar to the average CV of the electrode used in the brain. However, this approach relies on random resampling. This is problematic because the randomness limits the extent to which estimate accuracy can be improved and the slow speed of the resampling approach precludes the generation of estimates during data collection, which is critical to experiment success.  This proposal details the development of methods that leverage mixed integer programming to optimally generate training sets that combine data from multiple electrodes. By generating training sets that are specifically tailored to the electrode used for brain measurements, one can vastly improve dopamine concentration estimate accuracy. The speed of the integer programming methods will enable the use of this approach during data collection. This work will include validation of the methods on in vitro data as well as on data from published in vivo and slice experiments in rodents. By applying methods to published optogenetic experiments, one can compare estimates from the proposed methods and from standard methods. The asymptotic properties of the proposed methods will be characterized analytically assuming a linear mixed effects model and empirically through application of the methods to data simulated under this model.  This work will be conducted at the highly collaborative and innovative Harvard School of Public Health. The fellowship will support growth in statistical, computing and collaborative skills, and prepare the trainee for a productive career as a biostatistics professor who develops methods for neuroscience and addiction research. Project Narrative  Fast-scan cyclic voltammetry in humans offers an invaluable tool to study the neural mechanisms underlying reward by allowing for sub-second detection of dopamine during cognitive-behavioral tasks. However, conducting voltammetry in humans presents distinct statistical challenges that must be overcome to ensure optimal dopamine concentration estimates. We propose novel statistical methods that use mixed integer optimization and extend preliminary work that shows multi-study machine learning methods substantially improve dopamine concentration estimate accuracy.",Multi-Study Integer Programming Methods for Human Voltammery,10067624,F31DA052153,"['Achievement', 'Address', 'Algorithms', 'Behavioral', 'Biological Assay', 'Biometry', 'Brain', 'Cells', 'Chemicals', 'Cognitive', 'Complex Mixtures', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Dopamine', 'Electrodes', 'Ensure', 'Fellowship', 'Generations', 'Goals', 'Grant', 'Growth', 'Human', 'In Vitro', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Neurosciences', 'Neurotransmitters', 'Nucleus Accumbens', 'Performance', 'Periodicity', 'Property', 'Public Health Schools', 'Publications', 'Publishing', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Rewards', 'Rodent', 'Scanning', 'Scheme', 'Signal Transduction', 'Slice', 'Specificity', 'Speed', 'Statistical Computing', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Training', 'Validation', 'Work', 'addiction', 'algorithm training', 'career', 'effective therapy', 'experimental study', 'improved', 'in vivo', 'innovation', 'insight', 'machine learning method', 'method development', 'multiple data sources', 'neuromechanism', 'novel', 'optogenetics', 'predictive modeling', 'professor', 'relating to nervous system', 'response', 'skills', 'success', 'therapy development', 'tool']",NIDA,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2020,37235,0.01664668479482147
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9981804,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Models', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in silico', 'in vivo', 'insight', 'intracranial artery', 'microSPECT', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'multiple omics', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'spatiotemporal', 'supervised learning', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2020,601275,0.008390115177288274
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9900790,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'feature selection', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'machine learning method', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistical and machine learning', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2020,293003,0.017837106152089062
"Multi-level statistical classification of substance use disorder ABSTRACT This application represents our ongoing commitment to developing an innovative and interdisciplinary research program on the classification of substance use disorders (SUDs). This research is achieved through quantitative analysis of multidimensional data that combine clinical symptoms and diagnoses, imaging markers, and genotypes. The team has a PI with expertise in computational science and the development and implementation of innovative statistical algorithms to understand multidimensional data; a PI with extensive experience in systems, imaging and addiction neuroscience; and a co-I who has expertise in the genetics of SUDs. Our previous R01 project employed a sample of ~12,000 individuals aggregated from multiple genetic studies of alcohol and drug dependence to generate SUD subtypes based on clinical symptoms. Because clinical manifestations are distal endpoints in the biological pathway, the genetic effects identified are often weak and inconsistent, and consequently difficult to detect even in large samples. As championed by the NIMH Research Domain Criteria (RDoC) research, the etiologies of psychiatric disorders, including SUDs, can be fruitfully characterized by dimensional neural features. This project thus extends our ongoing work to include imaging neural features in the classification of SUDs. Specifically, we will utilize a large database from the UK Biobank Project that provides both genetic and multi-modality magnetic resonance imaging (MRI) data. Building on our work with the US Human Connectome Project, we aim in the current project to integrate clinical, imaging, and genotype data to investigate the neurobiological substrates of SUD diagnostic labels, and to derive SUD subtypes that are optimized for gene finding. Methodologically, we replace the classic statistical analysis that is confirmatory and biased to an a priori hypothesis by an approach that emphasizes pattern discoveries from big data. Our specific aims are to: (I): identify neuroimaging features that represent robust markers of addiction and differentiate SUD subtypes that can be confirmed by multi-modality evidence; (II) employ a novel brain connectivity model, on the basis of graph convolutional neural networks, to identify neural markers that precisely characterize the differences in structural changes and functional circuits related to SUDs; and (III) derive an innovative machine learning model to identify highly heritable neurobiological subtypes of SUDs that facilitate investigation of the genetic basis of addiction. We will focus on alcohol and nicotine use disorders to demonstrate the conceptual and methodological approaches. We believe that, by providing a productive conceptual and methodological platform to integrate imaging and genetic data to understand the etiologies of SUDs, this research is highly responsive to the RFA “Leveraging Big Data Science to Elucidate the Neural Mechanisms of Addiction and SUD.” The machine learning tools developed for this project will provide an innovative and reliable foundation to enhance the aggregation and analysis of multidimensional data, and to meet the diagnostic and predictive challenges in mental health research. PROJECT NARRATIVE We will develop novel statistical and quantitative tools to identify and differentiate subtypes of alcohol and nicotine use disorders by combining neural markers, genotype data, and clinical diagnostic information. The proposed analytics will be tested in a large database of alcohol and nicotine use disorders from the UK Biobank project. The proposed approaches will advance precision medicine by refining disease classification and facilitating gene finding for addictive and other mental disorders.",Multi-level statistical classification of substance use disorder,10056455,R01DA051922,"['Alcohol consumption', 'Alcohol dependence', 'Behavioral', 'Big Data', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Brain region', 'Classification', 'Clinical', 'Clinical Data', 'Cluster Analysis', 'Collaborations', 'Computational Science', 'Computer software', 'Data', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Distal', 'Drug Addiction', 'Emotional', 'Emotions', 'Etiology', 'Exhibits', 'Foundations', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Graph', 'Heritability', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Interdisciplinary Study', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Label', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'National Institute of Mental Health', 'Neurobiology', 'Neurosciences', 'Nicotine Use Disorder', 'Pathway interactions', 'Pattern', 'Phenotype', 'Process', 'Research', 'Research Domain Criteria', 'Rewards', 'Sampling', 'Single Nucleotide Polymorphism', 'Statistical Algorithm', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Substance Use Disorder', 'Symptoms', 'System', 'Testing', 'Variant', 'Work', 'addiction', 'base', 'big-data science', 'biobank', 'clinical diagnostics', 'cocaine use', 'cognitive neuroscience', 'connectome', 'convolutional neural network', 'data structure', 'disease classification', 'disorder subtype', 'endophenotype', 'executive function', 'experience', 'falls', 'genetic analysis', 'genetic variant', 'genome wide association study', 'genome-wide', 'gray matter', 'imaging biomarker', 'imaging genetics', 'imaging modality', 'individual variation', 'innovation', 'multidimensional data', 'multimodal data', 'multimodality', 'network models', 'neural correlate', 'neurogenetics', 'neuroimaging', 'neuromechanism', 'neuropsychiatric disorder', 'novel', 'precision medicine', 'programs', 'relating to nervous system', 'response', 'risk variant', 'statistical and machine learning', 'structured data', 'tool', 'trait', 'treatment response', 'whole genome']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,R01,2020,465370,-0.07032028552223213
"Leveraging Heterogeneity in Preclinical Traumatic Brain Injury to Drive Discovery and Reproducibility Traumatic brain injury (TBI) is a leading cause of neurological disorders and affects over 2.5 million people each year, yet no treatment has successfully translated from bench to clinic. TBI is a broad term and encompasses an extremely heterogeneous set of injuries differing by cause, severity, biomechanics, and the varied, complex secondary injury responses that collectively result in chronic disabilities. Current preclinical research circumvents the issue of TBI heterogeneity by relying on specific preclinical animal models that mimic subpopulations of patients and particular secondary injury mechanisms with each study focusing on limited, individual pathways. This proposal instead aims to tackle TBI heterogeneity by approaching TBI as a “big data” problem and aggregating and analyzing the multidimensional data collectively. A framework for data harmonization and curation will be developed, and datasets from a consortium of preclinical labs employing a variety of preclinical TBI models will be collected and curated into an open data commons (ODC-TBI). Utilizing machine learning and multidimensional analytics, the proposed research will directly leverage TBI heterogeneity in the merged dataset to identify persistent features of TBI to empower translational research. By creating a preclinical TBI ODC and applying machine learning to integrate the heterogeneity of preclinical TBI models, the project will reveal multidimensional features of TBI across heterogeneous injuries and characterize how diverse secondary injury mechanisms interact and ultimately affect injury outcome. Throughout the project's timeline, new datasets will continue to be harmonized into the ODC-TBI according to the established framework. The ODC-TBI will be the first open multicenter, multi-model repository of preclinical TBI data and will enable the application of data science to the field of TBI. Furthermore, the ODC-TBI and the methods implemented throughout the project will be openly shared to improve reproducibility of TBI research. Together with the multidimensional analysis that will provide quantitative and qualitative understanding of TBI heterogeneity, the project aims to ultimately accelerate data- driven discovery and precision medicine for TBI. Reflecting the complexities of clinical traumatic brain injury (TBI), preclinical TBI research is confounded by the extreme heterogeneity prevalent across possible injury models and resulting biological responses. The proposed research will aggregate and curate an extensive open data commons (ODC) of preclinical TBI research with multiple TBI models and utilize machine learning to tackle TBI heterogeneity directly. The project will create an ODC for preclinical TBI research to improve data sharing and scientific reproducibility, and will empower translational TBI research by identifying multidimensional features of TBI that best predict functional outcome.",Leveraging Heterogeneity in Preclinical Traumatic Brain Injury to Drive Discovery and Reproducibility,10042756,F32NS117728,"['Address', 'Affect', 'Animal Model', 'Big Data', 'Biological', 'Biological Markers', 'Biomechanics', 'Brain region', 'Chronic', 'Clinic', 'Clinical', 'Closed head injuries', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Element', 'Data Science', 'Data Set', 'Development', 'Foundations', 'Goals', 'Heterogeneity', 'Incidence', 'Individual', 'Inflammation', 'Informatics', 'Injury', 'Institutes', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Multivariate Analysis', 'National Institute of Neurological Disorders and Stroke', 'Outcome', 'Pathway interactions', 'Pattern', 'Pharmacologic Substance', 'Population', 'Positioning Attribute', 'Pre-Clinical Model', 'Principal Component Analysis', 'Publishing', 'Reproducibility', 'Research', 'Severities', 'Standardization', 'Synaptic plasticity', 'Therapeutic', 'TimeLine', 'Translating', 'Translational Research', 'Translations', 'Traumatic Brain Injury', 'behavioral outcome', 'bench to bedside', 'biomarker discovery', 'controlled cortical impact', 'data curation', 'data framework', 'data harmonization', 'data sharing', 'disability', 'experimental study', 'functional outcomes', 'genetic manipulation', 'improved', 'insight', 'multidimensional data', 'multiple datasets', 'nerve injury', 'nervous system disorder', 'neuroinflammation', 'open data', 'patient subsets', 'pre-clinical', 'pre-clinical research', 'precision medicine', 'repository', 'response', 'response to injury']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2020,69810,-0.010233716803352598
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,10002249,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2020,593966,-0.017342924907988427
"Designing neutralization antibodies against Sars-Cov-2 Project Summary COVID-19 has become a worldwide pandemic whose rapid spread and mortality rate threatens millions of lives and the global economic system. Developing effective treatment such as neutralization antibodies is an urgent need. We propose here to develop a new method to design antibodies strongly bind to the SARS-CoV-2 receptor binding domain (RBD) that is necessary for viral entrance to human cells. We will develop a novel approach that combines directed evolution, deep sequencing and interpretable neural network models to efficiently identify strong and specific antibodies. This method will allow analyzing large sequencing data sets of antibody variants against the SARS-CoV-2 RBD in order to derive superior binders that do not exist in the original library. Iteration through directed evolution and computational design will efficiently identify neutralization antibody candidates that can be used as potent therapeutics to treat COVID-19. Narrative: Developing neutralization antibodies is critical to provide effective treatment for Covid-19.",Designing neutralization antibodies against Sars-Cov-2,10173204,R21AI158114,"['2019-nCoV', 'Affinity', 'Amino Acids', 'Antibodies', 'Binding', 'COVID-19', 'Cells', 'Cessation of life', 'Clinical Trials', 'Consumption', 'Data', 'Data Set', 'Development', 'Directed Molecular Evolution', 'Economics', 'Epitopes', 'Future', 'Gene Library', 'Histones', 'Human', 'Human Engineering', 'Immunoglobulin G', 'Lead', 'Libraries', 'Machine Learning', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Mutate', 'Mutation', 'Nature', 'Network-based', 'Neural Network Simulation', 'Peptides', 'Positioning Attribute', 'Process', 'Reporting', 'Resistance', 'Screening procedure', 'Solubility', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Variant', 'Viral', 'Virus', 'Virus Diseases', 'base', 'clinical efficacy', 'data archive', 'deep learning', 'deep sequencing', 'design', 'drug candidate', 'effective therapy', 'machine learning method', 'mortality', 'mutant', 'neural network', 'neutralizing antibody', 'novel strategies', 'pandemic disease', 'receptor binding', 'screening', 'trend']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2020,433750,-0.010248027529415484
"Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis Neuropsychiatric disorders are characterized by highly heterogeneous and frequently overlapping clinical phenotypes. Understanding the neurobiological underpinnings of these clinical symptoms has been a central goal in neuropsychiatric research and has been largely facilitated by MRI and associated analytical methods that have found reproducible neuroanatomical abnormalities. However, the neuroanatomical heterogeneity in these disorders is also high. Therefore, attempting to find a unique neuroanatomical signature of a complex neuropsychiatric disorder using commonly used current techniques is hampered by such heterogeneity. Personalized disease treatment calls for fine quantification of heterogeneity and for more precise placement of each individual patient into a multi-dimensional spectrum of neuroanatomical alterations found in neuropsychiatric disorders. In the proposed project we focus on the neuroanatomy of psychosis. To this end, we leverage a unique set of pooled cohorts from 10 sites, including (1) adults with chronic schizophrenia-spectrum (non-affective) psychotic disorders (n=749), (2) individuals with first-episode (FE) psychosis (n=665), and matched healthy controls (N=1,483). This large cohort will allow us to test our first hypothesis, namely that neuroanatomical phenotypes of these patients will display high heterogeneity, which will allow us to define neuroanatomical dimensions of pathology. Our second hypothesis is that this heterogeneity will relate to clinical phenotypes in chronic schizophrenia spectrum patients, as well as to longitudinal outcome in FE psychosis. We leverage newly developed pattern analysis and semi-supervised machine learning techniques designed to quantify heterogeneity of complex patterns of neuroanatomical abnormalities. Our goal is to arrive at a new “NeuroAnatomical Coordinate system of PSychosis”(NAC-PS), with each dimension reflecting a different neuroanatomical pattern of brain alterations in this spectrum, which will allow us to measure patient positions and trajectories in this spectrum, as they evolve across time and treatment. We propose to: Aim1: Develop inter-site harmonization methods for imaging data, and hence establish a methodological platform for constructive integration of structural imaging data from multiple sites. Using these methods, we will generate a resource of 2,897 datasets with advanced neuroanatomical measurements; Aim 2: investigate the heterogeneity of anatomical patterns related to psychosis at the population level, using novel group analysis methods which model the neuroanatomical phenotype of disease as a collection of directions of deviation from normal anatomy. This will define a spectrum of neuroanatomical patterns of psychosis, rather than seeking a single dominant pattern; Aim 3: Develop MRI- based classification, subtyping, and outcome prediction on an individual patient basis, under this heterogeneity; Aim 4: Relate baseline neuroanatomical patterns to longitudinal clinical outcome in FE patients, and build individualized prognostic predictors. Additional/ancillary site-specific projects that link detailed, site-specific clinical data to NAC-PS axes will be further facilitated in the future by our foundational project. Project narrative This proposal aims to use advanced pattern analysis and machine learning methods to structural MRI data, in order to elucidate patterns of neuroanatomical change in psychosis, and use those to derive diagnostic and predictive indices on an individual patient basis. Data from over 3,000 individuals across 3 continents will be pooled together and harmonized, thereby allowing us to analyze the heterogeneity of neuroanatomy of psychosis, to relate it to clinical measures, and to construct predictors of clinical outcome in first episode patients.",Mapping Heterogeneity of Neuroanatomical Imaging Signatures of Psychosis via Pattern Analysis,9942277,R01MH112070,"['Address', 'Adult', 'Affective', 'Anatomy', 'Brain', 'Brain imaging', 'Chronic', 'Chronic Schizophrenia', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collection', 'Complex', 'Data', 'Data Set', 'Diagnostic', 'Dimensions', 'Disease', 'Exposure to', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Libraries', 'Link', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neuroanatomy', 'Neurobiology', 'Outcome', 'Pathology', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Positioning Attribute', 'Psychotic Disorders', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Sampling', 'Site', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'analytical method', 'base', 'clinical phenotype', 'cohort', 'data harmonization', 'data sharing', 'design', 'disease phenotype', 'first episode psychosis', 'follow-up', 'imaging modality', 'indexing', 'individual patient', 'interest', 'machine learning method', 'morphometry', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'outcome prediction', 'patient population', 'patient stratification', 'patient subsets', 'personalized medicine', 'predict clinical outcome', 'prognostic', 'supervised learning', 'treatment effect']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,658144,0.001391543287281894
"Next Generation Testing Strategies for Assessment of Genotoxicity Project Summary  It is well recognized that current batteries of genetic toxicology assays exhibit two critical deficiencies. First, the throughput capacity of in vitro mammalian cell genotoxicity tests is low, and does not meet current needs. Second, conventional assays provide simplistic binary calls, genotoxic or non-genotoxic. In this scheme there is little or no consideration for potency, and virtually no information is provided about molecular targets and mechanisms. These deficiencies in hazard characterization prevent genotoxicity data from optimally contributing to modern risk assessments, where this information is essential. We will address these major problems with current in vitro mammalian cell genetic toxicity assays by developing methods and associated commercial assay kits that dramatically enhance throughput capacity, and delineate genotoxicants' primary molecular targets, while simultaneously providing information about potency. Once biomarkers and a family of multiplexed assays have been developed for these purposes, an interlaboratory trial will be performed with prototype assay kits to assess the transferability of the methods. Project Narrative  DNA damage that cannot be faithfully repaired results in gene mutation and/or chromosomal aberrations, and these effects are known to contribute to cancer and other severe diseases. Thus, there is an important need for sensitive assays to evaluate chemicals for genotoxic and other deleterious effects. The work proposed herein will address issues that have plagued genotoxicity assessments for the last several decades: low throughput, lack of potency metrics, and little to no information about molecular targets. We will address these major problems with current genetic toxicity assays by developing new methods and associated commercial assay kits.",Next Generation Testing Strategies for Assessment of Genotoxicity,9838229,R44ES029014,"['Address', 'Affect', 'Aneugens', 'Antioxidants', 'Appearance', 'Benchmarking', 'Biological Assay', 'Biological Markers', 'Biological Response Modifiers', 'Bleomycin', 'Caspase', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Chemicals', 'Chromosome abnormality', 'Chromosomes', 'Classification', 'Cleaved cell', 'Colcemid', 'Companions', 'Complex', 'Computer Models', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'DNA Repair', 'DNA-PKcs', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Dose', 'Epitopes', 'Etoposide', 'Exhibits', 'Family', 'GADD45A gene', 'Gamma-H2AX', 'Gene Mutation', 'Genetic', 'Goals', 'Harvest', 'Histone H3', 'Human', 'In Vitro', 'Intercalating Agents', 'Investigation', 'Kinetics', 'Label', 'Laboratories', 'Logistic Regressions', 'Machine Learning', 'Malignant Neoplasms', 'Mammalian Cell', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Modification', 'Molecular Target', 'Mutagenicity Tests', 'NF-kappa B', 'Nuclear', 'Pathway interactions', 'Phase', 'Physiologic pulse', 'Procedures', 'Protocols documentation', 'Reagent', 'Reference Values', 'Risk Assessment', 'Schedule', 'Scheme', 'Series', 'Stains', 'TP53 gene', 'Testing', 'Time', 'Toxic effect', 'Toxicogenetics', 'Training', 'Validation', 'Work', 'aurora kinase', 'base', 'clastogen', 'computerized tools', 'design', 'experimental study', 'genotoxicity', 'hazard', 'inhibitor/antagonist', 'next generation', 'prediction algorithm', 'prevent', 'prototype', 'random forest', 'repaired', 'response', 'targeted agent', 'tool', 'treatment optimization', 'virtual']",NIEHS,"LITRON LABORATORIES, LTD.",R44,2020,474671,-0.011311930286514673
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9990809,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'statistical and machine learning', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2020,332952,-0.007884935431126188
"Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease ABSTRACT Rapid progress in biomedical informatics has generated massive high-dimensional data sets (“big data”), ranging from clinical information and medical imaging to genomic sequence data. The scale and complexity of these data sets hold great promise, yet present substantial challenges. To fully exploit the potential informativeness of big data, there is an urgent need to find effective ways to integrate diverse data from different levels of informatics technologies. Existing approaches and methods for data integration to date have several important limitations. In this project, we propose novel statistical methods and strategies to integrate neuroimaging, multi-omics, and clinical/behavioral data sets. To increase power for association analysis compared to existing methods, we propose a novel multi-phenotype multi-variant association method that can evaluate the cumulative effect of common and rare variants in genes or regions of interest, incorporate prior biological knowledge on the multiple phenotype structure, identify associated phenotypes among multiple phenotypes, and be computationally efficient for high-dimensional phenotypes. To improve the prediction of clinical outcomes, we propose a novel machine learning strategy that can integrate multimodal neuroimaging and multi-omics data into a mathematical model and can incorporate prior biological knowledge to identify genomic interactions associated with clinical outcomes. The ongoing Alzheimer's Disease Neuroimaging Initiative (ADNI) and Indiana Memory and Aging Study (IMAS) projects as a test bed provide a unique opportunity to evaluate/validate the proposed methods. Specific Aims: Aim 1: to develop powerful statistical methods for multivariate tests of associations between multiple phenotypes and a single genetic variant or set of variants (common and rare) in regions of interest, and to develop methods for mediation analysis to integrate neuroimaging, genetic, and clinical data to test for direct and indirect genetic effects mediated through neuroimaging phenotypes on clinical outcomes; Aim 2: to develop a novel multivariate model that combines multi-omics and neuroimaging data using a machine learning strategy to predict individuals with disease or those at high-risk for developing disease, and to develop a novel multivariate model incorporating prior biological knowledge to identify genomic interactions associated with clinical outcomes; Aim 3: to evaluate and validate the proposed methods using real data from the ADNI and IMAS cohorts; and Aim 4: to disseminate and support publicly available user-friendly software that efficiently implements the proposed methods. RELEVANCE TO PUBLIC HEALTH: Alzheimer's disease (AD) as an exemplar is an increasingly common progressive neurodegenerative condition with no validated disease modifying treatment. The proposed multivariate methods are likely to help identify novel diagnostic biomarkers and therapeutic targets for AD. Identifying new susceptibility loci/biomarkers for AD has important implications for gaining greater insight into the molecular mechanisms underlying AD. NARRATIVE In this project, we propose novel statistical methods and strategies to integrate high-dimensional neuroimaging, multi-omics, and clinical/behavioral data sets, which aim to increase detection power for association analysis and improve the prediction of clinical outcomes. The development of an advanced integrative analysis platform will provide more comprehensive and integrated approaches to answering complex biological questions. The proposed multivariate analysis methods have a high potential impact on and important implications for gaining greater insight into the molecular mechanisms underlying complex diseases, as well as helping the development of earlier diagnostic tests and novel therapeutic targets.","Integrating Neuroimaging, Multi-omics, and Clinical Data in Complex Disease",9916801,R01LM012535,"['Address', 'Advanced Development', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer’s disease biomarker', 'Beds', 'Behavioral', 'Big Data', 'Biological', 'Brain', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Complex', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic tests', 'Discipline', 'Disease', 'Disease Progression', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Genomics', 'Genotype', 'Health', 'Heterogeneity', 'Indiana', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Medical Imaging', 'Memory', 'Meta-Analysis', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Multivariate Analysis', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Phenotype', 'Positron-Emission Tomography', 'Proteomics', 'Public Health', 'Science', 'Statistical Methods', 'Structure', 'Susceptibility Gene', 'Technology', 'Testing', 'Time', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cohort', 'data integration', 'diagnostic biomarker', 'disease classification', 'diverse data', 'endophenotype', 'epigenomics', 'genetic association', 'genetic variant', 'high dimensionality', 'high risk', 'improved', 'insight', 'interest', 'learning strategy', 'mathematical model', 'metabolomics', 'multidimensional data', 'multimodality', 'multiple omics', 'neuroimaging', 'new therapeutic target', 'novel', 'novel diagnostics', 'predict clinical outcome', 'rare variant', 'risk variant', 'therapeutic target', 'transcriptomics', 'user friendly software']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2020,341471,-0.02994355284113031
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9870944,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2020,397125,-0.019001882930572683
"Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center The “clinical high risk” (CHR) for psychosis syndrome is an antecedent period characterized by attenuated psychotic symptoms that are marked by subtle deviations from normal development in thinking, motivation, affect, behavior, and a decline in functioning. Early intervention in this CHR population is critical to prevent psychosis onset as well as other adverse outcomes. However, the presentation of symptoms and subsequent course is highly variable, and there is a paucity of biomarkers to guide treatment development. Thus, to improve predictive models that are clinically relevant, several issues need to be addressed: 1) focusing on outcomes beyond psychosis; 2) taking into account heterogeneity in samples and outcomes; and 3) integrating data sets with a broad array of variables using innovative algorithms to overcome variability across studies. To address these challenges, the proposed “Psychosis Risk Evaluation Data Integration and Computational Technologies: Data Processing, Analysis, and Coordination Center” (PREDICT-DPACC) brings together a multidisciplinary team of highly experienced researchers with proven capabilities in all aspects of large-scale studies, CHR studies, as well as computational expertise. The ultimate goal is to identify new CHR biomarkers, and CHR subtypes that will enhance future clinical trials. To do so, the PREDICT-DPACC will 1) aggregate extant CHR- related data sets from legacy datasets; 2) provide collaborative management, direction, data processing and coordination for new U01 multisite network(s); and 3) develop and apply advanced algorithms to identify biomarkers that predict outcomes, and to stratify CHR into subtypes based on outcome trajectories, first from the extant data and then refined and applied to the new data. The PREDICT-DPACC team has the broad, comprehensive, and robust infrastructure that is sufficiently flexible to accommodate the inclusion of multiple data types and to optimally address the needs of the CHR U01 network(s). Carefully selected extant data will be rapidly obtained, processed, and uploaded to the NIMH Data Archive (NDA). Proposed analysis methods are powerful and robust, leveraging the expertise and experience of computer scientist developers, and experienced clinical researchers. The U01 network(s) will be coordinated by a team that is experienced in managing large studies, familiar with the needs of such studies, flexible, and is knowledgeable in all aspects of CHR studies, including measures, outcomes, biomarkers, and cohorts. Upon meeting the goals of this U24, and the supported U01 network(s), the expected outcomes of the PREDICT-DPACC will be new predictive biomarkers for CHR outcomes, new definitions of CHR subtypes that are clinically useful, and new curated and comprehensive CHR datasets (extant and new) as well as processing tools and prediction algorithms that are shared with the research community through the NIMH Data Archive. NARRATIVE The “Clinical High Risk” (CHR) for psychosis syndrome in young people represents an opportune window for early intervention to prevent the onset of psychosis and other disorders, and to forestall disability; however, clinical heterogeneity and the paucity of biomarkers have hampered the development of effective intervention. To address these challenges, working with NIMH and key stakeholders, we will harmonize and aggregate existing “legacy” CHR data, and guide and coordinate the collection of new data across a network of sites, to develop biomarker algorithms that can predict individual trajectories for diverse outcomes. This proposal leverages a multidisciplinary team with broad and CHR-specific experience in large-scale multisite and multimodal studies (including clinical trials), along with expertise in data type-specific processing, coordination, analysis, and computational analyses (e.g., machine and deep learning tools from artificial intelligence, and advanced statistical approaches), ethics, community outreach, and data dissemination, all of which will ensure the success of this project.","Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center",10092398,U24MH124629,"['Address', 'Adolescent', 'Affect', 'Algorithms', 'Anxiety Disorders', 'Artificial Intelligence', 'Attenuated', 'Behavior', 'Big Data', 'Biological Markers', 'Child', 'Clinical', 'Clinical Trials', 'Collection', 'Common Data Element', 'Communities', 'Community Outreach', 'Computer Analysis', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease remission', 'Early Intervention', 'Early identification', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'FAIR principles', 'Follow-Up Studies', 'Funding', 'Future', 'Goals', 'Heterogeneity', 'Human Resources', 'Impaired cognition', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Intervention', 'Lead', 'Leadership', 'Longterm Follow-up', 'Machine Learning', 'Measures', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Monitor', 'Moods', 'Motivation', 'National Institute of Mental Health', 'Online Systems', 'Outcome', 'Output', 'Perception', 'Procedures', 'Process', 'Protocols documentation', 'Psychotic Disorders', 'Quality Control', 'Recovery', 'Research', 'Research Personnel', 'Risk', 'Risk stratification', 'Safety', 'Sampling', 'Scientist', 'Secure', 'Site', 'Social Functioning', 'Standardization', 'Substance Use Disorder', 'Suggestion', 'Symptoms', 'Technology', 'Thinking', 'Time', 'Training', 'Transact', 'United States', 'Validation', 'Visualization software', 'adverse outcome', 'analytical tool', 'attenuated psychosis syndrome', 'base', 'bioinformatics infrastructure', 'candidate marker', 'clinical heterogeneity', 'clinical risk', 'clinical subtypes', 'clinically relevant', 'cloud based', 'cohort', 'computerized data processing', 'data acquisition', 'data archive', 'data dictionary', 'data dissemination', 'data harmonization', 'data infrastructure', 'data integration', 'data tools', 'deep learning', 'demographics', 'design', 'disability', 'effective intervention', 'experience', 'flexibility', 'functional decline', 'functional disability', 'high risk', 'high risk population', 'improved', 'inclusion criteria', 'innovation', 'meetings', 'member', 'multidisciplinary', 'multimodal data', 'multimodality', 'multiple data types', 'outcome prediction', 'persistent symptom', 'prediction algorithm', 'predictive marker', 'predictive modeling', 'prevent', 'prospective', 'psychotic symptoms', 'quality assurance', 'recruit', 'research study', 'resilience', 'response', 'success', 'therapy development', 'tool', 'working group']",NIMH,BRIGHAM AND WOMEN'S HOSPITAL,U24,2020,3935239,-0.009785242347735848
"An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis Project Summary  Improved methods for the bedside diagnosis and evaluation of neuromuscular disorders are needed. One technology that is finding increasing use for this purpose is electrical impedance myography (EIM). In EIM, a very weak, high frequency electrical current is passed through a muscle of interest and the resulting surface voltages are measured. Disease associated alterations in the composition and microstructural features of the muscle produce characteristic changes that can be used to help classify specific conditions and grade disease severity. To date, most studies using EIM analysis have utilized a fairly limited data set for disease assessment. While effective, this approach ignores a great deal of information locked within the impedance data, including those values that can assist in predicting specific muscle features (such as myofiber diameter) and the presence of pathological change (e.g., fat or connective tissue deposition). In addition, as it stands, the data set is challenging for the clinician to understand without a detailed knowledge of impedance theory. Myolex, Inc is a small business concern located in Boston, MA has as its main focus the development of EIM technologies for clinical use. Myolex recently completed a Phase 1 SBIR that demonstrated the potential capability of machine learning based classification algorithms to effectively discriminate healthy muscle from diseased and to discriminate one disease from another. In this proposed work, we will greatly advance this concept by embodying classification algorithms into a powerful new software suite for Myolex’s current EIM system, the mView. Our underlying hypothesis is that EIM data analysis can be automated to the point that classification systems can provide data on disease diagnosis as well as disease severity for improved ease-of-use. We propose to study this hypothesis via 2 specific aims. In Specific Aim 1, we will design a software suite capable of assisting with artifact-free data collection to be incorporated into our current EIM system, the mViewTM. Then using classification paradigms based on a prodigious amount of previous collected data, we will develop an automated data analysis tool to help provide data on disease category as well as microscopic features, muscle based on the impedance data alone using Microsoft’s Azure Cloud platform. In Specific Aim 2, we will test this developed software suite in a total of180 adult and pediatric neuromuscular disease patients and healthy participants evaluated at Ohio State University Wexner Medical Center (adults) and Boston Children’s Hospital (children). During this data collection period, the Ohio State and Boston Children’s researchers will have real- time access to Myolex staff to provide feedback and have questions/problems answered and addressed. The user interface will continue to be refined and classification algorithms improved. At the conclusion of this work, a new diagnostic tool will be developed for potential 510(k) FDA approval. It will serve as the basis for a continuously self-refining system as additional data sets are collected by end-users employing them in regular clinical use. Project Narrative  Electrical impedance myography (EIM) is a valuable technique to assist with the evaluation of a variety of conditions affecting nerve and muscle. However, to date, only simplistic EIM outcomes have been utilized to assess muscle condition. In this proposed work, we will develop a software platform using machine learning to be incorporated into current EIM technology to allow for automated diseased classification and characterization using the entire large EIM data set collected with each muscle measurement. This will serve as the basis for a new, powerful and convenient tool for neuromuscular diagnosis that will continue to advance over time.",An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis,10002324,R44NS113756,"['Address', 'Adult', 'Affect', 'Algorithmic Software', 'Amyotrophic Lateral Sclerosis', 'Area', 'Back Pain', 'Boston', 'Businesses', 'Caliber', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Duchenne muscular dystrophy', 'Effectiveness', 'Electrodes', 'Ensure', 'Evaluation', 'Fatty acid glycerol esters', 'Feedback', 'Fiber', 'Frequencies', 'Functional disorder', 'Health', 'Inclusion Body Myositis', 'Individual', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Medical Technology', 'Medical center', 'Methods', 'Microscopic', 'Morphologic artifacts', 'Muscle', 'Muscular Dystrophies', 'Myography', 'Myopathy', 'Myositis', 'Nerve', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Ohio', 'Outcome', 'Participant', 'Pathologic', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phase', 'Physicians', 'Play', 'Positioning Attribute', 'Provider', 'Radiculopathy', 'Research Personnel', 'Role', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'Work', 'advanced analytics', 'base', 'classification algorithm', 'cloud based', 'cloud platform', 'commercialization', 'complex data ', 'data acquisition', 'design', 'diagnosis evaluation', 'disease classification', 'disease diagnosis', 'electric impedance', 'feature extraction', 'improved', 'indexing', 'interest', 'machine learning algorithm', 'method development', 'nerve injury', 'neuromuscular', 'novel diagnostics', 'pediatric patients', 'physical therapist', 'prototype', 'sarcopenia', 'software development', 'success', 'theories', 'tool', 'usability', 'user friendly software', 'user-friendly', 'voltage']",NINDS,"MYOLEX, INC.",R44,2020,869698,-0.010246351354311838
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,10000112,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'ZIKA', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'data infrastructure', 'data modeling', 'data streams', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'heterogenous data', 'improved', 'innovation', 'mathematical methods', 'multiple data sources', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2020,365601,0.012015233652750595
"A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth This project provides a data science framework and a toolbox of best practices for systematic and reproducible data-driven methods for validating and deriving RDoC constructs with relevance to psychopathology. Despite recent advances in methods for data-driven constructs, results are often hard to reproduce using samples from other studies. There is a lack of systematic statistical methods and analytical design for enhancing reproducibility. To fill this gap, we will develop a data science framework, including novel scalable algorithms and software, to derive and validate RDoC constructs. Although the proposed methods will generally apply to all RDoC domains and constructs, we focus specifically on furthering understanding of the RDoC domains of cognitive control (CC) and attention (ATT) constructs implicated in attention deficit disorder (ADHD) and obsessive-compulsive disorder (OCD). Our application will use multi-modal neuroimaging, behavioral, and clinical/self-report data from large, nationally representative samples from the on Adolescent Brain Cognitive Development (ABCD) study and multiple local clinical samples with ADHD and OCD. Specifically, using the baseline ABCD samples, in aim 1, we will apply and develop methods to assess and validate the current configuration of RDoC for CC and ATT using confirmatory latent variable modeling. We will implement and develop new unsupervised learning methods to construct new computational-driven, brain-based domains from multi-modal image data. In Aim 2, We will introduce network analysis (via Gaussian graphical models) to characterize heterogeneity in the interrelationship of RDoC measurements due to observed characteristics (i.e., age and sex). We will further model the heterogeneity of the population due to unobserved characteristics by introducing the data-driven precision phenotypes, which are the subgroup of participants with similar RDoC dimensions. We propose a Hierarchical Bayesian Generative Model and scalable algorithm for simultaneous dimension reduction and identify precision phenotypes. The model also serves as a tool to transfer information from the community sample ABCD to local clinical enriched studies. In aim 3, we will utilize the follow-up samples from ABCD and local clinical enriched data sets to validate the results from Aims 1 and 2 and assess the clinical utility of the precision phenotypes in predicting psychological development in follow-up time. Our project will provide a suite of analytical tools to validate existing RDoC constructs and derive new, reproducible constructs by accounting for various sources of heterogeneity. To advance the understanding of psychopathology using dimensional constructs of measurements from multiple units of analysis, we propose reproducible statistical framework for validating and deriving RDoC constructs with relevance to psychopathology. We will use multi-modal neuroimaging, behavioral and clinical/self-report data from multiple samples to develop this framework. The design of our study consists of analyzing large, nationally representative samples, validating the results in local clinically enriched samples, and transfer information from the large community samples to local clinical samples.",A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth,10058921,R01MH124106,"['11 year old', 'Accounting', 'Adolescent', 'Age', 'Algorithmic Software', 'Algorithms', 'Attention', 'Attention Deficit Disorder', 'Base of the Brain', 'Behavioral', 'Brain', 'Characteristics', 'Child', 'Chronology', 'Clinical', 'Clinical Data', 'Communities', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Dimensions', 'Ensure', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Goals', 'Heterogeneity', 'Image', 'Knowledge', 'Learning', 'Link', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Obsessive-Compulsive Disorder', 'Participant', 'Pathway Analysis', 'Patient Self-Report', 'Phenotype', 'Population Heterogeneity', 'Prediction of Response to Therapy', 'Psychological Transfer', 'Psychopathology', 'Reproducibility', 'Reproducibility of Results', 'Research Domain Criteria', 'Sampling', 'Source', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'Time', 'Variant', 'Youth', 'age effect', 'analytical tool', 'autoencoder', 'base', 'biological sex', 'cognitive control', 'cognitive development', 'deep learning', 'design', 'follow up assessment', 'follow-up', 'high dimensionality', 'independent component analysis', 'insight', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'multimodality', 'network models', 'neuroimaging', 'novel', 'psychologic', 'response', 'sex', 'tool', 'unsupervised learning']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2020,710101,0.011645911412105275
"Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures Neuropsychiatric disorders pose an immense burden on patients, families, and health care systems, thus underscoring the urgent need to develop disease-modifying treatment. Research on neuropsychiatric disorders (e.g., Alzheimer’s disease, Parkinson’s disease) faces unique challenges, including the fact that these disorders typically have a late onset and slow progression, the diagnostic criteria are based on subjective clinical symptoms, and there is substantial disease and subject heterogeneity. In the proposed work, we aim to tackle these challenges by leveraging complementary contributions from multiple biomarkers, including genome-wide polymorphisms, whole brain neuroimaging, biofluids, and comprehensive neuropsychiatric assessments. We develop sophisticated analytic tools with higher resolution and improved accuracy by accounting for biological mechanisms of disease, synthesizing dynamic system-wide information, and integrating multiple sources of biomarkers. These methods are applied to clinical data collected by the investigative team or available from large international consortia in order to model the earliest pathological changes of neurodegenerative disease, assess treatment responses, and inform the design of early-intervention clinical trials and the discovery of optimal personalized therapies. Specifically, in Aim 1, we develop efficient methods for multi-level semiparametric transformation models to estimate and test the risk of genetic variants on various types of complex phenotypes to inform genetic counseling and improve clinical trial efficiency. Our methods do not rely on full pedigree genotyping and provide family-specific substructure, in addition to population substructure, to better control confounding and reduce false discovery rates in genome-wide association studies. In Aim 2, we develop large-scale nonlinear dynamic systems through ordinary differential equations with random inflections to understand early pathological changes and identify subjects with preclinical signs. Our method provides multi-domain integration of ensembles of biomarker dynamics. In Aim 3, we develop dynamic hazards models and incorporate dynamic network structures to estimate biomarker profiles that evolve smoothly with disease progression for earlier disease diagnosis. We account for irregularly measured biomarkers and biological network dependence among biomarkers. In Aim 4, we develop doubly robust and efficient machine learning methods to identify predictive markers, estimate optimal individualized therapies, and identify subgroups who may receive the greatest benefit from therapy, with minimal risk. In each aim, we will validate the proposed methods through extensive simulation studies and demonstrate their practical value via application to real-world clinical studies. We establish theoretical properties of the proposed methods using modern empirical process theory and statistical learning theory. Together, the state-of-the-art analytic methods proposed here will substantially improve analytic accuracy, and our combined statistical and clinical expertise will ensure that our methods are translated directly back to the clinical and translational research community. Project Narrative:  The ultimate goal of neuropsychiatric research is to develop experimental therapeutics to delay disease on- set, slow disease progression, and provide effective treatment at each stage of disease. This proposal aims to develop new statistical approaches to integrate complementary sources of information from genomic measures, brain imaging biomarkers, and early clinical signs to characterize disease mechanism, progression, and treatment responses, and thereby inform the design of clinical trials and the discovery of optimal personalized therapies.",Statistical methods for early disease prediction and treatment strategy estimation using biomarker signatures,9927686,R01NS073671,"['Accounting', 'Age', 'Alzheimer&apos', 's Disease', 'Back', 'Benefits and Risks', 'Biological', 'Biological Markers', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Data', 'Dependence', 'Diagnosis', 'Diagnostic', 'Differential Equation', 'Dimensions', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Ensure', 'Equilibrium', 'Event', 'Face', 'Family', 'Family health status', 'Family member', 'First Degree Relative', 'Funding', 'Genetic Counseling', 'Genetic Polymorphism', 'Genetic Risk', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Hazard Models', 'Healthcare Systems', 'Heterogeneity', 'Impact evaluation', 'Individual', 'International', 'Intervention', 'Investigational Therapies', 'Late-Onset Disorder', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Neurodegenerative Disorders', 'Non-linear Models', 'Nonlinear Dynamics', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Pathologic', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Process', 'Property', 'Radiation exposure', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Safety', 'Source', 'Spinal Puncture', 'Staging', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Testing', 'Time', 'Translating', 'Translational Research', 'Treatment Efficacy', 'Work', 'analytical method', 'analytical tool', 'base', 'clinical decision-making', 'design', 'disease diagnosis', 'dynamic system', 'effective therapy', 'genetic pedigree', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging biomarker', 'improved', 'individualized medicine', 'machine learning method', 'minimal risk', 'nervous system disorder', 'neuroimaging', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'personalized medicine', 'pre-clinical', 'predictive marker', 'predictive modeling', 'randomized trial', 'semiparametric', 'simulation', 'statistical learning', 'theories', 'treatment effect', 'treatment response', 'treatment strategy', 'validation studies']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,340025,0.01593159219368432
New Approaches to Dementia Heterogeneity  ,New Approaches to Dementia Heterogeneity,10053649,P30AG062422,"['Aging', 'Area', 'Artificial Intelligence', 'Astrocytes', 'Behavior', 'Big Data', 'Biological Markers', 'Blood', 'Blood Vessels', 'Brain', 'Brain imaging', 'Cells', 'Cognition', 'Cognitive', 'Data', 'Data Analytics', 'Data Collection', 'Degenerative Disorder', 'Dementia', 'Disease', 'Disease Progression', 'Dissection', 'Early Diagnosis', 'Emotional', 'Endothelium', 'Etiology', 'Failure', 'Fibrosis', 'Functional disorder', 'Funding', 'Goals', 'Heterogeneity', 'Homeostasis', 'Image', 'Impaired cognition', 'Impairment', 'Inflammation', 'Injury', 'Intervention', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mediating', 'Mediation', 'Mentors', 'Mentorship', 'Modeling', 'Molecular', 'National Institute of Neurological Disorders and Stroke', 'Natural regeneration', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Neuroglia', 'Neurons', 'Outcome', 'Pathologic Neovascularization', 'Pathway interactions', 'Patients', 'Pattern', 'Perfusion', 'Phase', 'Phenotype', 'Play', 'Population', 'Proteomics', 'Research Personnel', 'Risk', 'Role', 'Scientist', 'Site', 'Synapses', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Training', 'Vascular Dementia', 'Vascular Diseases', 'Veterans', 'angiogenesis', 'base', 'biomedical informatics', 'cerebrovascular biology', 'clinical phenotype', 'cognitive testing', 'cohort', 'disorder subtype', 'drug development', 'endophenotype', 'exosome', 'hypoperfusion', 'molecular marker', 'multidimensional data', 'neuroinflammation', 'neurovascular', 'novel', 'novel strategies', 'phenotypic biomarker', 'precision medicine', 'predictive signature', 'prospective', 'proteostasis', 'resilience', 'retinal imaging', 'tau Proteins', 'tool', 'unsupervised learning', 'vascular cognitive impairment and dementia']",NIA,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",P30,2020,298759,-0.02058850902380363
"Modeling Homeostasis of Human Blood Metabolites PROJECT SUMMARY  Metabolite levels in human blood are regulated by a relatively strict system of homeostatic control. Previous investigations of homeostasis have taken a number of approaches, and models of glucose and a few other metabolites have been developed, typically focused on a single organ. However, while potentially extremely useful, an accurate and quantitative model of blood metabolite levels under homeostasis does not currently exist.  It is well known that numerous demographic and clinical factors such as gender, age, BMI, smoking, etc., as well as pre-analytical factors and many diseases, significantly affect the levels of blood metabolites. Numerous studies in the field of metabolomics have attempted to account for the effects of many such factors. However, efforts to quantify these effects and validate them across different studies have so far been challenging, and resulted in consistent failures to validate discovered putative biomarkers. The challenges to integrate metabolite profiles with clinical and demographic factors are complicated by the high dimensionality of the data and the numerous correlations among the metabolites. Traditional statistical methods are incapable of accounting for these factors, and hence, investigations suffer from a high false discovery rate (FDR).  To overcome these challenges, we propose to develop quantitative statistical models of blood metabolite levels in healthy adults, and thereby produce a predictive model of homeostasis. Our preliminary work indicates that we can predict metabolite levels with much reduced variance using the reproducibly measured levels of a large pool of blood metabolites and clinical and demographic variables. We propose to develop sophisticated models of homeostasis based on advanced statistical methods and evaluate their predictive performance across different sample sets and metabolite classes.  The proposed project has four main Aims: (1) Obtain broad-based metabolomics data on blood samples collected from geographically distinct sites to explore the effects of a range of confounding effects on metabolite levels. (2) Model individual or biologically related groups of metabolite levels using multivariate statistical approaches to determine the contribution of clinical/demographic and pre-analytical variables and their predictability across collection site. (3) Investigate the interactions between metabolites and clinical/demographic variables using machine learning approaches to identify stable metabolites and key interactions. (4) Provide the community with user-friendly software packages for the prediction of blood metabolite levels under homeostasis.  An overall model of the metabolite concentrations in blood will be highly useful for a number of applications that include a better understanding of systems biology at the whole organism level, and ultimately improved risk prediction, disease diagnosis, treatment monitoring and outcomes analysis. PROJECT NARRATIVE A quantitative model of blood homeostasis based on predicting the normal levels of small molecules in the blood can help identify diseases or other stresses that cause changes to those levels. The proposed statistical methods that will be used to develop this homeostasis model have the potential to efficiently identify more reliable disease markers and to more accurately predict disease risk.",Modeling Homeostasis of Human Blood Metabolites,9995722,R01GM131491,"['Accounting', 'Address', 'Adult', 'Affect', 'Age', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Clinical', 'Collection', 'Communities', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Demographic Factors', 'Development', 'Dimensions', 'Disease', 'Disease Marker', 'Evaluation', 'Failure', 'Gases', 'Gender', 'Geographic Locations', 'Geography', 'Glucose', 'Homeostasis', 'Human', 'Individual', 'Investigation', 'Ions', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolite Interaction', 'Modeling', 'Monitor', 'NMR Spectroscopy', 'Organ', 'Outcome', 'Performance', 'Risk', 'Sampling', 'Site', 'Smoking', 'Source', 'Statistical Methods', 'Statistical Models', 'Stress', 'Supervision', 'System', 'Systems Biology', 'Technology', 'Temperature', 'Time', 'Training', 'Validation', 'Whole Organism', 'Work', 'base', 'clinical effect', 'cohort', 'computerized tools', 'data quality', 'disease diagnosis', 'disorder risk', 'improved', 'interoperability', 'metabolome', 'metabolomics', 'multidimensional data', 'predictive modeling', 'sample collection', 'small molecule', 'software development', 'user friendly software', 'user-friendly', 'validation studies']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,395738,0.006163305798401437
"Detection and characterization of critical under-immunized hotspots Detection and characterization of critical under-immunized hotspots  Emergence of undervaccinated geographical clusters for diseases like measles has become a national concern. A number of measles outbreaks have occurred in recent months, despite high MMR coverage in the United States ( 95%). Such undervaccinated clusters can act as reservoirs of infection that can transmit the disease to a wider population, magnifying their importance far beyond what their absolute numbers might indicate. The existence and growth of such undervaccinated clusters is often known to public health agencies and health provider networks, but they typically do not have enough resources to target people in each such cluster, to attempt to improve the vaccination rate. Preliminary results show that not all undervaccinated clusters are “equal” in terms of their potential for causing a big outbreak (referred to as its “criticality”), and the rate of undervaccination in a cluster does not necessarily correlate with its criticality.  However, there are no existing methods to estimate the potential risk of such clusters, and to identify the most “critical” ones. Some of the key reasons are: (i) purely data-driven spatial statistics methods rely only on immunization coverage, which does not give any indication of the risk of an outbreak; and (ii) current causal epidemic models need to be combined with detailed incidence data, which has not been easily available.  This proposal brings together a systems science approach, combining agent-based stochastic epidemic models, and techniques from machine learning, high performance computing, data mining, and spatial statistics, along with novel public and private datasets on immunization and incidence, to develop a novel methodology for identifying critical clusters, through the following tasks: (i) Identify spatial clusters with signiﬁcantly low immunization rates, or strong anti-vaccine sentiment; (ii) Develop an agent based model for the spread of measles that incorporates detailed immunization data, and is calibrated using a novel source of incidence data; (iii) Develop methods to ﬁnd and characterize critical spatial clusters, with respect to different metrics, which capture both epidemic and economic burden, and order underimmunized clusters based on their criticality; and (iv) Use the methodology to evaluate interventions in terms of their effect on criticality. A highly interdisciplinary team involving two universities, a health care delivery organization and a state department of Health, will work together to develop this methodology. Characterization of such clusters will enable public health departments and policy makers in targeted surveillance of their regions and a more efﬁcient allocation of resources. Project Narrative  This project will develop a new methodology to quantify the potential risks of under-vaccinated spatial clusters for highly infectious diseases. It will rank the clusters based on their economic and epidemic burden which will enable public health ofﬁcials in targeted surveillance and interventions, to mitigate their risk.",Detection and characterization of critical under-immunized hotspots,9887876,R01GM109718,"['Affect', 'Bayesian Method', 'Behavioral Model', 'California', 'Characteristics', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Detection', 'Disease', 'Disease Clusterings', 'Disease Outbreaks', 'Disease model', 'Economic Burden', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Funding', 'Geography', 'Growth', 'Health', 'Health Personnel', 'Herd Immunity', 'High Performance Computing', 'Immunization', 'Immunize', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Machine Learning', 'Measles', 'Measles-Mumps-Rubella Vaccine', 'Medical', 'Methodology', 'Methods', 'Minnesota', 'Modeling', 'New Jersey', 'New York', 'Oregon', 'Outcome', 'Pathway interactions', 'Policies', 'Policy Maker', 'Population', 'Population Analysis', 'Privatization', 'Public Health', 'Records', 'Registries', 'Resolution', 'Resource Allocation', 'Resources', 'Risk', 'Scanning', 'Schools', 'Science', 'Source', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Uncertainty', 'United States', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccines', 'Washington', 'Work', 'base', 'data mining', 'demographics', 'diverse data', 'economic cost', 'economic outcome', 'health care delivery', 'health disparity', 'health organization', 'improved', 'interest', 'novel', 'novel strategies', 'population based', 'provider networks', 'public health intervention', 'social', 'social media', 'spatiotemporal', 'statistics', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,324178,-0.009486030412208631
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9924432,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device', 'wearable sensor technology']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,298890,0.022087369998143157
"Dynamic imaging-genomic models for characterizing and predicting psychosis and mood disorders Project Summary/Abstract  Disorders of mood and psychosis such as schizophrenia, bipolar disorder, and unipolar depression are  incredibly complex, influenced by both genetic and environmental factors, and the clinical characterizations are primarily based on symptoms rather than biological information. Current diagnostic approaches are based on symptoms, which overlap extensively in some cases, and there is growing consensus that we should approach mental illness as a continuum, rather than as a categorical entity. Since both genetic and environmental factors play a large role in mental illness, the combination of brain imaging and genomic data are poised to play an important role is clarifying our understanding of mental illness. However, both imaging and genomic data are high dimensional and include complex relationships that are poorly understood. To characterize the available information, we are in need of approaches that can deal with high-dimensional data exhibiting interactions at multiple levels (i.e., data fusion), while providing interpretable solutions (i.e., a focus on brain and genomic  networks). An additional challenge exists because the available data has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. To address these challenges, we introduce a new unified framework called flexible subspace analysis (FSA) that can automatically identify subspaces (groupings of unimodal or multimodal  components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus  leveraging the power of deep learning. We will apply the developed models to a large (N>60,000) dataset of  individuals along the mood and psychosis spectrum to evaluate the important question of disease categorization. We will compute fully cross-validated genomic-neuro-behavioral profiles of individuals including a comparison of the predictive accuracy of 1) standard categories from the diagnostic and statistical manual of mental disorders (DSM), 2) data-driven subgroups, and 3) dimensional relationships. We will also evaluate the single subject predictive power of these profiles in independent data to maximize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data  promises to advance our understanding of the nosology of mood and psychosis disorders in addition to providing new tools that can be widely applied to other studies of complex disease. Project Narrative  It is clear that mood and psychosis disorders, largely diagnosed without biological criteria, include a multitude of inter-related genetic and environmental factors. We propose to develop new flexible models to capture  multiscale (dynamic) brain imaging and genomics data, which we will use to study individuals along the mood and psychosis spectrum using a large aggregated dataset including a comparison of the predictive accuracy of two dichotomous approaches (standard diagnostic categories and unsupervised/data-driven) as well as a  dimensional approach to diagnosis.",Dynamic imaging-genomic models for characterizing and predicting psychosis and mood disorders,9889183,R01MH118695,"['3-Dimensional', 'Address', 'Algorithms', 'Behavior', 'Behavioral', 'Benchmarking', 'Biological', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Brain region', 'Categories', 'Clinical', 'Communities', 'Complex', 'Consensus', 'Data', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Environmental Risk Factor', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genetic Risk', 'Genomics', 'Goals', 'Grouping', 'Image', 'Individual', 'Joints', 'Lead', 'Link', 'Major Depressive Disorder', 'Maps', 'Mental disorders', 'Methods', 'Modeling', 'Mood Disorders', 'Moods', 'Noise', 'Pathway interactions', 'Patients', 'Pattern', 'Play', 'Property', 'Psychotic Disorders', 'Research Personnel', 'Role', 'Sampling', 'Schizoaffective Disorders', 'Schizophrenia', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Source', 'Structure', 'Subgroup', 'Supervision', 'Symptoms', 'Syndrome', 'Time', 'Unipolar Depression', 'Work', 'base', 'bipolar patients', 'blind', 'connectome', 'data anonymization', 'data fusion', 'data warehouse', 'deep learning', 'disease classification', 'flexibility', 'genomic data', 'genomic locus', 'independent component analysis', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'profiles in patients', 'psychiatric genomics', 'psychotic symptoms', 'statistics', 'tool', 'user friendly software']",NIMH,GEORGIA STATE UNIVERSITY,R01,2020,698402,-0.015045206189287364
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,10011756,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'effectiveness evaluation', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2020,247413,0.02640136259307316
"Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine A fundamental challenge in precision medicine is to understand the patterns of differentiation between individuals. To address this challenge, we propose to go beyond the traditional `one disease--one model' view of bioinformatics and pursue a new view built upon personalized patient models that facilitates precision medicine by leveraging both commonalities within a patient cohort as well as signatures unique to every individual patient. With the emergence of large-scale databases such as The Cancer Genome Atlas (TCGA), the International Cancer Genome Consortium (ICGC), and the Gene Expression Omnibus (GEO), which collect multi-omic data on many different diseases, a new “pan-omics” and “pan-disease” paradigm has emerged to jointly analyze all patients in a disease cohort while accounting for patient-specific effects. An example of this is the recently released Pan-Cancer Atlas. At the same time, next generation statistical tools to accurately and rigorously draw the necessary inferences are lacking. In this project we propose a series of mathematically rigorous, statistically sound, and computationally feasible approaches to infer sample-specific models, providing a more complete view of heterogeneous datasets. By bringing together ideas from the machine learning, statistics, and mathematical optimization communities, we provide a rigorous framework for precision medicine via sample-specific statistical models. Crucially, we propose to analyze this framework and prove strong theoretical guarantees under weak assumptions--this dramatically distinguishes our framework from much of the existing literature. Towards these goals, we propose the following aims: Aim 1: Discovery of new molecular profiles with sample-specific statistical models. We propose a general framework for inferring sample-specific models with low-rank structure based on the novel concept of distance-matching. This allows us to infer statistical models at the level of a single patient without overfitting, and is general enough to be applied for prediction, classification, and network inference as well as a variety of diseases and phenotypes. Aim 2: Multimodal approaches to personalized diagnosis--contextually interpretable models for actionable clinical decision support. In order to translate these models into practice, we propose a novel interpretable predictive model that supports complex, multimodal data types such as images and text combined with high-level interpretable features such as SNP data, gender, age, etc. This framework simultaneously boosts the accuracy of clinical predictions by exploiting sample heterogeneity while providing human-digestable explanations for the predictions being made. Aim 3: Next-generation precision medicine--algorithms and software for personalized estimation. To put our models into practical use, we will develop new algorithms for interpretable prediction of personalized clinical outcomes and visualization of personalized statistical models. All of our tools will be combined into a user-friendly software package called PrecisionX that will be freely available to researchers and clinicians everywhere. RELEVANCE (See instructions): Personalization with data is a critical challenge whenever decisions must be made at scale, and has applications that go beyond precision medicine; businesses, educational institutions, and financial institutions are among the many players that have acknowledged a stake in this complex problem. We expect the proposed work to provide a rigorous foundation for personalization with large and high-dimensional datasets, finding use throughout the broader scientific community as well as with industry and educational institutions. Alongside our collaboration with Pitt/UPMC, we will work with physicians and data scientists for practical feedback as well as provide training in the methods developed. n/a",Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine,10133782,R01GM140467,"['Accounting', 'Address', 'Age', 'Algorithmic Software', 'Algorithms', 'Atlases', 'Bioinformatics', 'Businesses', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Scientist', 'Data Set', 'Disease', 'Feedback', 'Foundations', 'Gender', 'Gene Expression', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Industry', 'Institution', 'Instruction', 'International', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modeling', 'Molecular Profiling', 'Multiomic Data', 'Outcome', 'Patients', 'Pattern', 'Physicians', 'Portraits', 'Research Personnel', 'Sampling', 'Series', 'Statistical Models', 'Structure', 'Text', 'The Cancer Genome Atlas', 'Time', 'Training', 'Translating', 'Visualization', 'Work', 'base', 'cancer genome', 'clinical decision support', 'clinically actionable', 'cohort', 'disease phenotype', 'heterogenous data', 'high dimensionality', 'individual patient', 'large-scale database', 'molecular modeling', 'multimodal data', 'multimodality', 'next generation', 'novel', 'personalized diagnostics', 'personalized predictions', 'precision medicine', 'predictive modeling', 'sound', 'statistics', 'tool', 'user friendly software']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,305566,0.00985101811384823
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9920211,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'data quality', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,709525,-0.007082797919691069
"Evaluation of multiple medication exposures concurrently using a novel algorithm PROJECT SUMMARY The development of large observational health databases (OHD) has expanded the data available for analysis by pharmacoepidemiology research. The efficiency of these studies may be improved by simultaneously studying the association of multiple medications with a disease of interest. Unfortunately, prior research has demonstrated that it is difficult to distinguish true-positive from false-positive results when studying multiple exposures simultaneously, thus limiting the conclusions drawn from these types of studies and representing a major gap in the field. The objective of this proposal, which is the first step in achieving the applicant's long- term goal of improving the diagnosis and treatment of gastrointestinal diseases using insights derived from OHD, is to evaluate and validate medication class enrichment analysis (MCEA), a novel set-based signal-to- noise enrichment algorithm developed by the applicant to analyze multiple exposures from OHD with high sensitivity and specificity. The central hypothesis of this proposal is that MCEA has equal sensitivity and greater specificity compared to logistic regression, the most widely used analytic method for OHD, for identifying true associations between medications and clinical outcomes. The applicant will complete the following two interrelated specific aims to test the hypothesis: Aim 1 – to calculate the sensitivity and specificity of medication class enrichment analysis (MCEA) and logistic regression (LR) for identifying medication associations with Clostridium difficile infection (CDI) and Aim 2 – to calculate the sensitivity and specificity of MCEA and LR for identifying medication associations with gastrointestinal hemorrhage (GIH). The rationale for these aims is that by reproducing known medication-disease associations without false positives, MCEA can be used to identify novel pharmacologic associations with gastrointestinal diseases in future studies. The expected outcome for the proposed research is that it will demonstrate MCEA as a valid method for pharmacoepidemiology research, opening new research opportunities for the study of multi-exposure OHD. These new research opportunities may lead to more rapid identification of potential pharmacologic causes of emerging diseases and discovery of unanticipated beneficial medication effects, allowing such medications to be repurposed for new indications. To attain the expected outcome, the applicant will complete additional coursework that builds on his Master of Science in Clinical Epidemiology to learn computational biology, machine learning, and econometrics techniques. With the support of this grant and his institution, he will also directly apply these techniques to pharmacoepidemiology applications under the close mentorship of a carefully selected team of faculty with extensive experience in gastroenterology, pharmacoepidemiology, medical informatics, and mentoring prior K-award grant recipients. Through these activities, the applicant will develop the skills necessary to obtain NIH R01-level funding and become a leader in developing novel techniques for application to the epidemiologic study of gastrointestinal diseases. PROJECT NARRATIVE Traditionally, research studying medications associated with diseases are limited to analyzing one medication at a time. This novel proposal will validate medication class enrichment analysis, a recently developed algorithm to study multiple medications simultaneously for association with a disease of interest. Validation of this method will allow researchers to use existing medical databases to more rapidly identify potential medication causes of emerging diseases and identify medications with unanticipated beneficial effects, allowing such medications to be repurposed for new indications.",Evaluation of multiple medication exposures concurrently using a novel algorithm,9853783,K08DK119475,"['Address', 'Algorithms', 'Aminoglycosides', 'Antibiotics', 'Anticoagulants', 'Antiplatelet Drugs', 'Big Data to Knowledge', 'Biological', 'Carbapenems', 'Case-Control Studies', 'Cephalosporins', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Computational Biology', 'Computer software', 'Data', 'Databases', 'Development', 'Development Plans', 'Diagnosis', 'Digestive System Disorders', 'Disease', 'Electronic Health Record', 'Epidemiologic Methods', 'Evaluation', 'Faculty', 'Fluoroquinolones', 'Funding', 'Future', 'Gastroenterology', 'Gastrointestinal Diseases', 'Gastrointestinal Hemorrhage', 'Generations', 'Genomics', 'Goals', 'Grant', 'Health', 'Infection', 'Informatics', 'Inpatients', 'Institution', 'K-Series Research Career Programs', 'Lead', 'Learning', 'Logistic Regressions', 'Machine Learning', 'Master of Science', 'Medical', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Noise', 'Non-Steroidal Anti-Inflammatory Agents', 'Outcome', 'Penicillins', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacology', 'Research', 'Research Design', 'Research Personnel', 'Sensitivity and Specificity', 'Signal Transduction', 'Specificity', 'Techniques', 'Testing', 'Time', 'United Kingdom', 'United States National Institutes of Health', 'Validation', 'Veterans', 'analytical method', 'base', 'beta-Lactams', 'career development', 'clinical epidemiology', 'econometrics', 'epidemiology study', 'experience', 'improved', 'inhibitor/antagonist', 'insight', 'interest', 'novel', 'research study', 'simulation', 'skills', 'usability']",NIDDK,UNIVERSITY OF PENNSYLVANIA,K08,2020,167900,0.00029899459731194694
"Assessment of ultrasound features of knee osteoarthritis in a population-based community cohort Project summary Our long-term goal is to demonstrate the utility of ultrasound for OA assessment, standardize its acquisition and scoring, and promote increased uptake of US for use in clinical, research, and trial settings. Knee osteoarthritis (KOA) is highly prevalent and frequently debilitating. Development of potential treatments has been hampered by the heterogenous nature of this common chronic condition, which is characterized by a number of subgroups, or phenotypes, with different underlying pathophysiological mechanisms. Imaging, genetics, biochemical biomarkers, and other features can be used to characterize phenotypes, but variations in data types can make it difficult to harmonize definitions. While radiography is widely used in KOA imaging, it is limited in its ability to assess early disease (when interventions are most likely to succeed) and is insensitive to change. Ultrasound (US) is a widely accessible, time-efficient and cost-effective imaging modality that can provide detailed and reliable information about all joint tissues (e.g., cartilage, meniscus, synovium, bone), and could therefore inform phenotypes in KOA (e.g., by presence of synovitis, effusion, cartilage damage, calcium crystal deposition, and popliteal cysts). Use of US is currently limited by the lack of systematically performed studies in well-characterized non-clinical populations. To address this gap and further the use of this advantageous imaging modality for KOA, we will obtain standardized US and radiography in the population- based Johnston County Health Study (JoCoHS), the new enrollment phase of the 25+ year Johnston County OA Project which includes white, African American, and Hispanic men and women aged 35-70, to achieve three aims. In Aim 1, we will determine the population prevalence (n~3000) of knee US features including cartilage and meniscal damage, synovitis/effusion, calcium crystal deposition, popliteal cysts and osteophytes overall and in key subgroups by age, sex, race/ethnicity, and symptom status. Aim 2 will allow quantification of the associations between these US features and radiographic findings and symptom scores overall and in key subgroups (e.g., those with and without radiographic KOA, by sex, by race/ethnicity). For Aim 3, we will apply novel machine learning methodologies (e.g., Direction-projection-permutation [DiProPerm] hypothesis testing, Joint and Individual Variation [JIVE], and Distance-Weighted Discrimination [DWD]) to a) develop an overall US score for symptomatic KOA and b) identify the contribution of US variables to phenotypes relevant to KOA based on general health, physical activity, and functional assessments. This study is a crucial step to establish the foundation for US as an assessment tool for clinical use, research, and clinical trials in KOA, providing unique population-based cross-sectional data regarding the utility of US and forming the basis for future longitudinal work evaluating its value and performance characteristics related to incident and progressive KOA. Project narrative Osteoarthritis is an enormous and increasing public health problem that, like many other chronic conditions, is not a single disease but a heterogeneous condition consisting of multiple subgroups, or phenotypes, with differing underlying mechanisms. Ultrasound is an accessible, time-efficient, and cost-effective imaging modality that provides invaluable data about all joint tissues involved in osteoarthritis and has the potential to identify important phenotypes. The proposed work is relevant to the NIAMS mission and represents a crucial step to establish the foundation for ultrasound as an assessment tool for use in clinics, research, and clinical trials in osteoarthritis.",Assessment of ultrasound features of knee osteoarthritis in a population-based community cohort,9944803,R01AR077060,"['Address', 'African American', 'Age', 'Area', 'Assessment tool', 'Bilateral', 'Biochemical', 'Biological Markers', 'Bone Spur', 'Calcium', 'Cartilage', 'Categories', 'Characteristics', 'Chronic', 'Claustrophobias', 'Clinic', 'Clinical', 'Clinical Assessment Tool', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Cohort Studies', 'Communities', 'County', 'Crystal Formation', 'Crystallization', 'Data', 'Data Set', 'Degenerative polyarthritis', 'Development', 'Diagnostic radiologic examination', 'Discrimination', 'Disease', 'Enrollment', 'Ethnic Origin', 'Etiology', 'Foundations', 'Future', 'General Population', 'Goals', 'Health', 'Hispanics', 'Image', 'Implant', 'Individual', 'Infrastructure', 'Intervention', 'Joints', 'Knee', 'Knee Osteoarthritis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Meniscus structure of joint', 'Methodology', 'Mission', 'Modality', 'Musculoskeletal', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Nature', 'Outcome', 'Pain', 'Participant', 'Pathology', 'Performance', 'Phase', 'Phenotype', 'Physical activity', 'Popliteal Cyst', 'Population', 'Population Study', 'Prevalence', 'Public Health', 'Race', 'Receiver Operating Characteristics', 'Research', 'Risk Factors', 'Sex Differences', 'Specialist', 'Standardization', 'Subgroup', 'Symptoms', 'Syndrome', 'Synovial Membrane', 'Synovitis', 'Testing', 'Time', 'Tissues', 'Ultrasonography', 'Variant', 'Woman', 'Work', 'aged', 'base', 'bone', 'cohort', 'cost', 'cost effective', 'effusion', 'follow-up', 'imaging genetics', 'imaging modality', 'individual variation', 'interest', 'men', 'novel', 'point of care', 'population based', 'recruit', 'rheumatologist', 'sex', 'uptake']",NIAMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,342100,-0.0010179711218222265
"Multi-Resolution Docking Methods for Electron Microscopy Summary In the past decade, we have witnessed a revolutionary progress in camera technology and the attainable resolution of macromolecular assemblies via cryogenic electron microscopy (cryo-EM) and in the development of computational algorithms that relate the resulting 3D maps to atomic resolution structures. Whereas single- particle cryo-EM today is capable of directly solving atomic structures of biomolecular assemblies in isolation, electron tomography (ET) in unstained frozen-hydrated samples is widely used to capture the 3D organization of supramolecular complexes in their native (organelle, cell, or tissue) environments. We have identified three inter-related research areas where our computational modeling experience (historically rooted in pre-revolution multi-scale approaches) offers the biggest value to today's post-revolution EM community: (1) medium resolution cryo-EM modeling, (2) the segmentation and denoising of cryo-ET data, and (3) the validation of atomic models and their corresponding maps. The first aim is an extension of promising new ideas in flexible fitting as well as secondary structure prediction for medium resolution maps, which have been our key research areas in the past. medium resolution (5-10Å) maps are still widely used in EM and can be of significant biological importance. This is particularly true in the case of cryo-ET maps, which are harder to read than single particle cryo-EM maps because they often exhibit considerable noise, anisotropic resolution, and anisotropic density variations due to the low dose requirements and the missing wedge in the Fourier space. In the case of tightly packed or crowded macromolecular structures, the fusion of nearby biomolecular densities prevents an automated segmentation of geometric shapes, requiring a labor-intensive manual tracing by human experts. We are currently developing novel computational approaches to provide a more objective strategy for missing wedge correction in homogeneous specimen areas of tomograms. Our hybrid approach combines deconvolution and denoising with template matching in a unified mathematical framework that allows modeling constraints to be imposed in a least-squares optimization process. Our approach can also be extended to the flexible refinement of atomic structures using our damped dynamics flexible fitting approach by tuning the internal point-spread functions to the missing wedge of the ET data. To support these aims, we will quantitatively measure the fitness of an atomic model in local density regions and characterize the fitness of maps with reliable reference structures. The collaborative efforts supported by this grant will include the refinement of cytoskeletal filaments, molecular motors, bacterial chemoreceptor arrays, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established Internet-based mechanisms used by the Situs and Sculptor packages and as plugins for the popular UCSF Chimera graphics program. Project Narrative This project will help biological electron microscopists bridge a broad range of resolution levels, from the atomic to the living organism. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,10120245,R01GM062968,"['3-Dimensional', 'Algorithms', 'Architecture', 'Area', 'Biological', 'Cells', 'Characteristics', 'Chemoreceptors', 'Chimera organism', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational algorithm', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Crowding', 'Cryo-electron tomography', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Databases', 'Deposition', 'Detection', 'Development', 'Docking', 'Dose', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Elements', 'Environment', 'Equilibrium', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Human', 'Hybrids', 'Hydration status', 'Internet', 'Laboratories', 'Least-Squares Analysis', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Mathematics', 'Measures', 'Medical', 'Methods', 'Microscope', 'Modeling', 'Modernization', 'Molecular Motors', 'Molecular Structure', 'Morphologic artifacts', 'Nature', 'Noise', 'Organelles', 'Organism', 'Pattern', 'Plant Roots', 'Research', 'Resolution', 'Sampling', 'Shapes', 'Specimen', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Tomogram', 'Training', 'Validation', 'Variant', 'Visualization software', 'Work', 'algorithmic methodologies', 'automated segmentation', 'base', 'beta pleated sheet', 'computer code', 'cryogenics', 'data warehouse', 'deep learning', 'denoising', 'density', 'electron tomography', 'experience', 'feature detection', 'fitness', 'flexibility', 'fundamental research', 'heuristics', 'high standard', 'image reconstruction', 'improved', 'interest', 'learning network', 'macromolecular assembly', 'novel', 'particle', 'prevent', 'process optimization', 'programs', 'reconstruction', 'structured data', 'theories', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2020,313572,-0.023435691466007005
"FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control PROJECT SUMMARY In this proposal we plan to contribute addressing the above foundational and operational challenges by advancing the science of influenza modeling and contributing novel methods and data sources that will increase the accuracy and availability of seasonal and pandemic influenza models. To address these challenges, we plan to build on the unique mechanistic spatially structured modeling approaches developed by our consortium, that includes stochastic metapopulation models and fully developed agent-based models nested together in our global epidemic and mobility modeling (GLEAM) approach. The objective of this project is to generate novel and actionable scientific insights from dynamic transmission models of influenza transmission that effectively integrate key socio-demographic indicators of the focus population, as well as a wide spectrum of pharmaceutical and non-pharmaceutical interventions. Our proposed work in specific aim 1 (A1) will leverage our global modeling (from the global to local scale) framework that can be used to explore the multi-year impact of influenza vaccination, antiviral prophylaxis/treatment, and community mitigation during influenza seasons and pandemics. Our specific aim 2 (A2) will focus on using high quality data to model heterogeneous transmission drivers and novel contact pattern stratifications that will allow us to guide mitigation strategies and prioritization for interventions. In our Aim 3 (A3) we will use artificial intelligence approaches to identify interventions that are particularly synergistic and well-suited to particular epidemic scenarios, for seasonal and pandemic influenza. Our overarching goal is to provide a modeling portfolio with flexible and innovative mathematical and computational approaches. We aim to address several questions commonly asked about seasonal and pandemic influenza and match these with analytical methods and outbreak projections. The modeling and data developed in this project can help facilitate and justify transparent public health decisions, while contributing to the definition of standard methods for model selection and validation. Finally, our influenza modeling platform can also benefit the broader network of modeling teams and can be used to improve result sharing and harmonization of modeling approaches. The objective of this proposal is to advance the science of modeling and contribute novel methods and data analytics tools that will increase the understanding of seasonal and pandemic influenza in the context of the network of modeling teams coordinated by the CDC. To address these challenges, we plan to develop a novel global modeling framework, contribute new data and methods for improve the accuracy and validation of flu modeling approaches, and evolve successful methodologies to advance the analysis of layered intervention with artificial Intelligence.",FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control,10071782,U01IP001137,[' '],NCIRD,NORTHEASTERN UNIVERSITY,U01,2020,371721,0.011642857925857316
"Novel Statistical Inference for Biomedical Big Data Project Summary This project develops novel statistical inference procedures for biomedical big data (BBD), including data from diverse omics platforms, various medical imaging technologies and electronic health records. Statistical inference, i.e., assess- ing uncertainty, statistical signiﬁcance and conﬁdence, is a key step in computational pipelines that aim to discover new disease mechanisms and develop effective treatments using BBD. However, the development of statistical inference procedures for BBD has lagged behind technological advances. In fact, while point estimation and variable selection procedures for BBD have matured over the past two decades, existing inference procedures are either limited to simple methods for marginal inference and/or lack the ability to integrate biomedical data across multiple studies and plat- forms. This paucity is, in large part, due to the challenges of statistical inference in high-dimensional models, where the number of features is considerably larger than the number of subjects in the study. Motivated by our team's extensive and complementary expertise in analyzing multi-omics data from heterogenous studies, including the TOPMed project on which multiple team members currently collaborate, the current proposal aims to address these challenges. The ﬁrst aim of the project develops a novel inference procedure for conditional parameters in high-dimensional models based on dimension reduction, which facilitates seamless integration of external biological information, as well as biomedical data across multiple studies and platforms. To expand the application of this method to very high-dimensional models that arise in BBD applications, the second aim develops a data-adaptive screening procedure for selecting an optimal subset of relevant variables. The third aim develops a novel inference procedure for high-dimensional mixed linear models. This method expands the application domain of high-dimensional inference procedures to studies with longitu- dinal data and repeated measures, which arise commonly in biomedical applications. The fourth aim develops a novel data-driven procedure for controlling the false discovery rate (FDR), which facilitates the integration of evidence from multiple BBD sources, while minimizing the false negative rate (FNR) for optimal discovery. Upon evaluation using ex- tensive simulation experiments and application to multi-omics data from the TOPMed project, the last aim implements the proposed methods into easy-to-use open-source software tools leveraging the R programming language and the capabilities of the Galaxy workﬂow system, thus providing an expandable platform for further developments for BBD methods and tools. Public Health Relevance Biomedical big data (BBD), including large collections of omics data, medical imaging data, and electronic health records, offer unprecedented opportunities for discovering disease mechanisms and developing effective treatments. However, despite their tremendous potential, discovery using BBD has been hindered by computational challenges, including limited advances in statistical inference procedures that allow biomedical researchers to investigate uncon- founded associations among biomarkers of interest and various biological phenotypes, while integrating data from multiple BBD sources. The current proposal bridges this gap by developing novel statistical machine learning methods and easy-to-use open-source software for statistical inference in BBD, which are designed to facilitate the integration of data from multiple studies and platforms.",Novel Statistical Inference for Biomedical Big Data,9969887,R01GM133848,"['Address', 'Adoption', 'Behavioral', 'Big Data Methods', 'Biological', 'Biological Assay', 'Biological Markers', 'Code', 'Collection', 'Communities', 'Computer software', 'Data', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Galaxy', 'Genetic study', 'Goals', 'Heart', 'Imaging technology', 'Individual', 'Linear Models', 'Measurement', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Outcome', 'Phenotype', 'Procedures', 'R programming language ', 'Research Personnel', 'Sample Size', 'Scientist', 'Screening procedure', 'Software Tools', 'Structure', 'System', 'Testing', 'Trans-Omics for Precision Medicine', 'Uncertainty', 'Work', 'base', 'big biomedical data', 'computational pipelines', 'data integration', 'design', 'diverse data', 'effective therapy', 'experimental study', 'heterogenous data', 'high dimensionality', 'interest', 'machine learning method', 'member', 'novel', 'open source', 'public health relevance', 'screening', 'simulation', 'statistical and machine learning', 'structured data', 'tool', 'treatment strategy', 'user friendly software']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,456980,-0.004555595396358849
"Combining Voice and Genetic Information to Detect Heterogeneity in Major Depressive Disorder PROJECT SUMMARY This application aims to advance our understanding of major depressive disorder (MDD) by combining genetic information and analyzing speech patterns of those with MDD to identify subtypes. MDD is the leading cause of disability throughout the world, yet, relative to other common disorders, less is known about its origins. There are less effective treatments and much less is spent on trying to understand how it arises and how to cure it. Current treatments are relatively ineffective, with up 50% of patients refractory and many suffering severe recurrence. Understanding the mechanisms underlying MDD has been recognized as a grand challenge in global mental health. Thus, developing new treatments for MDD is a major priority for public health. A major challenge for MDD research is the presence of heterogeneity. The existence of multiple subtypes of MDD has been suspected for a long time, and likely confounds the ability to treat the disorder appropriately with existing treatments, as well as making it hard to identify the causes of MDD as a prelude to developing new treatments. However finding subtypes has been hard. Given that the way people talk can reflect alterations in mood, we expect voice to be able to predict mood, and hence potentially be used as biomarker to recognize heterogeneity. In preliminary data show that in combination with genetic data high-dimensional vocal features extracted from recordings can be used to identify subtypes. Furthermore, the use of genetic data allows us to impute voice features into large biobanks where no recordings exist, making it possible to explore the relationship between vocal features and a rich array of clinically important indicators. We explore the power of voice to make a diagnosis of MDD, to predict severity and other clinical features. Applying our approach to will inform clinical management, improving diagnosis, refine treatment and aid the development of new treatments PROJECT NARRATIVE The research proposed here will contribute to an understanding of major depressive disorder, the commonest psychiatric disorder and a leading cause of disability throughout the world. The proposal will combine information from voice recordings and genetics to identify subtypes of depression and develop robust predictors of mood, severity of illness and other clinical indicators. Our research will thereby provide new insights into disease, and well enable the more effective targeting of therapy to those who will most benefit at the appropriate time.",Combining Voice and Genetic Information to Detect Heterogeneity in Major Depressive Disorder,9943508,R01MH122569,"['Affect', 'Alleles', 'Anxiety', 'Behavioral Genetics', 'Biological Markers', 'Biology', 'Case-Control Studies', 'China', 'Chinese People', 'Classification', 'Clinical', 'Clinical Management', 'Collection', 'Data', 'Data Set', 'Depressed mood', 'Development', 'Diagnosis', 'Disease', 'Disease remission', 'Engineering', 'Ensure', 'Far East', 'Frequencies', 'Genetic', 'Genetic study', 'Genomics', 'Genotype', 'Heritability', 'Heterogeneity', 'Interview', 'Investigation', 'Linkage Disequilibrium', 'Major Depressive Disorder', 'Manuals', 'Maps', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Moods', 'Morphologic artifacts', 'Neurobiology', 'Participant', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacological Treatment', 'Phenotype', 'Population', 'Psychiatry', 'Psychological Transfer', 'Recurrence', 'Refractory', 'Research', 'Resources', 'Sampling', 'Scheme', 'Severities', 'Severity of illness', 'Signal Transduction', 'Specificity', 'Speech', 'Suicide', 'System', 'Testing', 'Time', 'Ursidae Family', 'Voice', 'Voice Quality', 'Woman', 'accurate diagnosis', 'base', 'biobank', 'clinical application', 'clinical phenotype', 'comorbidity', 'computer science', 'data sharing', 'deep neural network', 'disability', 'disorder subtype', 'effective therapy', 'flexibility', 'genetic analysis', 'genetic architecture', 'genetic information', 'genetic predictors', 'improved', 'innovation', 'insight', 'long short term memory', 'multidimensional data', 'neuroimaging', 'preservation', 'psychogenetics', 'public health priorities', 'statistics', 'targeted treatment', 'trait', 'treatment response', 'vector']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,733478,-0.016276756303457852
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9962426,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Pooling', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'multiple data types', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,323659,-0.003158192652285
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9970407,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2020,453846,-0.0010415836612067647
"Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences Abstract A major approach in causal inference literature aimed at mitigating bias due to unmeasured confounding is the so- called instrumental variable (IV) design which relies on identifying a variable which (i) influences the treatment process, (ii) has no direct effect on the outcome other than through the treatment, and (iii) is independent of any unmeasured confounder. IV methods are very well developed and widely used in social and health science, although validity of IV inferences may not be reliable if any of required assumptions (i)-(iii) is violated. This proposal aims to develop (a) new IV methods robust to violation of any of (i)-(iii); (b) New negative control methods that can be used to detect and sometimes to nonparametrically account for unmeasured confounding bias; (c) New bracketing methods for partial inference about causal effects in comparative interrupted time series studies. The proposed methods will be used to address current scientific queries in three major substantive public health areas:(1) to understand the health effects of air pollution; (2) to quantify the causal effects of modifiable risk factors for Alzheimer's disease and related disorders; (3) To uncover the mechanism by which a randomized package of interventions produced a substantial reduction of HIV incidence in a recent major cluster randomized trial of treatment as prevention in Botswana, Africa. Our proposal will provide the best available analytical methods to date to resolve confounding concerns in these high impact public health applications and more broadly in observational studies in the health sciences. Summary This proposal aims to develop new causal inference methods to tame bias due to hidden confounding factors in obser- vational studies as well as in randomized experiments subject to non-adherence. The proposed methods are firmly grounded in modern semiparametric theory which will be used to obtain more robust and efficient inferences about causal effects in a broad range of public health applications including in Epidemiology of Aging, Environmental Health Epidemiology and HIV/AIDS Prevention.",Novel Designs and Methods to Remove Hidden Confounding Bias in Health Sciences,9859751,R01AG065276,"['AIDS prevention', 'Address', 'Adherence', 'Africa', 'Aging', 'Air Pollution', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Area', 'Blood Pressure', 'Botswana', 'Clinical Treatment', 'Cluster randomized trial', 'Data', 'Diabetes Mellitus', 'Disease', 'Environmental Health', 'Epidemiology', 'Genetic', 'HIV', 'Health', 'Health Sciences', 'Incidence', 'Interruption', 'Intervention', 'Learning', 'Linkage Disequilibrium', 'Literature', 'Machine Learning', 'Masks', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Observational Study', 'Outcome', 'Participant', 'Prevention', 'Process', 'Public Health', 'Public Health Applications Research', 'Randomized', 'Research Design', 'Research Personnel', 'Risk Factors', 'Series', 'Social Sciences', 'Testing', 'Thromboplastin', 'Time', 'ambient air pollution', 'analytical method', 'c new', 'comparative', 'design', 'experimental study', 'genetic variant', 'high dimensionality', 'intervention effect', 'modifiable risk', 'mortality', 'novel', 'pleiotropism', 'semiparametric', 'simulation', 'theories', 'treatment effect', 'uptake', 'user friendly software']",NIA,UNIVERSITY OF PENNSYLVANIA,R01,2020,502013,-0.03290485759673561
"Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents PROJECT ABSTRACT  Antimicrobial resistance is a critical public health issue. Infections with drug resistant pathogens are estimated to cause an additional eight million hospitalization days annually over the hospitalizations that would be seen for infections with susceptible agents. The use of antibiotics (in both clinical and agricultural settings) is being viewed as precursor for these infections and thus, is a major public health concern—particularly as outbreaks become more frequent and severe. However, scientiﬁc evidence describing the hazards associated with antibiotic use is lacking due to inability to quantify the risk of these practices. One promising avenue to elucidate this risk is to use shotgun metagenomics to identify the AMR genes in samples taken through systematic spatiotemporal surveillance. The goal of this proposed work is to develop algorithms that will provide such a means for analysis. The algorithms need to be scalable to very large datasets and thus, will require the development and use succinct data structures.  In order to achieve this goal, the investigative team will develop the theoretical foundations and applied meth- ods needed to study AMR through the use of shotgun metagenomics. A major focus of the proposed work is developing algorithms that can handle very large datasets. To achieve this scalability, we will create novel means to create, compress, reconstruct and update very large de Bruijn graphs that metagenomics data in a manner needed to study AMR. In addition, we will pioneer the study of AMR through long read data by proposing new algorithmic problems and solutions that use data. For example, identifying the location of speciﬁc genes in a metagenomics sample using long read data has not been proposed or studied. Thus, the algorithmic ideas and techniques developed in this project will not only advance the study of AMR, but contribute to the growing domain of big data analysis and pan-genomics.  Lastly, we plan to apply our methods to samples collected from both agricultural and clinical settings in Florida. Analysis of preliminary and new data will allow us to conclude about (1) the public risk associated with antimicro- bial use in agriculture; (2) the effectiveness of interventions used to reduce resistant bacteria, and lastly, (3) the factors that allow resistant bacteria to grow, thrive and evolve. A–1 PROJECT NARRATIVE  Antibiotic use in agriculture is a major public health concern that is receiving a lot of media attention, par- ticularly as antibiotic-resistant infections in become more frequent and severe. This research will build a novel bioinformatics framework for determining how antimicrobial resistant genes evolve, grow, and persist in a system that has been affected by antibiotic use. This will, in turn, facilitate the development of effective intervention methods that reduce resistant pathogens in clinical and agricultural settings. N–1",Developing Computational Methods for Surveillance of Antimicrobial Resistant Agents,9828618,R01AI141810,"['Affect', 'Agriculture', 'Algorithms', 'Antibiotic Resistance', 'Antibiotics', 'Antimicrobial Resistance', 'Attention', 'Bacteria', 'Base Pairing', 'Big Data', 'Bioinformatics', 'Clinical', 'Collaborations', 'Combating Antibiotic Resistant Bacteria', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Compression', 'Data Set', 'Development', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Florida', 'Food production', 'Foundations', 'Genes', 'Genomics', 'Goals', 'Graph', 'Hospitalization', 'Infection', 'International', 'Investigation', 'Length', 'Location', 'Measures', 'Memory', 'Metagenomics', 'Methods', 'Monitor', 'Noise', 'Organism', 'Pathogenicity', 'Plasmids', 'Prevention', 'Public Health', 'Research', 'Resistance', 'Risk', 'Sampling', 'Shotguns', 'Surveillance Methods', 'System', 'Techniques', 'Time', 'Translating', 'Update', 'Work', 'antibiotic resistant infections', 'bacterial resistance', 'base', 'combinatorial', 'drug resistant pathogen', 'effective intervention', 'foodborne outbreak', 'genetic variant', 'hazard', 'improved', 'large datasets', 'machine learning algorithm', 'method development', 'microbial', 'microbiome analysis', 'microbiome research', 'multiple datasets', 'novel', 'pathogen', 'petabyte', 'reconstruction', 'research and development', 'resistance gene', 'spatiotemporal', 'standard care', 'structured data']",NIAID,UNIVERSITY OF FLORIDA,R01,2020,422334,-0.01059400044206242
"MUFA-SIRT1 signaling as a central node regulating healthspan PROJECT SUMMARY Macronutrients serve a multitude of roles beyond provision of energy, with numerous nutrients and/or their downstream metabolites acting as signaling molecules to coordinate cellular metabolism and function. Indeed, numerous nutrient sensing pathways (e.g. mTOR, AMPK and sirtuins) have evolved allowing us to respond to specific nutrients/metabolites, which in turn impacts healthspan. Sirtuins are largely thought to be driven by redox, whereby high levels of NAD, a cofactor in the sirtuin reaction and indicator of low energy charge, drives sirtuin-catalyzed deacylation of target proteins. SIRT1, the most-studied sirtuin, is a key nutrient sensing node that regulates a plethora of cellular functions to promote lifespan extension and healthy aging. As a result, there is immense interest in the use of SIRT1 activating compounds (STACs) to prevent or treat a wide range of aging-related disease. The links between dietary macronutrients, nutrient sensing and healthspan have historically focused upon caloric or protein restriction with limited attention given to dietary lipids. However, a small and growing body of literature has linked monounsaturated fatty acids (MUFAs) to improved healthspan. In addition to positive effects on lifespan and healthy aging in model organisms, dietary MUFAs have been linked to wide-ranging health benefits in epidemiological studies and, since they are a primary constituent of olive oil, thought to contribute to the benefits of the Mediterranean Diet. Despite these studies, little is known about the biological underpinnings through which MUFAs elicit their beneficial health effects. We have previously shown that lipid droplet catabolism (i.e. lipolysis) increases SIRT1 and downstream PGC-1a/PPAR- a signaling as a means to increase mitochondrial biogenesis and function during times of nutrient deprivation. Our preliminary data show for the first time that MUFAs released specifically from lipolysis are trafficked to the nucleus where they allosterically activate SIRT1 towards select acetylated peptide substrates. This discovery makes MUFAs the first-known endogenous allosteric activators of SIRT1. Moreover, we show that MUFAs activate SIRT1 through a similar mechanism to resveratrol suggesting that MUFA signaling may modulate the response to exogenous SIRT1 activators. Based on these preliminary data, the objective of this application is to further characterize the role of MUFAs as endogenous SIRT1 activators. We hypothesize that MUFAs selectively activate SIRT1 to modulate the response to numerous dietary interventions known to impact healthspan. To test our objective, we propose the following aims: Aim 1: To define how MUFAs modulate SIRT1 substrate selectivity. Aim 2: To characterize the SIRT1-dependent effects of MUFAs/olive oil on healthspan. Aim 3: To determine the contribution of MUFAs in mediating the response to STACs or caloric restriction. Upon completion of the proposes studies, we will have further expanded our understanding of SIRT1 biology allowing for refined approaches to activate SIRT1 to promote healthy aging. NARRATIVE The proposed studies will advance our understanding into the underlying biology linking dietary factors to healthspan. The data gleaned from these studies will help refine therapeutic or nutritional avenues to modulate lifespan and aging-related diseases resulting in a direct, positive impact on human health.",MUFA-SIRT1 signaling as a central node regulating healthspan,10092409,R01AG069768,"['Aging', 'Animal Model', 'Animals', 'Attention', 'Biogenesis', 'Biological', 'Biology', 'Caloric Restriction', 'Catabolism', 'Cell Nucleus', 'Cell physiology', 'Charge', 'Clinical Trials', 'Data', 'Deacetylation', 'Development', 'Diet', 'Dietary Factors', 'Dietary Fats', 'Dietary Intervention', 'Disease', 'Dose', 'FRAP1 gene', 'Fasting', 'Glean', 'Gold', 'Health', 'Health Benefit', 'Human', 'Link', 'Lipids', 'Lipolysis', 'Literature', 'Longevity', 'Machine Learning', 'Macronutrients Nutrition', 'Maps', 'Mediating', 'Mediterranean Diet', 'Metabolism', 'Mitochondria', 'Modeling', 'Monounsaturated Fatty Acids', 'Mus', 'Nutrient', 'Nutritional', 'Oils', 'Olive oil preparation', 'Olives - dietary', 'Outcome', 'Oxidation-Reduction', 'PPAR alpha', 'Pathway interactions', 'Peptides', 'Pharmacologic Substance', 'Proteins', 'Proteomics', 'Reaction', 'Research', 'Resveratrol', 'Role', 'SIRT1 gene', 'Signal Transduction', 'Signaling Molecule', 'Sirtuins', 'Source', 'Testing', 'Therapeutic', 'Time', 'Work', 'analog', 'base', 'cofactor', 'deacylation', 'detection of nutrient', 'epidemiology study', 'healthspan', 'healthy aging', 'improved', 'innovation', 'interest', 'middle age', 'mutant mouse model', 'novel', 'nutrient deprivation', 'polyphenol', 'prevent', 'red wine', 'response']",NIA,UNIVERSITY OF MINNESOTA,R01,2020,315700,-0.016579816504127137
"Estimating Mediation Effects in Prevention Studies The purpose of this competing continuation grant proposal is to develop, evaluate and apply  methodological and statistical procedures to investigate how prevention programs change outcome  variables. These mediation analyses assess the link between program effects on the constructs targeted  by a prevention program and effects on the outcome. As noted by many researchers and federal  agencies, mediation analyses identify the most effective program components and increase  understanding of the underlying mechanisms leading to changing outcome variables. Information from  mediation analysis can make interventions more powerful, more efficient, and shorter. The P. I. of this grant received a one-year NIDA small grant and four multi-year grants to develop and evaluate mediation  analysis in prevention research. This work led to many publications and innovations. The proposed  five-year continuation focuses on the further development and refinement of exciting new mediation  analysis statistical developments. Four statistical topics represent next steps in this research and include  analytical and simulation research as well as applications to etiological and prevention data. The work expands on our development of causal mediation and Bayesian mediation methods that hold great promise for mediation analysis. In Study 1, practical causal mediation and Bayesian mediation analyses  for research designs are developed and evaluated. This approach will clarify methods and develop  approaches for dealing with violation of testable and untestable assumptions. Study 2 investigates  important measurement issues for the investigation of mediation. This work will focus on methods to identify critical facets of mediating variables, approaches to understanding whether mediators and  outcomes are redundant, and develop methods for studies with big data. Study 3 continues the development and evaluation of new longitudinal mediation methods for ecological momentary assessment data and other studies with massive data collection. These new methods promise to more accurately model change over time for both individuals and groups of individuals. Study 4 develops methods to  uncover subgroups in mediation analysis including causal mediation methods, multilevel models, and new  approaches based on residuals for identifying individuals for whom mediating processes differ in  effectiveness from other individuals. For each study, we will investigate unique issues with mediation analysis of prevention data including methods for small N and also massive data collection (big data), the RcErLitEicVaANl rCoEle(Soeef imnsetruacstiounrse):ment for mediating mechanisms, and the application of the growing literature on  causal methods and Bayesian methods. Study 5 applies new statistical methods to data from several NIH  The project further develops a method, statistical mediation analysis, that extracts more information from  funded prevention studies providing important feedback about the usefulness of the methods. Study 6  research. Mediation analysis explains how and why prevention and treatments are successful. Mediation  disseminates new information about mediation analysis through our website and other media, by  analysis improves prevention and treatment so that their effects are greater and even cost less. communication with researchers, and publications from the project. n/a",Estimating Mediation Effects in Prevention Studies,9851457,R37DA009757,"['Address', 'Alcohol or Other Drugs use', 'Applications Grants', 'Bayesian Method', 'Behavioral Mechanisms', 'Big Data', 'Biological Models', 'Communication', 'Complex', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Ecological momentary assessment', 'Educational workshop', 'Effectiveness', 'Etiology', 'Evaluation', 'Feedback', 'Funding', 'Grant', 'Individual', 'Individual Differences', 'Intervention', 'Investigation', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Outcome', 'Persons', 'Prevention', 'Prevention Research', 'Prevention program', 'Principal Investigator', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Residual state', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Testing', 'Time', 'Translating', 'United States National Institutes of Health', 'Work', 'base', 'computer program', 'cost', 'data space', 'design', 'dynamic system', 'improved', 'innovation', 'interest', 'longitudinal design', 'model design', 'multilevel analysis', 'novel strategies', 'programs', 'simulation', 'successful intervention', 'theories', 'therapy design', 'tool', 'treatment research', 'web site']",NIDA,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R37,2020,382893,0.02908349489527154
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9965720,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2020,741796,-0.02201862869038289
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9855035,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'disease phenotype', 'driver mutation', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'machine learning method', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistical learning', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,305167,0.018509227725715983
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9911975,R01AR068456,"['3-Dimensional', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Visualization', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'osteoporosis with pathological fracture', 'patient response', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2020,276227,0.0025080198162745434
"Refining and Validating Borderline Personality Disorder Phenotypes Through Factor Mixture Modeling The proposed research seeks to clarify the symptomatic heterogeneity of borderline personality disorder (BPD) by examining BPD phenotypes through advanced latent variable modeling. A second, innovative aim is to validate these findings through intensive longitudinal assessment in daily life. BPD is associated with high rates of emergency room visits and costly healthcare service utilization, affecting 10-20% of psychiatric outpatients and 20-40% of psychiatric inpatients. BPD also contributes to impaired social and occupational functioning and significant suicide risk, with 1 in 10 individuals with BPD completing suicide. Recent research has aimed to enhance treatment effectiveness for BPD by identifying prototypical patterns of symptom manifestation that may suggest ideographic treatment targets. However, no research has simultaneously included: a) a sufficiently large patient sample; b) ecologically sound validation of results; and c) use of appropriate statistical techniques. The proposed project builds on this research through two aims. Aim 1: Utilize a model comparison approach to identify BPD phenotypes in a large psychiatric outpatient sample assessed via semi-structured diagnostic interviews (Study 1). Aim 2: Validate the results of Study 1 by applying phenotype classification algorithms produced in Study 1 to a smaller sample of patients who have completed 21 days of momentary surveys on symptoms and clinical outcomes (Study 2). To address Aim 1, factor mixture modeling (FMM)—a novel, flexible, and integrative latent variable modeling approach—will be compared to standard factor analysis and latent class analysis in order to evaluate the dimensional and categorical structure of BPD. We expect a single-factor, multi-class FMM will best explain heterogeneity in BPD, over and above other sources of heterogeneity (e.g., gender, comorbidity). To address Aim 2, we will use a prototype-matching approach to algorithmically assign patients in the validation sample to phenotypes identified in Aim 1 and determine their predictive validity in terms of daily clinical outcomes. Results of this project will provide empirically grounded personalized prediction tools for BPD intervention and treatment development, in line with the NIMH’s goal of “developing, testing, and refining tools and methodologies… for personalized risk and trajectory prediction and intervention.” This fellowship will allow the applicant to receive tailored consultation from experts in methodology, data analysis, and BPD theory and assessment, as well as advanced statistical training and grantsmanship courses and workshops. This training will be enhanced by the resource-rich environment and explicit support of student research and funding provided by the Pennsylvania State University, as well as the support of Dr. Kenneth Levy and his lab. This promising young researcher will gain training in computational modeling, proficiency in working with “big data,” increased understanding of conceptual and nosological models of BPD, and further skills in disseminating research findings through publication and presentation, as vital steps towards an independent research career in translational clinical science. PROJECT NARRATIVE The proposed research aims to elucidate the underlying psychopathology of borderline personality disorder (BPD), a prevalent, costly, and deadly psychiatric condition, through the identification of ecologically valid phenotypes of the disorder. Accurate identification of BPD phenotypes promises to reveal patterns of symptoms that can be targeted in treatment development, improving treatment effectiveness for this impairing disorder. This research will support scientifically and clinically useful algorithms for personalized intervention, enhancing treatment outcomes and reducing the overall burden of BPD.",Refining and Validating Borderline Personality Disorder Phenotypes Through Factor Mixture Modeling,9911299,F31MH121020,"['Address', 'Affect', 'Algorithms', 'Archives', 'Assessment tool', 'Behavior', 'Big Data', 'Biological Markers', 'Borderline Personality Disorder', 'Categories', 'Cellular Phone', 'Clinical', 'Clinical Sciences', 'Computer Models', 'Consultations', 'Data', 'Data Analyses', 'Diagnostic', 'Dimensions', 'Disease', 'Ecological momentary assessment', 'Educational workshop', 'Emergency department visit', 'Emotional', 'Ensure', 'Environment', 'Factor Analysis', 'Fellowship', 'Foundations', 'Funding', 'Gender', 'Goals', 'Health Care Costs', 'Heterogeneity', 'Impairment', 'Individual', 'Inpatients', 'Intervention', 'Interview', 'Life', 'Machine Learning', 'Mental disorders', 'Methodology', 'Modeling', 'Monitor', 'National Institute of Mental Health', 'Occupational', 'Onset of illness', 'Outcome', 'Outcome Study', 'Outpatients', 'Patients', 'Pattern', 'Pennsylvania', 'Phenotype', 'Prediction of Response to Therapy', 'Process', 'Psychopathology', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Scientist', 'Self Concept', 'Severities', 'Source', 'Structure', 'Students', 'Suicide', 'Surveys', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment outcome', 'Universities', 'Validation', 'Variant', 'Work', 'accomplished suicide', 'career', 'classification algorithm', 'comorbidity', 'cost', 'data archive', 'effective therapy', 'flexibility', 'health care service utilization', 'improved', 'innovation', 'novel', 'personalized intervention', 'personalized medicine', 'personalized predictions', 'prospective', 'prototype', 'psychologic', 'recruit', 'response', 'skills', 'social', 'sound', 'success', 'suicidal risk', 'theories', 'therapy development', 'tool']",NIMH,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,F31,2020,25358,-0.05055955789068418
"Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration PROJECT SUMMARY The cell nucleus is a heterogeneous organelle that consists of nuclear bodies such as nuclear lamina, speckles, nucleoli and PML bodies. These structures continuously tether and tug chromatin at the small and large scales to synergistically orchestrate dynamic functions in distinct spatio-temporal compartments. A major obstacle to the production of navigable 4D reference maps and relating structure to function in the nucleus remains understanding how these different scales of organization influence each other. In particular, we have a poor understanding of the large-scale genome organization. Growing evidence suggests that such nuclear compartmentalization is causally connected with vital genome functions in human health and disease. However, the principles of this nuclear compartmentalization, its dynamics during changes in cell conditions, and its functional relevance are poorly understood. One lesson from Phase 1 4DN was the huge gap in throughput between imaging methods, that directly measure large-scale multi-landmark relationships, and genomic methods, that aim for whole genome high-resolution maps but are indirect measurements and provide limited information about large-scale compartments. For this 4DN UM1 Center application, we propose to meet these needs through the following Aims: (1) Generate multi-modal imaging and genomic datasets to reveal the structure, dynamics, and function of nuclear compartmentalization; (2) Develop and apply computational tools for data-driven genome structure modeling and integrative analysis of nuclear compartmentalization; (3) Develop an integrative analysis and visualization platform with navigable 4D reference maps of nuclear organization. The combined datasets and results of our proposed approaches will advance our understanding of nuclear compartmentalization, the interwoven connections among different nuclear components, and their functional significance. Our new integrative analysis tools and data-driven predictive models will produce more complete nuclear organization reference maps that integrate large-scale chromosome structure data from live and super-resolution microscopy with multi-modal genomic data including smaller scale chromatin interaction maps and predict functional relationships and dynamic responses. Our navigable reference maps will be publicly accessible through an analysis platform that provides interactive visualization of multiple data types, thus enabling investigators with diverse expertise to simultaneously explore their own data and related datasets/tools and promoting collaborations that will open new horizons into the role of the 4D nucleome in human health and disease. PROJECT NARRATIVE The proposed research is relevant to public health because it will enhance our understanding of nuclear genome organization and functions that are increasingly being linked to health and disease. Because we develop tools to disseminate this information and enable others to work with our data and their own data, we will also bring nuclear architecture to bear on a broad range of ongoing health related research. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration,10156141,UM1HG011593,"['Address', 'Architecture', 'Atlases', 'Binding', 'Biochemical', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Chromatin', 'Chromatin Loop', 'Chromatin Structure', 'Chromosome Structures', 'Chromosomes', 'Collaborations', 'Communities', 'Complement', 'Computing Methodologies', 'Cytology', 'DNA Replication Timing', 'Data', 'Data Set', 'Development', 'Disease', 'Formulation', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Interphase Chromosome', 'Intuition', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Methods', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nuclear', 'Nuclear Lamina', 'Nuclear Structure', 'Organelles', 'Outcome', 'Output', 'Phase', 'Population', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Structural Models', 'Structure', 'Technology', 'Three-Dimensional Imaging', 'United States National Institutes of Health', 'Ursidae Family', 'Validation', 'Variant', 'Visualization', 'Work', 'base', 'cell cycle genetics', 'cell type', 'computer framework', 'computerized tools', 'data exploration', 'data integration', 'data tools', 'experimental study', 'genome-wide', 'genomic data', 'histone modification', 'imaging modality', 'improved', 'insight', 'machine learning algorithm', 'mental function', 'multimodal data', 'multimodality', 'multiple data types', 'multiscale data', 'predictive modeling', 'response', 'spatiotemporal', 'structured data', 'tool', 'transcription factor', 'transcriptome sequencing', 'user-friendly', 'whole genome']",NHGRI,CARNEGIE-MELLON UNIVERSITY,UM1,2020,2075409,-0.02623547560617776
"Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study PROJECT SUMMARY ABSTRACT The overarching goals of this R01 proposal are to improve scientific understanding of potential mechanisms by which ophthalmic diseases lead to the risk of Alzheimer’s disease. The investigators will leverage modern ophthalmic data with state-of-the-art imaging and extensive archived clinical data from a well-characterized cohort of older adults. The investigators propose to examine the effect of structural and functional changes in retina and longitudinal severity of ophthalmic diseases on Alzheimer’s disease and related neuropathology. The proposal builds on the resources of the Adult Changes in Thought (ACT) study, a prospective longitudinal, population-based, dementia-free cohort of over 5,500 people to date established in 1994 which has detected >1,014 research quality diagnoses of Alzheimer’s disease and >1,254 dementia to date. ACT follows consenting participants to autopsy and has performed state-of-the arts autopsy on >781 decedents to date. In this extremely well-characterized cohort, the investigators found that several ophthalmic diseases (diabetic retinopathy, glaucoma, age-related macular degeneration) are significantly associated with the risk of developing Alzheimer’s disease. The investigators will use three advanced ophthalmic imaging modalities at both home and clinical research study visits: fundus photography, optical coherence tomography (OCT), and OCT angiography (OCTA), to obtain quantitative data relevant to these ophthalmic diseases. The study team will establish the distribution (Aim 1a) and 2- and 4-year evolution of ophthalmic imaging characteristics found in older adults in the community and determine associations with change in cognition (Aim 1 b, c). Additionally, magnetic resonance imaging (MRI) and MRI angiography (MRA) will be obtained in a subset of participants to investigate the contribution of small (retinal) and large (cerebral) vascular disease towards cognitive changes (Aim 1d). The study team will continue ACT study’s strong commitment for meaningful data sharing. In collaboration with the Laboratory of Neuro Imaging at University of Southern California, the study team will promulgate these ophthalmic data in addition to neuroimaging data to the research community (Aim 1e). In Aim 2, the investigators will use extensive clinical ophthalmology data captured over many decades and incorporate them in novel longitudinal models of eye disease severity. The investigators will analyze eye disease severity along with extensive neuropathology data from the ACT study, including both standard (Aim 2a) and novel quantitative (Aim 2b) neuropathology data, to further scientific understanding of neuropathological mechanisms underlying associations between eye conditions and Alzheimer’s disease risk. The brain is not amenable to direct observations during life. In contrast, the eye is an anterior extension of the central nervous system and may provide a valuable window to illuminate neurodegenerative processes in the aging brain. Proposed investigations will substantially enhance scientific understanding of the role of modern ophthalmic evaluations in delineating risk of Alzheimer's disease and other forms of neuropathology. PROJECT NARRATIVE Using a large, well-characterized, longitudinal, prospective, cohort study, the study team previously found that diabetic retinopathy, glaucoma, and age-related macular degeneration were significantly associated with Alzheimer’s disease risk. The team proposes to use three cutting edge ophthalmic imaging modalities to obtain quantitative data at both home and clinic research study visits in addition to MRI and MRA in a subset of participants to evaluate their associations with change in cognition over time (Aim 1). The team will leverage extensive ophthalmic clinical and neuropathological data already available for 781 study participants to date as well as new state-of-the-art quantitative measures of beta amyloid (Aβ1-42) and phosphorylated tau to elucidate mechanisms underlying associations between ophthalmic conditions and Alzheimer’s disease (Aim 2).",Aging eyes and aging brains in studying alzheimer's disease: Modern ophthalmic data collection in the adult changes in thought (ACT) study,10005108,R01AG060942,"['Adult', 'Age related macular degeneration', 'Age-associated memory impairment', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease diagnosis', 'Alzheimer&apos', 's disease risk', 'Amyloid beta-Protein', 'Angiography', 'Anterior', 'Archives', 'Autopsy', 'Bayesian Modeling', 'Biological Markers', 'Blood Vessels', 'Brain', 'California', 'Cerebrovascular Disorders', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cognition', 'Collaborations', 'Communities', 'Consent', 'Data', 'Data Collection', 'Dementia', 'Diabetic Retinopathy', 'Diagnosis', 'Disease', 'Disease model', 'Drusen', 'Elderly', 'Evaluation', 'Evolution', 'Eye', 'Eye diseases', 'Fundus', 'Fundus photography', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Home environment', 'Image', 'Impaired cognition', 'Investigation', 'Laboratories', 'Lead', 'Life', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Microvascular Dysfunction', 'Modeling', 'Modernization', 'Nerve Degeneration', 'Nerve Fibers', 'Neuraxis', 'Occipital lobe', 'Ophthalmology', 'Optical Coherence Tomography', 'Participant', 'Pathology', 'Perfusion', 'Persons', 'Predisposition', 'Process', 'Prospective cohort study', 'Provider', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Retina', 'Risk', 'Role', 'Scanning', 'Selection Bias', 'Series', 'Severities', 'Severity of illness', 'Structure', 'Technology', 'Testing', 'Time', 'Universities', 'Vascular Diseases', 'Visit', 'aging brain', 'area striata', 'automated algorithm', 'base', 'cognitive change', 'cohort', 'data sharing', 'deep learning', 'diagnosis quality', 'epidemiology study', 'fiber cell', 'follow-up', 'high risk', 'imaging biomarker', 'imaging modality', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuropathology', 'novel', 'paired helical filament', 'population based', 'prospective', 'public repository', 'research study', 'resilience', 'tau Proteins', 'tau-1', 'vascular contributions']",NIA,UNIVERSITY OF WASHINGTON,R01,2020,3349913,-0.07313002067785726
