text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Automated domain adaptation for clinical natural language processing Project Summary Automatic extraction of useful information from clinical texts enables new clinical research tasks and new technologies at the point of care. The natural language processing (NLP) systems that perform this extraction rely on supervised machine learning. The learning process uses manually labeled datasets that are limited in size and scope, and as a result, applying NLP systems to unseen datasets often results in severely degraded performance. Obtaining larger and broader datasets is unlikely due to the expense of the manual labeling process and the difficulty of sharing text data between multiple different institutions. Therefore, this project develops unsupervised domain adaptation algorithms to adapt NLP systems to new data. Domain adaptation describes the process of adapting a machine learning system to new data sources. The proposed methods are unsupervised in that they do not require manual labels for the new data. This project has three aims. The first aim makes use of multiple existing datasets for the same task to study the differences in domains, and uses this information to develop new domain adaptation algorithms. Evaluation uses standard machine learning metrics, and analysis of performance is tightly bounded by strong baselines from below and realistic upper bounds, both based on theoretical research on machine learning generalization. The second aim develops open source software tools to simplify the process of incorporating domain adaptation into clinical text processing workflows. This software will have input interfaces to connect to methods developed in Aim 1 and output interfaces to connect with Apache cTAKES, a widely used open- source NLP tool. Aim 3 tests these methods in an end-to-end use case, adverse drug event (ADE) extraction on a dataset of pediatric pulmonary hypertension notes. ADE extraction relies on multiple NLP systems, so this use case is able to show how broad improvements to NLP methods can improve downstream methods. This aim also creates new manual labels for the dataset for an end-to-end evaluation that directly measures how improvements to the NLP systems lead to improvement in ADE extraction. Project Narrative Software systems that use machine learning to understand clinical text often suffer severe performance loss when they are applied to new data that looks different than the data that they originally learned from. In this project, we develop and implement methods that allow these systems to automatically adapt to the characteristics of a new data source. We evaluate these methods on the clinical research task of adverse drug event detection, which relies on many different variables found in the text of electronic health records.",Automated domain adaptation for clinical natural language processing,9768545,R01LM012918,"['Adult', 'Adverse drug event', 'Algorithms', 'Apache', 'Area', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Colon Carcinoma', 'Communities', 'Computer software', 'Computers', 'Conceptions', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Dimensions', 'Ecosystem', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation', 'Human', 'Institution', 'Knowledge', 'Label', 'Language', 'Lead', 'Learning', 'Linguistics', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Network-based', 'Output', 'Pathology', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Pulmonary Hypertension', 'Radiology Specialty', 'Research', 'Software Tools', 'Source', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'TimeLine', 'Training', 'Update', 'Vision', 'Work', 'base', 'case finding', 'improved', 'learning strategy', 'malignant breast neoplasm', 'method development', 'natural language', 'neural network', 'new technology', 'news', 'novel', 'open source', 'point of care', 'side effect', 'social media', 'software systems', 'statistics', 'supervised learning', 'tool', 'tumor', 'unsupervised learning']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2019,383874,0.012589197505892616
"Improving Specialty Care Delivery in the Safety Net with Natural Language Processing Project Summary  Safety net providers treat a substantial share of socioeconomically vulnerable patients in their communities, but struggle to provide timely access to high quality specialty care for their patients. Delayed access to specialty care is associated with worse health outcomes and potentially contributes to health disparities across socioeconomic groups. Given their limited resources, safety net providers must seek creative approaches to improve specialty access. However, to choose what programs to implement, safety net providers need to understand the specialty care needs of their populations. Fortunately, the adoption of eConsult systems by safety net providers across the US provides a valuable opportunity to systematically measure patterns of specialty care referrals for minority, underserved populations.  In this project, we propose using state-of-the-art methods in machine learning and natural language processing (NLP) to help safety net providers extract actionable, population wide data from their electronic consultation systems. We will do this in partnership with three of the most prominent safety net health systems in the US in Los Angeles, San Francisco and New York City. Using specialty request databases from our collaborators, we will build NLP systems to automatically classify specialty requests along two dimensions: the “clinical issue” motivating the request (e.g., chest pain), and the “question type” (e.g., request for a procedure, help with medication management). This automated classification of electronic specialty requests can enable identification of promising targets for interventions to improve specialty access and quality of care.  After developing these NLP systems, we will analyze >1 million specialty requests to describe trends in how safety net patients are referred to specialists and examine variation in referral patterns by clinic and individual provider. The goal is to identify the most impactful opportunities to improve specialty access and quality. For example, a high rate of referrals for esophageal reflux, which most PCPs can treat on their own with specialist guidance, could lead to new treatment algorithms, potentially reducing the need for these requests and improving access for other patients.  This proposal is a “high-risk high-reward” project that creates new research tools to identify and evaluate data-driven interventions to improve specialty care delivery for underserved populations. Project Narrative Access to timely, high-quality specialty care is a fundamental component of a well-functioning health system, yet safety net health care providers face persistent challenges delivering such care. Quality improvement efforts to improve specialty access have been thwarted in part because safety net providers lack the data to understand a basic question – why patients are referred for specialty care. Taking advantage of the growing use of electronic specialty referral systems by safety net providers, we propose using natural language processing to conduct automated analysis and classification of specialty requests in safety net populations, which will enable the design of targeted interventions to improve specialty care access and delivery.",Improving Specialty Care Delivery in the Safety Net with Natural Language Processing,9789060,R21MD012693,"['Acute', 'Adopted', 'Adoption', 'Algorithms', 'Caring', 'Chest Pain', 'Chronic Kidney Failure', 'Classification', 'Clinic', 'Clinical', 'Communities', 'Consultations', 'County', 'Data', 'Data Set', 'Databases', 'Education', 'Epidemiology', 'Face', 'Federally Qualified Health Center', 'Gastroesophageal reflux disease', 'Goals', 'Health', 'Health Personnel', 'Health Services', 'Health Services Accessibility', 'Health system', 'Heart failure', 'Hospitals', 'Improve Access', 'Individual', 'Intervention', 'Lead', 'Los Angeles', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Education', 'Medical center', 'Medication Management', 'Methods', 'Minority', 'Morbidity - disease rate', 'Natural Language Processing', 'New York City', 'Online Systems', 'Ophthalmology', 'Outcome', 'Patients', 'Pattern', 'Play', 'Population', 'Primary Care Physician', 'Procedures', 'Provider', 'Public Hospitals', 'Quality of Care', 'Research', 'Resources', 'Retinal Diseases', 'Role', 'San Francisco', 'Specialist', 'System', 'Taxonomy', 'Telemedicine', 'Text', 'Time', 'Transplantation', 'Triage', 'Underserved Population', 'Variant', 'Visit', 'automated analysis', 'care delivery', 'design', 'diabetic', 'disease classification', 'ethnic minority population', 'follow-up', 'health disparity', 'high reward', 'high risk', 'improved', 'medical specialties', 'medically underserved', 'minority communities', 'mortality', 'performance tests', 'programs', 'racial and ethnic', 'safety net', 'screening', 'socioeconomic disadvantage', 'socioeconomics', 'tool', 'trend', 'two-dimensional']",NIMHD,BOSTON CHILDREN'S HOSPITAL,R21,2019,94263,0.00939439874836095
"Learning Universal Patient Representations from Clinical Text with Hierarchical Recurrent Neural Networks Project Summary In this project we develop new methods for extracting important information from electronic health records based on recurrent neural networks. These methods represent the hierarchical and sequential nature of human language, leverage large scale datasets to make learning sophisticated representations possible, and make use of novel sources of supervision that are available at this scale. The model architecture we propose is a hierarchical recurrent neural network (RNN). This architecture explicitly represents temporality at multiple different time scales, with stacked RNN layers representing words, sentences, paragraphs, and documents. At the word level, the model is trained to predict important pieces of clinical information, such as negation and temporality, using existing labeled data sets. Training for clinical information extraction at the lowest level ensures that the higher-level models have a foundation of medically relevant inputs. We are still left with the challenge of training higher-level networks, because these models require massive amounts of labeled training data to learn. We solve this problem by taking advantage of the temporal aspect of information in an EHR, and having each higher-level recurrent layer train getting supervision from the future. For example, the document RNN is trained to predict billing codes and NLP concept codes that were found in the subsequent document. This source of supervision is scalable, and our preliminary data shows that it is effective at learning how to generate generalizable patient representations. The patient representations that our model learns are shareable across multiple tasks, potentially streamlining EHR-based research by eliminating what was previously a manual step – designing text-based variables to represent patients. We demonstrate a new workflow for text-based EHR research, showing how the same representations can be used for two completely distinct phenotyping tasks. These phenotyping studies make use of high-quality datasets of patients with pulmonary hypertension and autism spectrum disorder at Boston Children’s Hospital. PH is relatively rare, so finding every patient with a phenotyping algorithm is important for clinical research. ASD has several sub-phenotypes, and finding large numbers of patients from each sub- phenotype can help to better understand the mechanisms of ASD. Along with demonstrating the applicability of our representations on these specific clinical research use cases, we incorporate our patient representations into the i2b2 clinical research software, making them available to all clinical investigators using this platform at Boston Children’s Hospital. Project Narrative This project develops methods for extracting universal patient representations from unstructured text in electronic health records. These methods leverage huge amounts of clinical data, recurrent neural network architectures, and novel training techniques to incorporate information at multiple time scales. These methods are evaluated using public datasets to promote reproducibility, and applied to clinical research tasks that extend the knowledge of patients with pulmonary hypertension and autism spectrum disorder at Boston Children’s Hospital.",Learning Universal Patient Representations from Clinical Text with Hierarchical Recurrent Neural Networks,9642922,R01LM012973,"['Algorithms', 'Architecture', 'Boston', 'Brain', 'Childhood', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Research', 'Code', 'Comorbidity', 'Computer software', 'Data', 'Data Set', 'Electronic Health Record', 'Ensure', 'Event', 'Face', 'Felis catus', 'Foundations', 'Future', 'Healthcare Systems', 'Human', 'Human Characteristics', 'Human Resources', 'Intensive Care Units', 'Israel', 'Knowledge', 'Label', 'Language', 'Learning', 'Left', 'Linguistics', 'Location', 'Logistic Regressions', 'Machine Learning', 'Manuals', 'Medical', 'Medical center', 'Methods', 'Modeling', 'Natural Language Processing', 'Neural Network Simulation', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phenotype', 'Problem Solving', 'Process', 'Pulmonary Hypertension', 'Rare Diseases', 'Records', 'Recurrence', 'Reproducibility', 'Research', 'Research Personnel', 'Source', 'Statistical Methods', 'Supervision', 'System', 'Text', 'Time', 'Training', 'Training Technics', 'Uncertainty', 'autism spectrum disorder', 'base', 'clinically relevant', 'cohort', 'data resource', 'deep neural network', 'design', 'disease phenotype', 'learning strategy', 'machine translation', 'neural network', 'neural network architecture', 'novel', 'recurrent neural network', 'relating to nervous system']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2019,386910,0.0008928203646112171
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9645433,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2019,92070,0.013148180497727814
"Big Data Methods for Comprehensive Similarity based Risk Prediction Project Summary Electronic health records (EHR) provide rich source of data about representative populations and are yet to be fully utilized to enhance clinical decision-making. Conventional approaches in clinical decision-making start with the identification of relevant biomarkers based on subject-matter knowledge, followed by detailed but limited analysis using these biomarkers exclusively. As the current scientific literature indicates, many human disorders share a complex etiological basis and exhibit correlated disease progression. Therefore, it is desirable to use comprehensive patient data for patient similarity. This proposal focuses on deriving a comprehensive and integrated score of patient similarity from complete patient characteristics currently available, including but not limited to 1) demographic similarity; 2) genetic similarity; 3) clinical phenotype similarity; 4) treatment similarity; and 5) exposome similarity (here exposome defined as all available attributes of the living environment an individual is exposed to), when some of the aspects may overlap and interact. We will optimize information fusion and task-dependent feature selection for assessing patient similarity for clinical risk prediction. Since currently there does not exist a pipeline that is able to extract executable complete patient determinant data, to achieve the research goal described above, we propose first deliver an open- source data preparation pipeline that is based on a widely used clinical data standard, the OMOP (Observational Medical Outcomes Partnership) Common Data Model (CMD) version 5.2. Moreover, to mitigate common missingness and sparsity challenges in clinical data, we describe the first attempt to represent patients' sparse clinical information with missingness, including diagnosis information, medication data, treatment intervention, with a fixed-length feature vector (i.e. the Patient2Vec). This project has four specific aims. Aim 1 is to develop a clinical data processing pipeline for harmonizing patient information from multiple sources into a standards-based uniformed data representation and to evaluate its efficiency, interoperability, and accuracy. Aim 2 is to leverage a powerful machine learning technique, Document2Vec, from the natural language processing literature, to create an open-source Patient2Vec framework for the derivation of informative numerical representations of patients. Aim 3 is to develop a unified machine learning clinical- outcome-prediction framework for Optimized Patient Similarity Fusion (OptPSF) that integrates traditional medical covariates with the derived numerical patient representations from Patient2Vec (Aim 2) for improved clinical risk prediction. Aim 4 is to evaluate our similarity framework for predicting 1) the risk of end-stage kidney disease (ESKD) in general EHR patient population and 2) the risk of death among patients with chronic kidney disease (CKD). The project focus on developing a novel data science pipeline which includes a clinical data processing pipeline to format comprehensive patient health determinants from a variety of sources of clinical, genomic, socioenvironmental data, and a clinical-outcome-prediction framework that optimally fuses relevant patient health determinants to define patient similarity for improved clinical risk predictions.",Big Data Methods for Comprehensive Similarity based Risk Prediction,9687065,R01LM013061,"['Address', 'Automation', 'Big Data', 'Big Data Methods', 'Biological Markers', 'Biological Process', 'Biometry', 'Case Study', 'Characteristics', 'Chronic', 'Chronic Disease', 'Chronic Kidney Failure', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complex', 'Data', 'Data Reporting', 'Data Science', 'Derivation procedure', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronic Health Record', 'End stage renal failure', 'Environment', 'Etiology', 'Exhibits', 'Exposure to', 'Genetic', 'Genomics', 'Goals', 'Health', 'Health Professional', 'Healthcare', 'Heterogeneity', 'Human', 'Individual', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Knowledge', 'Length', 'Life', 'Literature', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Preparation', 'Reporting', 'Reproducibility', 'Research', 'Risk', 'Source', 'Surveys', 'Techniques', 'base', 'biomedical informatics', 'clinical decision support', 'clinical decision-making', 'clinical phenotype', 'clinical risk', 'computerized data processing', 'data modeling', 'design', 'disease diagnosis', 'health data', 'improved', 'interoperability', 'mortality risk', 'novel', 'open data', 'open source', 'outcome prediction', 'patient population', 'precision medicine', 'predict clinical outcome', 'socioeconomics', 'support tools', 'vector']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,448622,-0.0005807661037539767
"Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records PROJECT SUMMARY  Today, more patients can access their health records online than ever before. However, clinical acronyms hinder patients' comprehension of their records and decrease the benefits of transparency. An automated system for expanding clinical acronyms should have major clinical significance and far-reaching consequences for improving patient-provider communication, shared decision-making, and health outcomes. Existing systems have limited power to expand clinical acronyms, primarily due to the lack of comprehensiveness (or generali- zability) of existing acronym sense inventories. Because developing comprehensive sense inventories is difficult, existing knowledge engineering methods have primarily focused on developing institution-specific sense inventories. Institution-specific sense inventories may not be generalizable to other geographical regions and medical specialties. Furthermore, developing an institution-specific sense inventory at every US healthcare organization is not feasible, especially without automated methods which currently do not exist.  I developed advanced knowledge engineering methods to overcome these limitations through the use of fully automated techniques to generalize existing sense inventories from different geographical regions and medical specialties. My methods leverage the extensive resources already devoted to developing institution- specific sense inventories in the U.S., and may help generalize existing sense inventories to institutions without the resources to develop them. Although promising, challenges remain with the optimization and evaluation of these methods. The objective of the proposed project is to use knowledge engineering to improve patients' comprehension of their health records, focusing specifically on clinical acronyms. In Aim 1, I will develop new knowledge engineering methods to facilitate the automated integration of sense inventories, using literature- based quality heuristics and a Siamese neural network to establish synonymy. I will evaluate these methods using multiple metrics to assess redundancy, quality, and coverage in two test corpora with over 17 million clinical notes. In Aim 2, I will evaluate whether the knowledge engineering methods improve comprehension of doctors' notes in 60 hospitalized patients with advanced heart failure. With success, I will create novel, automated knowledge engineering methods that can be directly applied to improve patient care. This research is in support of my mentored doctoral training at Columbia University Department of Biomedical Informatics (DBMI) under Drs. David Vawdrey, George Hripcsak, Carol Friedman, Suzanne Bakken, and Chunhua Weng, and will include coursework on deep learning, oral presentations at major annual conferences, and career development planning, among other activities. DBMI is frequently recognized as one of the oldest and best programs of its kind in the world, and provides an exception training environment for my development into an independent and productive academic investigator. PROJECT NARRATIVE Clinical acronyms make it difficult for patients to understand their medical records, decreasing the benefits of transparency. This project applies advanced knowledge engineering methods and machine learning to generate comprehensive acronym sense inventories used to aid consumers' comprehension of their health records. The project is in support of the applicant's mentored doctoral dissertation research.",Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records,9681711,F31LM013054,"['Abbreviations', 'Award', 'Clinical', 'Clinical Medicine', 'Comprehension', 'Controlled Vocabulary', 'Development', 'Development Plans', 'Engineering', 'Environment', 'Equipment and supply inventories', 'Evaluation', 'Future', 'Geographic Locations', 'Goals', 'Grant', 'Health', 'Healthcare', 'Heart failure', 'Hospitals', 'Informatics', 'Information Resources', 'Institution', 'Knowledge', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Mentors', 'Mentorship', 'Methods', 'Natural Language Processing', 'Oral', 'Outcome', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Positioning Attribute', 'Publishing', 'Questionnaires', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Safety', 'Source', 'Support System', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Unified Medical Language System', 'Universities', 'acronyms', 'base', 'biomedical informatics', 'career development', 'clinically significant', 'deep learning', 'doctoral student', 'federal policy', 'health care service organization', 'health record', 'heuristics', 'improved', 'information organization', 'medical specialties', 'method development', 'multidisciplinary', 'neural network', 'novel', 'patient portal', 'patient-clinician communication', 'programs', 'shared decision making', 'success', 'symposium', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,F31,2019,50016,0.009671118362497032
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,9774338,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Infrastructure', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Meta-Analysis', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Research', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phrases', 'portability', 'preservation', 'recruit', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2019,1521748,0.020017800382485663
"Statistical Methods for Incorporating Machine Learning Tools in Inference and Large-Scale Surveillance using Electronic Medical Records Data SUMMARY The modernization and standardization of clinical care information systems is creating large networks of linked electronic health records (EHR) that capture key treatments and select patient outcomes for millions of patients throughout the country. The observational data emerging from these systems provide an unparalleled opportunity to learn about the effectiveness of existing and novel treatments, and to monitor potential safety issues that may arise when interventions are used in broad patient populations. However, observational clinical data have exposures that are driven by many factors and therefore aggressive adjustment is needed to remove as much confounding bias as possible in order to make attribution regarding select exposures. The field of machine learning provides a powerful collection of data-driven approaches for performing flexible, thorough confounding adjustment, but performing reliable statistical inference is particularly challenging when these techniques are used as part of the analytic strategy. We propose to advance reproducible research methods by developing and illustrating novel targeted learning tools that leverage the flexibility of machine learning methods to detect and characterize health effect signals using large-scale EHR data. Specifically, we will first develop techniques for making efficient, statistically valid and robust inference for treatment effects using state-of-the-art machine learning tools. We will also develop online learning techniques to make such inference in the context of streaming EHR data. Methodological advances will enable us to formulate a formal, rigorous and practical framework for conducting continuous, effective and reliable surveillance for safety endpoints. Finally, we will develop statistical approaches for incorporating prior information -- including demographic, epidemiologic or pharmacodynamic knowledge, for example -- to improve health effect estimation and inference when the health outcome of interest is rare and the statistical problem is thus difficult, as often occurs in safety surveillance. The ultimate goal of the proposed research is to enable biomedical researchers and public health regulators to carefully monitor and protect the health of the public by allowing them to more effectively and more reliably detect critical health effect signals that may be contained in population-scale EHR data. PROJECT NARRATIVE The modernization and standardization of clinical care information systems is creating large networks of linked electronic medical records that capture key treatments and select patient outcomes for millions of U.S. subjects. The population scale of contemporary health care data is opening new opportunities for quickly learning from observational data, and is now supporting on-going national surveillance that will monitor the risks and benefits of both existing and novel treatment paths. The objective of this proposal is to provide an inferential framework that leverages the flexibility of machine learning methods to detect health effect signals, including in the important setting of high-dimensional confounders and/or rare events, and to develop a real-time sequential updating methodology for safety signal detection.",Statistical Methods for Incorporating Machine Learning Tools in Inference and Large-Scale Surveillance using Electronic Medical Records Data,9816009,R01HL137808,"['Algorithms', 'Benefits and Risks', 'Characteristics', 'Clinical Data', 'Complex', 'Computer software', 'Computerized Medical Record', 'Confidence Intervals', 'Country', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Dimensions', 'E-learning', 'Early Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Estimation Techniques', 'Event', 'Frequencies', 'Goals', 'Health', 'Healthcare', 'Heterogeneity', 'Information Systems', 'Infrastructure', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modernization', 'Monitor', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Pharmacodynamics', 'Population', 'Population Surveillance', 'Procedures', 'Public Health', 'Records', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Safety', 'Sentinel', 'Signal Transduction', 'Standardization', 'Statistical Methods', 'Stream', 'Structural Models', 'Subgroup', 'Surveillance Program', 'System', 'Techniques', 'Testing', 'Time', 'Treatment outcome', 'Update', 'base', 'clinical care', 'comparative treatment', 'flexibility', 'high dimensionality', 'improved', 'interest', 'learning strategy', 'national surveillance', 'novel', 'open source', 'patient population', 'patient subsets', 'risk minimization', 'software development', 'surveillance data', 'tool', 'treatment effect', 'user-friendly']",NHLBI,UNIVERSITY OF WASHINGTON,R01,2019,502815,0.03754270073101666
"Multi-objective representation learning methods for interpetable predictions of patient outcomesusing electronic health records Project Summary/Abstract This project proposes new methods for representing data in electronic health records (EHR) to improve pre- dictive modeling and interpretation of patient outcomes. EHR data offer a promising opportunity for advancing the understanding of how clinical decisions and patient conditions interact over time to inﬂuence patient health. However, EHR data are difﬁcult to use for predictive modeling due to the various data types they contain (con- tinuous, categorical, text, etc.), their longitudinal nature, the high amount of non-random missingness for certain measurements, and other concerns. Furthermore, patient outcomes often have heterogenous causes and re- quire information to be synthesized from several clinical lab measures and patient visits. The core challenge at hand is overcoming the mismatch between data representations in the EHR and the assumptions underly- ing commonly used statistical and machine learning (ML) methods. To this end, this project proposes novel wrapper-based methods for learning informative features from EHR data. Both methods propose specialized operators to handle sequential data, time delays, and variable interactions, and have the capacity to discover underlying clinical rules/decisions that affect patient outcomes. Importantly, both methods also produce archives of possible models that represent the best trade-offs between complexity and accuracy, which assists in model interpretation. These method advances are made possible by encoding a rich set of data operations as nodes in a directed acyclic graph, and optimizing the graph structures using multi-objective optimization. The central hypothesis of this research is that multi-objective optimization can learn effective data representations from the EHR to produce accurate, explanatory models of patient outcomes. Preliminary work has shown that these methods can effectively learn low-order data representations that improve the predictive ability of several state- of-the-art ML methods. This technique demonstrates good scaling properties with high-dimensional biomedical data. Aim 1 (K99) is to develop a multi-objective feature engineering method that pairs with existing ML methods to iteratively improve their performance by constructing new features from the raw data and using feedback from the trained model to guide feature construction. In Aim 2 (K99), this method is applied to form predictive models of the risk of heart disease and heart failure using longitudinal EHR data. The resultant models will be inter- preted with the help of mentors in order to translate predictions into clinical recommendations. For Aim 3 (R00), a second method is proposed that uses a similar framework to optimize existing neural network approaches in order to simplify their structure as much as possible while maintaining accuracy. The goal of Aim 4 (R00) is to identify hospital patients who are at risk of readmission and propose point-of-care strategies to mitigate that risk. This goal is facilitated through the application of the proposed methods to patient data collected from the Hospital of the University of Pennsylvania, the Geisinger Health System, and publicly available EHR databases. Project Narrative  Understanding how clinical decisions interact with a patient's health and environmental over time to inﬂuence patient outcomes is central to the goals of enhancing health, reducing illness and improving quality of life. The proposed research provides important methodological advances for extracting these insights from widely available patient health records.",Multi-objective representation learning methods for interpetable predictions of patient outcomesusing electronic health records,9744166,K99LM012926,"['Address', 'Affect', 'Archives', 'Area', 'Automobile Driving', 'Cardiovascular Diseases', 'Categories', 'Clinical', 'Communities', 'Complex', 'Couples', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Development', 'Disease', 'Electronic Health Record', 'Engineering', 'Feedback', 'Goals', 'Graph', 'Hand', 'Health', 'Health Sciences', 'Health system', 'Heart Diseases', 'Heart failure', 'Hospitals', 'Inpatients', 'Knowledge', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical Records', 'Mentors', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Nature', 'Outcome', 'Pathology', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Pennsylvania', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Process', 'Property', 'Protocols documentation', 'Quality of life', 'Recommendation', 'Replacement Arthroplasty', 'Research', 'Research Personnel', 'Risk', 'Structure', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'University Hospitals', 'Visit', 'Work', 'base', 'care costs', 'cluster computing', 'data archive', 'deep learning', 'deep neural network', 'design', 'disease diagnosis', 'disorder subtype', 'health record', 'heart disease risk', 'high dimensionality', 'hospital readmission', 'improved', 'insight', 'learning strategy', 'network architecture', 'neural network', 'novel', 'open source', 'operation', 'point of care', 'predictive modeling', 'readmission rates', 'readmission risk', 'tool']",NLM,UNIVERSITY OF PENNSYLVANIA,K99,2019,89260,0.04414177805538221
"National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives Project Summary and Abstract Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Natural language processing (NLP) technologies can extract this information and convert it in to a structured format that is more readily accessible by computerized systems. However, the development of NLP systems is contingent on access to relevant data and EHRs are notoriously difficult to obtain because of privacy reasons. Despite the recent efforts to de-identify and release narrative EHRs for research, these data are still very rare. As a result, clinical NLP, as a field has lagged behind. To address this problem, since 2006, we organized thirteen shared tasks, accompanied with workshops and journal publications. Twelve of these shared tasks have focused on the development of clinical NLP systems and the remaining one on the usability of these systems. We have covered both depth and breadth in terms of shared tasks, preparing tasks that study cutting-edge NLP problems on a variety of EHR data from multiple institutions. Our shared tasks are the longest running series of clinical NLP shared tasks, with ever growing EHR data sets, tasks, and participation. Our most popular three data sets have been cited 495 (2010 data), 284 (2006 de-id data), and 274 (2009 data) times, respectively, representing hundreds of articles that have come out of these three data sets alone. Our goal in this proposal is to continue the efforts we started in 2006 under i2b2 shared task challenges (i2b2, NIH NLM U54LM008748, PI: Kohane and R13 LM011411, PI: Uzuner) to de-identify EHRs, annotate them with gold- standard annotations for clinical NLP tasks, and release them to the research community for the development and head-to-head comparison of clinical NLP systems, for the advancement of the state of the art. Continuing our efforts under National NLP Clinical Challenges (n2c2) based at the Health Data Science program of the newly established Department of Biomedical Informatics at Harvard Medical School, we aim to form partnerships with the community to grow the shared task efforts in several ways: (1) grow the available de-identified EHR data sets through partnerships that can contribute to the volume and variety of the data, and (2) grow the available gold-standard annotations in terms of depth and breadth of NLP tasks. Given these aims and partnerships, we plan to hold a series of shared tasks. We will complement these shared tasks with workshops that meet in conjunction with the Fall Symposium of the American Medical Informatics Association and with journal special issues so that advancement of the state of the art can be sped up and future generations can build on the past. Project Narrative We propose to organize a series of shared tasks, workshops, and journal publications for fostering the continuous development of clinical Natural Language Processing (NLP) technologies that can extract information from narratives of Electronic Health Records (EHRs). Our aim is to grow the annotated gold standard EHR data sets that are available to the research community through partnerships and to bring together clinical NLP researchers with informatics researchers for building collaborations. We will engage the community in shared tasks and disseminate the knowledge generated by these shared tasks through workshops and journal special issues for the advancement of the state of the art.",National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives,9759499,R13LM013127,"['Access to Information', 'Address', 'American', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Community Developments', 'Complement', 'Data', 'Data Science', 'Data Set', 'Development', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Future', 'Future Generations', 'Goals', 'Gold', 'Grant', 'Growth', 'Hand', 'Head', 'Healthcare', 'Improve Access', 'Individual', 'Informatics', 'Institution', 'Israel', 'Journals', 'Knowledge', 'Measures', 'Medical Informatics', 'Medical center', 'Methodology', 'Natural Language Processing', 'Outcome', 'Paper', 'Peer Review', 'Performance', 'Privacy', 'Publications', 'Publishing', 'Records', 'Research', 'Research Personnel', 'Rest', 'Running', 'Series', 'Source', 'Structure', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'clinical development', 'computerized', 'falls', 'head-to-head comparison', 'health data', 'indexing', 'medical schools', 'meetings', 'practical application', 'programs', 'symposium', 'usability', 'working group']",NLM,GEORGE MASON UNIVERSITY,R13,2019,20000,0.015645435850698625
"Discovering and Applying Knowledge in Clinical Databases PROJECT SUMMARY / ABSTRACT The long-term goal of our ongoing project, “Discovering and applying knowledge in clinical databases,” is to learn from data in the electronic health record (EHR) and to apply that knowledge to understand and improve health. The EHR, because of its broad capture of human health, greatly amplifies our ability to carry out observational research, opening the possibility of covering emerging problems, diverse populations, rare diseases, and chronic diseases in long-term longitudinal studies. Unfortunately, the strength of EHR data—its breadth and flexible nature—imposes additional challenges. We have found that the biggest challenge comes from the inaccuracy, incompleteness, complexity, and resulting bias inherent in the recording of the health care process. We previously showed that health care process bias exists to the extent, for example, that simple use of the data can create signals implying the opposite of what we know to be true. One of the most important factors is sparse, irregular sampling; we found that sampling bias can be reduced by reparameterizing time and that prediction techniques that can accommodate EHR-specific data and resist their biases like data assimilation can be used on EHR data to produce good estimates of glucose and HA1c. The previous cycle of this project produced 75 publications. We propose to develop methods to accommodate health care process bias, using both knowledge engineering and experience with health care process bias as well as advanced statistical techniques that employ dynamical models and latent variables. We hypothesize that heuristics and models combined with knowledge can improve our ability to generate inferences and learn phenotypes despite health care process bias. Our aims are as follows: (1) Taking a knowledge engineering approach, study the effect of preprocessing and analytic choices on reducing health care process bias, and using machine learning techniques, learn more about health care process bias. (2) Taking a more empirical approach, use dynamic latent factor modeling and variation inference to accommodate health care process bias, learning how a patient's health state and health processes affect censoring, exploiting information from many variables at once. (3) Use data assimilation and mechanistic models to learn otherwise unmeasurable physiologic phenotypes despite irregular, sparse sampling typical of electronic health records. (4) Use the developed models and generated phenotypes to answer clinical questions, and disseminate the results. PROJECT NARRATIVE This project studies the biases that health care processes bring to electronic health record data, and it develops methods to overcome those biases to improve reuse of the data for purposes such as clinical research and quality improvement.",Discovering and Applying Knowledge in Clinical Databases,9659641,R01LM006910,"['Active Learning', 'Adopted', 'Adverse drug effect', 'Affect', 'Algorithms', 'Area', 'Assimilations', 'Award', 'Chronic Disease', 'Clinical', 'Clinical Research', 'Complex', 'Data', 'Data Reporting', 'Electronic Health Record', 'Engineering', 'Functional disorder', 'Generations', 'Glucose', 'Goals', 'Health', 'Healthcare', 'Human', 'Insulin', 'Knowledge', 'Learning', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Metabolism', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Nonlinear Dynamics', 'Observational Study', 'Patients', 'Performance', 'Phenotype', 'Physiological', 'Physiology', 'Population Heterogeneity', 'Process', 'Publications', 'Rare Diseases', 'Research', 'Sampling', 'Sampling Biases', 'Signal Transduction', 'Structure', 'Techniques', 'Testing', 'Time', 'Training', 'Variant', 'Work', 'abstracting', 'clinical database', 'deep learning', 'experience', 'flexibility', 'heuristics', 'improved', 'novel', 'precision medicine', 'predictive modeling']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,596772,0.03496250850292797
"From enrichment to insights Project Summary Most medical decisions are made without the support of rigorous evidence in large part due to the cost and complexity of performing randomized trials for most clinical situations. In practice, clinicians must use their judgement, informed by their own and the collective experience of their colleagues. The advent of the electronic health record (EHR) enables the modern practitioner to algorithmically check the records of thousands or millions of patients to rapidly find similar cases and compare outcomes. In addition to filling the inferential gap in actionable evidence, these kinds of analyses avoid issues of ethics, practicality, and generalizability that plague randomized clinical trials (RCTs). Unfortunately, identifying patients with the appropriate phenotypes, properly leveraging available data to adjust results, and matching similar patients to reduce confounding remain critical challenges in every study that uses EHR data. Overcoming these challenges to improve the accuracy of observational studies conducted with EHR data is of paramount importance. Studies using EHR data begin by defining a set of patients with specific phenotypes, analogous to amassing a cohort for a clinical trial. This process of electronic phenotyping, is typically done via a set of rules defined by experts. Machine learning approaches are increasingly used to complement consensus definitions created by experts and we propose several advances to validate and improve this practice. We will explore and quantify the effects of feature engineering choices to transform the diagnoses, procedures, medications, laboratory tests and clinical notes in the EHR into a computable feature matrix. Finally, building on recent advances, we plan to characterize the performance of existing methods and develop EHR-specific strategies for patient matching. Our work is significant because we will take on three challenging problems--electronic phenotyping, feature engineering, and patient matching--that stand in the way of generating insights via EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes. Narrative The advent of the electronic health record (EHR) enables the search of thousands or millions of patients to rapidly find similar cases and compare outcomes. We will develop methods for feature engineering, electronic phenotyping and patient matching from real-world EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes.",From enrichment to insights,9759984,R01LM011369,"['Address', 'Algorithms', 'Area', 'Clinical', 'Clinical Trials', 'Code', 'Complement', 'Consensus', 'Data', 'Data Element', 'Data Set', 'Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evaluation', 'Frequencies', 'Future', 'Goals', 'Health system', 'Healthcare Systems', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Modernization', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Plague', 'Procedures', 'Process', 'Randomized Clinical Trials', 'Records', 'Resources', 'Scheme', 'Source', 'Statistical Data Interpretation', 'Test Result', 'Testing', 'Time', 'Training', 'Work', 'base', 'cohort', 'cost', 'electronic data', 'experience', 'health data', 'improved', 'innovation', 'insight', 'machine learning algorithm', 'novel', 'portability', 'randomized trial', 'simulation', 'treatment effect', 'vector']",NLM,STANFORD UNIVERSITY,R01,2019,640832,0.06356173447648324
"Investigating the documentation of E-cigarette use in the VA EHR PROJECT SUMMARY Electronic cigarettes were developed in China in the early 2000s and first introduced to the US market in 2007. Once established in the US, the product experienced explosive growth, with the number of electronic cigarette users doubling every year between 2008 and 2012. In 2012, it was estimated that 75% of US adults had heard of electronic cigarettes, and 8% had tried them. While electronic cigarettes have been studied over the last sev- eral years, no scientific consensus has emerged regarding either the safety of electronic cigarettes, or their po- tential as a smoking cessation aid. With this proposal, we will investigate how electronic cigarette use is documented in the Veterans Association Electronic Health Record, focusing specifically on the relationship between electronic cigarette use and com- bustible tobacco use, with the goal of understanding both how electronic cigarette use is documented in the context of the United States’ only nationwide health system, and how electronic cigarette related information can be reliably extracted from narrative clinical text using fully automated Natural Language Processing meth- ods. PROJECT NARRATIVE The proposed research focuses on the use of Natural Language Processing methods to automatically extract mentions of electronic cigarette use from the Veterans Association Electronic Health Record. The research will provide insight into important, currently unresolved questions regarding how clinicians record electronic cigarette use in the context of a nationwide health system, and whether patients report the use of electronic cigarettes as a smoking cessation aid or use the devices in conjunction with combustible tobacco.",Investigating the documentation of E-cigarette use in the VA EHR,9652537,R03DA047577,"['Address', 'Adult', 'Algorithms', 'American', 'Area', 'China', 'Cities', 'Clinical', 'Consensus', 'Dangerousness', 'Data', 'Data Set', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Electronic Nicotine Delivery Systems', 'Electronic cigarette', 'Environment', 'Epidemiology', 'Evaluation', 'Geography', 'Goals', 'Government', 'Growth', 'Health', 'Health system', 'Healthcare Systems', 'Hearing', 'Individual', 'Methods', 'Natural Language Processing', 'Patients', 'Pattern', 'Professional Organizations', 'Public Health', 'Public Health Applications Research', 'Reporting', 'Research', 'Risk', 'Safety', 'Scheme', 'Smoking', 'Sodium Chloride', 'Source', 'Structure', 'Technology', 'Text', 'Tobacco', 'Tobacco use', 'Tweens', 'United States', 'Universities', 'Utah', 'Variant', 'Veterans', 'Work', 'authority', 'electronic cigarette use', 'electronic cigarette user', 'electronic hookah', 'evidence base', 'experience', 'information model', 'innovation', 'insight', 'men', 'smoking cessation', 'success', 'systems research', 'tobacco control', 'tool', 'vaping']",NIDA,UNIVERSITY OF UTAH,R03,2019,76250,-0.0005753108772693665
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics. PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,9901995,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Base Sequence', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'DNA sequencing', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Elements', 'Ethics', 'Evaluation', 'Faculty', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Genes', 'Genetic', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Infrastructure', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Recommendation', 'Records', 'Research', 'Research Institute', 'Research Personnel', 'Ritalin', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'autism spectrum disorder', 'base', 'biobank', 'clinical care', 'clinical decision support', 'clinically actionable', 'cohort', 'cost', 'data modeling', 'data warehouse', 'database of Genotypes and Phenotypes', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'learning strategy', 'member', 'next generation sequencing', 'patient oriented', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'public health relevance', 'response', 'skills', 'social', 'success', 'support tools', 'targeted sequencing', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2019,742762,0.017226561382087276
"Efficient Statistical Learning Methods for Personalized Medicine Using Large Scale Biomedical Data Project Summary/Abstract  Current medical treatment guidelines largely rely on data from randomized controlled trials that study  average effects, which may be inadequate for making individualized decisions for real-world patients. Large-scale electronic health records (EHRs) data provide unprecedented opportunities to optimize personalized treatment strategies and generate evidence relevant to real-world patients. However, there are inherent challenges in the use of EHRs, including non-experimental nature of data collection processes, heterogeneous data types with complex dependencies, irregular measurement patterns, multiple dynamic treatment sequences, and the need to balance risk and benefit of treatments. Using two high-quality EHR databases, Columbia University Medical Center's clinical data warehouse and the Indiana Network for Patient Care database, and focusing on type 2 diabetes (T2D), this proposal will develop novel and scalable statistical learning approaches that overcome these challenges to discover optimal personalized treatment strategies for T2D from real-world patients. Specifically, under Aim 1, we will develop a unified framework to learn latent temporal processes for feature extraction and dynamic patient records representation. Our approach will accommodate large-scale variables of mixed types (continuous, binary, counts) measured at irregular intervals. They extract lower-dimensional components to reflect patients' dynamic health status, account for informative healthcare documentation processes, and characterize similarities between patients. Under Aim 2, we will develop fast and efficient multi-category machine learning methods, in order to evaluate treatment propensities and adaptively learn optimal dynamic treatment regimens (DTRs) among the extensive number of treatment options observed in the EHRs. The methods will provide  sequential decisions that determine the best treatment sequence for a T2D patient given his/her EHRs. Under Aim 3, we will develop statistical learning methods to assist multi-faceted treatment decision-making, which balances risks versus benefits when evaluating a DTR. Our approach will ensure maximizing benefit to the greatest extent while controlling all risk outcomes under the safety margins. For all aims, we will develop efficient stochastic resampling algorithms to scale up the optimization for massive data sizes. We will identify optimal DTRs for T2D using the extracted information from patients' comorbidity conditions, medications, and laboratory tests, as well as records-collection processes. Our methodologies will be applied and cross-validated between the two EHR databases. The treatment strategies learned from the representative EHR databases with a diverse patient  population will be beneficial for individual patient care, assisting clinicians to adaptively choose the optimal treatment for a patient. Finally, we will disseminate our methods and results through freely available software and outreach to the informatics and clinical experts at our Centers for Translational Science and elsewhere. Project Narrative  This proposal aims to develop novel and scalable statistical learning methods to analyze electronic health records (EHRs) and use two real-world, high-quality EHR databases for personalized medicine research. The methods will handle the non-experimental nature of data collection processes, along with heterogeneous data types, dynamic treatment sequences, and the trade-off between benefit and risk outcomes. The results will complement the current knowledge base for individual patient care using evidence generated from patients in real-world clinical practices.",Efficient Statistical Learning Methods for Personalized Medicine Using Large Scale Biomedical Data,9659349,R01GM124104,"['Academic Medical Centers', 'Address', 'Adverse event', 'Algorithms', 'Benefits and Risks', 'Categories', 'Center for Translational Science Activities', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Databases', 'Decision Making', 'Dependence', 'Dimensions', 'Documentation', 'Electronic Health Record', 'Ensure', 'Equilibrium', 'Exclusion Criteria', 'Formulation', 'Gaussian model', 'Goals', 'Health', 'Health Status', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Indiana', 'Informatics', 'International', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Process', 'Quality Control', 'Randomized Controlled Trials', 'Records', 'Research', 'Risk', 'Safety', 'Sampling', 'Structure', 'Testing', 'Time', 'Treatment Protocols', 'adaptive learning', 'adverse event risk', 'algorithmic methodologies', 'analytical tool', 'base', 'big biomedical data', 'clinical data warehouse', 'clinical decision-making', 'clinical practice', 'data modeling', 'data space', 'design', 'evidence base', 'individual patient', 'individualized medicine', 'knowledge base', 'learning strategy', 'novel', 'optimal treatments', 'outreach', 'patient population', 'personalized decision', 'personalized medicine', 'population based', 'scale up', 'temporal measurement', 'theories', 'treatment effect', 'treatment guidelines', 'treatment response', 'treatment strategy']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,329020,0.020845512188004574
"Improve medical decision making in emergency medicine via a new interface to visualize and understand relevant prior history for complex patients at the point-of-care. PROJECT SUMMARY/ABSTRACT Patient Insight is working to drive more informed medical decisions by designing a real-time interface that dynamically captures and presents prior history for complex patients in less time and with more completeness than using their electronic health record (EHR) system. The company is developing proprietary data mining and natural language processing algorithms that show promise for identifying, aggregating, and displaying the most essential EHR data for patients with complex conditions while at the same time working with clinicians to design the visual interface to reduce the cognitive burden and support diagnosis and treatment decisions. The specific goals of this phase I project are to test the feasibility of developing Patient Insight’s core solution to (1) use advanced analytics and natural language processing to mine and prioritize the most relevant information for use by (ED) physicians and (2) develop an clinician-informed, visual display of that essential information to timely support clinical decision-making sufficient to reduce medical errors, improve health, and control costs. In phase I, Patient Insight will work with Universal Health Systems, the parent company of George Washington University Hospital, to co-develop and test the platform for efficacy in the ED as a precursor to continued development of this core solution/product. This will involve using EHR data to assess comprehensiveness to reconstruct history for five groups of chronically ill patients (e.g., advanced cancer, coronary heart disease, insulin-dependent diabetes, chronic pain, and schizophrenia) who are admitted with high-frequency chief complaints of abdominal pain, chest pain, and fever. This will include mining, prioritizing, and presenting information to clinicians using a total of 30 dimensions of data regarding patient’s prior ED and hospital visits including both structured (e.g., diagnosis, medications, labs, imaging test types, EKG date/time) and unstructured data (e.g., images, physician notes, consult notes) to establish physician consensus on the data most valuable to medical decision-making and co-design data visualization tools to render a clear patient history. Our aims include: (Aim 1) Establishing the feasibility of leveraging provider-as-user-centered design to guide development of a data visualization platform that enables providers to readily “see” the most complete patient record for the five groups of complex patients, and (Aim 2) Conducting usability testing and compare the completeness and efficiency of using the enhanced platform on the five patient categories with usual care. Success in phase I will enable the company to validate a ‘proof of concept’ for developing its clinician-as-user-centered design process to inform a phase II research and development effort to design a commercially viable clinical decision support tool for hospitals and health systems. PROJECT NARRATIVE Patient Insight seeks to determine the feasibility of its proposed platform to aggregate and present the clearest picture possible of a patient’s history for time-constrained emergency department (ED) providers sufficient to drive improved medical decision-making, safety, and improved health outcomes for complex patients at the point-of-care and across disparate EHR systems. The strategy is to leverage our capability to extract discreet and unstructured data from select EHR systems and engage providers in the design of key elements of information display to reduce duplicate testing, lower medical errors related to misinformed treatment choices, and reduce unnecessary treatment and costs. Proving feasibility will enable Patient Insight to structure a partnership with Universal Health Services and inform further development and testing of its core solution across multiple ED facilities.",Improve medical decision making in emergency medicine via a new interface to visualize and understand relevant prior history for complex patients at the point-of-care.,9778547,R43LM013130,"['Abdominal Pain', 'Accident and Emergency department', 'Adoption', 'Advanced Malignant Neoplasm', 'Algorithms', 'Automated Clinical Decision Support', 'Award', 'Caring', 'Categories', 'Chest Pain', 'Chronically Ill', 'Clinical', 'Cognitive', 'Communities', 'Comorbidity', 'Complex', 'Consensus', 'Consult', 'Coronary heart disease', 'Cost Control', 'Critical Illness', 'Data', 'Data Display', 'Data Element', 'Decision Making', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Electrocardiogram', 'Electronic Health Record', 'Elements', 'Emergency Medicine', 'Environment', 'Event', 'Fever', 'Frequencies', 'Goals', 'Grant', 'Health', 'Health Services', 'Health system', 'Hospitalization', 'Hospitals', 'Iatrogenesis', 'Image', 'Imagery', 'Inpatients', 'Insulin-Dependent Diabetes Mellitus', 'Intuition', 'Medical', 'Medical Errors', 'Medical History', 'Mining', 'Modeling', 'Natural Language Processing', 'Outcome', 'Output', 'Parents', 'Patients', 'Personal Satisfaction', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Primary Health Care', 'Process', 'Provider', 'Psyche structure', 'Recording of previous events', 'Records', 'Research', 'Safety', 'Schizophrenia', 'Small Business Innovation Research Grant', 'Software Engineering', 'Structure', 'Symptoms', 'System', 'Technology', 'Testing', 'Time', 'Treatment Cost', 'Universities', 'University Hospitals', 'Visit', 'Visual', 'Visualization software', 'Washington', 'Work', 'chronic pain', 'clinical decision support', 'clinical decision-making', 'cognitive load', 'cost', 'cost effective', 'data mining', 'data visualization', 'design', 'health information technology', 'improved', 'information display', 'innovation', 'insight', 'interoperability', 'medical specialties', 'novel', 'opioid misuse', 'point of care', 'research and development', 'stem', 'success', 'support tools', 'tool', 'treatment as usual', 'treatment choice', 'unnecessary treatment', 'usability', 'user centered design']",NLM,"PATIENT INSIGHT, INC.",R43,2019,155520,0.012709127252300285
"Collaborative Research: Statistical algorithms for anomaly  detection and patterns recognition in patient care and safety event reports     Medical errors have been shown to be the third leading cause of death in the United States. The Institute of Medicine and several state legislatures have recommended the use of patient safety event reporting systems (PSRS) to better understand and improve safety hazards. Numerous healthcare providers have adopted these systems, which provide a framework for healthcare provlder staff to report patient safety events. Public databases like MAUDE and VAERS have also been created to collect and trend safety events across healthcare systems. A patient safety event (PSE) report generally consists of both structured and unstructured data elements. Structured data are pre-defined, fixed fields that solicit specific information about the event. The unstructured data fields generally include a free text field where the reporter can enter a text description of the event. The text descriptions are often a rich data source in that the reporter ls not constrained to limited categories or selection options and is able to freely descrlbe the details of the event. The goal of this project is to develop novel statistical methods to analyze unstructured text like patient safety event reports arising in healthcare, which can lead to significant improvements to patient safety and enable timely intervention strategies. We address three problems: (a) Building realistic and meaningful baseline models for near misses, and detecting systematic deterioration of adverse outcomes relative to such baselines; (b) Understanding critical factors that lead to near misses & quantifying severity of outcomes; and (c) ldentifylng document groups of interest. We will use novel statistical approaches that combine Natural Language Processing with Statistical Process Monitoring, Statistical Networks Analysis, and Spatio-temporal Modeling to build a generalizable toolbox that can address these issues in healthcare. An important advantage of our research team is the involvement of healthcare domain experts and access to frontline staff, and we will leverage this strength to develop our algorithms. A key feature of our work is the generalizability of our methods, which will be applicable to biomedical documents arising across a remarkable variety of areas, such as patient safety and equipment malfunction reports, electronic health records, adverse drug or vaccine reports, etc. We will also release open source software via R packages & GitHub, which will enable healthcare staff and researchers to execute our methods on their datasets. Estimates of preventable adverse events in healthcare are staggering, despite the frequently cited Institute of Medicine (IOM) report that first brought attention to the problem over ten years ago. Identifying temporal trends and patterns in the data is particularly important to improving patient safety and patient care. Using our algorithms to effectively analyze documents from reporting systems has the potential to dramatically improve the safety and quality of care by exposing possible weaknesses in the care process.",Collaborative Research: Statistical algorithms for anomaly  detection and patterns recognition in patient care and safety event reports    ,9914443,R01LM013309,"['Address', 'Adopted', 'Adverse event', 'Algorithms', 'Area', 'Attention', 'Caring', 'Categories', 'Cause of Death', 'Computer software', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Deterioration', 'Electronic Health Record', 'Equipment Malfunction', 'Event', 'Goals', 'Health Personnel', 'Healthcare', 'Healthcare Systems', 'Institute of Medicine (U.S.)', 'Instruction', 'Interest Group', 'Intervention', 'Lead', 'Medical Errors', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Outcome', 'Pathway Analysis', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Process', 'Quality of Care', 'Report (document)', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Statistical Algorithm', 'Statistical Computing', 'Statistical Methods', 'Structure', 'System', 'Text', 'Time', 'Time trend', 'United States', 'Vaccines', 'Work', 'adverse outcome', 'hazard', 'improved', 'novel', 'open source', 'patient safety', 'spatiotemporal', 'trend']",NLM,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2019,279001,0.03948500345771068
"Health Information Technology for Surveillance of Health Care-Associated Infections PROJECT SUMMARY/ABSTRACT Health care-associated infections (HAIs) affect one in every 20 hospitalized patients and account for $10 billion dollars in potentially preventable health care expenditures annually. Current efforts at detection of HAIs are limited to manual chart review which hinders the generalizability and scalability of HAI detection. My goal in seeking a Mentored Clinical Scientist Career Development Award is to acquire the necessary training, practical experience, and knowledge to develop a health services research career as a principal investigator focusing on leveraging novel health information technology (HIT) tools to improve the measurement of surgical health care quality, safety, and effectiveness. To continue my progress towards this goal, the objective of this project is to address the challenges of HAI detection by developing a robust and portable automated HAI surveillance toolkit. This toolkit will combine structured electronic health record (EHR) data with rich information locked in clinical notes using machine learning and natural language processing (NLP) to identify HAIs after surgical procedures. Our overall hypothesis is that combining structured variables from the EHR supplemented with NLP will improve our ability to identify HAIs after surgical procedures. To test the central hypothesis and accomplish the objectives for this application, I will pursue the following three specific aims: 1) Determine the EHR data elements indicative of postoperative HAIs and evaluate the performance of a novel HAI surveillance algorithm; 2) Identify the presence of postoperative SSIs from clinical notes using an automated portable NLP-based algorithm; 3) Apply user-centered design to create a high fidelity prototype of a surgical quality dashboard incorporating our HAI case detection methodology. This contribution is a significant first step in a continuum of research that utilizes the large amounts of data in the EHR combined with novel HIT methods to improve the measurement of surgical health-care quality, safety, and effectiveness. This approach is significant because the tools developed in this proposal have potential to serve as a prototype for identification and monitoring hospitals adverse events and could be replicated on a national scale. The proposed research is innovative in its approach using a combination of structured and unstructured data in the EHR along with novel machine learning and NLP tools to create a generalizable surveillance toolkit for the detection of HAIs. This proposal is responsive to the AHRQ Special Emphasis Notice (NOT-HS-13-011) specifically addressing the use of HIT to improve quality measurement. I have assembled a mentoring team who all internationally recognized experts with long and successful track records of funding and trainee mentorship. This project will provide the means to place me on a trajectory towards a health services research career focused on improving the measurement of surgical health-care quality, safety, and effectiveness using novel HIT tools. In summary, my previous training and experience, innovative research plan, high-quality training plan, first-rate mentorship team, and supportive research environment give me the highest likelihood of success to research independence with the proposed K08 award. PROJECT NARRATIVE/RELEVANCE TO PUBLIC HEALTH The proposed research and career development plan are relevant to public health because health care- associated infections (HAIs) affect about one in every 20 hospitalized patients and account for $30 billion dollars in potentially preventable health care expenditures annually. The objective of this project is to develop robust and portable automated surveillance toolkit that combines structured EHR data with rich information locked in clinical notes using natural language processing to identify HAIs after surgical procedures. The proposed research has the potential to serve as a prototype for identification and monitoring of numerous hospitals adverse events and could be replicated on a national scale.",Health Information Technology for Surveillance of Health Care-Associated Infections,9731453,K08HS025776,[' '],AHRQ,UNIVERSITY OF UTAH,K08,2019,160164,0.003230982708041627
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9937918,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Database Management Systems', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Serious Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'risk prediction model', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2019,97055,0.014580961630196563
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9637319,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Database Management Systems', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Serious Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'risk prediction model', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2019,619389,0.014580961630196563
"Enhance Arthroplasty Research through Electronic Health Records and Nlp-Enabled Informatics ABSTRACT Total joint arthroplasty (TJA) is the most common and fastest growing surgical procedure in the nation. Despite the high procedure volume, the evidence base for TJA procedures and associated interventions are limited. This is mainly due to lack of high quality data sources and the logistical difficulties associated with manually extracting TJA information from the unstructured text of the Electronic Health Records (EHR). Meanwhile, the rapid adoption of EHR and the advances in health information technology offer the potential to transform unstructured EHR notes into structured, codified format that can then be analyzed and shared with local and national arthroplasty registries and other agencies. We therefore propose to leverage unique data resources and natural language processing (NLP) technologies to build an informatics infrastructure for automated EHR data extraction and analysis. We will (1) develop a high performance, externally validated and user centric NLP- enabled algorithm for extraction of complex TJA-specific data elements from the structured and unstructured text of the EHR, (2) validate the algorithm externally in multiple EHR platforms and hospital settings, and (3) conduct a demonstration project focused on prediction of prosthetic joint infections using data elements collected by the NLP-enabled algorithm. Our overarching goal is to develop valid, open source and portable NLP-enabled data collection and risk prediction tools and disseminate them widely to hospitals participating in regional and national TJA registries. This research is significant as it leverages strong data resources and expertise to tackle the pressing need for high quality data and accurate prediction models in TJA. Automated data collection and processing capabilities will lead to an upsurge in secondary use of EHR to advance scientific knowledge on TJA risk factors, healthcare quality and patient outcomes. Accurate prediction of high risk patients for prosthetic joint infections will guide prevention and treatment decisions resulting in significant health benefits to TJA patients. The research is innovative because TJA-specific bioinformatics technology will shift TJA research from current under-powered, single-center studies to large, multi-center registry-based observational studies and clinical trials. Our deliverables have the potential to exert a sustained downstream effect on future TJA research, practice and policy. PUBLIC HEALTH RELEVANCE Lack of high quality data is a critical barrier to progress in total joint arthroplasty (TJA) research. We will utilize health information technology to automate extraction of rich TJA information from the electronic health records, and develop a robust risk prediction score for prosthetic joint infections, a devastating and yet preventable complication of TJA. Widespread adoption of these tools will enhance data collection capabilities and enable affordable large scale studies for practice improvements through secondary use of real-world data. Accurate prediction of post- operative infection risk among TJA candidates will guide individualized preventive strategies for modifiable risk factors, thereby reducing the burden of prosthetic joint infections in TJA patients.",Enhance Arthroplasty Research through Electronic Health Records and Nlp-Enabled Informatics,9657648,R01AR073147,"['Adoption', 'Algorithms', 'American', 'Bioinformatics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Complex', 'Complication', 'Data', 'Data Collection', 'Data Element', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Making', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Epidemic', 'Evidence based practice', 'Future', 'Goals', 'Gold', 'Guide prevention', 'Health Benefit', 'Hospitals', 'Individual', 'Informatics', 'Institution', 'Intervention', 'Joint Prosthesis', 'Knowledge', 'Logistics', 'Manuals', 'Marketing', 'Medicare', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Policies', 'Postoperative Period', 'Prevention', 'Prevention strategy', 'Procedures', 'Provider', 'Publishing', 'Registries', 'Replacement Arthroplasty', 'Research', 'Risk', 'Risk Factors', 'Safety', 'Scientific Advances and Accomplishments', 'Source', 'Structure', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States', 'age group', 'base', 'computerized data processing', 'cost', 'data access', 'data resource', 'electronic data', 'electronic structure', 'epidemiology study', 'evidence base', 'health care quality', 'health information technology', 'high risk', 'improved', 'individual patient', 'infection risk', 'informatics infrastructure', 'informatics\xa0tool', 'innovation', 'joint infection', 'modifiable risk', 'novel', 'open source', 'outcome prediction', 'patient population', 'portability', 'pragmatic trial', 'predictive modeling', 'prototype', 'public health relevance', 'risk prediction model', 'surgery outcome', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,R01,2019,636882,0.05081544354267375
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities. PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9774751,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Benefits and Risks', 'CCL4 gene', 'Cancer Intervention', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical and Translational Science Awards', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Public Health Informatics', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical implementation', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'informatics\xa0tool', 'interest', 'learning algorithm', 'longitudinal dataset', 'novel', 'open source', 'optimal treatments', 'pharmacovigilance', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2019,600683,0.013963725226351946
"Enhancing Intimate Partner Violence (IPV) Identification through Automated EHR Summarization Project summary Intimate partner violence (IPV) is a significant public health and criminal justice problem that negatively impacts millions of victims yearly in the United States, primarily women (85%). Most IPV-related healthcare visits (83%) occurred in an emergency department (ED), and these clinical encounters are unique opportunities to identify IPV victims and potentially provide assistance. Although numerous health professional organizations have endorsed universal screening and counseling for IPV since 1992, actual screening rates, detection of IPV victims, and referrals to IPV services remain low in the ED. As a result, many IPV victims pass through the ED unidentified and untreated. Computerized screening tools have been developed and implemented in clinical settings in order to assist providers in screening and detecting IPV. However, these tools have a great limitation in that they rely on information collected from the patient and do not utilize the longitudinal data in electronic health records (EHR). Recently, researchers demonstrated that a history of IPV diagnoses and associated clinical symptoms highly predict current and future IPV (OR=7.8), and these important IPV data could serve as red flags that trigger providers to assess patients further for IPV. In order to enhance IPV screening in the ED, we propose to develop and assess an automatic clinical data summarizer that extracts, abstracts and synthesizes patient historical IPV data (structured and unstructured), and delivers patient historical IPV data to ED providers through an intuitive interface. The specific aims are: 1) develop and evaluate natural language processing (NLP) strategies to identify and extract patient historical IPV incidents and timelines from clinic notes; 2) develop and evaluate a web service-based summary tool (IPV-Summary-Service) that synthesizes patient- specific IPV information from both NLP-processed data elements and structured data; and 3) develop and pilot test an enhanced IPV screening strategy that delivers clinical evidence generated by the IPV-Summary- Service through a specific EHR (Epic) to providers during the patient universal IPV screening in the ED. This automatic clinical date summary for IPV will be piloted in one ED at MUSC. There are two major outcomes to be measured for the IPV-Summary enhanced screening for 6 months before and after the index date of pilot testing: 1) rate of successful referral to the IPV 24-hour dedicated IPV nurse; and 2) rate of initiation of referral and identification of persons at high risk of IPV. We will use mixed effect generalized linear regression models to estimate the effects of IPV-Summary on the referral rate and IPV case identification rate. Through survey studies, we will assess secondary outcomes including factors of system feasibility, usability, and providers' satisfaction. These analyses can identify potentially important correlates of the major outcomes and may help us improve the design of the intervention. The results from this study will form the foundation for a broader implementation in a regional health information exchange for EDs. Narrative Computer-based approaches for intimate partner violence (IPV) universal screening have led to significantly higher screening rate and detection rate, as well as receipt of IPV services in the emergency department (ED). However, these approaches rely on information collected from the patient and do not utilize the longitudinal IPV data existing in electronic health records (EHR), which have high predictive power of IPV risk. In order to enhance the effectiveness of IPV screening, we propose to develop and assess an automatic clinical data summary tool that extracts, abstracts, and synthesizes patient historical IPV information from EHR and then delivers that critical information to ED providers at the point of care.",Enhancing Intimate Partner Violence (IPV) Identification through Automated EHR Summarization,9739347,R21LM012945,"['Accident and Emergency department', 'Acute', 'Address', 'Adopted', 'Adult', 'American', 'Caring', 'Centers for Disease Control and Prevention (U.S.)', 'Chronic', 'Client', 'Clinic', 'Clinical', 'Clinical Data', 'Complex', 'Computers', 'Counseling', 'Crime', 'Criminal Justice', 'Data', 'Data Element', 'Decision Making', 'Detection', 'Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Emergency Department patient', 'Emergency department visit', 'Event', 'Face', 'Female', 'Foundations', 'Future', 'Health', 'Health Care Visit', 'Health Professional', 'Hour', 'Injury', 'Intuition', 'Linear Regressions', 'Link', 'Masks', 'Measures', 'Medical', 'Modeling', 'Natural Language Processing', 'Nurses', 'Outcome', 'Patient Self-Report', 'Patients', 'Persons', 'Process', 'Professional Organizations', 'Provider', 'Public Health', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Screening procedure', 'Services', 'South Carolina', 'Structure', 'Surveys', 'Symptoms', 'System', 'Testing', 'Time', 'TimeLine', 'Trauma', 'United States', 'Universities', 'Victimization', 'Violent injury', 'Woman', 'Work', 'base', 'clinical decision support', 'clinical decision-making', 'clinical practice', 'clinically relevant', 'computerized', 'experience', 'high risk', 'improved', 'indexing', 'intimate partner violence', 'point of care', 'pressure', 'satisfaction', 'screening', 'secondary outcome', 'therapy design', 'tool', 'usability', 'web services']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R21,2019,168188,0.029970703837786283
"Advancing Quality and Outcomes Measurement in Rheumatology PROJECT SUMMARY Healthcare has changed rapidly in the last decade with the widespread use of electronic health records (EHRs) and the creation of national EHR-based data networks that aim to improve the quality of care. The American College of Rheumatology's RISE registry is a federally Qualified Clinical Data Registry that collects EHR data from the practices of almost 1000 rheumatologists nationally, analyzes these data centrally, and continuously feeds back performance on quality measures to practices via a web-based dashboard. In this K24 proposal, the applicant proposes to utilize novel methods in clinical informatics to increase the accuracy of quality measurement, while also developing and testing new EHR-based quality measures relevant to rheumatic diseases. The proposed research will leverage her strong research portfolio, including grants from the National Institute of Arthritis and Musculoskeletal and Skin Diseases and the Agency for Healthcare Research and Quality, her successful track record of achieving national endorsement for EHR-based quality measures, existing data from over 1.4 million patients in the RISE database, and the outstanding institutional environment at the University of California, San Francisco. It will also support her ongoing career development in clinical informatics methods relevant to EHR-based clinical research. For this five year K24 award proposal, she plans to increase the time spent mentoring junior investigators in the field or quality and outcomes measurement in rheumatology, with the goal of helping trainees successfully launch academic research careers in patient- oriented research. Aligned with a comprehensive mentoring plan, the proposal outlines two specific aims, including using natural language processing to increase the accuracy of EHR-based quality measurement in RISE, and developing and validating new, prototype electronic clinical quality measures to monitor and address high impact gaps in care for patients with rheumatic disease. The work will prioritize outcome measures and use eMeasurement standards, including the Quality Data Model and Health Quality Measures Format to develop, specify and test measures. Measures developed through this research and mentoring program will be candidates for nationwide dissemination across rheumatology practices to improve care for individuals with rheumatic disease. PROJECT NARRATIVE This mid-career investigator award will support a program in patient-oriented research in rheumatic diseases at the University of California, San Francisco. The award will allow the applicant to expand her research on the development and validation of health care quality measures and support her mentoring of early investigators. The proposed research aims to create quality measures that can be deployed across rheumatology practices to improve the quality of care, while training new researchers to perform innovative patient-oriented research in the area of electronic health record-based quality and outcomes measurement.",Advancing Quality and Outcomes Measurement in Rheumatology,9645973,K24AR074534,"['Address', 'Algorithms', 'American', 'Area', 'Award', 'Back', 'Benchmarking', 'California', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Collection', 'Comorbidity', 'Computer software', 'Consensus', 'Country', 'Data', 'Data Analyses', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Dictionary', 'Disease', 'Electronic Health Record', 'Environment', 'Feeds', 'Foundations', 'Funding', 'Goals', 'Gout', 'Grant', 'Growth', 'Health', 'Healthcare', 'Healthcare Systems', 'High Prevalence', 'Individual', 'Informatics', 'Infrastructure', 'Institute of Medicine (U.S.)', 'Leadership', 'Learning', 'Measurement', 'Measures', 'Mentors', 'Mentorship', 'Methods', 'Mid-Career Clinical Scientist Award (K24)', 'Monitor', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Osteoporosis', 'Outcome', 'Outcome Measure', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Preventive care', 'Process', 'Public Health', 'Quality of Care', 'Registries', 'Research', 'Research Personnel', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Role', 'Safety', 'San Francisco', 'Scientist', 'Specific qualifier value', 'Structure', 'Testing', 'Text', 'Time', 'Training', 'United States', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Universities', 'Validation', 'Vision', 'Work', 'base', 'career', 'career development', 'college', 'dashboard', 'data mining', 'data modeling', 'data registry', 'data warehouse', 'design', 'digital', 'evidence base', 'feeding', 'health care quality', 'improved', 'informatics infrastructure', 'innovation', 'interest', 'learning progression', 'novel', 'patient oriented', 'patient oriented research', 'patient registry', 'patient safety', 'programs', 'prototype', 'research study', 'rheumatologist', 'success', 'tool']",NIAMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K24,2019,189666,0.030355616316148117
"Interdisciplinary Infrastructure for Aging Research: Rochester Epidemiology Project PROJECT SUMMARY The National Institutes on Aging (NIA) has recommended strengthening research infrastructures to address future aging research questions (2016 Data Infrastructure Review Committee Report and PAR-16-367). In particular, they recommend: 1) integrating biological data into larger population-based studies; 2) increasing use of electronic health record (EHR) data and linking to medical care claims data; and 3) developing new approaches to collecting data to answer important scientific questions about mechanisms of aging. The Rochester Epidemiology Project (REP; NIA R01 AG034676) is a unique infrastructure for studies of aging, because the REP collects longitudinal EHR data on all health conditions that come to medical attention for a large, Midwestern population. Therefore, the REP allows investigators to study all age-related diseases and outcomes. However, the REP has three significant gaps. First, the REP does not include biospecimens. Second, the REP is missing health care delivered outside of the health care institutions that partner with the REP, and it does not include information on filled prescriptions. Third, a significant proportion of EHR data is difficult to access due to two factors: 1) the full text of the EHRs includes extensive clinical notes about aging outcomes and geriatric syndromes, but these notes are not routinely coded for billing, and can only be accessed through laborious manual review; and 2) the REP health care partners use three different EHR systems, making it difficult to apply electronic data extraction tools across all partners. To address these three gaps, we will develop an interdisciplinary collaboration across experts in aging research, epidemiologic methods, biobanking, and medical informatics to create a new, comprehensive research infrastructure (“Bio-REP”) to support aging research. In the R21 phase, we will develop a comprehensive research infrastructure that combines the REP data with Mayo Clinic Biobank biospecimens, medical claims data from the Centers for Medicare and Medicaid Services (CMS; Aim 1), and geriatric syndrome data that are included in the unstructured EHR clinical notes using Natural Language Processing techniques (NLP; Aim 2). In the R33 phase, we will deploy NLP algorithms developed in Aim 2 in the clinical notes from two additional EHR systems (Aim 3), and we will conduct two demonstration projects. First, we will measure associations between novel aging-related biomarkers and aging-related outcomes (Aim 4). Second, we will determine whether two common medications that are hypothesized to impact aging (metformin and angiotensin receptor blockers) modify associations between aging biomarkers and aging outcomes (Aim 5). The new, robust Bio-REP infrastructure will support a wide range of efficient, cost-effective observational studies to characterize associations between aging-related biomarkers and specific diseases, geriatric syndromes, and drug utilization. Such studies are urgently needed to design effective clinical trials to improve the health span of the aging population. PROJECT NARRATIVE We will establish an interdisciplinary collaboration across experts in aging research, epidemiologic methods, biobanking, and medical informatics to develop and test a new, comprehensive research infrastructure (“Bio- REP”) to support aging research. The Bio-REP will support efficient, cost-effective studies to characterize associations between aging-related biomarkers and specific diseases, geriatric syndromes, and drug utilization. Such studies are urgently needed to design effective clinical trials to improve the health span of the aging population.",Interdisciplinary Infrastructure for Aging Research: Rochester Epidemiology Project,9749915,R21AG058738,"['Address', 'Age-Years', 'Aging', 'Agreement', 'Algorithms', 'Angiotensin Receptor', 'Area', 'Benchmarking', 'Biological', 'Biological Markers', 'Caring', 'Ceramides', 'Clinic', 'Clinical', 'Clinical Trials', 'Code', 'Data', 'Delirium', 'Diagnosis', 'Diagnostic Services', 'Disease', 'Disease Outcome', 'Drug Utilization', 'Elderly', 'Electronic Health Record', 'Enrollment', 'Epidemiologic Methods', 'Epidemiology', 'Future', 'Grant', 'Health', 'Healthcare', 'Infrastructure', 'Institution', 'Link', 'Manuals', 'Measures', 'Medical', 'Medical Informatics', 'Medical Records', 'Metformin', 'National Institute on Aging', 'Natural Language Processing', 'Observational Study', 'Outcome', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Polypharmacy', 'Population', 'Population Study', 'Proteins', 'Recommendation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Review Committee', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'United States Centers for Medicare and Medicaid Services', 'age related', 'aging population', 'biobank', 'cost effective', 'design', 'electronic data', 'falls', 'healthspan', 'improved', 'interdisciplinary collaboration', 'medical attention', 'novel', 'novel marker', 'novel strategies', 'senescence', 'tool']",NIA,MAYO CLINIC ROCHESTER,R21,2019,198750,0.0073072534533122225
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9615037,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Development', 'Diagnostic', 'Disease Surveillance', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Source', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'optimal treatments', 'outcome forecast', 'population based', 'prevent', 'sociodemographics', 'socioeconomics', 'surveillance data', 'treatment guidelines', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2019,784820,0.014684016853767775
"Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes ?DESCRIPTION (provided by applicant): The applicant and principal investigator (PI) is a board-certified cardiologist, Assistant Professor of Medicine and Senior Associate Consultant in the Mayo Clinic Cardiovascular Division. [The long-term goal of the PI is to become an independent clinician-investigator in translational informatics focused on the development, validation and deployment of electronic tools to the point-of-care to improve outcomes for patients with cardiovascular disease. The PI intends to leverage the EHR to conduct population-based studies using electronic algorithms that include NLP and by application of state-of-the-art informatics approaches to deliver the knowledge at the point-of-care in support of patient-centered decision-making for patients with peripheral arterial disease (PAD). The electronic tools developed will be portable to different institutions and other health conditions. The PI will be uniquely positioned as a cardiologist within a large academic health system working with novel NLP techniques at the interface of the EHR and CDS to rapidly translate knowledge acquired in community-based epidemiologic investigations to patient care.] The PI has proposed a research career development plan to acquire expertise in translational informatics including electronic phenotyping, and automated calculation and visual display of prognostic scores using a novel application integrated with clinical decision support (CDS). The PI will complete comprehensive coursework in informatics science and gain additional patient-oriented and epidemiologic research experience. The primary mentor will be Iftikhar Kullo, M.D., cardiologist and PI of the Mayo Clinic electronic MEdical Records and GEnomics (eMERGE) network and a leader in the development and application of electronic phenotyping for genomic studies of PAD. The mentoring team also includes Dr. Christopher Chute, M.D., Head of Medical Informatics at Mayo Clinic, an expert in informatics and epidemiology, and co-PI of the Mayo eMERGE grant and co-investigator of the Rochester Epidemiology Project; Hongfang Liu, Ph.D., an expert in natural language processing (NLP) and its application for EHR-based phenotyping; and Kent Bailey, Ph.D., senior biostatistician with expertise in development of prognostic risk scores. The specific aims of the proposal are: Aim 1 - Apply phenotyping algorithms that include NLP of clinical notes to [a] identify PAD cases and controls without PAD in the community from 1998-2011; and [b] ascertain adverse cardiovascular and limb outcomes through December 2013. Aim 2 - Create multivariable prognostic risk models and scores for adverse outcomes in PAD cases from the community. We will estimate the relative risk of death in PAD patients compared to controls without PAD. Aim 3 - Develop and evaluate usefulness of a novel electronic application within the Mayo EHR to retrieve relevant data elements, calculate, and display individualized prognostic scores. On completion of the proposed investigations the deliverables will be new knowledge and an e-health prognostication tool for PAD patients to be disseminated to stakeholders including clinicians, patients, and researchers portable to other institutions and other cardiovascular diseases. PUBLIC HEALTH RELEVANCE: The applicant, a board certified cardiovascular specialist, proposes to acquire training in translational informatics science which will enable application of novel electronic algorithms to electronic health records to more efficiently conduct community-based studies of peripheral arterial disease, a prevalent but undertreated health condition. These studies will concurrently develop novel electronic tools which will inform patients and providers of risk, facilitate shared-decision making at the point-of-care, and promote compliance to guideline recommended strategies. These tools will be portable to other institutions and to other cardiovascular diseases.",Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes,9655950,K01HL124045,"['Address', 'Adult', 'Adverse event', 'Algorithms', 'Amputation', 'Atherosclerosis', 'Atrial Fibrillation', 'Attention', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinic', 'Clinical', 'Clinical Investigator', 'Communities', 'Computerized Medical Record', 'Coronary heart disease', 'Data', 'Data Element', 'Decision Making', 'Dependence', 'Development', 'Development Plans', 'Diagnosis', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Epidemiology', 'Genomics', 'Goals', 'Government Agencies', 'Grant', 'Guidelines', 'Head', 'Health', 'Health system', 'Healthcare Systems', 'Heart failure', 'High Prevalence', 'Image', 'Informatics', 'Institution', 'Investigation', 'Ischemia', 'Knowledge', 'Laboratories', 'Limb structure', 'Link', 'Manuals', 'Medical Informatics', 'Medicine', 'Mentors', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Peripheral arterial disease', 'Pharmaceutical Preparations', 'Phenotype', 'Population Study', 'Positioning Attribute', 'Principal Investigator', 'Provider', 'Relative Risks', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Risk stratification', 'Safety', 'Science', 'Site', 'Specialist', 'Stroke', 'Symptoms', 'System', 'Techniques', 'Time', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Visual', 'Work', 'adverse outcome', 'base', 'career development', 'case control', 'clinical decision support', 'clinical decision-making', 'cost', 'demographics', 'didactic education', 'eHealth', 'epidemiology study', 'experience', 'health information technology', 'high risk', 'improved', 'improved outcome', 'informatics\xa0tool', 'investigator training', 'mortality', 'mortality risk', 'multidisciplinary', 'novel', 'patient oriented', 'patient oriented research', 'point of care', 'portability', 'predictive modeling', 'professor', 'prognostic', 'prognostic tool', 'public health relevance', 'repository', 'research and development', 'risk prediction model', 'shared decision making', 'skills', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,K01,2019,170856,0.051582619427197725
"National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms PROJECT SUMMARY With the rapidly growing adoption of patient electronic health record systems (EHRs) due to Meaningful Use, and linkage of EHRs to research biorepositories, evaluating the suitability of EHR data for clinical and translational research is becoming ever more important, with ramifications for genomic and observational research, clinical trials, and comparative effectiveness studies. A key component for identifying patient cohorts in the EHR is to define inclusion and exclusion criteria that algorithmically select sets of patients based on stored clinical data. This process is commonly referred to, as “EHR-driven phenotyping” is time-consuming and tedious due to the lack of a widely accepted and standards-based formal information model for defining phenotyping algorithms. To address this overall challenge, the proposed project will design, build and promote an open-access community infrastructure for standards-based development and sharing of phenotyping algorithms, as well as provide tools and resources for investigators, researchers and their informatics support staff to implement and execute the algorithms on native EHR data. PROJECT NARRATIVE The identification of patient cohorts for clinical and genomic research is a costly and time-consuming process. This bottleneck adversely affects public health by delaying research findings, and in some cases by making research costs prohibitively high. To address this issue, leveraging electronic health records (EHRs) for identifying patient cohorts has become an increasingly attractive option. This proposal will investigate and implement standards based approaches for computable phenotype identification from multiple EHRs.",National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms,9774075,R01GM105688,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Architecture', 'Benchmarking', 'Benign Prostatic Hypertrophy', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computerized Medical Record', 'Computing Methodologies', 'Consensus', 'Consumption', 'Data', 'Data Element', 'Data Quality', 'Data Reporting', 'Development', 'Educational workshop', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Engineering', 'Event', 'Exclusion Criteria', 'Fast Healthcare Interoperability Resources', 'Flowcharts', 'Genomics', 'Gold', 'Grant', 'Health', 'Health system', 'Healthcare Systems', 'Human', 'Informatics', 'Infrastructure', 'Intuition', 'Knowledge', 'Logic', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Needs Assessment', 'Observational Study', 'Outcome', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Precision Medicine Initiative', 'Process', 'Public Health', 'Public Health Informatics', 'Rare Diseases', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk Factors', 'Running', 'Scientist', 'Services', 'Standardization', 'Structure', 'System', 'Techniques', 'Text', 'Time', 'Translational Research', 'United States National Institutes of Health', 'University Hospitals', 'authority', 'base', 'biobank', 'clinical phenotype', 'cohort', 'comparative effectiveness', 'cost', 'data modeling', 'data warehouse', 'database query', 'deep learning', 'design', 'endophenotype', 'experience', 'inclusion criteria', 'informatics training', 'information model', 'knowledge base', 'meetings', 'portability', 'precision medicine', 'repository', 'syntax', 'tool', 'usability']",NIGMS,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2019,706851,0.045371923383010944
"Data-Mining Clinical Decision Support from Electronic Health Records ﻿    DESCRIPTION (provided by applicant)    Data-Mining Clinical Decision Support from Electronic Health Records Public Health Motivation: National healthcare quality is compromised by undesirable variability, reflected in different locales having anywhere from 20-80% compliance with evidence-based guidelines. Much of this is due to uncertainty, with half of clinical practice guidelines lacking adequate evidence to confirm their efficacy. This is unsurprising when clinical trials cost >$15 million to answer individual clinical questions. The result is medical practice routinely driven by individual opinio and anecdotal experience. While Big Data has revolutionized how society processes internet scale information, the status quo in clinical decision making remains the manual interpretation of literature and isolated decision aids. The adoption of electronic health records (EHR) creates a new opportunity to answer a ""grand challenge in clinical decision support (CDS)."" In a learning health system, we could automatically adapt knowledge from the collective expertise embedded in the EHR practices of real clinicians and close the loop by disseminating that knowledge back as executable decision support. Candidate Goals and Objectives: The unifying goal of this BD2K K01 proposal is the mentored career development of Jonathan H. Chen, MD, PhD. This proposal will accelerate his transition into an independent physician scientist, towards his long-term goals to produce Big Data technologies that answer such grand challenges in clinical decision support. His near-term objective is developing methods to translate EHR data into useful knowledge in the form of patient- specific, point-of-care clinical order recommendations for acute medical hospitalizations. His doctoral background in computer science gives him the technical capability to achieve these objectives, while his medical training will ensure clinically meaningful results. His preliminary work to build an order recommender, analogous to commercial product recommenders, demonstrates the proposal's overall feasibility. Institutional Environment and Career Development: The research facilities and training opportunities at Stanford University provide the ideal environment to achieve these objectives, with established and growing Centers for Biomedical Informatics Research, the Biomedical Data Science Initiative, and the first Clinical Informatics Fellowship accredited in the nation. Prof. Russ Altman, Director of the Biomedical Informatics Training Program, will lead a collaborative team of mentors with expertise in clinical decision support (Mary Goldstein), implementation science (Steven Asch), data-mining electronic health records (Nigam Shah), statistical learning algorithms (Lester Mackey), and healthcare statistics (Michael Baiocchi). Combined with respective didactic training, this mentorship will enable Dr. Chen to achieve his objectives through a series of research aims. Research Aims: The overriding hypothesis of the proposal is that clinical knowledge reflected in clinical order patterns from historical EHR data can improve medical decision making when adapted into functional clinical decision support. The specific aims each address components of this concept, as they seek to: (1) Develop the algorithms to learn clinical order patterns from historical EHR data, building on a preliminary recommender system; (2) Assess how underlying clinician proficiency affects the quality of those learned clinical order patterns through observational data inference against external standards; and (3) Determine the impact of automatically learned clinical decision support (CDS) on (simulated) clinical workflows through a randomized controlled crossover trial of human-computer interfaces with real clinicians. Expected Results and General Significance: By the completion of the proposed work, Dr. Chen will answer the grand challenge in clinical decision support (CDS) by automating much of the CDS production process, and have direct translational impact with a prototype system. This will advance the field with new paradigms of generating and disseminating clinical knowledge, which can then improve the consistency and quality of healthcare delivery. Additional benefits will include methods to identify and monitor areas of high practice variability for targeted optimization and improve predictive models that inform precision medicine. With this applied research experience and career development, Dr. Chen can compete for R01 funding and become an independent physician scientist developing Big Data approaches to solve national healthcare problems in clinical decision making. PUBLIC HEALTH RELEVANCE    National healthcare quality is compromised by undesirable practice variability and medical uncertainty, with most medical practice routinely driven by individual opinions and anecdotal experience. With methods analogous to commercial product recommender systems, the proposed project will automatically learn patterns in raw clinical transaction data to capture the undocumented knowledge of real-world clinicians, and close the loop in a learning health system by disseminating that knowledge back as clinical decision support to improve patient care.",Data-Mining Clinical Decision Support from Electronic Health Records,9749154,K01ES026837,"['Accreditation', 'Achievement', 'Acute', 'Address', 'Admission activity', 'Adoption', 'Affect', 'Algorithms', 'Applied Research', 'Area', 'Automated Clinical Decision Support', 'Back', 'Big Data', 'Big Data to Knowledge', 'Caring', 'Chiroptera', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Practice Guideline', 'Clinical Trials', 'Cost efficiency', 'Cross-Over Trials', 'Crowding', 'Data', 'Data Science', 'Decision Aid', 'Decision Making', 'Diagnosis', 'Doctor of Philosophy', 'Educational process of instructing', 'Electronic Health Record', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Failure', 'Fellowship', 'Funding', 'Future', 'Goals', 'Health system', 'Healthcare', 'Hospitalization', 'Hospitals', 'Image', 'Individual', 'Institute of Medicine (U.S.)', 'Internet', 'Investments', 'Knowledge', 'Lead', 'Learning', 'Literature', 'Locales', 'Machine Learning', 'Manuals', 'Medical', 'Medical Residency', 'Mentors', 'Mentorship', 'Methods', 'Monitor', 'Motivation', 'Noise', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Privatization', 'Process', 'Production', 'Public Health', 'Quality of Care', 'Randomized', 'Recommendation', 'Research', 'Research Training', 'Scientist', 'Series', 'Services', 'Societies', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Training', 'Training Programs', 'Transact', 'Translating', 'Uncertainty', 'Universities', 'User-Computer Interface', 'Validation', 'Weight', 'Work', 'base', 'biomedical informatics', 'career development', 'clinical care', 'clinical decision support', 'clinical decision-making', 'clinical practice', 'clinical predictors', 'computer science', 'cost', 'data mining', 'design', 'evidence based guidelines', 'experience', 'health care delivery', 'health care quality', 'implementation science', 'improved', 'learning algorithm', 'multidisciplinary', 'point of care', 'precision medicine', 'predictive modeling', 'prototype', 'public health relevance', 'research facility', 'routine practice', 'statistics', 'support tools', 'training opportunity', 'translational impact']",NIEHS,STANFORD UNIVERSITY,K01,2019,178606,0.025559366617838974
"Improving missing data analysis in distributed research networks ABSTRACT Electronic health record (EHR) databases collect data that reflect routine clinical care. These databases are increasingly used in comparative effectiveness research, patient-centered outcomes research, quality improvement assessment, and public health surveillance to generate actionable evidence that improves patient care. It is often necessary to analyze multiple databases that cover large and diverse populations to improve the statistical power of the study or generalizability of the findings. A common approach to analyzing multiple databases is the use of a distributed research network (DRN) architecture, in which data remains under the physical control of data partners. Although EHRs are generally thought to contain rich clinical information, the information is not uniformly collected. Certain information is available only for some patients, and only at some time points for a given patient. There are generally two types of missing information in EHRs. The first is the conventionally understood and obvious missing data in which some data fields (e.g., body mass index) are not complete for various reasons, e.g., the clinician does not collect the information or the patient chooses not to provide the information. The second is less obvious because the data field is not empty but the recorded value may be incorrect due to missing data. For example, EHRs generally do not have complete data for care that occurs in a different delivery system. A medical condition (e.g., asthma) may be coded as “no” but the true value would have been “yes” if more complete data had been available, e.g., from claims data as the other delivery system would submit a claim to the patient’s health plan for the care provided. In other words, one may incorrectly treat “absence of evidence” as “evidence of absence”. EHRs hold great promise but we must address several outstanding methodological challenges inherent in the databases, specifically missing data. Addressing missing data is more challenging in DRNs due to different missing data mechanisms across databases. The specific aims of the study are: (1) Apply and assess missing data methods developed in single-database settings to handle obvious and well-recognized missing data in DRNs; (2) Apply and assess machine learning and predictive modeling techniques to address less obvious and under-recognized missing data for select variables in DRNs; and (3) Apply and assess a comprehensive analytic approach that combines conventional missing data methods and machine learning techniques to address missing data in DRNs. The analytic methods developed in this project, including the extension of existing missing data methods to DRNs, the innovative use of machine learning techniques to address missing data, and their integration with privacy- protecting analytic methods, will have direct impact on the design and analysis of future comparative effectiveness and safety studies, and patient-centered outcomes research conducted in DRNs. PROJECT NARRATIVE The proposed project will refine existing methods and develop new methods to address missing data issues in electronic health record databases.",Improving missing data analysis in distributed research networks,9783740,R01HS026214,[' '],AHRQ,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2019,399886,0.031938493203818
"Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign Project Summary/Abstract Written The objective of the proposed research is to reduce the patient safety hazards associated with electronic medication administration records (eMARs) by, (1) understanding current usability and safety gaps, and (2) creating design and development documents, wireframes, and prototypes to serve as the foundation for future eMARs that will eliminate these gaps. In particular, we focus on communication and information flow challenges between nurses, pharmacists, and physicians during medication administration and use of the eMAR. The proposed research is in direct response to special emphasis notice (NOT-HS-16-009). We will develop a broad understanding of usability and safety hazards associated with eMARs by analyzing a large dataset of 1.7 million patient safety event reports and detailed medication error related narratives. We will then conduct heuristic analyses of current eMARs, and interviews and observations of physicians, nurses, and pharmacists. These data will serve to inform the development of eMAR design documents, wireframes, and prototypes as the foundation for future development. This project utilizes the extensive expertise of the research team in human factors and safety science, health information technology (health IT), informatics, and data science. Our research team includes physicians, nurses, pharmacists, and human factors engineers, and experts in natural language processing. In addition, our partnerships include a patient safety organization and a health IT vendor. The proposal addresses fundamental aspects of the call for proposals by providing new insights on the safety of health IT and improves current practices by developing use cases and new prototypes for immediate use by healthIT vendors. Contributions from this research will include a fundamental understanding of the role of health IT during medication administration with a focus on communication and information flow, design, development and testing documents for vendors and providers, and eMAR wireframes and prototypes to improve development. Our research will also provide organizations like the Office of the National Coordinator with medication related test scenarios to assess current health IT systems. Project Narrative This project is relevant to public health because it applies the sciences of human factors and informatics to improve the usability and safety of the electronic medication administration records (eMARs), which will ultimately improve patient care. Patient safety event report data will be analyzed to identify usability and safety hazards in eMARs, and interviews and observations will be conducted to identify clinician needs. Based on this knowledge eMAR design documents, wireframes, and prototypes will be developed, tested, and disseminated with the goal of improving communication and information flow to reduce medication related errors.",Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign,9731456,R01HS025136,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2019,399999,0.0028799540734190804
"EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia PROJECT SUMMARY/ABSTRACT Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by elevated plasma low-density lipoprotein cholesterol (LDL-C) and a dramatically increased lifetime risk for premature atherosclerotic cardiovascular disease (ASCVD). Available data suggest substantial under treatment of individuals with FH, and it is estimated that <5% of prevalent FH cases in the US are diagnosed and treated. The proposed research will develop electronic health record (EHR)-based strategies to reduce mortality and morbidity from FH. We will develop and validate a phenotyping algorithm for rapid and efficient identification of FH cases thereby enabling EHR-based surveillance of FH. We will deploy the phenotyping algorithm in the population-based setting of Olmsted County, Minnesota, to estimate prevalence and provide hitherto unavailable data on awareness, detection and control of FH. We will develop CDS to help care providers manage FH patients and an FH-specific decision aid to facilitate shared decision making related to lipid-lowering therapy and screening of family members. To accomplish these goals, we will leverage the following resources: a) the electronic phenotyping expertise available in the electronic Medical Records and Genomics (eMERGE) network; b) the Rochester Epidemiology Project (REP), that links medical records of Olmsted County MN residents thereby capturing nearly all health care delivered to residents of the community; and c) expertise in developing and deploying CDS in the EHR and in creating decision aids for disclosing cardiovascular risk and the benefits of lipid-lowering drugs. Our specific aims are: Aim 1. Develop and validate an electronic phenotyping algorithm to rapidly identify FH cases from the EHR. Aim 2. Conduct an e- epidemiology study to obtain hitherto unknown data regarding prevalence, awareness, detection, control of FH in a population-based setting in the US. Aim 3. a) Develop EHR-based tools to help care providers manage FH and facilitate shared decision making and cascade screening and b) assess outcomes after implementation of CDS and decision aid. The proposed research will enable rapid identification of FH in EHRs, provide hitherto unavailable data on the burden of FH in the community, facilitate EHR-based strategies for early detection, increase awareness of FH among care providers, provide guidance for management of FH at point of care and help both patients and providers make informed decisions about drug therapy and screening of family members. These are critical steps for early detection and treatment of FH to reduce the burden of premature ASCVD due to this condition. PROJECT NARRATIVE Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by high cholesterol levels and increased risk of heart attack or sudden cardiac death. The proposed research will develop electronic health record (EHR)-based strategies to prevent adverse outcomes such as heart attack in FH patients. These include methods to rapidly identify FH patients, estimate prevalence of FH and develop clinical decision support to help care providers manage FH patients. The proposed work will have a significant impact on clinical management of FH patients.",EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia,9730585,R01HL135879,"['Academy', 'Address', 'Algorithms', 'Atherosclerosis', 'Awareness', 'Benefits and Risks', 'Cholesterol', 'Clinic', 'Clinical', 'Clinical Management', 'Communities', 'County', 'Data', 'Data Set', 'Decision Aid', 'Decision Making', 'Detection', 'Diagnosis', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Early treatment', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Epidemiology', 'Europe', 'Familial Hypercholesterolemia', 'Family', 'Family history of', 'Family member', 'Genetic Diseases', 'Genetic screening method', 'Genomics', 'Goals', 'Healthcare', 'Healthcare Systems', 'Heritability', 'Individual', 'Infrastructure', 'Institutes', 'LDL Cholesterol Lipoproteins', 'Label', 'Laboratories', 'Learning', 'Link', 'Lipids', 'Lipoprotein (a)', 'Low-Density Lipoproteins', 'Manuals', 'Medical Records', 'Medicine', 'Methods', 'Minnesota', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phenotype', 'Plasma', 'Population Study', 'Predictive Value', 'Prevalence', 'Prevalence Study', 'Primary Prevention', 'Provider', 'Public Health', 'Recommendation', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Resources', 'Risk', 'Structure', 'Test Result', 'Time', 'Work', 'World Health Organization', 'Xanthomas', 'adverse outcome', 'base', 'cardiovascular risk factor', 'care providers', 'case-based', 'clinical decision support', 'clinical implementation', 'epidemiology study', 'evaluation/testing', 'implementation science', 'improved', 'improved outcome', 'inhibitor/antagonist', 'innovation', 'lifetime risk', 'mortality', 'novel therapeutics', 'point of care', 'population based', 'precision medicine', 'premature', 'prevent', 'screening', 'screening program', 'shared decision making', 'sudden cardiac death', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,R01,2019,675136,0.03351939063687697
"Semi-supervised Algorithms for Risk Assessment with Noisy EHR Data PROJECT SUMMARY Large electronic health record research (EHR) data integrated with -omics data from linked biorepositories have expanded opportunities for precision medicine research. These integrated datasets open opportunities for developing accurate EHR-based personalized cancer risk and progression prediction models, which can be easily incorporated into clinical practice and ultimately realize the promise of precision oncology. However, efficiently and effectively using EHR for cancer research remains challenging due to practical and methodological obstacles. For example, obtaining precise event time information such as time of cancer recurrence is a major bottleneck in using EHR for precision medicine research due to the requirement of laborious medical record review and the lack of documentation. Simple estimates of the event time based on billing or procedure codes may poorly approximate the true event time. Naive use of such estimated event times can lead to highly biased estimates due to the approximation error. Such biases impose challenges to performing pragmatic trials when the study endpoint is time to events and captured using EHR. The overall goal of this proposal is to fill these methodological gaps in risk assessment for cancer research using EHR data, which will advance our ability to achieve the promise of precision oncology. Statistical algorithms and software will be developed to (i) automatically assign event time information using longitudinally recorded EHR information; and (ii) to perform accurate risk assessment using noisy proxies of event times. The proposed tools for risk assessment using imperfect EHR data without requiring extensive manual chart review could greatly improve the utility of EHR for oncology research. PROJECT NARRATIVE This proposal addresses major methodological gaps in effectively utilizing EHR data for risk assessment due to the noisy nature of EHR data. We propose novel statistical algorithms and software to (i) efficiently annotate event time by combining multiple longitudinally recorded code information and (ii) to provide precise risk estimates using noisy proxies of event times. By drawing upon the rich albeit imperfect data from bio-repository linked EHR, our algorithms will advance our ability to use EHR data for precision oncology research.",Semi-supervised Algorithms for Risk Assessment with Noisy EHR Data,9827744,R21CA242940,"['Address', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Biological', 'Boston', 'Cancer Patient', 'Clinical', 'Clinical Trials', 'Code', 'Cohort Studies', 'Data', 'Data Set', 'Diagnosis', 'Documentation', 'Electronic Health Record', 'Event', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Infrastructure', 'Label', 'Laboratories', 'Lead', 'Link', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measurement', 'Medical Records', 'Methodology', 'Methods', 'Nature', 'Patients', 'Phenotype', 'Procedures', 'Progression-Free Survivals', 'Proxy', 'Registries', 'Research', 'Risk', 'Risk Assessment', 'Risk Estimate', 'Software Tools', 'Source', 'Statistical Algorithm', 'Supervision', 'Testing', 'Time', 'adjudicate', 'anticancer research', 'base', 'biobank', 'cancer recurrence', 'cancer risk', 'clinical practice', 'cohort', 'electronic data', 'evidence base', 'genomic data', 'improved', 'learning algorithm', 'multimodality', 'novel', 'oncology', 'patient population', 'patient subsets', 'pragmatic trial', 'precision medicine', 'precision oncology', 'predictive modeling', 'repository', 'supervised learning', 'tool', 'tumor progression', 'user friendly software', 'web site']",NCI,HARVARD SCHOOL OF PUBLIC HEALTH,R21,2019,192385,0.030555736051897923
"Automated domain adaptation for clinical natural language processing Project Summary Automatic extraction of useful information from clinical texts enables new clinical research tasks and new technologies at the point of care. The natural language processing (NLP) systems that perform this extraction rely on supervised machine learning. The learning process uses manually labeled datasets that are limited in size and scope, and as a result, applying NLP systems to unseen datasets often results in severely degraded performance. Obtaining larger and broader datasets is unlikely due to the expense of the manual labeling process and the difficulty of sharing text data between multiple different institutions. Therefore, this project develops unsupervised domain adaptation algorithms to adapt NLP systems to new data. Domain adaptation describes the process of adapting a machine learning system to new data sources. The proposed methods are unsupervised in that they do not require manual labels for the new data. This project has three aims. The first aim makes use of multiple existing datasets for the same task to study the differences in domains, and uses this information to develop new domain adaptation algorithms. Evaluation uses standard machine learning metrics, and analysis of performance is tightly bounded by strong baselines from below and realistic upper bounds, both based on theoretical research on machine learning generalization. The second aim develops open source software tools to simplify the process of incorporating domain adaptation into clinical text processing workflows. This software will have input interfaces to connect to methods developed in Aim 1 and output interfaces to connect with Apache cTAKES, a widely used open- source NLP tool. Aim 3 tests these methods in an end-to-end use case, adverse drug event (ADE) extraction on a dataset of pediatric pulmonary hypertension notes. ADE extraction relies on multiple NLP systems, so this use case is able to show how broad improvements to NLP methods can improve downstream methods. This aim also creates new manual labels for the dataset for an end-to-end evaluation that directly measures how improvements to the NLP systems lead to improvement in ADE extraction. Project Narrative Software systems that use machine learning to understand clinical text often suffer severe performance loss when they are applied to new data that looks different than the data that they originally learned from. In this project, we develop and implement methods that allow these systems to automatically adapt to the characteristics of a new data source. We evaluate these methods on the clinical research task of adverse drug event detection, which relies on many different variables found in the text of electronic health records.",Automated domain adaptation for clinical natural language processing,9579181,R01LM012918,"['Adult', 'Adverse drug event', 'Adverse effects', 'Algorithms', 'Apache', 'Area', 'Biological Neural Networks', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Colon Carcinoma', 'Communities', 'Computer software', 'Computers', 'Conceptions', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Dimensions', 'Ecosystem', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation', 'Human', 'Institution', 'Knowledge', 'Label', 'Language', 'Lead', 'Learning', 'Linguistics', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Network-based', 'Output', 'Pathology', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Pulmonary Hypertension', 'Radiology Specialty', 'Research', 'Software Tools', 'Source', 'Statistical Models', 'Structure', 'Supervision', 'System', 'Testing', 'Text', 'TimeLine', 'Training', 'Update', 'Vision', 'Work', 'base', 'case finding', 'improved', 'learning strategy', 'malignant breast neoplasm', 'method development', 'natural language', 'new technology', 'news', 'novel', 'open source', 'point of care', 'social media', 'software systems', 'statistics', 'tool', 'tumor', 'unsupervised learning']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2018,416066,0.012589197505892616
"Improving Specialty Care Delivery in the Safety Net with Natural Language Processing Project Summary  Safety net providers treat a substantial share of socioeconomically vulnerable patients in their communities, but struggle to provide timely access to high quality specialty care for their patients. Delayed access to specialty care is associated with worse health outcomes and potentially contributes to health disparities across socioeconomic groups. Given their limited resources, safety net providers must seek creative approaches to improve specialty access. However, to choose what programs to implement, safety net providers need to understand the specialty care needs of their populations. Fortunately, the adoption of eConsult systems by safety net providers across the US provides a valuable opportunity to systematically measure patterns of specialty care referrals for minority, underserved populations.  In this project, we propose using state-of-the-art methods in machine learning and natural language processing (NLP) to help safety net providers extract actionable, population wide data from their electronic consultation systems. We will do this in partnership with three of the most prominent safety net health systems in the US in Los Angeles, San Francisco and New York City. Using specialty request databases from our collaborators, we will build NLP systems to automatically classify specialty requests along two dimensions: the “clinical issue” motivating the request (e.g., chest pain), and the “question type” (e.g., request for a procedure, help with medication management). This automated classification of electronic specialty requests can enable identification of promising targets for interventions to improve specialty access and quality of care.  After developing these NLP systems, we will analyze >1 million specialty requests to describe trends in how safety net patients are referred to specialists and examine variation in referral patterns by clinic and individual provider. The goal is to identify the most impactful opportunities to improve specialty access and quality. For example, a high rate of referrals for esophageal reflux, which most PCPs can treat on their own with specialist guidance, could lead to new treatment algorithms, potentially reducing the need for these requests and improving access for other patients.  This proposal is a “high-risk high-reward” project that creates new research tools to identify and evaluate data-driven interventions to improve specialty care delivery for underserved populations. Project Narrative Access to timely, high-quality specialty care is a fundamental component of a well-functioning health system, yet safety net health care providers face persistent challenges delivering such care. Quality improvement efforts to improve specialty access have been thwarted in part because safety net providers lack the data to understand a basic question – why patients are referred for specialty care. Taking advantage of the growing use of electronic specialty referral systems by safety net providers, we propose using natural language processing to conduct automated analysis and classification of specialty requests in safety net populations, which will enable the design of targeted interventions to improve specialty care access and delivery.",Improving Specialty Care Delivery in the Safety Net with Natural Language Processing,9600732,R21MD012693,"['Acute', 'Adopted', 'Adoption', 'Algorithms', 'Caring', 'Chest Pain', 'Chronic Kidney Failure', 'Classification', 'Clinic', 'Clinical', 'Communities', 'Consultations', 'County', 'Data', 'Data Set', 'Databases', 'Education', 'Epidemiology', 'Face', 'Federally Qualified Health Center', 'Gastroesophageal reflux disease', 'Goals', 'Health', 'Health Personnel', 'Health Services', 'Health Services Accessibility', 'Health system', 'Heart failure', 'Hospitals', 'Improve Access', 'Individual', 'Intervention', 'Lead', 'Los Angeles', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Education', 'Medical center', 'Medication Management', 'Methods', 'Minority', 'Morbidity - disease rate', 'Natural Language Processing', 'New York City', 'Online Systems', 'Ophthalmology', 'Outcome', 'Patients', 'Pattern', 'Play', 'Population', 'Primary Care Physician', 'Procedures', 'Provider', 'Public Hospitals', 'Quality of Care', 'Research', 'Resources', 'Retinal Diseases', 'Role', 'San Francisco', 'Specialist', 'System', 'Taxonomy', 'Telemedicine', 'Text', 'Time', 'Transplantation', 'Triage', 'Underserved Population', 'Variant', 'Visit', 'care delivery', 'design', 'diabetic', 'disease classification', 'ethnic minority population', 'follow-up', 'health disparity', 'high reward', 'high risk', 'improved', 'medical specialties', 'medically underserved', 'minority communities', 'mortality', 'performance tests', 'programs', 'racial and ethnic', 'safety net', 'screening', 'socioeconomic disadvantage', 'socioeconomics', 'tool', 'trend', 'two-dimensional']",NIMHD,BOSTON CHILDREN'S HOSPITAL,R21,2018,278902,0.00939439874836095
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,9547946,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Meta-Analysis', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phrases', 'portability', 'preservation', 'recruit', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2018,1542081,0.020017800382485663
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,9534183,R01LM011934,"['Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Ontology', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'information model', 'novel', 'open source', 'profiles in patients', 'public health relevance', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2018,387966,0.03795178485971105
"A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications DESCRIPTION (provided by applicant): Electronic Health Records (EHRs) can improve the quality of healthcare delivery in the United States, by providing automated best-practice reminders to clinicians and patients. However such functionality is currently limited to narrow areas of clinical practice, as existing decision support systems can process only structured data, due to lack of a suitable framework and concerns about accuracy and portability. Preliminary work by the PI has shown that rule-based approach can be used to develop broad-domain reminder systems that can utilize free-text in addition to the structured data. The PI has developed prototype systems for cervical and colorectal cancer prevention. These systems consist of rule-based composite models of national guidelines, and rule-based Natural Language Processing (NLP) parsers. The NLP parsers extract the patient variables required for applying the guidelines. However further research is needed to extend the systems and to ensure their accuracy for clinical deployment. In the mentored phase, the PI will collaborate with clinicians to extend and iteratively optimize and validate the systems, and will make them available in open-source so that they can be adapted for deployment at other institutions (aim 1 - K99). In the independent phase, the PI will research methods to facilitate rapid development, deployment and cross- institutional portability of similar systems. Specifically, the PI will develp a hybrid design for the parsers and investigate domain adaptation and active learning methods, for reducing the manual effort for development and adaptation of the NLP parsers (aim 2 - R00). To enable other researchers to reuse the developed methodologies and software resources, a toolkit will be developed that will support the construction and deployment of similar systems (aim 3 - R00). The toolkit will consist of user-friendly tools and templates to replicate the processes engineered in the case studies, and will build on the SHARPn data normalization tooling and other open-source tools. The independent phase will be in collaboration with Intermountain Healthcare. The PI's career goal is to become a scientific leader in clinical informatics with a focus on optimizing clinical decision making. The PI has strong background in clinical medicine and medical informatics, and will receive mentoring from Drs. Hongfang Liu, Christopher Chute, Robert Greenes and Rajeev Chaudhry, who have complimentary areas of expertise. The mentored (K99) phase will be for 2 years at Mayo Clinic Rochester, wherein the PI will undertake courses on decision support and will get mentored training in NLP and health information standards. This will prepare the PI for independent research in R00 phase on portability and tooling. Completion of the proposed work will enable the PI to seek further funding for piloting clinical deployment of the developed systems, measuring their clinical impact, and for scaling the approach to other clinical domains and institutions. The career grant will enable the PI to establish himself as an independent investigator and to make significant contributions towards advancing clinical decision support for improving care delivery. PUBLIC HEALTH RELEVANCE STATEMENT The potential of Electronic Health Records (EHRs) to improve care delivery by providing best-practice reminders is unrealized, because reminder systems currently operate in narrow areas of clinical practice, as they can process only structured data. The proposed framework will enable construction of reminder systems that can encompass broader areas of practice, due to their capability to utilize free-text as well as structured EHR data. This pioneering research directly impacts public health by improving the quality of care through enhanced reminder functionality in the EHRs.",A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications,9406887,R00LM011575,"['Active Learning', 'Address', 'Area', 'Caregivers', 'Caring', 'Case Study', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Collaborations', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Decision Support Systems', 'Development', 'Electronic Health Record', 'Engineering', 'Ensure', 'Fostering', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Care Costs', 'Healthcare', 'Hybrids', 'Institution', 'Language', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Manuals', 'Measures', 'Medical Informatics', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Phase', 'Process', 'Public Health', 'Quality of Care', 'Reminder Systems', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Standardization', 'Structure', 'Supervision', 'System', 'Text', 'Training', 'United States', 'Validation', 'Work', 'base', 'care delivery', 'career', 'clinical application', 'clinical decision support', 'clinical decision-making', 'clinical practice', 'colorectal cancer prevention', 'colorectal cancer screening', 'design', 'electronic structure', 'health care delivery', 'health care quality', 'improved', 'learning strategy', 'open source', 'portability', 'prevent', 'prototype', 'public health relevance', 'tool', 'user-friendly']",NLM,MASSACHUSETTS GENERAL HOSPITAL,R00,2018,248969,0.0423270050500431
"From enrichment to insights Project Summary Most medical decisions are made without the support of rigorous evidence in large part due to the cost and complexity of performing randomized trials for most clinical situations. In practice, clinicians must use their judgement, informed by their own and the collective experience of their colleagues. The advent of the electronic health record (EHR) enables the modern practitioner to algorithmically check the records of thousands or millions of patients to rapidly find similar cases and compare outcomes. In addition to filling the inferential gap in actionable evidence, these kinds of analyses avoid issues of ethics, practicality, and generalizability that plague randomized clinical trials (RCTs). Unfortunately, identifying patients with the appropriate phenotypes, properly leveraging available data to adjust results, and matching similar patients to reduce confounding remain critical challenges in every study that uses EHR data. Overcoming these challenges to improve the accuracy of observational studies conducted with EHR data is of paramount importance. Studies using EHR data begin by defining a set of patients with specific phenotypes, analogous to amassing a cohort for a clinical trial. This process of electronic phenotyping, is typically done via a set of rules defined by experts. Machine learning approaches are increasingly used to complement consensus definitions created by experts and we propose several advances to validate and improve this practice. We will explore and quantify the effects of feature engineering choices to transform the diagnoses, procedures, medications, laboratory tests and clinical notes in the EHR into a computable feature matrix. Finally, building on recent advances, we plan to characterize the performance of existing methods and develop EHR-specific strategies for patient matching. Our work is significant because we will take on three challenging problems--electronic phenotyping, feature engineering, and patient matching--that stand in the way of generating insights via EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes. Narrative The advent of the electronic health record (EHR) enables the search of thousands or millions of patients to rapidly find similar cases and compare outcomes. We will develop methods for feature engineering, electronic phenotyping and patient matching from real-world EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes.",From enrichment to insights,9535477,R01LM011369,"['Address', 'Algorithms', 'Area', 'Clinical', 'Clinical Trials', 'Code', 'Complement', 'Consensus', 'Data', 'Data Element', 'Data Set', 'Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evaluation', 'Frequencies', 'Future', 'Goals', 'Health system', 'Healthcare Systems', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Modernization', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Plague', 'Procedures', 'Process', 'Randomized Clinical Trials', 'Records', 'Resources', 'Scheme', 'Source', 'Statistical Data Interpretation', 'Test Result', 'Testing', 'Time', 'Training', 'Work', 'base', 'cohort', 'cost', 'electronic data', 'experience', 'health data', 'improved', 'innovation', 'insight', 'novel', 'portability', 'randomized trial', 'simulation', 'treatment effect', 'vector']",NLM,STANFORD UNIVERSITY,R01,2018,694405,0.06356173447648324
"Bayesian Generative Methods for Extracting and Modeling Relations in EHR Narratives Project Summary Medicine has evolved into an era where the entire hospital progressively adopts more real-time monitoring for the patients and generates ICU like clinical data. The rapidly growing data makes ICU a snapshot for tomorrow’s standard of care that should benefit from computer-aided decision making. These data contain not only numerical or coded information, but also a large volume of unstructured narrative text such as physicians’ and nurses' notes, specialists' reports, and discharge summaries. Both types of data have been shown to be highly informative for tasks such as cohort selection, and work best in combination. However, to achieve this, specific bits of information must be extracted from the narrative reports and coded in formal representation. These bits include medical concepts such as symptoms, diseases, medications and procedures; characteristics such as certainty, severity, dose; assertions about these items, such as whether they pertain to the patient or a family member, etc.; relations among these mentions, including indications of what condition is treated by what action and its degree of success, the time sequence and duration of events, and interpretations of laboratory test results as relations among medical concepts such as cells and antigens (e.g., “[large atypical cells] express [CD30]”). Concepts and assertions can be regarded as simple relations, and our proposal focuses on modeling narrative relations to augment structured data for predicting patient outcomes. Most existing techniques for interpreting clinical narratives either rely on hand-crafted rule systems and large medical thesauri or are based on machine learning models that create classification or regression models from large annotated data sets. The former are difficult and laborious to generalize, whereas the latter require large volumes of human-labeled data and may result in models whose operation is difficult to interpret and is therefore considered unsuitable for computer-aided decision making. We propose to build on our previous work to use unsupervised learning methods that identify frequent patterns in un-annotated narratives and identify informative patterns by tensor factorization. Although existing methods can also identify patterns that are meaningful in a data-driven sense, these patterns are difficult for clinicians to understand. Our specific goal is to develop a novel method that uses a Bayesian generative model that integrates relation mining with tensor factorization to learn patterns that correspond to an understanding of the clinical domain and can be used for evidence based patient outcome prediction. Our framework represents relations in clinical narratives as graphs, then mines subgraphs for important relations. These relations are used as features in building up a tensor model in order to reduce dimensionality, discover coherent groups of relations, and explore the group interactions. We develop Bayesian formulation to integrate relation mining and tensor modeling in a generative model, to incorporate existing medical knowledge as probability priors, as well as to reliably estimate the posterior probabilities and confidence intervals of any findings from the model. Rapid growth in the hospital adoption of large volume of Electronic Health Records (EHRs) has led to an unprecedented availability of narrative dataset for clinical and translational research. We propose the development of a novel Bayesian generative framework to enable extraction of accurate and clinically meaningful patterns of EHR narratives in order to support evidence based diagnostic reasoning and outcome risk prediction.",Bayesian Generative Methods for Extracting and Modeling Relations in EHR Narratives,9535479,R21LM012618,"['Admission activity', 'Adopted', 'Adoption', 'Algorithms', 'Antigens', 'Appearance', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Cognitive', 'Computer Assisted', 'Confidence Intervals', 'Data', 'Data Set', 'Decision Making', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Dose', 'Electronic Health Record', 'Event', 'Family member', 'Formulation', 'Goals', 'Graph', 'Hand', 'Hodgkin Disease', 'Hospitals', 'Human', 'Individual', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Lymphoma', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Morphology', 'National Research Council', 'Nurses', 'Outcome', 'Output', 'Pathology Report', 'Patient Monitoring', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Probability', 'Procedures', 'Reporting', 'Research', 'Risk', 'Risk Factors', 'Severities', 'Source', 'Specialist', 'Structure', 'Symptoms', 'System', 'TNFRSF8 gene', 'Techniques', 'Test Result', 'Text', 'Thesauri', 'Time', 'To specify', 'Translational Research', 'Work', 'adverse outcome', 'base', 'clinical decision support', 'cohort', 'data warehouse', 'diagnostic panel', 'evidence base', 'improved', 'learning strategy', 'mortality', 'novel', 'operation', 'outcome prediction', 'patient oriented', 'rapid growth', 'real time monitoring', 'standard of care', 'success', 'unsupervised learning']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2018,181755,0.03776614795429494
"Algorithms to Identify Systemic Lupus from Electronic Health Record Data Abstract  Systemic lupus erythematosus (SLE) is a chronic, autoimmune, multisystem disease that is often difficult to diagnose because of the diverse manifestations that occur over time and across care sites leading to increased damage and early mortality. The personal and economic costs in decreased quality of life and increase in healthcare expenditures, respectively, highlight the critical unmet need to develop new therapeutic strategies to treat lupus, so that treatment or participation in clinical trials occurs as early as possible to mitigate against disease-related damage. Therefore, it is important to find better ways to identify SLE patents.  Electronic health records (EHR) are now used in a majority of health care settings throughout the country, and present a rich source of information about patients which can be mined for earlier diagnosis and identification to improve quality of care, or enable high throughput clinical studies. Despite this potential, to date few accurate algorithms have been developed to identify SLE patients using EHR data. Construction of an effective algorithm, either by rule-based or machine learning methods, requires access to at two data resources not commonly available: 1) a validated “gold standard” patient data set with clear documentation of criteria that are indicative of SLE that can be compared against EHR data and 2) an integrated health record dataset that contains data from multiple health care institutions and reflects that SLE patients receive healthcare at multiple institutions and healthcare providers given their chronic, progressive disease. Over the past several years, our team has created both key resources: the Chicago Lupus Database (CLD), a physician-validated registry of 880 patients and gold standard data set and the Chicago HealthLNK Data Repository (HDR), a regional data resource including integrated medical records for 2.1 million patients across multiple institutions. Jointly, these two datasets enable the creation, testing and validation of algorithms for the identification of SLE in EHR data and provide a more complete picture of a patient population at risk for lupus.  We propose three specific aims to address the need to reduce the time to identify those with SLE in order to initiate treatment in a timelier fashion and to identify candidates for clinical trials. These aims are: 1) To create and validate a series of algorithms to identify SLE patients in EHR data against a gold standard curated registry, CLD, using validated classification criteria for SLE to build concepts for rule-based and machine learning methods that incorporate structured data, laboratory data, and unstructured data, e.g., physician notes, 2) To determine whether identification of SLE patients is improved when algorithms to identify SLE patients are extended to an integrated medical record dataset that includes data from multiple health care institutions, and 3) To use clustering techniques on SLE patients identified from EHR data to isolate clinically distinct sub-populations of patients, which could inform patient selection for participation in clinical trials. Project Narrative  Systemic lupus erythematosus (SLE) is a chronic, autoimmune, multisystem disease that is often difficult to diagnose because of the diverse manifestations that occur over time and across care sites leading to increased damage and early mortality. Electronic health records (EHR) are now used in a majority of health care settings throughout the country, and present a rich source of information about patients which can be mined for earlier diagnosis and identification to improve quality of care, or enable high throughput clinical studies. This study will test and validate algorithms for the identification of SLE from EHR data and thus will help provide a more complete picture of a patient population at risk for lupus.",Algorithms to Identify Systemic Lupus from Electronic Health Record Data,9536683,R21AR072263,"['Address', 'Affect', 'Algorithms', 'American', 'Autoimmune Process', 'Caring', 'Chicago', 'Chronic', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Collection', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Documentation', 'Early Diagnosis', 'Early identification', 'Electronic Health Record', 'Flare', 'General Population', 'Goals', 'Gold', 'Health Expenditures', 'Health Personnel', 'Health system', 'Healthcare', 'Institution', 'Laboratories', 'Legal patent', 'Lupus', 'Machine Learning', 'Medical Care Costs', 'Medical Records', 'Medicine', 'Patient Selection', 'Patients', 'Phenotype', 'Physicians', 'Populations at Risk', 'Progressive Disease', 'Quality of Care', 'Quality of life', 'Registries', 'Research', 'Resources', 'Sensitivity and Specificity', 'Series', 'Site', 'Source', 'Structure', 'Symptoms', 'Systemic Lupus Erythematosus', 'Techniques', 'Testing', 'Time', 'Validation', 'base', 'clinical candidate', 'clinical practice', 'data resource', 'data warehouse', 'economic cost', 'health care settings', 'health record', 'improved', 'learning strategy', 'mortality', 'novel therapeutic intervention', 'patient population', 'patient subsets', 'personalized medicine', 'prevent', 'standard of care', 'targeted treatment', 'therapeutic evaluation']",NIAMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2018,173800,0.03811240117610095
"Evaluating the Impact of Changes in Opioid Prescribing Across Health Systems Implementing Zero Suicide This timely supplement would support our goals for the current award: An Evaluation of the National Zero Suicide Model Across Learning Healthcare Systems (U01MH114087) by capitalizing on a natural experiment, the planned the national roll-out of safety planning templates in behavioral health departments across five Kaiser Permanente regions and Henry Ford Health System in 2019. Safety planning is a highly recommended practice within the Zero Suicide (ZS) framework, but little is known about the effectiveness of the individual elements that can make up a safety plan, such as lethal means assessment, identification of supportive contacts, coping skills, warning signs, and sources of distraction. The current Zero Suicide award proposes to examine the impact of safety planning and lethal means assessment using a stepped-wedged interrupted time- series (ITS) approach, measuring each as a binary variable (e.g. safety planning did or did not occur). The ITS approach requires that some sites implement safety planning (intervention sites for safety planning), while others do not (control sites for safety planning). The proposed ITS approach is now problematic without further work for two reasons: 1) All Kaiser Permanente sites and Henry Ford have decided to uniformly implement safety planning around the same time, therefore there are no control sites 2) Without control sites, metrics that can accurately measure variation in safety planning/lethal means assessment at baseline and then longitudinally thereafter would enable our evaluation to take place, but all of the documentation lives in text- based clinical narratives. In working with our health system leads on the development of Zero Suicide metrics, we have been informed that the rate for safety planning and lethal means assessment at baseline is not zero, but the actual rate is unknown. This supplement will support development of new metrics using Natural Language Processing to determine baseline rates, from which, we can quantify the change in safety planning and lethal means assessment practice longitudinally after implementation of new safety planning templates using our Zero Suicide main award. Furthermore, we propose to take advantage of the newly implemented templates to address an important mediator of the effect of safety planning on suicide outcomes, the impact of fidelity to the new templates, which we define as quality, completeness, and level of integration with ongoing care. We propose the following three specific aims for this supplemental work: 1) Identify key terms for safety planning and lethal means assessment 1.) Develop Natural Language Processing (NLP) metrics to assess the occurrence of safety planning and lethal means assessment at three Zero Suicide sites 2) Implement NLP queries for identification of safety planning and lethal means assessment and measure baseline rates 3) Upon implementation of electronic safety planning templates in medical records, develop and implement metrics using NLP for assessing fidelity (completeness, quality, integration with care) to safety planning templates. Project Narrative: This supplement to the current award: An Evaluation of the National Zero Suicide Model Across Learning Healthcare Systems (U01MH114087) will take advantage of a national roll-out for safety planning templates in the electronic medical record across 5 Kaiser Permanente regions and Henry Ford Health System. It will support the development of innovative measures using natural language processing to efficiently quantify the baseline rates of safety planning and lethal means assessment across multiple sites, which will enable a rigorous evaluation to take place upon implementation of the new electronic templates. Furthermore, we propose to take advantage of the safety planning template roll-out by measuring fidelity (quality, completeness, and level of integration with care), because it may be an important mediator of the relationship between safety planning and suicide outcomes.",Evaluating the Impact of Changes in Opioid Prescribing Across Health Systems Implementing Zero Suicide,9676620,U01MH114087,"['Accident and Emergency department', 'Address', 'Adopted', 'Algorithms', 'Award', 'Caring', 'Clinical', 'Code', 'Colorado', 'Computerized Medical Record', 'Coping Skills', 'Country', 'Development', 'Disease', 'Documentation', 'Effectiveness', 'Elements', 'Evaluation', 'Firearms', 'Frequencies', 'Future', 'Goals', 'Health', 'Health system', 'Healthcare Systems', 'Individual', 'Institutes', 'Interruption', 'Intervention', 'Investigation', 'Learning', 'Measures', 'Mediator of activation protein', 'Medical Records', 'Mental Health', 'Mining', 'Natural Language Processing', 'Natural experiment', 'Outcome', 'Pharmaceutical Preparations', 'Predictive Value', 'Process', 'Research', 'Research Personnel', 'Safety', 'Series', 'Site', 'Source', 'Standardization', 'Suicide', 'Suicide prevention', 'System', 'Text', 'Time', 'Variant', 'Work', 'base', 'behavioral health', 'behavioral outcome', 'design', 'distraction', 'health plan', 'health record', 'improved', 'innovation', 'method development', 'prescription opioid', 'reducing suicide', 'safety practice', 'suicidal behavior', 'suicide model', 'tool']",NIMH,HENRY FORD HEALTH SYSTEM,U01,2018,262124,-0.09447556204346201
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics. PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,9515026,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Base Sequence', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'DNA sequencing', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Elements', 'Ethics', 'Evaluation', 'Faculty', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Recommendation', 'Records', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Ritalin', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'base', 'biobank', 'clinical care', 'clinical decision support', 'clinically actionable', 'cohort', 'cost', 'data modeling', 'data warehouse', 'database of Genotypes and Phenotypes', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'learning strategy', 'member', 'next generation sequencing', 'patient oriented', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'public health relevance', 'response', 'skills', 'social', 'success', 'support tools', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2018,855289,0.017226561382087276
"Efficient Statistical Learning Methods for Personalized Medicine Using Large Scale Biomedical Data Project Summary/Abstract  Current medical treatment guidelines largely rely on data from randomized controlled trials that study av- erage effects, which may be inadequate for making individualized decisions for real-world patients. Large-scale electronic health records (EHRs) data provide unprecedented opportunities to optimize personalized treatment strategies and generate evidence relevant to real-world patients. However, there are inherent challenges in the use of EHRs, including non-experimental nature of data collection processes, heterogeneous data types with complex dependencies, irregular measurement patterns, multiple dynamic treatment sequences, and the need to balance risk and beneﬁt of treatments. Using two high-quality EHR databases, Columbia University Medical Center's clinical data warehouse and the Indiana Network for Patient Care database, and focusing on type 2 diabetes (T2D), this proposal will develop novel and scalable statistical learning approaches that overcome these challenges to discover optimal personalized treatment strategies for T2D from real-world patients. Speciﬁcally, under Aim 1, we will develop a uniﬁed framework to learn latent temporal processes for feature extraction and dynamic patient records representation. Our approach will accommodate large-scale variables of mixed types (continuous, binary, counts) measured at irregular intervals. They extract lower-dimensional components to reﬂect patients' dynamic health status, account for informative healthcare documentation processes, and characterize similarities between patients. Under Aim 2, we will develop fast and efﬁcient multi-category machine learning methods, in order to evaluate treatment propensities and adaptively learn optimal dynamic treatment regimens (DTRs) among the extensive number of treatment options observed in the EHRs. The methods will provide se- quential decisions that determine the best treatment sequence for a T2D patient given his/her EHRs. Under Aim 3, we will develop statistical learning methods to assist multi-faceted treatment decision-making, which balances risks versus beneﬁts when evaluating a DTR. Our approach will ensure maximizing beneﬁt to the greatest extent while controlling all risk outcomes under the safety margins. For all aims, we will develop efﬁcient stochastic resampling algorithms to scale up the optimization for massive data sizes. We will identify optimal DTRs for T2D using the extracted information from patients' comorbidity conditions, medications, and laboratory tests, as well as records-collection processes. Our methodologies will be applied and cross-validated between the two EHR databases. The treatment strategies learned from the representative EHR databases with a diverse patient pop- ulation will be beneﬁcial for individual patient care, assisting clinicians to adaptively choose the optimal treatment for a patient. Finally, we will disseminate our methods and results through freely available software and outreach to the informatics and clinical experts at our Centers for Translational Science and elsewhere. Project Narrative  This proposal aims to develop novel and scalable statistical learning methods to analyze electronic health records (EHRs) and use two real-world, high-quality EHR databases for personalized medicine research. The methods will handle the non-experimental nature of data collection processes, along with heterogeneous data types, dynamic treatment sequences, and the trade-off between benefit and risk outcomes. The results will complement the current knowledge base for individual patient care using evidence generated from patients in real-world clinical practices.",Efficient Statistical Learning Methods for Personalized Medicine Using Large Scale Biomedical Data,9519452,R01GM124104,"['Academic Medical Centers', 'Address', 'Adverse event', 'Algorithms', 'Benefits and Risks', 'Categories', 'Center for Translational Science Activities', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Comorbidity', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Databases', 'Decision Making', 'Dependence', 'Dimensions', 'Documentation', 'Electronic Health Record', 'Ensure', 'Equilibrium', 'Exclusion Criteria', 'Formulation', 'Gaussian model', 'Goals', 'Health', 'Health Status', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Indiana', 'Informatics', 'International', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Process', 'Quality Control', 'Randomized Controlled Trials', 'Records', 'Research', 'Risk', 'Safety', 'Sampling', 'Structure', 'Testing', 'Time', 'Treatment Protocols', 'adaptive learning', 'algorithmic methodologies', 'analytical tool', 'base', 'clinical data warehouse', 'clinical decision-making', 'clinical practice', 'data modeling', 'data space', 'design', 'evidence base', 'individual patient', 'individualized medicine', 'knowledge base', 'learning strategy', 'novel', 'optimal treatments', 'outreach', 'patient population', 'personalized decision', 'personalized medicine', 'population based', 'scale up', 'temporal measurement', 'theories', 'treatment guidelines', 'treatment response', 'treatment strategy']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,352653,0.021190246650461524
"Improving accreditation, certification and quality improvement programs through automated abstraction of electronic health data. ABSTRACT SBIR Phase I: Reduce the burden of data abstraction for certification, accreditation and quality improvement programs via a new platform to automate the abstraction of measures and a novel interface to improve human abstractor workflow. The broad impact/commercial potential of this SBIR Phase I project is to drive down the burden of participation in quality improvement programs, specifically, the resource intensive manual data abstraction process to derive and report quality measures that are instrumental to improving health outcomes and controlling costs. Patient Insight proposes improvements through automated abstraction methods and an interface for human abstractors to better visualize electronic health record (EHR) data and complete their work. The existing approach is a barrier to broader uptake of quality improvement initiatives such as accreditation. Patient Insight’s proposed solution consists of leveraging both proven and proprietary technologies to extract data via EHR agnostic application programming interfaces (APIs), better target eligible patients, and calculate measures via natural language processing (NLP) algorithms and custom queries. The core innovation is a novel data mining engine and user interface that improves the process of human data abstraction for clinical and quality documentation such as those required to achieve accreditation. The proposed project will allow Patient Insight, in partnership with the American College of Cardiology, a leader in the hospital based accreditation space, to develop a proof-of-concept pilot project at a select hospital and compare human and automated data abstraction methods for a pre-selected number of measures that are mandated as part of the ACC’s Heart Failure Accreditation program. Once completed, Patient Insight will expand on the technology, design and user research data/requirements with a roadmap for product enhancements for Phase II that will enable the building of a commercially viable ‘add-on’ service for sites participating in the ACC’s suite of accreditation programs and eventually expand to additional ACC service lines as well as other accreditation and certifying bodies. Success will represent a transformative change in general-purpose abstraction and an interface that will support a broad array of accreditation measures and abstraction workflows thereby solving a critical inefficiency both clinically and financially for hospitals while improving patient health outcomes. PROJECT NARRATIVE Health service accreditation and certification programs are a critical mechanism to direct care quality improvements and ensure compliance with regulations. Reporting on requisite measures is a resource intensive and costly process requiring human data abstractors to interpret heterogeneous and disparately presented data elements from the electronic health record (EHR). Replacing human elements using evolving automated data abstraction methods and an interface for abstractors to better visualize EHR data and manage their workflow would solve an important, unmet need and offer a dramatic improvement over the status quo. Patient Insight’s commercial technology solution will facilitate efficiencies in automated data abstraction and human-to-computer interactions in quality improvement reporting.","Improving accreditation, certification and quality improvement programs through automated abstraction of electronic health data.",9622944,R43LM012955,"['Accreditation', 'Algorithms', 'American', 'Appointment', 'Back', 'Beds', 'Budgets', 'Cardiac rehabilitation', 'Cardiology', 'Caring', 'Certification', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Computers', 'Cost Control', 'Custom', 'Data', 'Data Element', 'Diuretics', 'Documentation', 'Electronic Health Record', 'Elements', 'Ensure', 'Event', 'Expenditure', 'Feedback', 'Hand', 'Health', 'Health Services', 'Health Services Accessibility', 'Health Status', 'Healthcare', 'Heart failure', 'Hospitals', 'Human', 'Information Technology', 'Investments', 'Licensing', 'Manuals', 'Measures', 'Methods', 'Mining', 'Natural Language Processing', 'Outcome', 'Outcome Measure', 'Paper', 'Patient Care', 'Patients', 'Phase', 'Pilot Projects', 'Population', 'Process', 'Quality of Care', 'Regulation', 'Reporting', 'Research', 'Resources', 'Schedule', 'Services', 'Site', 'Small Business Innovation Research Grant', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Work', 'application programming interface', 'base', 'cardiology service', 'certificate program', 'college', 'cost', 'data management', 'data mining', 'data visualization', 'design', 'exercise program', 'follow-up', 'health assessment', 'health data', 'human data', 'improved', 'innovation', 'insight', 'new technology', 'novel', 'product development', 'programs', 'readmission rates', 'success', 'tool', 'uptake', 'usability']",NLM,"PATIENT INSIGHT, INC.",R43,2018,259060,0.018471866023617178
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9419767,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Severe Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2018,619389,0.014580961630196563
"Health Information Technology for Surveillance of Health Care-Associated Infections PROJECT SUMMARY/ABSTRACT Health care-associated infections (HAIs) affect one in every 20 hospitalized patients and account for $10 billion dollars in potentially preventable health care expenditures annually. Current efforts at detection of HAIs are limited to manual chart review which hinders the generalizability and scalability of HAI detection. My goal in seeking a Mentored Clinical Scientist Career Development Award is to acquire the necessary training, practical experience, and knowledge to develop a health services research career as a principal investigator focusing on leveraging novel health information technology (HIT) tools to improve the measurement of surgical health care quality, safety, and effectiveness. To continue my progress towards this goal, the objective of this project is to address the challenges of HAI detection by developing a robust and portable automated HAI surveillance toolkit. This toolkit will combine structured electronic health record (EHR) data with rich information locked in clinical notes using machine learning and natural language processing (NLP) to identify HAIs after surgical procedures. Our overall hypothesis is that combining structured variables from the EHR supplemented with NLP will improve our ability to identify HAIs after surgical procedures. To test the central hypothesis and accomplish the objectives for this application, I will pursue the following three specific aims: 1) Determine the EHR data elements indicative of postoperative HAIs and evaluate the performance of a novel HAI surveillance algorithm; 2) Identify the presence of postoperative SSIs from clinical notes using an automated portable NLP-based algorithm; 3) Apply user-centered design to create a high fidelity prototype of a surgical quality dashboard incorporating our HAI case detection methodology. This contribution is a significant first step in a continuum of research that utilizes the large amounts of data in the EHR combined with novel HIT methods to improve the measurement of surgical health-care quality, safety, and effectiveness. This approach is significant because the tools developed in this proposal have potential to serve as a prototype for identification and monitoring hospitals adverse events and could be replicated on a national scale. The proposed research is innovative in its approach using a combination of structured and unstructured data in the EHR along with novel machine learning and NLP tools to create a generalizable surveillance toolkit for the detection of HAIs. This proposal is responsive to the AHRQ Special Emphasis Notice (NOT-HS-13-011) specifically addressing the use of HIT to improve quality measurement. I have assembled a mentoring team who all internationally recognized experts with long and successful track records of funding and trainee mentorship. This project will provide the means to place me on a trajectory towards a health services research career focused on improving the measurement of surgical health-care quality, safety, and effectiveness using novel HIT tools. In summary, my previous training and experience, innovative research plan, high-quality training plan, first-rate mentorship team, and supportive research environment give me the highest likelihood of success to research independence with the proposed K08 award. PROJECT NARRATIVE/RELEVANCE TO PUBLIC HEALTH The proposed research and career development plan are relevant to public health because health care- associated infections (HAIs) affect about one in every 20 hospitalized patients and account for $30 billion dollars in potentially preventable health care expenditures annually. The objective of this project is to develop robust and portable automated surveillance toolkit that combines structured EHR data with rich information locked in clinical notes using natural language processing to identify HAIs after surgical procedures. The proposed research has the potential to serve as a prototype for identification and monitoring of numerous hospitals adverse events and could be replicated on a national scale.",Health Information Technology for Surveillance of Health Care-Associated Infections,9599186,K08HS025776,[' '],AHRQ,UNIVERSITY OF UTAH,K08,2018,160164,0.003230982708041627
"Patient Medical History Representation, Extraction, and Inference from EHR Data DESCRIPTION (provided by applicant): The significance of developing tools for automatically harvesting temporal constraints of clinical events from Electronic Health Records (EHR) cannot be overestimated. Efficient analysis of the temporal aspects in EHR data could boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine.     One big challenge we are facing is to automatically untangle and linearize the temporal constraints of clinical events embedded in highly diverse large-scale EHR data. Barriers to temporal data modeling, normalization, extraction, and reasoning have precluded the efficient use of EHR data sources for event history evaluation and trending analysis: (1) The current federally-supported EHR data normalization tools do not focus on the time aspect of unstructured data yet; (2) Existing time models focus only on structured data with absolute time, lack of supporting reasoning systems, or only offer application-specific partial solutions which cannot be adopted by the complex EHR data; (3) Current temporal information extraction approaches are either difficult to be adopted to EHR data, not scalable, or only offers application-specific partial solution.     This proposed project fills in the current gaps among ontologies, Natural Language Processing (NLP), and EHR-based clinical research for temporal data representation, normalization, extractions, and reasoning. We propose to develop novel approaches for automatic temporal data representation, normalization and reasoning for large, diverse, and heterogeneous EHR data and prepare the integrated data for further analysis. We will build new reasoning and extraction capacities on our TIMER (Temporal Information Modeling, Extracting, and Reasoning) framework to provide an end-to-end, open-source, standard-conforming software package. TIMER will be built on strong prior work by our team. We will develop new features in our CNTRO (Clinical Narrative Temporal Relation Ontology) for semantically defining the time domain and representing temporal data in complex EHR data. On top of the new developed CNTRO semantics, we will implement temporal relation reasoning capacities to automatically normalize temporal expressions, compute and infer temporal relations, and resolve ambiguities. We will leverage existing NLP tools and work on top of these tools to develop new extraction approaches to fill in the current gaps between NLP approaches and ontology-based reasoning approaches. We will adapt the SHARPn EHR data normalization pipeline and cTAKES for extracting and normalizing clinical event mentions from clinical narratives. We will explore an innovative approach for temporal relation extraction and event coreference, and make it work with the TIMER framework. We will evaluate the system using Diabetes Mellitus (DM) and colorectal cancer (CRC) patient cohorts from two insititutions. Each component will be tested separately first followed by an evaluation of the whole framework. Results such as precision, recall, and f-measure will be reported. Project Narrative The significance of developing capabilities for automatically harvesting temporal constraints for clinical events from Electronic Health Records (EHR) cannot be overestimated. A substantial portion of the information in the EHR is historical in nature. Patient medical history can be long, especially in complex patients. The proposed work, by offering an end-to-end open-source framework for automatically extracting, normalizing, and reasoning clinically-important time-relevant information from large-scale EHR data, can boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine; as well as facilitate clinical practice for early disease detection, post-treatment care, and patient-clinician communication.","Patient Medical History Representation, Extraction, and Inference from EHR Data",9534182,R01LM011829,"['Address', 'Adopted', 'Aftercare', 'Archives', 'Automated Annotation', 'Big Data', 'Caring', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Colorectal Cancer', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Data Reporting', 'Data Set', 'Data Sources', 'Databases', 'Decision Support Systems', 'Detection', 'Diabetes Mellitus', 'Disease', 'Disease Progression', 'Electronic Health Record', 'Evaluation', 'Event', 'Goals', 'Gold', 'Harvest', 'Human', 'Institutes', 'Maps', 'Measures', 'Medical History', 'Medical Records', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Performance', 'Recording of previous events', 'Registries', 'Reporting', 'Resolution', 'Semantics', 'Structure', 'Supervision', 'System', 'Testing', 'Time', 'Translational Research', 'Work', 'application programming interface', 'base', 'clinical practice', 'cohort', 'colon cancer patients', 'data modeling', 'data structure', 'information model', 'innovation', 'novel strategies', 'open source', 'patient-clinician communication', 'personalized medicine', 'tool', 'trend analysis']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2018,335217,0.058406506835919574
"Enhance Arthroplasty Research through Electronic Health Records and Nlp-Enabled Informatics ABSTRACT Total joint arthroplasty (TJA) is the most common and fastest growing surgical procedure in the nation. Despite the high procedure volume, the evidence base for TJA procedures and associated interventions are limited. This is mainly due to lack of high quality data sources and the logistical difficulties associated with manually extracting TJA information from the unstructured text of the Electronic Health Records (EHR). Meanwhile, the rapid adoption of EHR and the advances in health information technology offer the potential to transform unstructured EHR notes into structured, codified format that can then be analyzed and shared with local and national arthroplasty registries and other agencies. We therefore propose to leverage unique data resources and natural language processing (NLP) technologies to build an informatics infrastructure for automated EHR data extraction and analysis. We will (1) develop a high performance, externally validated and user centric NLP- enabled algorithm for extraction of complex TJA-specific data elements from the structured and unstructured text of the EHR, (2) validate the algorithm externally in multiple EHR platforms and hospital settings, and (3) conduct a demonstration project focused on prediction of prosthetic joint infections using data elements collected by the NLP-enabled algorithm. Our overarching goal is to develop valid, open source and portable NLP-enabled data collection and risk prediction tools and disseminate them widely to hospitals participating in regional and national TJA registries. This research is significant as it leverages strong data resources and expertise to tackle the pressing need for high quality data and accurate prediction models in TJA. Automated data collection and processing capabilities will lead to an upsurge in secondary use of EHR to advance scientific knowledge on TJA risk factors, healthcare quality and patient outcomes. Accurate prediction of high risk patients for prosthetic joint infections will guide prevention and treatment decisions resulting in significant health benefits to TJA patients. The research is innovative because TJA-specific bioinformatics technology will shift TJA research from current under-powered, single-center studies to large, multi-center registry-based observational studies and clinical trials. Our deliverables have the potential to exert a sustained downstream effect on future TJA research, practice and policy. PUBLIC HEALTH RELEVANCE Lack of high quality data is a critical barrier to progress in total joint arthroplasty (TJA) research. We will utilize health information technology to automate extraction of rich TJA information from the electronic health records, and develop a robust risk prediction score for prosthetic joint infections, a devastating and yet preventable complication of TJA. Widespread adoption of these tools will enhance data collection capabilities and enable affordable large scale studies for practice improvements through secondary use of real-world data. Accurate prediction of post- operative infection risk among TJA candidates will guide individualized preventive strategies for modifiable risk factors, thereby reducing the burden of prosthetic joint infections in TJA patients.",Enhance Arthroplasty Research through Electronic Health Records and Nlp-Enabled Informatics,9486584,R01AR073147,"['Adoption', 'Algorithms', 'American', 'Bioinformatics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Complex', 'Complication', 'Data', 'Data Collection', 'Data Element', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Making', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Epidemic', 'Evidence based practice', 'Future', 'Goals', 'Gold', 'Guide prevention', 'Health Benefit', 'Hospitals', 'Individual', 'Infection', 'Informatics', 'Institution', 'Intervention', 'Joint Prosthesis', 'Knowledge', 'Logistics', 'Manuals', 'Marketing', 'Medicare', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Patient risk', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Policies', 'Postoperative Period', 'Prevention', 'Prevention strategy', 'Procedures', 'Provider', 'Publishing', 'Registries', 'Replacement Arthroplasty', 'Research', 'Risk', 'Risk Factors', 'Safety', 'Scientific Advances and Accomplishments', 'Source', 'Structure', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States', 'age group', 'base', 'computerized data processing', 'cost', 'data access', 'data resource', 'electronic data', 'electronic structure', 'epidemiology study', 'evidence base', 'health care quality', 'health information technology', 'high risk', 'improved', 'individual patient', 'informatics infrastructure', 'innovation', 'joint infection', 'modifiable risk', 'novel', 'open source', 'outcome prediction', 'patient population', 'portability', 'pragmatic trial', 'predictive modeling', 'prototype', 'public health relevance', 'surgery outcome', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,R01,2018,484387,0.05081544354267375
"Secondary use of EMRs for surgical complication surveillance DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.",Secondary use of EMRs for surgical complication surveillance,9476980,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'base', 'clinical decision support', 'clinical implementation', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'patient safety', 'public health relevance', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2018,300000,0.03231005138540235
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics ﻿    DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities. PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9674607,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Benefits and Risks', 'CCL4 gene', 'Cancer Intervention', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical and Translational Science Awards', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Learning', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Public Health Informatics', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical implementation', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'interest', 'longitudinal dataset', 'novel', 'open source', 'optimal treatments', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2018,79500,0.013963725226351946
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics ﻿    DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities. PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9789497,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Benefits and Risks', 'CCL4 gene', 'Cancer Intervention', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical and Translational Science Awards', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Learning', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Public Health Informatics', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical implementation', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'interest', 'longitudinal dataset', 'novel', 'open source', 'optimal treatments', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2018,79500,0.013963725226351946
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics ﻿    DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities. PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9548987,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Benefits and Risks', 'CCL4 gene', 'Cancer Intervention', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical and Translational Science Awards', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Learning', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Public Health Informatics', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical implementation', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'interest', 'longitudinal dataset', 'novel', 'open source', 'optimal treatments', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2018,620106,0.013963725226351946
"Interdisciplinary Infrastructure for Aging Research: Rochester Epidemiology Project PROJECT SUMMARY The National Institutes on Aging (NIA) has recommended strengthening research infrastructures to address future aging research questions (2016 Data Infrastructure Review Committee Report and PAR-16-367). In particular, they recommend: 1) integrating biological data into larger population-based studies; 2) increasing use of electronic health record (EHR) data and linking to medical care claims data; and 3) developing new approaches to collecting data to answer important scientific questions about mechanisms of aging. The Rochester Epidemiology Project (REP; NIA R01 AG034676) is a unique infrastructure for studies of aging, because the REP collects longitudinal EHR data on all health conditions that come to medical attention for a large, Midwestern population. Therefore, the REP allows investigators to study all age-related diseases and outcomes. However, the REP has three significant gaps. First, the REP does not include biospecimens. Second, the REP is missing health care delivered outside of the health care institutions that partner with the REP, and it does not include information on filled prescriptions. Third, a significant proportion of EHR data is difficult to access due to two factors: 1) the full text of the EHRs includes extensive clinical notes about aging outcomes and geriatric syndromes, but these notes are not routinely coded for billing, and can only be accessed through laborious manual review; and 2) the REP health care partners use three different EHR systems, making it difficult to apply electronic data extraction tools across all partners. To address these three gaps, we will develop an interdisciplinary collaboration across experts in aging research, epidemiologic methods, biobanking, and medical informatics to create a new, comprehensive research infrastructure (“Bio-REP”) to support aging research. In the R21 phase, we will develop a comprehensive research infrastructure that combines the REP data with Mayo Clinic Biobank biospecimens, medical claims data from the Centers for Medicare and Medicaid Services (CMS; Aim 1), and geriatric syndrome data that are included in the unstructured EHR clinical notes using Natural Language Processing techniques (NLP; Aim 2). In the R33 phase, we will deploy NLP algorithms developed in Aim 2 in the clinical notes from two additional EHR systems (Aim 3), and we will conduct two demonstration projects. First, we will measure associations between novel aging-related biomarkers and aging-related outcomes (Aim 4). Second, we will determine whether two common medications that are hypothesized to impact aging (metformin and angiotensin receptor blockers) modify associations between aging biomarkers and aging outcomes (Aim 5). The new, robust Bio-REP infrastructure will support a wide range of efficient, cost-effective observational studies to characterize associations between aging-related biomarkers and specific diseases, geriatric syndromes, and drug utilization. Such studies are urgently needed to design effective clinical trials to improve the health span of the aging population. PROJECT NARRATIVE We will establish an interdisciplinary collaboration across experts in aging research, epidemiologic methods, biobanking, and medical informatics to develop and test a new, comprehensive research infrastructure (“Bio- REP”) to support aging research. The Bio-REP will support efficient, cost-effective studies to characterize associations between aging-related biomarkers and specific diseases, geriatric syndromes, and drug utilization. Such studies are urgently needed to design effective clinical trials to improve the health span of the aging population.",Interdisciplinary Infrastructure for Aging Research: Rochester Epidemiology Project,9503117,R21AG058738,"['Address', 'Age-Years', 'Aging', 'Agreement', 'Algorithms', 'Angiotensin Receptor', 'Area', 'Benchmarking', 'Biological', 'Biological Markers', 'Caring', 'Ceramides', 'Clinic', 'Clinical', 'Clinical Trials', 'Code', 'Data', 'Delirium', 'Diagnosis', 'Diagnostic Services', 'Disease', 'Disease Outcome', 'Drug Utilization', 'Elderly', 'Electronic Health Record', 'Enrollment', 'Epidemiologic Methods', 'Epidemiology', 'Future', 'Grant', 'Health', 'Healthcare', 'Institution', 'Link', 'Manuals', 'Measures', 'Medical', 'Medical Informatics', 'Medical Records', 'Metformin', 'National Institute on Aging', 'Natural Language Processing', 'Observational Study', 'Outcome', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Polypharmacy', 'Population', 'Population Study', 'Proteins', 'Recommendation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Review Committee', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'United States Centers for Medicare and Medicaid Services', 'age related', 'aging population', 'biobank', 'cost effective', 'design', 'electronic data', 'falls', 'healthspan', 'improved', 'interdisciplinary collaboration', 'medical attention', 'novel', 'novel marker', 'novel strategies', 'senescence', 'tool']",NIA,MAYO CLINIC ROCHESTER,R21,2018,238500,0.0073072534533122225
"Enhancing Intimate Partner Violence (IPV) Identification through Automated EHR Summarization Project summary Intimate partner violence (IPV) is a significant public health and criminal justice problem that negatively impacts millions of victims yearly in the United States, primarily women (85%). Most IPV-related healthcare visits (83%) occurred in an emergency department (ED), and these clinical encounters are unique opportunities to identify IPV victims and potentially provide assistance. Although numerous health professional organizations have endorsed universal screening and counseling for IPV since 1992, actual screening rates, detection of IPV victims, and referrals to IPV services remain low in the ED. As a result, many IPV victims pass through the ED unidentified and untreated. Computerized screening tools have been developed and implemented in clinical settings in order to assist providers in screening and detecting IPV. However, these tools have a great limitation in that they rely on information collected from the patient and do not utilize the longitudinal data in electronic health records (EHR). Recently, researchers demonstrated that a history of IPV diagnoses and associated clinical symptoms highly predict current and future IPV (OR=7.8), and these important IPV data could serve as red flags that trigger providers to assess patients further for IPV. In order to enhance IPV screening in the ED, we propose to develop and assess an automatic clinical data summarizer that extracts, abstracts and synthesizes patient historical IPV data (structured and unstructured), and delivers patient historical IPV data to ED providers through an intuitive interface. The specific aims are: 1) develop and evaluate natural language processing (NLP) strategies to identify and extract patient historical IPV incidents and timelines from clinic notes; 2) develop and evaluate a web service-based summary tool (IPV-Summary-Service) that synthesizes patient- specific IPV information from both NLP-processed data elements and structured data; and 3) develop and pilot test an enhanced IPV screening strategy that delivers clinical evidence generated by the IPV-Summary- Service through a specific EHR (Epic) to providers during the patient universal IPV screening in the ED. This automatic clinical date summary for IPV will be piloted in one ED at MUSC. There are two major outcomes to be measured for the IPV-Summary enhanced screening for 6 months before and after the index date of pilot testing: 1) rate of successful referral to the IPV 24-hour dedicated IPV nurse; and 2) rate of initiation of referral and identification of persons at high risk of IPV. We will use mixed effect generalized linear regression models to estimate the effects of IPV-Summary on the referral rate and IPV case identification rate. Through survey studies, we will assess secondary outcomes including factors of system feasibility, usability, and providers' satisfaction. These analyses can identify potentially important correlates of the major outcomes and may help us improve the design of the intervention. The results from this study will form the foundation for a broader implementation in a regional health information exchange for EDs. Narrative Computer-based approaches for intimate partner violence (IPV) universal screening have led to significantly higher screening rate and detection rate, as well as receipt of IPV services in the emergency department (ED). However, these approaches rely on information collected from the patient and do not utilize the longitudinal IPV data existing in electronic health records (EHR), which have high predictive power of IPV risk. In order to enhance the effectiveness of IPV screening, we propose to develop and assess an automatic clinical data summary tool that extracts, abstracts, and synthesizes patient historical IPV information from EHR and then delivers that critical information to ED providers at the point of care.",Enhancing Intimate Partner Violence (IPV) Identification through Automated EHR Summarization,9520862,R21LM012945,"['Accident and Emergency department', 'Acute', 'Address', 'Adopted', 'Adult', 'American', 'Caring', 'Centers for Disease Control and Prevention (U.S.)', 'Chronic', 'Client', 'Clinic', 'Clinical', 'Clinical Data', 'Complex', 'Computers', 'Counseling', 'Crime', 'Criminal Justice', 'Data', 'Data Element', 'Decision Making', 'Detection', 'Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Emergency Department patient', 'Emergency department visit', 'Event', 'Face', 'Female', 'Foundations', 'Future', 'Health', 'Health Care Visit', 'Health Professional', 'Hour', 'Injury', 'Intuition', 'Linear Regressions', 'Link', 'Masks', 'Measures', 'Medical', 'Modeling', 'Natural Language Processing', 'Nurses', 'Outcome', 'Patient Self-Report', 'Patients', 'Persons', 'Process', 'Professional Organizations', 'Provider', 'Public Health', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Screening procedure', 'Services', 'South Carolina', 'Structure', 'Surveys', 'Symptoms', 'System', 'Testing', 'Time', 'TimeLine', 'Trauma', 'United States', 'Universities', 'Victimization', 'Violent injury', 'Woman', 'Work', 'base', 'clinical decision support', 'clinical decision-making', 'clinical practice', 'clinically relevant', 'computerized', 'experience', 'high risk', 'improved', 'indexing', 'intimate partner violence', 'point of care', 'pressure', 'satisfaction', 'screening', 'secondary outcome', 'therapy design', 'tool', 'usability', 'web services']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R21,2018,201825,0.029970703837786283
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9395941,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Data Sources', 'Development', 'Diagnostic', 'Disease Surveillance', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'optimal treatments', 'outcome forecast', 'population based', 'prevent', 'socioeconomics', 'surveillance data', 'treatment guidelines', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2018,803425,0.014684016853767775
"Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding Project Summary/Abstract  With increasing use of electronic medical records for a variety of patients, a large investment is being made in a resource still vastly underused. Especially in mental health, where problems are highly individualized, requiring personalized intervention, and often accompanied by rich data not easily captured in structured templates, the need for extracting information from free text in existing records for use as large-scale stand- alone datasets or in combination with other data is real. Without scalable and effective computational approaches to capture this data, much time, effort and money is used to create limited-use records that instead could be leveraged into precious data sources to inform existing research and lead to new insights, progress and treatments. Our broad, long-term goal is processing free text in EHR in mental health. We focus on Autism Spectrum Disorders (ASD), a particularly interesting example of both shortcomings and opportunities.  ASD’s prevalence has increased over the years, and estimates range from 1 in 150 in 2000 to 1 in 68 in 2010(1-5). These numbers are based on surveillance using electronic health records. The increasing prevalence is not well understood, and hypotheses range from changing diagnostic criteria to environmental factors. The lines of inquiry used to find cures are similarly broad and range from brain scans and genetics, resulting in large structured datasets, to highly individualized therapies, resulting in rich but unstructured data. Currently the text information in the electronic records is not being leveraged on a large scale.  The proposed project continues our preliminary work and uses a data-driven approach to create human- interpretable models that allow automated extraction of relevant structured data from free text. The Diagnostic and Statistical Manual of Mental Disorders (DSM) is the starting point for identifying features. A database of thousands of records is leveraged to design and test the algorithms. The two specific aims are: 1) design and test natural language processing (NLP) algorithms to detect DSM criteria for ASD in free text in EHR, and 2) demonstrate feasibility and usefulness of the models for large-scale analysis of ASD cases, which is inconceivable today with current approaches. Our methods include analysis of free text in electronic records and end-user annotations to create a large gold standard of instances of DSM criteria for ASD, application of machine learning and rule-based approaches to create human-interpretable models for automated annotation of diagnostic patterns in textual records, and demonstrate usefulness with new research (e.g., Automatically detect ASD vs. no-ASD status for challenging cases; evaluate prevalence of symptoms over time). Through NLP algorithms, this project has the potential to significantly shift away from the current paradigm of attempting to understand ASD by relying on small-scale data from individual interventions and lack of integration between different data sources, to leveraging information from existing large-scale data sources to propose novel analyses and hypotheses. Project Narrative  Lack of sophisticated tools to extract relevant diagnostic patterns from free text from the increasingly large number of electronic medical/health records is a critical barrier in the field of mental health to leverage and utilize the already available data. Natural language processing (NLP) algorithms designed specifically for mental health can make new data analysis and integration with other sources possible at a scale previously unseen. Using a data-driven process, this project will design NLP algorithms to annotate free text with criteria from the Diagnostic and Statistical Manual of Mental Disorders (DSM) and demonstrate scope, feasibility and usefulness by focusing on Autism Spectrum Disorders (ASD) where prevalence is increasing and much rich clinical text is stored in electronic health records (EHR).",Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding,9547263,R21HS024988,[' '],AHRQ,UNIVERSITY OF ARIZONA,R21,2018,146202,-0.05761058777977941
"Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes ﻿    DESCRIPTION (provided by applicant): The applicant and principal investigator (PI) is a board-certified cardiologist, Assistant Professor of Medicine and Senior Associate Consultant in the Mayo Clinic Cardiovascular Division. [The long-term goal of the PI is to become an independent clinician-investigator in translational informatics focused on the development, validation and deployment of electronic tools to the point-of-care to improve outcomes for patients with cardiovascular disease. The PI intends to leverage the EHR to conduct population-based studies using electronic algorithms that include NLP and by application of state-of-the-art informatics approaches to deliver the knowledge at the point-of-care in support of patient-centered decision-making for patients with peripheral arterial disease (PAD). The electronic tools developed will be portable to different institutions and other health conditions. The PI will be uniquely positioned as a cardiologist within a large academic health system working with novel NLP techniques at the interface of the EHR and CDS to rapidly translate knowledge acquired in community-based epidemiologic investigations to patient care.] The PI has proposed a research career development plan to acquire expertise in translational informatics including electronic phenotyping, and automated calculation and visual display of prognostic scores using a novel application integrated with clinical decision support (CDS). The PI will complete comprehensive coursework in informatics science and gain additional patient-oriented and epidemiologic research experience. The primary mentor will be Iftikhar Kullo, M.D., cardiologist and PI of the Mayo Clinic electronic MEdical Records and GEnomics (eMERGE) network and a leader in the development and application of electronic phenotyping for genomic studies of PAD. The mentoring team also includes Dr. Christopher Chute, M.D., Head of Medical Informatics at Mayo Clinic, an expert in informatics and epidemiology, and co-PI of the Mayo eMERGE grant and co-investigator of the Rochester Epidemiology Project; Hongfang Liu, Ph.D., an expert in natural language processing (NLP) and its application for EHR-based phenotyping; and Kent Bailey, Ph.D., senior biostatistician with expertise in development of prognostic risk scores. The specific aims of the proposal are: Aim 1 - Apply phenotyping algorithms that include NLP of clinical notes to [a] identify PAD cases and controls without PAD in the community from 1998-2011; and [b] ascertain adverse cardiovascular and limb outcomes through December 2013. Aim 2 - Create multivariable prognostic risk models and scores for adverse outcomes in PAD cases from the community. We will estimate the relative risk of death in PAD patients compared to controls without PAD. Aim 3 - Develop and evaluate usefulness of a novel electronic application within the Mayo EHR to retrieve relevant data elements, calculate, and display individualized prognostic scores. On completion of the proposed investigations the deliverables will be new knowledge and an e-health prognostication tool for PAD patients to be disseminated to stakeholders including clinicians, patients, and researchers portable to other institutions and other cardiovascular diseases. PUBLIC HEALTH RELEVANCE: The applicant, a board certified cardiovascular specialist, proposes to acquire training in translational informatics science which will enable application of novel electronic algorithms to electronic health records to more efficiently conduct community-based studies of peripheral arterial disease, a prevalent but undertreated health condition. These studies will concurrently develop novel electronic tools which will inform patients and providers of risk, facilitate shared-decision making at the point-of-care, and promote compliance to guideline recommended strategies. These tools will be portable to other institutions and to other cardiovascular diseases.",Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes,9443655,K01HL124045,"['Address', 'Adult', 'Adverse event', 'Algorithms', 'Amputation', 'Atherosclerosis', 'Atrial Fibrillation', 'Attention', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Investigator', 'Communities', 'Computerized Medical Record', 'Coronary heart disease', 'Data', 'Data Element', 'Decision Making', 'Dependence', 'Development', 'Development Plans', 'Diagnosis', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Epidemiology', 'Genomics', 'Goals', 'Government Agencies', 'Grant', 'Guidelines', 'Head', 'Health', 'Health system', 'Healthcare', 'Heart failure', 'High Prevalence', 'Image', 'Informatics', 'Institution', 'Investigation', 'Ischemia', 'Knowledge', 'Laboratories', 'Limb structure', 'Link', 'Manuals', 'Medical Informatics', 'Medicine', 'Mentors', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patient risk', 'Patient-Focused Outcomes', 'Patients', 'Peripheral arterial disease', 'Pharmaceutical Preparations', 'Phenotype', 'Population Study', 'Positioning Attribute', 'Principal Investigator', 'Provider', 'Relative Risks', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Risk stratification', 'Safety', 'Science', 'Site', 'Specialist', 'Stroke', 'Symptoms', 'System', 'Techniques', 'Time', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Visual', 'Work', 'adverse outcome', 'base', 'career development', 'case control', 'clinical decision support', 'clinical decision-making', 'cost', 'demographics', 'didactic education', 'eHealth', 'epidemiology study', 'experience', 'health information technology', 'high risk', 'improved', 'improved outcome', 'investigator training', 'mortality', 'multidisciplinary', 'novel', 'patient oriented', 'patient oriented research', 'point of care', 'portability', 'predictive modeling', 'professor', 'prognostic', 'prognostic tool', 'public health relevance', 'repository', 'research and development', 'shared decision making', 'skills', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,K01,2018,170856,0.051582619427197725
"National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms PROJECT SUMMARY With the rapidly growing adoption of patient electronic health record systems (EHRs) due to Meaningful Use, and linkage of EHRs to research biorepositories, evaluating the suitability of EHR data for clinical and translational research is becoming ever more important, with ramifications for genomic and observational research, clinical trials, and comparative effectiveness studies. A key component for identifying patient cohorts in the EHR is to define inclusion and exclusion criteria that algorithmically select sets of patients based on stored clinical data. This process is commonly referred to, as “EHR-driven phenotyping” is time-consuming and tedious due to the lack of a widely accepted and standards-based formal information model for defining phenotyping algorithms. To address this overall challenge, the proposed project will design, build and promote an open-access community infrastructure for standards-based development and sharing of phenotyping algorithms, as well as provide tools and resources for investigators, researchers and their informatics support staff to implement and execute the algorithms on native EHR data. PROJECT NARRATIVE The identification of patient cohorts for clinical and genomic research is a costly and time-consuming process. This bottleneck adversely affects public health by delaying research findings, and in some cases by making research costs prohibitively high. To address this issue, leveraging electronic health records (EHRs) for identifying patient cohorts has become an increasingly attractive option. This proposal will investigate and implement standards based approaches for computable phenotype identification from multiple EHRs.",National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms,9547873,R01GM105688,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Architecture', 'Benchmarking', 'Benign Prostatic Hypertrophy', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computerized Medical Record', 'Computing Methodologies', 'Consensus', 'Data', 'Data Element', 'Data Quality', 'Data Reporting', 'Development', 'Educational workshop', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Engineering', 'Event', 'Exclusion Criteria', 'Flowcharts', 'Genomics', 'Gold', 'Grant', 'Health', 'Health system', 'Healthcare', 'Healthcare Systems', 'Human', 'Informatics', 'Intuition', 'Knowledge', 'Logic', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Needs Assessment', 'Observational Study', 'Outcome', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Precision Medicine Initiative', 'Process', 'Public Health', 'Public Health Informatics', 'Rare Diseases', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk Factors', 'Running', 'Scientist', 'Services', 'Standardization', 'Structure', 'System', 'Techniques', 'Text', 'Time', 'Translational Research', 'United States National Institutes of Health', 'University Hospitals', 'authority', 'base', 'biobank', 'clinical phenotype', 'cohort', 'comparative effectiveness', 'cost', 'data modeling', 'data warehouse', 'database query', 'deep learning', 'design', 'endophenotype', 'experience', 'inclusion criteria', 'informatics training', 'information model', 'interoperability', 'knowledge base', 'meetings', 'portability', 'precision medicine', 'repository', 'syntax', 'tool', 'usability']",NIGMS,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2018,706851,0.045371923383010944
"National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms PROJECT SUMMARY With the rapidly growing adoption of patient electronic health record systems (EHRs) due to Meaningful Use, and linkage of EHRs to research biorepositories, evaluating the suitability of EHR data for clinical and translational research is becoming ever more important, with ramifications for genomic and observational research, clinical trials, and comparative effectiveness studies. A key component for identifying patient cohorts in the EHR is to define inclusion and exclusion criteria that algorithmically select sets of patients based on stored clinical data. This process is commonly referred to, as “EHR-driven phenotyping” is time-consuming and tedious due to the lack of a widely accepted and standards-based formal information model for defining phenotyping algorithms. To address this overall challenge, the proposed project will design, build and promote an open-access community infrastructure for standards-based development and sharing of phenotyping algorithms, as well as provide tools and resources for investigators, researchers and their informatics support staff to implement and execute the algorithms on native EHR data. PROJECT NARRATIVE The identification of patient cohorts for clinical and genomic research is a costly and time-consuming process. This bottleneck adversely affects public health by delaying research findings, and in some cases by making research costs prohibitively high. To address this issue, leveraging electronic health records (EHRs) for identifying patient cohorts has become an increasingly attractive option. This proposal will investigate and implement standards based approaches for computable phenotype identification from multiple EHRs.",National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms,9707366,R01GM105688,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Architecture', 'Benchmarking', 'Benign Prostatic Hypertrophy', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computerized Medical Record', 'Computing Methodologies', 'Consensus', 'Data', 'Data Element', 'Data Quality', 'Data Reporting', 'Development', 'Educational workshop', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Engineering', 'Event', 'Exclusion Criteria', 'Flowcharts', 'Genomics', 'Gold', 'Grant', 'Health', 'Health system', 'Healthcare', 'Healthcare Systems', 'Human', 'Informatics', 'Intuition', 'Knowledge', 'Logic', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Needs Assessment', 'Observational Study', 'Outcome', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Precision Medicine Initiative', 'Process', 'Public Health', 'Public Health Informatics', 'Rare Diseases', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk Factors', 'Running', 'Scientist', 'Services', 'Standardization', 'Structure', 'System', 'Techniques', 'Text', 'Time', 'Translational Research', 'United States National Institutes of Health', 'University Hospitals', 'authority', 'base', 'biobank', 'clinical phenotype', 'cohort', 'comparative effectiveness', 'cost', 'data modeling', 'data warehouse', 'database query', 'deep learning', 'design', 'endophenotype', 'experience', 'inclusion criteria', 'informatics training', 'information model', 'interoperability', 'knowledge base', 'meetings', 'portability', 'precision medicine', 'repository', 'syntax', 'tool', 'usability']",NIGMS,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2018,429621,0.045371923383010944
"Development and Evaluation of a Learning Electronic Medical Record System ﻿    DESCRIPTION (provided by applicant):  The goal of this project to develop and evaluate a learning electronic medical records (L-EMR) system that draws a physician's attention to the right data, at the right time. It learns how to do so by analyzing patterns of patient data access f many physicians in many past cases in the EMR, and learning which EMR data to highlight that are relevant for making clinical decisions in a given patient.      The hypothesis underlying this research is that the L-EMR system will have sufficiently high precision and recall in highlighting relevant data, decrease the average time to assess an intensive care unit (ICU) patient case, and be judged by critical care medicine (CCM) physicians to be clinically useful.    The first aim of this project is develop a highly-usable L-EMR user interface. The L-EMR user interface will include zoomable time-series displays of lab-results, med-orders, and vital signs. Usability studies of the L-EMR user interface will guide revisions and enhancements.      The second aim of the project is to train statistical models that can be applied to a patient case to predict relevant lab-results, med-orders, and vital signs. We will enlist CCM physicians to review a set of retrospective ICU patient cases on a focused set of clinical conditions. Participants will review these cases as if they were active patients, identifying relevant lab- results, med-orders, and vital signs. We will train and evaluate statistical models to predict relevant data, and identify the best performing algorithm to include in the L-EMR system.      The third aim of the project is to evaluate the L-EMR system. We will recruit CCM physicians to evaluate an L-EMR system based on user interfaces from Aim 1 and statistical models trained using the best performing algorithm in Aim 2 to highlight relevant data items. We will measure the precision and recall of the data-highlighting functionality for assessing patient cases and making clinical decisions (e.g., lab and medication orders), the time required to assess cases with and without the highlighting, and physicians' assessments of the strengths and weaknesses of the L-EMR system.    If the results of these experiments are positive, as anticipated, this project will introduce a computational method that has significant potential to improve future EMR systems and enhance patient care. Narrative The purpose of this research is to develop and evaluate a learning electronic medical records (EMR) system that draws a physician's attention to the right data, at the right time. The system works by analyzing patterns of EMR usage of physicians, and learning which EMR data to highlight that are relevant in a given patient. The main idea underlying the approach is that patterns of past EMR usage patterns can be exploited to selectively highlight clinically useful patient data.",Development and Evaluation of a Learning Electronic Medical Record System,9521586,R01LM012095,"['Address', 'Adult', 'Algorithms', 'American', 'Attention', 'Bayesian Modeling', 'Blood', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical assessments', 'Computerized Medical Record', 'Computing Methodologies', 'Critical Care', 'Critical Illness', 'Data', 'Data Display', 'Data Set', 'Development', 'E-learning', 'Educational workshop', 'Evaluation', 'Face', 'Future', 'Gastrointestinal Hemorrhage', 'Goals', 'Healthcare Systems', 'Heart Rate', 'Hemoglobin', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Intensive Care Units', 'Intravenous', 'Laboratories', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Physiological', 'Provider', 'Reporting', 'Research', 'Research Personnel', 'Series', 'Statistical Models', 'System', 'Test Result', 'Time', 'Training', 'Work', 'base', 'clinical decision-making', 'computer human interaction', 'data access', 'design', 'experimental study', 'follow-up', 'improved', 'prospective', 'prototype', 'recruit', 'research clinical testing', 'stem', 'trend', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2018,452158,0.038107506533143216
"Data-Mining Clinical Decision Support from Electronic Health Records ﻿    DESCRIPTION (provided by applicant)    Data-Mining Clinical Decision Support from Electronic Health Records Public Health Motivation: National healthcare quality is compromised by undesirable variability, reflected in different locales having anywhere from 20-80% compliance with evidence-based guidelines. Much of this is due to uncertainty, with half of clinical practice guidelines lacking adequate evidence to confirm their efficacy. This is unsurprising when clinical trials cost >$15 million to answer individual clinical questions. The result is medical practice routinely driven by individual opinio and anecdotal experience. While Big Data has revolutionized how society processes internet scale information, the status quo in clinical decision making remains the manual interpretation of literature and isolated decision aids. The adoption of electronic health records (EHR) creates a new opportunity to answer a ""grand challenge in clinical decision support (CDS)."" In a learning health system, we could automatically adapt knowledge from the collective expertise embedded in the EHR practices of real clinicians and close the loop by disseminating that knowledge back as executable decision support. Candidate Goals and Objectives: The unifying goal of this BD2K K01 proposal is the mentored career development of Jonathan H. Chen, MD, PhD. This proposal will accelerate his transition into an independent physician scientist, towards his long-term goals to produce Big Data technologies that answer such grand challenges in clinical decision support. His near-term objective is developing methods to translate EHR data into useful knowledge in the form of patient- specific, point-of-care clinical order recommendations for acute medical hospitalizations. His doctoral background in computer science gives him the technical capability to achieve these objectives, while his medical training will ensure clinically meaningful results. His preliminary work to build an order recommender, analogous to commercial product recommenders, demonstrates the proposal's overall feasibility. Institutional Environment and Career Development: The research facilities and training opportunities at Stanford University provide the ideal environment to achieve these objectives, with established and growing Centers for Biomedical Informatics Research, the Biomedical Data Science Initiative, and the first Clinical Informatics Fellowship accredited in the nation. Prof. Russ Altman, Director of the Biomedical Informatics Training Program, will lead a collaborative team of mentors with expertise in clinical decision support (Mary Goldstein), implementation science (Steven Asch), data-mining electronic health records (Nigam Shah), statistical learning algorithms (Lester Mackey), and healthcare statistics (Michael Baiocchi). Combined with respective didactic training, this mentorship will enable Dr. Chen to achieve his objectives through a series of research aims. Research Aims: The overriding hypothesis of the proposal is that clinical knowledge reflected in clinical order patterns from historical EHR data can improve medical decision making when adapted into functional clinical decision support. The specific aims each address components of this concept, as they seek to: (1) Develop the algorithms to learn clinical order patterns from historical EHR data, building on a preliminary recommender system; (2) Assess how underlying clinician proficiency affects the quality of those learned clinical order patterns through observational data inference against external standards; and (3) Determine the impact of automatically learned clinical decision support (CDS) on (simulated) clinical workflows through a randomized controlled crossover trial of human-computer interfaces with real clinicians. Expected Results and General Significance: By the completion of the proposed work, Dr. Chen will answer the grand challenge in clinical decision support (CDS) by automating much of the CDS production process, and have direct translational impact with a prototype system. This will advance the field with new paradigms of generating and disseminating clinical knowledge, which can then improve the consistency and quality of healthcare delivery. Additional benefits will include methods to identify and monitor areas of high practice variability for targeted optimization and improve predictive models that inform precision medicine. With this applied research experience and career development, Dr. Chen can compete for R01 funding and become an independent physician scientist developing Big Data approaches to solve national healthcare problems in clinical decision making. PUBLIC HEALTH RELEVANCE    National healthcare quality is compromised by undesirable practice variability and medical uncertainty, with most medical practice routinely driven by individual opinions and anecdotal experience. With methods analogous to commercial product recommender systems, the proposed project will automatically learn patterns in raw clinical transaction data to capture the undocumented knowledge of real-world clinicians, and close the loop in a learning health system by disseminating that knowledge back as clinical decision support to improve patient care.",Data-Mining Clinical Decision Support from Electronic Health Records,9537252,K01ES026837,"['Accreditation', 'Achievement', 'Acute', 'Address', 'Admission activity', 'Adoption', 'Affect', 'Algorithms', 'Applied Research', 'Area', 'Back', 'Big Data', 'Big Data to Knowledge', 'Caring', 'Chiroptera', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Practice Guideline', 'Clinical Trials', 'Cost efficiency', 'Cross-Over Trials', 'Crowding', 'Data', 'Data Science', 'Decision Aid', 'Decision Making', 'Diagnosis', 'Doctor of Philosophy', 'Educational process of instructing', 'Electronic Health Record', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Failure', 'Fellowship', 'Funding', 'Future', 'Goals', 'Health system', 'Healthcare', 'Hospitalization', 'Hospitals', 'Image', 'Individual', 'Institute of Medicine (U.S.)', 'Internet', 'Investments', 'Knowledge', 'Lead', 'Learning', 'Literature', 'Locales', 'Machine Learning', 'Manuals', 'Medical', 'Medical Residency', 'Mentors', 'Mentorship', 'Methods', 'Monitor', 'Motivation', 'Noise', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Privatization', 'Process', 'Production', 'Public Health', 'Quality of Care', 'Randomized', 'Recommendation', 'Research', 'Research Training', 'Scientist', 'Series', 'Services', 'Societies', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Training', 'Training Programs', 'Transact', 'Translating', 'Uncertainty', 'Universities', 'User-Computer Interface', 'Validation', 'Weight', 'Work', 'base', 'biomedical informatics', 'career development', 'clinical care', 'clinical decision support', 'clinical decision-making', 'clinical practice', 'clinical predictors', 'computer science', 'cost', 'data mining', 'design', 'evidence based guidelines', 'experience', 'health care delivery', 'health care quality', 'implementation science', 'improved', 'multidisciplinary', 'point of care', 'precision medicine', 'predictive modeling', 'prototype', 'public health relevance', 'research facility', 'routine practice', 'statistics', 'support tools', 'training opportunity', 'translational impact']",NIEHS,STANFORD UNIVERSITY,K01,2018,178606,0.025559366617838974
"Improving missing data analysis in distributed research networks ABSTRACT Electronic health record (EHR) databases collect data that reflect routine clinical care. These databases are increasingly used in comparative effectiveness research, patient-centered outcomes research, quality improvement assessment, and public health surveillance to generate actionable evidence that improves patient care. It is often necessary to analyze multiple databases that cover large and diverse populations to improve the statistical power of the study or generalizability of the findings. A common approach to analyzing multiple databases is the use of a distributed research network (DRN) architecture, in which data remains under the physical control of data partners. Although EHRs are generally thought to contain rich clinical information, the information is not uniformly collected. Certain information is available only for some patients, and only at some time points for a given patient. There are generally two types of missing information in EHRs. The first is the conventionally understood and obvious missing data in which some data fields (e.g., body mass index) are not complete for various reasons, e.g., the clinician does not collect the information or the patient chooses not to provide the information. The second is less obvious because the data field is not empty but the recorded value may be incorrect due to missing data. For example, EHRs generally do not have complete data for care that occurs in a different delivery system. A medical condition (e.g., asthma) may be coded as “no” but the true value would have been “yes” if more complete data had been available, e.g., from claims data as the other delivery system would submit a claim to the patient’s health plan for the care provided. In other words, one may incorrectly treat “absence of evidence” as “evidence of absence”. EHRs hold great promise but we must address several outstanding methodological challenges inherent in the databases, specifically missing data. Addressing missing data is more challenging in DRNs due to different missing data mechanisms across databases. The specific aims of the study are: (1) Apply and assess missing data methods developed in single-database settings to handle obvious and well-recognized missing data in DRNs; (2) Apply and assess machine learning and predictive modeling techniques to address less obvious and under-recognized missing data for select variables in DRNs; and (3) Apply and assess a comprehensive analytic approach that combines conventional missing data methods and machine learning techniques to address missing data in DRNs. The analytic methods developed in this project, including the extension of existing missing data methods to DRNs, the innovative use of machine learning techniques to address missing data, and their integration with privacy- protecting analytic methods, will have direct impact on the design and analysis of future comparative effectiveness and safety studies, and patient-centered outcomes research conducted in DRNs. PROJECT NARRATIVE The proposed project will refine existing methods and develop new methods to address missing data issues in electronic health record databases.",Improving missing data analysis in distributed research networks,9575484,R01HS026214,[' '],AHRQ,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2018,400000,0.031938493203818
"Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign Project Summary/Abstract Written The objective of the proposed research is to reduce the patient safety hazards associated with electronic medication administration records (eMARs) by, (1) understanding current usability and safety gaps, and (2) creating design and development documents, wireframes, and prototypes to serve as the foundation for future eMARs that will eliminate these gaps. In particular, we focus on communication and information flow challenges between nurses, pharmacists, and physicians during medication administration and use of the eMAR. The proposed research is in direct response to special emphasis notice (NOT-HS-16-009). We will develop a broad understanding of usability and safety hazards associated with eMARs by analyzing a large dataset of 1.7 million patient safety event reports and detailed medication error related narratives. We will then conduct heuristic analyses of current eMARs, and interviews and observations of physicians, nurses, and pharmacists. These data will serve to inform the development of eMAR design documents, wireframes, and prototypes as the foundation for future development. This project utilizes the extensive expertise of the research team in human factors and safety science, health information technology (health IT), informatics, and data science. Our research team includes physicians, nurses, pharmacists, and human factors engineers, and experts in natural language processing. In addition, our partnerships include a patient safety organization and a health IT vendor. The proposal addresses fundamental aspects of the call for proposals by providing new insights on the safety of health IT and improves current practices by developing use cases and new prototypes for immediate use by healthIT vendors. Contributions from this research will include a fundamental understanding of the role of health IT during medication administration with a focus on communication and information flow, design, development and testing documents for vendors and providers, and eMAR wireframes and prototypes to improve development. Our research will also provide organizations like the Office of the National Coordinator with medication related test scenarios to assess current health IT systems. Project Narrative This project is relevant to public health because it applies the sciences of human factors and informatics to improve the usability and safety of the electronic medication administration records (eMARs), which will ultimately improve patient care. Patient safety event report data will be analyzed to identify usability and safety hazards in eMARs, and interviews and observations will be conducted to identify clinician needs. Based on this knowledge eMAR design documents, wireframes, and prototypes will be developed, tested, and disseminated with the goal of improving communication and information flow to reduce medication related errors.",Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign,9447854,R01HS025136,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2018,400000,0.0028799540734190804
"Automatic Generation of Computer Interpretable Guidelines PROJECT SUMMARY / ABSTRACT Compliance with clinical practice guidelines (CPGs) has been demonstrated to markedly improve patient care, but the tools and processes available for physicians to rapidly and meaningfully leverage these guidelines are currently sub-optimal. Compliance improves greatly with the introduction of clinical decision support systems (CDSS), which implement guideline recommendations and are integrated into electronic health record (EHR) systems. Unfortunately, CPGs, as commonly distributed, contain recommendations for care of only a single disorder (or class of disorders) and are not easily consumable by computers for integration with CDSS. Recent work has focused on methods to resolve conflicts between guidelines, but only once they are in a computer interpretable form - a Computer Interpretable Guideline (CIG). We aim: to (1) develop a system for computational understanding of CPGs from the unstructured text; (2) to ground the clinical terms in each guideline in its definition so that the produced CIG can better integrate with electronic health record systems; (3) to generate CIGs in a format already developed which allows guidelines to be mediated with each other and allows the creation of CDSS. The project will focus on four clinical guidelines, relating to diabetes, heart disease, non-small cell lung and prostate cancers. Samples of each of the guidelines will be annotated by clinicians with the appropriate output of systems accomplishing each of the three aims. These samples will be split into datasets for testing and for evaluation, with the overall goal to achieve human levels of competence for each aim. PROJECT NARRATIVE Clinical practice guidelines are paramount to the goals of evidence-based medicine but, being unstructured free text, they are not well integrated with clinician workflows and are not consistently adhered to. We aim to automatically generate computer interpretable versions of textual guidelines in a format which allows for automatic mediation between guidelines for patients with multiple morbidities. Mediated computer interpretable guidelines could then be integrated with electronic medical record systems, providing immediate impact on population health.",Automatic Generation of Computer Interpretable Guidelines,9654971,R15LM013030,"['Belief', 'Caring', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Practice Guideline', 'Competence', 'Complex', 'Computational Linguistics', 'Computational Technique', 'Computer software', 'Computerized Medical Record', 'Computers', 'Conflict (Psychology)', 'Data', 'Data Set', 'Diabetes Mellitus', 'Disease', 'Drug Prescriptions', 'Electronic Health Record', 'Evaluation', 'Evidence Based Medicine', 'Frequencies', 'Generations', 'Goals', 'Graph', 'Guidelines', 'Heart Diseases', 'Human', 'Knowledge', 'Language', 'Level of Evidence', 'Link', 'Malignant neoplasm of prostate', 'Manuals', 'Maps', 'Mediating', 'Mediation', 'Medical', 'Methods', 'Modality', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Non-Small-Cell Lung Carcinoma', 'Ontology', 'Output', 'Patient Care', 'Patients', 'Physicians', 'Positioning Attribute', 'Process', 'Recommendation', 'Research', 'Risk Factors', 'Sampling', 'Semantics', 'Source', 'Structure', 'Students', 'System', 'Techniques', 'Terminology', 'Testing', 'Text', 'Work', 'clinical care', 'clinical practice', 'computer generated', 'design', 'electronic structure', 'evidence base', 'experience', 'improved', 'novel', 'phrases', 'population health', 'prescription procedure', 'syntax', 'tool', 'undergraduate student']",NLM,COLLEGE AT OSWEGO,R15,2018,325760,0.034122100341028076
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,9567932,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2018,249135,0.016391971940315025
"Developing Evidence for Safety Surveillance from Device Adverse Event Reports Title: Developing Evidence for Safety Surveillance from Device Adverse Event Reports Project Summary/Abstract Population level studies have shown that the device-based hysteroscopic sterilization was associated with increased risks of reoperation during follow-up, when compared to traditional laparoscopic sterilization. However, secondary data sources often lack the granularity to understand the nature of patient and device complications related to the device removal and additional surgery. The Manufacturer and User Facility Device Experience (MAUDE) database houses medical device reports submitted to the FDA by mandatory and voluntary reporters. These reports contain detailed information of patient and device adverse events. But due to its narrative structure, research with the reports has been limited, partially due to the restrictions of keyword search and manual review. The proposed study will innovatively apply natural language processing (NLP) to analyze MAUDE reports of device removals. NLP is a powerful tool capable of extracting information efficiently from documents such as medical notes, allowing the summarization of thousands of adverse event reports in a cost-effective way. The primary aim is to develop an NLP program to extract and summarize patient- and device-specific complications associated with device removal and additional surgeries following hysteroscopic sterilization. Secondary objective is to evaluate the impact of regulatory activities on adverse event reporting behavior and structure. The hypotheses are that the majority of reported removals were associated with device-related complications as opposed to persistent symptoms only, and that after the FDA convened a panel discussion in September 2015, adverse event reports were more likely to be submitted by mandatory reporters, with improvement in structured presentation. Adverse event reports related to device removal will be selected from the MAUDE database using keyword search first, and 1,000 reports will be annotated and used to develop and validate the NLP tool. Applying the developed NLP to all reports, extracted information will be used for the analysis, and comparisons will be made before and after September 2015. The significance of the proposed research is that it will develop a method to better utilize adverse event reports to obtain crucial device safety information supplemental to regular population-level studies. By achieving this, the long term goal is to create a useful tool for future medical device safety surveillance to understand the nature of adverse events. The immediate next step will be to use the tool to investigate device safety in other areas. The comprehension of the nature of device adverse events and the elucidation of the crucial role of regulatory activity in facilitating reliable adverse event reporting will help promote patient safety evaluation and monitoring. Project Narrative The proposed research will focus on device adverse event reports and develop a powerful tool to analyze reports to understand the nature of patient and device complications related to device removals and additional surgeries. The study will also elucidate the impact of regulatory activities on adverse event reporting. Thus, the proposed research is relevant to part of AHRQ's mission to make health care safer.",Developing Evidence for Safety Surveillance from Device Adverse Event Reports,9586941,R03HS026291,[' '],AHRQ,WEILL MEDICAL COLL OF CORNELL UNIV,R03,2018,96907,0.007707671635297789
"EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia PROJECT SUMMARY/ABSTRACT Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by elevated plasma low-density lipoprotein cholesterol (LDL-C) and a dramatically increased lifetime risk for premature atherosclerotic cardiovascular disease (ASCVD). Available data suggest substantial under treatment of individuals with FH, and it is estimated that <5% of prevalent FH cases in the US are diagnosed and treated. The proposed research will develop electronic health record (EHR)-based strategies to reduce mortality and morbidity from FH. We will develop and validate a phenotyping algorithm for rapid and efficient identification of FH cases thereby enabling EHR-based surveillance of FH. We will deploy the phenotyping algorithm in the population-based setting of Olmsted County, Minnesota, to estimate prevalence and provide hitherto unavailable data on awareness, detection and control of FH. We will develop CDS to help care providers manage FH patients and an FH-specific decision aid to facilitate shared decision making related to lipid-lowering therapy and screening of family members. To accomplish these goals, we will leverage the following resources: a) the electronic phenotyping expertise available in the electronic Medical Records and Genomics (eMERGE) network; b) the Rochester Epidemiology Project (REP), that links medical records of Olmsted County MN residents thereby capturing nearly all health care delivered to residents of the community; and c) expertise in developing and deploying CDS in the EHR and in creating decision aids for disclosing cardiovascular risk and the benefits of lipid-lowering drugs. Our specific aims are: Aim 1. Develop and validate an electronic phenotyping algorithm to rapidly identify FH cases from the EHR. Aim 2. Conduct an e- epidemiology study to obtain hitherto unknown data regarding prevalence, awareness, detection, control of FH in a population-based setting in the US. Aim 3. a) Develop EHR-based tools to help care providers manage FH and facilitate shared decision making and cascade screening and b) assess outcomes after implementation of CDS and decision aid. The proposed research will enable rapid identification of FH in EHRs, provide hitherto unavailable data on the burden of FH in the community, facilitate EHR-based strategies for early detection, increase awareness of FH among care providers, provide guidance for management of FH at point of care and help both patients and providers make informed decisions about drug therapy and screening of family members. These are critical steps for early detection and treatment of FH to reduce the burden of premature ASCVD due to this condition. PROJECT NARRATIVE Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by high cholesterol levels and increased risk of heart attack or sudden cardiac death. The proposed research will develop electronic health record (EHR)-based strategies to prevent adverse outcomes such as heart attack in FH patients. These include methods to rapidly identify FH patients, estimate prevalence of FH and develop clinical decision support to help care providers manage FH patients. The proposed work will have a significant impact on clinical management of FH patients.",EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia,9490424,R01HL135879,"['Academy', 'Address', 'Algorithms', 'Atherosclerosis', 'Awareness', 'Benefits and Risks', 'Cholesterol', 'Clinic', 'Clinical', 'Clinical Management', 'Communities', 'County', 'Data', 'Data Set', 'Decision Aid', 'Decision Making', 'Detection', 'Diagnosis', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Early treatment', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Epidemiology', 'Europe', 'Familial Hypercholesterolemia', 'Family', 'Family history of', 'Family member', 'Genetic Diseases', 'Genetic screening method', 'Genomics', 'Goals', 'Healthcare', 'Healthcare Systems', 'Heritability', 'Individual', 'Institutes', 'LDL Cholesterol Lipoproteins', 'Label', 'Laboratories', 'Learning', 'Link', 'Lipids', 'Lipoprotein (a)', 'Low-Density Lipoproteins', 'Manuals', 'Medical Records', 'Medicine', 'Methods', 'Minnesota', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phenotype', 'Plasma', 'Population Study', 'Predictive Value', 'Prevalence', 'Prevalence Study', 'Primary Prevention', 'Provider', 'Public Health', 'Recommendation', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk', 'Structure', 'Test Result', 'Time', 'Work', 'World Health Organization', 'Xanthomas', 'adverse outcome', 'base', 'cardiovascular risk factor', 'care providers', 'case-based', 'clinical decision support', 'clinical implementation', 'epidemiology study', 'evaluation/testing', 'implementation science', 'improved', 'improved outcome', 'inhibitor/antagonist', 'innovation', 'lifetime risk', 'mortality', 'novel therapeutics', 'point of care', 'population based', 'precision medicine', 'premature', 'prevent', 'screening', 'screening program', 'shared decision making', 'sudden cardiac death', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,R01,2018,653490,0.03351939063687697
"Interactive machine learning methods for clinical natural language processing DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3. Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,9337267,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face Processing', 'Goals', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Supervision', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'learning strategy', 'model development', 'novel', 'open source', 'real world application', 'statistics', 'success', 'tool', 'usability', 'word learning']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,463061,0.025981778309663785
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,9385056,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Biological Preservation', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Meta-Analysis', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phrases', 'portability', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2017,1589604,0.020017800382485663
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,9325065,R01LM011934,"['Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Ontology', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'information model', 'novel', 'open source', 'profiles in patients', 'public health relevance', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2017,387966,0.03795178485971105
"A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications DESCRIPTION (provided by applicant): Electronic Health Records (EHRs) can improve the quality of healthcare delivery in the United States, by providing automated best-practice reminders to clinicians and patients. However such functionality is currently limited to narrow areas of clinical practice, as existing decision support systems can process only structured data, due to lack of a suitable framework and concerns about accuracy and portability. Preliminary work by the PI has shown that rule-based approach can be used to develop broad-domain reminder systems that can utilize free-text in addition to the structured data. The PI has developed prototype systems for cervical and colorectal cancer prevention. These systems consist of rule-based composite models of national guidelines, and rule-based Natural Language Processing (NLP) parsers. The NLP parsers extract the patient variables required for applying the guidelines. However further research is needed to extend the systems and to ensure their accuracy for clinical deployment. In the mentored phase, the PI will collaborate with clinicians to extend and iteratively optimize and validate the systems, and will make them available in open-source so that they can be adapted for deployment at other institutions (aim 1 - K99). In the independent phase, the PI will research methods to facilitate rapid development, deployment and cross- institutional portability of similar systems. Specifically, the PI will develp a hybrid design for the parsers and investigate domain adaptation and active learning methods, for reducing the manual effort for development and adaptation of the NLP parsers (aim 2 - R00). To enable other researchers to reuse the developed methodologies and software resources, a toolkit will be developed that will support the construction and deployment of similar systems (aim 3 - R00). The toolkit will consist of user-friendly tools and templates to replicate the processes engineered in the case studies, and will build on the SHARPn data normalization tooling and other open-source tools. The independent phase will be in collaboration with Intermountain Healthcare. The PI's career goal is to become a scientific leader in clinical informatics with a focus on optimizing clinical decision making. The PI has strong background in clinical medicine and medical informatics, and will receive mentoring from Drs. Hongfang Liu, Christopher Chute, Robert Greenes and Rajeev Chaudhry, who have complimentary areas of expertise. The mentored (K99) phase will be for 2 years at Mayo Clinic Rochester, wherein the PI will undertake courses on decision support and will get mentored training in NLP and health information standards. This will prepare the PI for independent research in R00 phase on portability and tooling. Completion of the proposed work will enable the PI to seek further funding for piloting clinical deployment of the developed systems, measuring their clinical impact, and for scaling the approach to other clinical domains and institutions. The career grant will enable the PI to establish himself as an independent investigator and to make significant contributions towards advancing clinical decision support for improving care delivery. PUBLIC HEALTH RELEVANCE STATEMENT The potential of Electronic Health Records (EHRs) to improve care delivery by providing best-practice reminders is unrealized, because reminder systems currently operate in narrow areas of clinical practice, as they can process only structured data. The proposed framework will enable construction of reminder systems that can encompass broader areas of practice, due to their capability to utilize free-text as well as structured EHR data. This pioneering research directly impacts public health by improving the quality of care through enhanced reminder functionality in the EHRs.",A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications,9201329,R00LM011575,"['Active Learning', 'Address', 'Area', 'Caregivers', 'Caring', 'Case Study', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Collaborations', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Decision Support Systems', 'Development', 'Electronic Health Record', 'Engineering', 'Ensure', 'Fostering', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Care Costs', 'Healthcare', 'Hybrids', 'Institution', 'Language', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Manuals', 'Measures', 'Medical Informatics', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Phase', 'Process', 'Public Health', 'Quality of Care', 'Reminder Systems', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Standardization', 'Structure', 'Supervision', 'System', 'Text', 'Training', 'United States', 'Validation', 'Work', 'base', 'care delivery', 'career', 'clinical application', 'clinical decision-making', 'clinical practice', 'colorectal cancer prevention', 'colorectal cancer screening', 'design', 'electronic structure', 'health care delivery', 'health care quality', 'improved', 'learning strategy', 'open source', 'portability', 'prevent', 'prototype', 'public health relevance', 'tool', 'user-friendly']",NLM,MASSACHUSETTS GENERAL HOSPITAL,R00,2017,248969,0.0423270050500431
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility. PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9270528,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'Supervision', 'System', 'Time', 'Training', 'clinical predictors', 'cohort', 'computer science', 'crowdsourcing', 'data sharing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2017,316000,0.015580595706349728
"From enrichment to insights Project Summary Most medical decisions are made without the support of rigorous evidence in large part due to the cost and complexity of performing randomized trials for most clinical situations. In practice, clinicians must use their judgement, informed by their own and the collective experience of their colleagues. The advent of the electronic health record (EHR) enables the modern practitioner to algorithmically check the records of thousands or millions of patients to rapidly find similar cases and compare outcomes. In addition to filling the inferential gap in actionable evidence, these kinds of analyses avoid issues of ethics, practicality, and generalizability that plague randomized clinical trials (RCTs). Unfortunately, identifying patients with the appropriate phenotypes, properly leveraging available data to adjust results, and matching similar patients to reduce confounding remain critical challenges in every study that uses EHR data. Overcoming these challenges to improve the accuracy of observational studies conducted with EHR data is of paramount importance. Studies using EHR data begin by defining a set of patients with specific phenotypes, analogous to amassing a cohort for a clinical trial. This process of electronic phenotyping, is typically done via a set of rules defined by experts. Machine learning approaches are increasingly used to complement consensus definitions created by experts and we propose several advances to validate and improve this practice. We will explore and quantify the effects of feature engineering choices to transform the diagnoses, procedures, medications, laboratory tests and clinical notes in the EHR into a computable feature matrix. Finally, building on recent advances, we plan to characterize the performance of existing methods and develop EHR-specific strategies for patient matching. Our work is significant because we will take on three challenging problems--electronic phenotyping, feature engineering, and patient matching--that stand in the way of generating insights via EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes. Narrative The advent of the electronic health record (EHR) enables the search of thousands or millions of patients to rapidly find similar cases and compare outcomes. We will develop methods for feature engineering, electronic phenotyping and patient matching from real-world EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes.",From enrichment to insights,9365759,R01LM011369,"['Address', 'Algorithms', 'Area', 'Clinical', 'Clinical Trials', 'Code', 'Complement', 'Consensus', 'Data', 'Data Element', 'Data Set', 'Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evaluation', 'Frequencies', 'Future', 'Goals', 'Health system', 'Healthcare Systems', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Modernization', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Plague', 'Procedures', 'Process', 'Randomized Clinical Trials', 'Records', 'Resources', 'Scheme', 'Source', 'Statistical Data Interpretation', 'Test Result', 'Testing', 'Time', 'Training', 'Work', 'base', 'cohort', 'cost', 'electronic data', 'experience', 'health data', 'improved', 'innovation', 'insight', 'novel', 'portability', 'randomized trial', 'simulation', 'treatment effect', 'vector']",NLM,STANFORD UNIVERSITY,R01,2017,636560,0.06356173447648324
"Algorithms to Identify Systemic Lupus from Electronic Health Record Data Abstract  Systemic lupus erythematosus (SLE) is a chronic, autoimmune, multisystem disease that is often difficult to diagnose because of the diverse manifestations that occur over time and across care sites leading to increased damage and early mortality. The personal and economic costs in decreased quality of life and increase in healthcare expenditures, respectively, highlight the critical unmet need to develop new therapeutic strategies to treat lupus, so that treatment or participation in clinical trials occurs as early as possible to mitigate against disease-related damage. Therefore, it is important to find better ways to identify SLE patents.  Electronic health records (EHR) are now used in a majority of health care settings throughout the country, and present a rich source of information about patients which can be mined for earlier diagnosis and identification to improve quality of care, or enable high throughput clinical studies. Despite this potential, to date few accurate algorithms have been developed to identify SLE patients using EHR data. Construction of an effective algorithm, either by rule-based or machine learning methods, requires access to at two data resources not commonly available: 1) a validated “gold standard” patient data set with clear documentation of criteria that are indicative of SLE that can be compared against EHR data and 2) an integrated health record dataset that contains data from multiple health care institutions and reflects that SLE patients receive healthcare at multiple institutions and healthcare providers given their chronic, progressive disease. Over the past several years, our team has created both key resources: the Chicago Lupus Database (CLD), a physician-validated registry of 880 patients and gold standard data set and the Chicago HealthLNK Data Repository (HDR), a regional data resource including integrated medical records for 2.1 million patients across multiple institutions. Jointly, these two datasets enable the creation, testing and validation of algorithms for the identification of SLE in EHR data and provide a more complete picture of a patient population at risk for lupus.  We propose three specific aims to address the need to reduce the time to identify those with SLE in order to initiate treatment in a timelier fashion and to identify candidates for clinical trials. These aims are: 1) To create and validate a series of algorithms to identify SLE patients in EHR data against a gold standard curated registry, CLD, using validated classification criteria for SLE to build concepts for rule-based and machine learning methods that incorporate structured data, laboratory data, and unstructured data, e.g., physician notes, 2) To determine whether identification of SLE patients is improved when algorithms to identify SLE patients are extended to an integrated medical record dataset that includes data from multiple health care institutions, and 3) To use clustering techniques on SLE patients identified from EHR data to isolate clinically distinct sub-populations of patients, which could inform patient selection for participation in clinical trials. Project Narrative  Systemic lupus erythematosus (SLE) is a chronic, autoimmune, multisystem disease that is often difficult to diagnose because of the diverse manifestations that occur over time and across care sites leading to increased damage and early mortality. Electronic health records (EHR) are now used in a majority of health care settings throughout the country, and present a rich source of information about patients which can be mined for earlier diagnosis and identification to improve quality of care, or enable high throughput clinical studies. This study will test and validate algorithms for the identification of SLE from EHR data and thus will help provide a more complete picture of a patient population at risk for lupus.",Algorithms to Identify Systemic Lupus from Electronic Health Record Data,9375416,R21AR072263,"['Address', 'Affect', 'Algorithms', 'American', 'Autoimmune Process', 'Caring', 'Chicago', 'Chronic', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Collection', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Documentation', 'Early Diagnosis', 'Early identification', 'Electronic Health Record', 'Expenditure', 'Flare', 'General Population', 'Goals', 'Gold', 'Health Personnel', 'Health system', 'Healthcare', 'Institution', 'Laboratories', 'Legal patent', 'Lupus', 'Machine Learning', 'Medical', 'Medical Records', 'Medicine', 'Patient Selection', 'Patients', 'Phenotype', 'Physicians', 'Populations at Risk', 'Progressive Disease', 'Quality of Care', 'Quality of life', 'Registries', 'Research', 'Resources', 'Sensitivity and Specificity', 'Series', 'Site', 'Source', 'Structure', 'Symptoms', 'Systemic Lupus Erythematosus', 'Techniques', 'Testing', 'Time', 'Validation', 'base', 'clinical candidate', 'clinical practice', 'cost', 'data resource', 'economic cost', 'health record', 'improved', 'learning strategy', 'mortality', 'novel therapeutic intervention', 'patient population', 'personalized medicine', 'prevent', 'standard of care', 'targeted treatment', 'therapeutic evaluation']",NIAMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2017,208395,0.03811240117610095
"Bayesian Generative Methods for Extracting and Modeling Relations in EHR Narratives Project Summary Medicine has evolved into an era where the entire hospital progressively adopts more real-time monitoring for the patients and generates ICU like clinical data. The rapidly growing data makes ICU a snapshot for tomorrow’s standard of care that should benefit from computer-aided decision making. These data contain not only numerical or coded information, but also a large volume of unstructured narrative text such as physicians’ and nurses' notes, specialists' reports, and discharge summaries. Both types of data have been shown to be highly informative for tasks such as cohort selection, and work best in combination. However, to achieve this, specific bits of information must be extracted from the narrative reports and coded in formal representation. These bits include medical concepts such as symptoms, diseases, medications and procedures; characteristics such as certainty, severity, dose; assertions about these items, such as whether they pertain to the patient or a family member, etc.; relations among these mentions, including indications of what condition is treated by what action and its degree of success, the time sequence and duration of events, and interpretations of laboratory test results as relations among medical concepts such as cells and antigens (e.g., “[large atypical cells] express [CD30]”). Concepts and assertions can be regarded as simple relations, and our proposal focuses on modeling narrative relations to augment structured data for predicting patient outcomes. Most existing techniques for interpreting clinical narratives either rely on hand-crafted rule systems and large medical thesauri or are based on machine learning models that create classification or regression models from large annotated data sets. The former are difficult and laborious to generalize, whereas the latter require large volumes of human-labeled data and may result in models whose operation is difficult to interpret and is therefore considered unsuitable for computer-aided decision making. We propose to build on our previous work to use unsupervised learning methods that identify frequent patterns in un-annotated narratives and identify informative patterns by tensor factorization. Although existing methods can also identify patterns that are meaningful in a data-driven sense, these patterns are difficult for clinicians to understand. Our specific goal is to develop a novel method that uses a Bayesian generative model that integrates relation mining with tensor factorization to learn patterns that correspond to an understanding of the clinical domain and can be used for evidence based patient outcome prediction. Our framework represents relations in clinical narratives as graphs, then mines subgraphs for important relations. These relations are used as features in building up a tensor model in order to reduce dimensionality, discover coherent groups of relations, and explore the group interactions. We develop Bayesian formulation to integrate relation mining and tensor modeling in a generative model, to incorporate existing medical knowledge as probability priors, as well as to reliably estimate the posterior probabilities and confidence intervals of any findings from the model. Rapid growth in the hospital adoption of large volume of Electronic Health Records (EHRs) has led to an unprecedented availability of narrative dataset for clinical and translational research. We propose the development of a novel Bayesian generative framework to enable extraction of accurate and clinically meaningful patterns of EHR narratives in order to support evidence based diagnostic reasoning and outcome risk prediction.",Bayesian Generative Methods for Extracting and Modeling Relations in EHR Narratives,9374626,R21LM012618,"['Admission activity', 'Adopted', 'Adoption', 'Algorithms', 'Antigens', 'Appearance', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Cognitive', 'Computer Assisted', 'Confidence Intervals', 'Data', 'Data Set', 'Decision Making', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Dose', 'Electronic Health Record', 'Event', 'Family member', 'Formulation', 'Goals', 'Graph', 'Hand', 'Hodgkin Disease', 'Hospitals', 'Human', 'Individual', 'Injectable', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Lymphoma', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Morphology', 'National Research Council', 'Nurses', 'Outcome', 'Output', 'Pathology Report', 'Patient Monitoring', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Probability', 'Procedures', 'Reporting', 'Research', 'Risk', 'Risk Factors', 'Severities', 'Source', 'Specialist', 'Structure', 'Symptoms', 'System', 'TNFRSF8 gene', 'Techniques', 'Test Result', 'Text', 'Thesauri', 'Time', 'To specify', 'Translational Research', 'Work', 'adverse outcome', 'base', 'cohort', 'diagnostic panel', 'evidence base', 'improved', 'learning strategy', 'mortality', 'novel', 'operation', 'outcome prediction', 'patient oriented', 'rapid growth', 'standard of care', 'success']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2017,208272,0.03776614795429494
"Automated Detection of Anomalous Accesses to Electronic Health Records DESCRIPTION (provided by applicant): Health information technology (HIT) can lower costs, strengthen productivity, and promote safety. To realize such benefits on a large scale, healthcare organizations (HCOs) are adopting electronic health records (EHRs) to provide various capabilities. Yet, as EHRs and the healthcare workforce grow in diversity, so does their complexity. This is a concern because evidence suggests complex HIT can interrupt care delivery, contribute to medical errors, and expose patient data to privacy breaches. Moreover, such events tend to be discovered only after they transpire en masse, leading to negative media coverage, loss of patients' trust, and sanctions. Federal regulations now enable patients to receive accountings of who accessed their medical records during treatment, payment, and operations related activities. Yet, for patients to make sense of such accountings, they need to be provided with explanations regarding the extent to which accesses are normal in the context of routine HCO activities. We believe that relating specific accesses to patterns of healthcare operations can help explain how medical records are utilized. Unfortunately, many of the aforementioned problems manifest because EHR utilization patterns rarely guide the design and refinement of healthcare management practices. Thus, the overarching objective of our research is to develop novel strategies to automatically learn HCO behavior based on EHR usage. The past several years has witnessed a flurry of activity in this field, but it remains in is infancy and has only scratched the surface of care patterns and the types of anomalies that can be detected. Through this project, we propose to develop anomaly detection methods that integrate the semantics of healthcare operations and allow for the detection of workflows over time. This will enable HCOs and patients to audit in a meaningful way. Moreover, we believe the innovation and dissemination of such data mining strategies will enable HCOs to detect anomalous events that indicate system misuse and patients who require special attention, but also effectively audit business practices and discover inefficient workflows. The specific aims of this project are (1) to develop machine learning approaches, based on intrasession utilization patterns, to streamline EHR interface configuration and detect anomalous sessions, (2) to design a data mining framework, based on intersession EHR access patterns, to characterize HCO departmental interactions in patient treatment and detect anomalous events, and (3) to infer patient management pathways to consolidate redundant processes and detect deviations from anticipated workflows. In support of these goals, we will evaluate, compare, and contrast the workflows and anomalies in the EHR systems of two large medical centers. Additionally, we will ensure that our methods are integrated into an open source software system that can assist HCOs to extract, transform, and load (ETL) access data from EHRs, analyze such data for anomalies, and visualize the results in interfaces that enable review by healthcare administrators and patients. In doing so, we will be able to compare and contrast behavior of the workflows and multiple institutions and develop methods that appropriately generalize across EHR systems. As electronic health record systems, and healthcare organizations, grow in diversity, so do their complexity, which can lead to inefficient documentation, management of patients, and expose patient data to privacy breaches. In this research, we will develop technologies, to be disseminated through an open source software suite, to learn patterns associated with healthcare operations, upon which anomaly detection techniques can be based. The specific goals of this project are to 1) model HCO users' intrasession behaviors when interacting with an EHR, 2) learn interdepartmental relations based on the accesses of common patients, and 3) infer patient management pathways based on the sequence of accesses to a patient's record.",Automated Detection of Anomalous Accesses to Electronic Health Records,9332474,R01LM010207,"['Administrator', 'Admission activity', 'Adopted', 'Architecture', 'Attention', 'Behavior', 'Businesses', 'Caring', 'Case Manager', 'Case Study', 'Clinical', 'Collaborations', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Security', 'Detection', 'Dimensions', 'Documentation', 'Electronic Health Record', 'Employee', 'Engineering', 'Ensure', 'Entropy', 'Event', 'Geography', 'Goals', 'Grant', 'Graph', 'Healthcare', 'Inpatients', 'Institution', 'Interdepartmental Relations', 'Interruption', 'Lead', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Errors', 'Medical Records', 'Medical center', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Movement', 'Neonatal Intensive Care', 'Neonatology', 'Pathway interactions', 'Patient Care', 'Patients', 'Pattern', 'Patterns of Care', 'Phase', 'Practice Management', 'Primary Health Care', 'Privacy', 'Process', 'Productivity', 'Provider', 'Recording of previous events', 'Regulation', 'Research', 'Research Personnel', 'Safety', 'Semantics', 'Surface', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trust', 'Variant', 'Work', 'base', 'care delivery', 'comparative', 'cost', 'data access', 'data mining', 'design', 'follow-up', 'health information technology', 'infancy', 'innovation', 'novel strategies', 'open source', 'operation', 'organizational structure', 'payment', 'programs', 'software systems', 'ward', 'web portal']",NLM,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2017,341475,0.06533299742828878
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics. PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,9282532,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Base Sequence', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'DNA sequencing', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Elements', 'Ethics', 'Evaluation', 'Faculty', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Methylphenidate', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Recommendation', 'Records', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'base', 'biobank', 'clinical care', 'clinically actionable', 'cohort', 'cost', 'data modeling', 'database of Genotypes and Phenotypes', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'learning strategy', 'member', 'next generation sequencing', 'patient oriented', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'public health relevance', 'response', 'skills', 'social', 'success', 'support tools', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2017,811897,0.017226561382087276
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9290660,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Severe Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,644888,0.014580961630196563
"Patient Medical History Representation, Extraction, and Inference from EHR Data DESCRIPTION (provided by applicant): The significance of developing tools for automatically harvesting temporal constraints of clinical events from Electronic Health Records (EHR) cannot be overestimated. Efficient analysis of the temporal aspects in EHR data could boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine.     One big challenge we are facing is to automatically untangle and linearize the temporal constraints of clinical events embedded in highly diverse large-scale EHR data. Barriers to temporal data modeling, normalization, extraction, and reasoning have precluded the efficient use of EHR data sources for event history evaluation and trending analysis: (1) The current federally-supported EHR data normalization tools do not focus on the time aspect of unstructured data yet; (2) Existing time models focus only on structured data with absolute time, lack of supporting reasoning systems, or only offer application-specific partial solutions which cannot be adopted by the complex EHR data; (3) Current temporal information extraction approaches are either difficult to be adopted to EHR data, not scalable, or only offers application-specific partial solution.     This proposed project fills in the current gaps among ontologies, Natural Language Processing (NLP), and EHR-based clinical research for temporal data representation, normalization, extractions, and reasoning. We propose to develop novel approaches for automatic temporal data representation, normalization and reasoning for large, diverse, and heterogeneous EHR data and prepare the integrated data for further analysis. We will build new reasoning and extraction capacities on our TIMER (Temporal Information Modeling, Extracting, and Reasoning) framework to provide an end-to-end, open-source, standard-conforming software package. TIMER will be built on strong prior work by our team. We will develop new features in our CNTRO (Clinical Narrative Temporal Relation Ontology) for semantically defining the time domain and representing temporal data in complex EHR data. On top of the new developed CNTRO semantics, we will implement temporal relation reasoning capacities to automatically normalize temporal expressions, compute and infer temporal relations, and resolve ambiguities. We will leverage existing NLP tools and work on top of these tools to develop new extraction approaches to fill in the current gaps between NLP approaches and ontology-based reasoning approaches. We will adapt the SHARPn EHR data normalization pipeline and cTAKES for extracting and normalizing clinical event mentions from clinical narratives. We will explore an innovative approach for temporal relation extraction and event coreference, and make it work with the TIMER framework. We will evaluate the system using Diabetes Mellitus (DM) and colorectal cancer (CRC) patient cohorts from two insititutions. Each component will be tested separately first followed by an evaluation of the whole framework. Results such as precision, recall, and f-measure will be reported. Project Narrative The significance of developing capabilities for automatically harvesting temporal constraints for clinical events from Electronic Health Records (EHR) cannot be overestimated. A substantial portion of the information in the EHR is historical in nature. Patient medical history can be long, especially in complex patients. The proposed work, by offering an end-to-end open-source framework for automatically extracting, normalizing, and reasoning clinically-important time-relevant information from large-scale EHR data, can boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine; as well as facilitate clinical practice for early disease detection, post-treatment care, and patient-clinician communication.","Patient Medical History Representation, Extraction, and Inference from EHR Data",9332464,R01LM011829,"['Address', 'Adopted', 'Aftercare', 'Archives', 'Automated Annotation', 'Big Data', 'Caring', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Colorectal Cancer', 'Communication', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Data Reporting', 'Data Set', 'Data Sources', 'Databases', 'Decision Support Systems', 'Detection', 'Diabetes Mellitus', 'Disease', 'Disease Progression', 'Electronic Health Record', 'Evaluation', 'Event', 'Goals', 'Gold', 'Harvest', 'Human', 'Institutes', 'Maps', 'Measures', 'Medical History', 'Medical Records', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Performance', 'Recording of previous events', 'Registries', 'Reporting', 'Resolution', 'Semantics', 'Structure', 'Supervision', 'System', 'Testing', 'Time', 'Translational Research', 'Work', 'application programming interface', 'base', 'clinical practice', 'cohort', 'colon cancer patients', 'data modeling', 'data structure', 'information model', 'innovation', 'novel strategies', 'open source', 'personalized medicine', 'tool', 'trend analysis']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,335217,0.058406506835919574
"Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients Project Summary Epilepsy is one of the leading neurological disorders in the United States, affecting more than 479,000 children and over 2 million adults. Approximately 30% of epileptic patients have poor seizure control despite antiepileptic medications and are potential candidates for neurosurgical intervention. Early identification and referral of children who are potential surgical candidates is complex and while relevant guidelines exist, there is no standard process to efficiently identify those patients meeting criteria for neurosurgical intervention. Given the large corpus of note-based data available in the electronic health record (EHR), it is challenging for providers to efficiently retain and process all the pertinent patient information. Natural Language Processing (NLP) and machine learning techniques have been successfully used to evaluate clinical notes and make recommendations in the research setting. However, NLP techniques are rarely integrated into practice to provide real-time clinical decision support. We developed and retrospectively evaluated a NLP system to help identify those patients who meet neurosurgical criteria and therefore enable surgical consults and evaluations to occur sooner. Knowing clinical decision support can improve outcomes of care, our proposed research will implement NLP into clinical practice and develop a decision support mechanism to improve the time to surgery for eligible patients. The objective of this project is to implement NLP directly into clinical care and determine the most effective decision support mechanism for provider adherence to epilepsy surgical consult recommendations. The long- term goal of this project is to reduce the time to initial surgery evaluation for patients with intractable epilepsy by integrating NLP-classification criteria into clinical practice. This project is one of the first in the field to study the integration of NLP recommendations into clinical care. We will use a human factors engineering framework to design and to analyze two different alerting methodologies for the best-fit for clinical workflow to produce the optimum provider adherence while reducing alert fatigue. Epilepsy progress notes can be classified across hospitals, and if successful, the system will be implemented in additional pediatric institutions around the United States. Project Narrative Epilepsy affects more than 479,000 children; 30% of those are not controlled with medication and may require surgery. We developed a novel Natural Language Processing (NLP) algorithm that can be integrated into neurology practice to detect patients who may be eligible for epilepsy surgical consults. Once implemented, this research can help to drive successful implementations of NLP and identify and use ideal alerting mechanisms in neurological care.",Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients,9355161,R21HS024977,[' '],AHRQ,CINCINNATI CHILDRENS HOSP MED CTR,R21,2017,159334,0.00010923999066842528
"Secondary use of EMRs for surgical complication surveillance DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.",Secondary use of EMRs for surgical complication surveillance,9251814,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'base', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'patient safety', 'public health relevance', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2017,300000,0.03231005138540235
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics ﻿    DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities. PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9331533,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Award', 'Benefits and Risks', 'CCL4 gene', 'Cancer Intervention', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Learning', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Public Health Informatics', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'interest', 'longitudinal dataset', 'novel', 'open source', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2017,623596,0.013963725226351946
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9337497,R01LM010090,"['Apache', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'Intuition', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translational Research', 'Trees', 'Vision', 'Visualization software', 'Work', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'electronic structure', 'experience', 'individual patient', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'symptom treatment', 'syntax', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2017,643621,0.019477782709814727
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9215012,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Data Sources', 'Development', 'Diagnostic', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'outcome forecast', 'population based', 'prevent', 'socioeconomics', 'surveillance data', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2017,836858,0.014684016853767775
"Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding Project Summary/Abstract  With increasing use of electronic medical records for a variety of patients, a large investment is being made in a resource still vastly underused. Especially in mental health, where problems are highly individualized, requiring personalized intervention, and often accompanied by rich data not easily captured in structured templates, the need for extracting information from free text in existing records for use as large-scale stand- alone datasets or in combination with other data is real. Without scalable and effective computational approaches to capture this data, much time, effort and money is used to create limited-use records that instead could be leveraged into precious data sources to inform existing research and lead to new insights, progress and treatments. Our broad, long-term goal is processing free text in EHR in mental health. We focus on Autism Spectrum Disorders (ASD), a particularly interesting example of both shortcomings and opportunities.  ASD’s prevalence has increased over the years, and estimates range from 1 in 150 in 2000 to 1 in 68 in 2010(1-5). These numbers are based on surveillance using electronic health records. The increasing prevalence is not well understood, and hypotheses range from changing diagnostic criteria to environmental factors. The lines of inquiry used to find cures are similarly broad and range from brain scans and genetics, resulting in large structured datasets, to highly individualized therapies, resulting in rich but unstructured data. Currently the text information in the electronic records is not being leveraged on a large scale.  The proposed project continues our preliminary work and uses a data-driven approach to create human- interpretable models that allow automated extraction of relevant structured data from free text. The Diagnostic and Statistical Manual of Mental Disorders (DSM) is the starting point for identifying features. A database of thousands of records is leveraged to design and test the algorithms. The two specific aims are: 1) design and test natural language processing (NLP) algorithms to detect DSM criteria for ASD in free text in EHR, and 2) demonstrate feasibility and usefulness of the models for large-scale analysis of ASD cases, which is inconceivable today with current approaches. Our methods include analysis of free text in electronic records and end-user annotations to create a large gold standard of instances of DSM criteria for ASD, application of machine learning and rule-based approaches to create human-interpretable models for automated annotation of diagnostic patterns in textual records, and demonstrate usefulness with new research (e.g., Automatically detect ASD vs. no-ASD status for challenging cases; evaluate prevalence of symptoms over time). Through NLP algorithms, this project has the potential to significantly shift away from the current paradigm of attempting to understand ASD by relying on small-scale data from individual interventions and lack of integration between different data sources, to leveraging information from existing large-scale data sources to propose novel analyses and hypotheses. Project Narrative  Lack of sophisticated tools to extract relevant diagnostic patterns from free text from the increasingly large number of electronic medical/health records is a critical barrier in the field of mental health to leverage and utilize the already available data. Natural language processing (NLP) algorithms designed specifically for mental health can make new data analysis and integration with other sources possible at a scale previously unseen. Using a data-driven process, this project will design NLP algorithms to annotate free text with criteria from the Diagnostic and Statistical Manual of Mental Disorders (DSM) and demonstrate scope, feasibility and usefulness by focusing on Autism Spectrum Disorders (ASD) where prevalence is increasing and much rich clinical text is stored in electronic health records (EHR).",Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding,9381416,R21HS024988,[' '],AHRQ,UNIVERSITY OF ARIZONA,R21,2017,146202,-0.05761058777977941
"Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes ﻿    DESCRIPTION (provided by applicant): The applicant and principal investigator (PI) is a board-certified cardiologist, Assistant Professor of Medicine and Senior Associate Consultant in the Mayo Clinic Cardiovascular Division. [The long-term goal of the PI is to become an independent clinician-investigator in translational informatics focused on the development, validation and deployment of electronic tools to the point-of-care to improve outcomes for patients with cardiovascular disease. The PI intends to leverage the EHR to conduct population-based studies using electronic algorithms that include NLP and by application of state-of-the-art informatics approaches to deliver the knowledge at the point-of-care in support of patient-centered decision-making for patients with peripheral arterial disease (PAD). The electronic tools developed will be portable to different institutions and other health conditions. The PI will be uniquely positioned as a cardiologist within a large academic health system working with novel NLP techniques at the interface of the EHR and CDS to rapidly translate knowledge acquired in community-based epidemiologic investigations to patient care.] The PI has proposed a research career development plan to acquire expertise in translational informatics including electronic phenotyping, and automated calculation and visual display of prognostic scores using a novel application integrated with clinical decision support (CDS). The PI will complete comprehensive coursework in informatics science and gain additional patient-oriented and epidemiologic research experience. The primary mentor will be Iftikhar Kullo, M.D., cardiologist and PI of the Mayo Clinic electronic MEdical Records and GEnomics (eMERGE) network and a leader in the development and application of electronic phenotyping for genomic studies of PAD. The mentoring team also includes Dr. Christopher Chute, M.D., Head of Medical Informatics at Mayo Clinic, an expert in informatics and epidemiology, and co-PI of the Mayo eMERGE grant and co-investigator of the Rochester Epidemiology Project; Hongfang Liu, Ph.D., an expert in natural language processing (NLP) and its application for EHR-based phenotyping; and Kent Bailey, Ph.D., senior biostatistician with expertise in development of prognostic risk scores. The specific aims of the proposal are: Aim 1 - Apply phenotyping algorithms that include NLP of clinical notes to [a] identify PAD cases and controls without PAD in the community from 1998-2011; and [b] ascertain adverse cardiovascular and limb outcomes through December 2013. Aim 2 - Create multivariable prognostic risk models and scores for adverse outcomes in PAD cases from the community. We will estimate the relative risk of death in PAD patients compared to controls without PAD. Aim 3 - Develop and evaluate usefulness of a novel electronic application within the Mayo EHR to retrieve relevant data elements, calculate, and display individualized prognostic scores. On completion of the proposed investigations the deliverables will be new knowledge and an e-health prognostication tool for PAD patients to be disseminated to stakeholders including clinicians, patients, and researchers portable to other institutions and other cardiovascular diseases. PUBLIC HEALTH RELEVANCE: The applicant, a board certified cardiovascular specialist, proposes to acquire training in translational informatics science which will enable application of novel electronic algorithms to electronic health records to more efficiently conduct community-based studies of peripheral arterial disease, a prevalent but undertreated health condition. These studies will concurrently develop novel electronic tools which will inform patients and providers of risk, facilitate shared-decision making at the point-of-care, and promote compliance to guideline recommended strategies. These tools will be portable to other institutions and to other cardiovascular diseases.",Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes,9210555,K01HL124045,"['Address', 'Adult', 'Adverse event', 'Algorithms', 'Amputation', 'Atherosclerosis', 'Atrial Fibrillation', 'Attention', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Investigator', 'Communities', 'Computerized Medical Record', 'Coronary heart disease', 'Data', 'Data Element', 'Decision Making', 'Dependence', 'Development', 'Development Plans', 'Diagnosis', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Electronic Health Record', 'Epidemiology', 'Genomics', 'Goals', 'Government Agencies', 'Grant', 'Guidelines', 'Head', 'Health', 'Health system', 'Healthcare', 'Heart failure', 'High Prevalence', 'Image', 'Informatics', 'Institution', 'Investigation', 'Ischemia', 'Knowledge', 'Laboratories', 'Limb structure', 'Link', 'Manuals', 'Medical Informatics', 'Medicine', 'Mentors', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patient risk', 'Patient-Focused Outcomes', 'Patients', 'Peripheral arterial disease', 'Pharmaceutical Preparations', 'Phenotype', 'Population Study', 'Positioning Attribute', 'Principal Investigator', 'Provider', 'Relative Risks', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Risk stratification', 'Safety', 'Science', 'Site', 'Specialist', 'Stroke', 'Symptoms', 'System', 'Techniques', 'Time', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Visual', 'Work', 'adverse outcome', 'base', 'career development', 'case control', 'clinical decision-making', 'cost', 'demographics', 'didactic education', 'eHealth', 'epidemiology study', 'experience', 'health information technology', 'high risk', 'improved', 'improved outcome', 'investigator training', 'mortality', 'multidisciplinary', 'novel', 'patient oriented', 'point of care', 'portability', 'predictive modeling', 'professor', 'prognostic', 'prognostic tool', 'public health relevance', 'repository', 'research and development', 'shared decision making', 'skills', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,K01,2017,170856,0.051582619427197725
"Development and Evaluation of a Learning Electronic Medical Record System ﻿    DESCRIPTION (provided by applicant):  The goal of this project to develop and evaluate a learning electronic medical records (L-EMR) system that draws a physician's attention to the right data, at the right time. It learns how to do so by analyzing patterns of patient data access f many physicians in many past cases in the EMR, and learning which EMR data to highlight that are relevant for making clinical decisions in a given patient.      The hypothesis underlying this research is that the L-EMR system will have sufficiently high precision and recall in highlighting relevant data, decrease the average time to assess an intensive care unit (ICU) patient case, and be judged by critical care medicine (CCM) physicians to be clinically useful.    The first aim of this project is develop a highly-usable L-EMR user interface. The L-EMR user interface will include zoomable time-series displays of lab-results, med-orders, and vital signs. Usability studies of the L-EMR user interface will guide revisions and enhancements.      The second aim of the project is to train statistical models that can be applied to a patient case to predict relevant lab-results, med-orders, and vital signs. We will enlist CCM physicians to review a set of retrospective ICU patient cases on a focused set of clinical conditions. Participants will review these cases as if they were active patients, identifying relevant lab- results, med-orders, and vital signs. We will train and evaluate statistical models to predict relevant data, and identify the best performing algorithm to include in the L-EMR system.      The third aim of the project is to evaluate the L-EMR system. We will recruit CCM physicians to evaluate an L-EMR system based on user interfaces from Aim 1 and statistical models trained using the best performing algorithm in Aim 2 to highlight relevant data items. We will measure the precision and recall of the data-highlighting functionality for assessing patient cases and making clinical decisions (e.g., lab and medication orders), the time required to assess cases with and without the highlighting, and physicians' assessments of the strengths and weaknesses of the L-EMR system.    If the results of these experiments are positive, as anticipated, this project will introduce a computational method that has significant potential to improve future EMR systems and enhance patient care. Narrative The purpose of this research is to develop and evaluate a learning electronic medical records (EMR) system that draws a physician's attention to the right data, at the right time. The system works by analyzing patterns of EMR usage of physicians, and learning which EMR data to highlight that are relevant in a given patient. The main idea underlying the approach is that patterns of past EMR usage patterns can be exploited to selectively highlight clinically useful patient data.",Development and Evaluation of a Learning Electronic Medical Record System,9297355,R01LM012095,"['Address', 'Adult', 'Algorithms', 'American', 'Attention', 'Bayesian Modeling', 'Blood', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical assessments', 'Computerized Medical Record', 'Computing Methodologies', 'Critical Care', 'Critical Illness', 'Data', 'Data Display', 'Data Set', 'Development', 'E-learning', 'Educational workshop', 'Evaluation', 'Face', 'Future', 'Gastrointestinal Hemorrhage', 'Goals', 'Healthcare Systems', 'Heart Rate', 'Hemoglobin', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Intensive Care Units', 'Intravenous', 'Laboratories', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Physiological', 'Provider', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Series', 'Statistical Models', 'System', 'Test Result', 'Time', 'Training', 'Work', 'base', 'clinical decision-making', 'computer human interaction', 'data access', 'design', 'experimental study', 'follow-up', 'improved', 'prospective', 'prototype', 'research clinical testing', 'stem', 'trend', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2017,444581,0.038107506533143216
"Data-Mining Clinical Decision Support from Electronic Health Records ﻿    DESCRIPTION (provided by applicant)    Data-Mining Clinical Decision Support from Electronic Health Records Public Health Motivation: National healthcare quality is compromised by undesirable variability, reflected in different locales having anywhere from 20-80% compliance with evidence-based guidelines. Much of this is due to uncertainty, with half of clinical practice guidelines lacking adequate evidence to confirm their efficacy. This is unsurprising when clinical trials cost >$15 million to answer individual clinical questions. The result is medical practice routinely driven by individual opinio and anecdotal experience. While Big Data has revolutionized how society processes internet scale information, the status quo in clinical decision making remains the manual interpretation of literature and isolated decision aids. The adoption of electronic health records (EHR) creates a new opportunity to answer a ""grand challenge in clinical decision support (CDS)."" In a learning health system, we could automatically adapt knowledge from the collective expertise embedded in the EHR practices of real clinicians and close the loop by disseminating that knowledge back as executable decision support. Candidate Goals and Objectives: The unifying goal of this BD2K K01 proposal is the mentored career development of Jonathan H. Chen, MD, PhD. This proposal will accelerate his transition into an independent physician scientist, towards his long-term goals to produce Big Data technologies that answer such grand challenges in clinical decision support. His near-term objective is developing methods to translate EHR data into useful knowledge in the form of patient- specific, point-of-care clinical order recommendations for acute medical hospitalizations. His doctoral background in computer science gives him the technical capability to achieve these objectives, while his medical training will ensure clinically meaningful results. His preliminary work to build an order recommender, analogous to commercial product recommenders, demonstrates the proposal's overall feasibility. Institutional Environment and Career Development: The research facilities and training opportunities at Stanford University provide the ideal environment to achieve these objectives, with established and growing Centers for Biomedical Informatics Research, the Biomedical Data Science Initiative, and the first Clinical Informatics Fellowship accredited in the nation. Prof. Russ Altman, Director of the Biomedical Informatics Training Program, will lead a collaborative team of mentors with expertise in clinical decision support (Mary Goldstein), implementation science (Steven Asch), data-mining electronic health records (Nigam Shah), statistical learning algorithms (Lester Mackey), and healthcare statistics (Michael Baiocchi). Combined with respective didactic training, this mentorship will enable Dr. Chen to achieve his objectives through a series of research aims. Research Aims: The overriding hypothesis of the proposal is that clinical knowledge reflected in clinical order patterns from historical EHR data can improve medical decision making when adapted into functional clinical decision support. The specific aims each address components of this concept, as they seek to: (1) Develop the algorithms to learn clinical order patterns from historical EHR data, building on a preliminary recommender system; (2) Assess how underlying clinician proficiency affects the quality of those learned clinical order patterns through observational data inference against external standards; and (3) Determine the impact of automatically learned clinical decision support (CDS) on (simulated) clinical workflows through a randomized controlled crossover trial of human-computer interfaces with real clinicians. Expected Results and General Significance: By the completion of the proposed work, Dr. Chen will answer the grand challenge in clinical decision support (CDS) by automating much of the CDS production process, and have direct translational impact with a prototype system. This will advance the field with new paradigms of generating and disseminating clinical knowledge, which can then improve the consistency and quality of healthcare delivery. Additional benefits will include methods to identify and monitor areas of high practice variability for targeted optimization and improve predictive models that inform precision medicine. With this applied research experience and career development, Dr. Chen can compete for R01 funding and become an independent physician scientist developing Big Data approaches to solve national healthcare problems in clinical decision making. PUBLIC HEALTH RELEVANCE    National healthcare quality is compromised by undesirable practice variability and medical uncertainty, with most medical practice routinely driven by individual opinions and anecdotal experience. With methods analogous to commercial product recommender systems, the proposed project will automatically learn patterns in raw clinical transaction data to capture the undocumented knowledge of real-world clinicians, and close the loop in a learning health system by disseminating that knowledge back as clinical decision support to improve patient care.",Data-Mining Clinical Decision Support from Electronic Health Records,9325317,K01ES026837,"['Accreditation', 'Achievement', 'Acute', 'Address', 'Admission activity', 'Adoption', 'Affect', 'Algorithms', 'Applied Research', 'Area', 'Back', 'Big Data', 'Big Data to Knowledge', 'Caring', 'Chiroptera', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Practice Guideline', 'Clinical Trials', 'Cross-Over Trials', 'Crowding', 'Data', 'Data Science', 'Decision Aid', 'Decision Making', 'Diagnosis', 'Doctor of Philosophy', 'Educational process of instructing', 'Electronic Health Record', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Failure', 'Fellowship', 'Funding', 'Future', 'Goals', 'Health system', 'Healthcare', 'Hospitalization', 'Hospitals', 'Image', 'Individual', 'Institute of Medicine (U.S.)', 'Internet', 'Investments', 'Knowledge', 'Lead', 'Learning', 'Literature', 'Locales', 'Machine Learning', 'Manuals', 'Medical', 'Medical Residency', 'Mentors', 'Mentorship', 'Methods', 'Monitor', 'Motivation', 'Noise', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Privatization', 'Process', 'Production', 'Public Health', 'Quality of Care', 'Randomized', 'Recommendation', 'Research', 'Research Training', 'Scientist', 'Series', 'Services', 'Societies', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Training', 'Training Programs', 'Transact', 'Translating', 'Uncertainty', 'Universities', 'User-Computer Interface', 'Validation', 'Weight', 'Work', 'base', 'biomedical informatics', 'career development', 'clinical care', 'clinical decision-making', 'clinical practice', 'clinical predictors', 'computer science', 'cost', 'data mining', 'design', 'evidence based guidelines', 'experience', 'health care delivery', 'health care quality', 'implementation science', 'improved', 'multidisciplinary', 'point of care', 'precision medicine', 'predictive modeling', 'prototype', 'public health relevance', 'research facility', 'routine practice', 'statistics', 'support tools', 'training opportunity', 'translational impact']",NIEHS,STANFORD UNIVERSITY,K01,2017,178606,0.025559366617838974
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9352296,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2017,249995,0.004696198921118965
"NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department Summary Timely identification of relevant or “need to know” clinical information about a patient’s history in the acute care setting can be critical for patient safety and medical decision-making. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. Currently, it is estimated that over 50% of the EHR is free-text. EHR search tools today are often inefficient, simplistic, and unable to rank or evoke the relevance of information for a particular problem or complaint. This is compounded by the fact that EHRs are amassing clinical information at an exponential rate. While the benefits of having a wealth of information at a provider’s fingertips seem obvious, the time and energy cost of culling through enormous amounts of data creates new issues of decreased efficiency and information overload for providers seeking to identify the most pertinent and relevant information about their patients. In the emergency department, where patients can present with life threatening conditions, timely unlocking of clinically relevant information for a patient’s problem or complaint at the point of care can be critical to medical decision-making and patient safety. In this study, we plan to address this challenge through the development of a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information based on the patients presenting complaint. We will accomplish this through the following specific aims: 1) identify and define complaint- specific information elements within a patient’s history and 2) develop and test an NLP-based information retrieval tool. Narrative Patient safety hinges on having right information about the right patient and the right time. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. The purpose of this study is to develop and evaluate a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information from EHRs that providers rely upon to make medical decisions for their patients. This study comes at an important time where data in the EHRs is increasing at an exponential rate, creating a new problem for clinicians, that of finding all the relevant information for patient’s particular problem. This is particularly true in the emergency department setting where providers have limited if any prior relationship and often have to make quick decisions for patients with life threatening conditions. This search tool will provide a snapshot of clinically relevant information that the providers can view alongside the structured information already in the EHR. We believe this approach has the potential to increase clinician efficiency, decrease healthcare costs by avoiding duplicate diagnostic tests, and provide clinicians with the tools they need to make well-informed medical decisions, thereby improving patient safety and reducing suffering.",NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department,9342692,R21HS024541,[' '],AHRQ,UNIVERSITY OF COLORADO DENVER,R21,2017,148922,0.03705509281247727
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,9352770,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,249135,0.016391971940315025
"National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms PROJECT SUMMARY With the rapidly growing adoption of patient electronic health record systems (EHRs) due to Meaningful Use, and linkage of EHRs to research biorepositories, evaluating the suitability of EHR data for clinical and translational research is becoming ever more important, with ramifications for genomic and observational research, clinical trials, and comparative effectiveness studies. A key component for identifying patient cohorts in the EHR is to define inclusion and exclusion criteria that algorithmically select sets of patients based on stored clinical data. This process is commonly referred to, as “EHR-driven phenotyping” is time-consuming and tedious due to the lack of a widely accepted and standards-based formal information model for defining phenotyping algorithms. To address this overall challenge, the proposed project will design, build and promote an open-access community infrastructure for standards-based development and sharing of phenotyping algorithms, as well as provide tools and resources for investigators, researchers and their informatics support staff to implement and execute the algorithms on native EHR data. PROJECT NARRATIVE The identification of patient cohorts for clinical and genomic research is a costly and time-consuming process. This bottleneck adversely affects public health by delaying research findings, and in some cases by making research costs prohibitively high. To address this issue, leveraging electronic health records (EHRs) for identifying patient cohorts has become an increasingly attractive option. This proposal will investigate and implement standards based approaches for computable phenotype identification from multiple EHRs.",National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms,9381197,R01GM105688,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Architecture', 'Benchmarking', 'Benign Prostatic Hypertrophy', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computerized Medical Record', 'Computing Methodologies', 'Consensus', 'Data', 'Data Element', 'Data Quality', 'Data Reporting', 'Development', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Event', 'Exclusion Criteria', 'Flowcharts', 'Genomics', 'Gold', 'Grant', 'Health', 'Health system', 'Healthcare', 'Healthcare Systems', 'Human', 'Informatics', 'Intuition', 'Knowledge', 'Learning', 'Logic', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Needs Assessment', 'Observational Study', 'Outcome', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Precision Medicine Initiative', 'Process', 'Public Health', 'Public Health Informatics', 'Rare Diseases', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk Factors', 'Running', 'Scientist', 'Services', 'Standardization', 'Structure', 'System', 'Techniques', 'Text', 'Time', 'Translational Research', 'United States National Institutes of Health', 'University Hospitals', 'authority', 'base', 'biobank', 'clinical phenotype', 'cohort', 'comparative effectiveness', 'cost', 'data modeling', 'database query', 'design', 'endophenotype', 'experience', 'inclusion criteria', 'informatics training', 'information model', 'interoperability', 'knowledge base', 'learning strategy', 'meetings', 'portability', 'precision medicine', 'repository', 'syntax', 'tool', 'usability']",NIGMS,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2017,745771,0.045371923383010944
"Automated Problem and Allergy Lists Enrichment Based on High Accuracy Information Extraction from the Electronic Health Record ﻿    DESCRIPTION (provided by applicant): Medical errors are recognized as the cause of numerous deaths, and even if some are difficult to avoid, many are preventable. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information in the electronic health record (EHR). Unfortunately, a substantial proportion of the information available in the EHR is only mentioned in narrative clinical documents. Electronic lists of problems and allergies are available in most EHRs, but they require manual management by their users, to add new problems, modify existing ones, and the removal of the ones that are irrelevant. Consequently, these electronic lists are often incomplete, inaccurate, and out of date. Clinacuity, Inc. proposed a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the EHR of patients suffering from cancer, and established its feasibility. To advance this new system from a prototype to an accurate, adaptable, and robust system, integrated into the commercial EHR system used in our implementation and testing site (Huntsman Cancer Institute and University of Utah Hospital, Salt Lake City, Utah), and ready for commercialization efforts, we will work on the following aims: 1) enhance the NLP system performance, scalability, and quality, 2) develop an advanced visualization interface for local adaptation of the NLP system, and 3) integrate the NLP system with a commercial EHR system. A large and varied reference standard for training and testing the information extraction application will also be developed, a reference standard including a random sample of de-identified clinical narratives from patients treated at the Huntsman Cancer Institute and at the University of Utah Hospital (Salt Lake City, Utah), with problems and allergies annotated by domain experts. Commercial application: The system Clinacuity proposes will not only help healthcare providers maintain complete and timely lists of problems and allergies, providing them with an efficient overview of a patient, but also help healthcare organizations attain meaningful use requirements. The proposed system has potential commercial applications in inpatient and outpatient settings, increasing the efficiency of busy healthcare providers by saving time, and aiding healthcare organizations in demonstrating ""meaningful use"" and obtaining Centers for Medicare & Medicaid Services incentive payments. Clinacuity will further extend the commercial potential of the system and its output, using modular design principles allowing utilization of each module independently, and enhancing its local adaptability for easier deployment. PUBLIC HEALTH RELEVANCE: Medical errors cause numerous deaths, and even if some are difficult to avoid, many could be prevented. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information such as entries in electronic lists of problems and allergies. Such lists are available in most electronic health records, but they require manual management and are often incomplete, inaccurate, and out of date. On the other hand, clinical text reports contain the majority of the patient information, including problems and allergies. The overall goal of this project is to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the electronic health record.",Automated Problem and Allergy Lists Enrichment Based on High Accuracy Information Extraction from the Electronic Health Record,9357564,R42CA180190,"['Adverse drug event', 'Cessation of life', 'Cities', 'Clinical', 'Code', 'Communications Media', 'Complex', 'Development', 'Disease', 'Electronic Health Record', 'Ensure', 'Environment', 'Excision', 'Goals', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'Hospitals', 'Huntsman Cancer Institute at the University of Utah', 'Hybrids', 'Hypersensitivity', 'Imagery', 'Incentives', 'Injury', 'Inpatients', 'Institutes', 'Laboratories', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Medical Errors', 'Medication Errors', 'Methods', 'Modernization', 'Natural Language Processing', 'Outpatients', 'Output', 'Patient Care', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Process', 'Reference Standards', 'Reporting', 'Risk', 'Sampling', 'Savings', 'Secure', 'Site', 'Sodium Chloride', 'Speed', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Universities', 'Utah', 'Work', 'base', 'cancer care', 'commercial application', 'commercialization', 'computerized physician order entry', 'cost', 'design', 'improved', 'payment', 'prevent', 'processing speed', 'prototype', 'public health relevance', 'software development', 'standard measure', 'usability', 'web services']",NCI,"CLINACUITY,INC.",R42,2017,767485,0.06384527246241604
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,9307936,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Databases', 'Disease', 'Drug Exposure', 'Drug Modelings', 'Drug toxicity', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'cost', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'genomic data', 'improved', 'longitudinal dataset', 'novel', 'open source', 'personalized medicine', 'phenotypic data', 'public health relevance', 'rapid growth', 'rare variant', 'response', 'study population', 'success', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2017,600474,0.009438619806228447
"EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia PROJECT SUMMARY/ABSTRACT Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by elevated plasma low-density lipoprotein cholesterol (LDL-C) and a dramatically increased lifetime risk for premature atherosclerotic cardiovascular disease (ASCVD). Available data suggest substantial under treatment of individuals with FH, and it is estimated that <5% of prevalent FH cases in the US are diagnosed and treated. The proposed research will develop electronic health record (EHR)-based strategies to reduce mortality and morbidity from FH. We will develop and validate a phenotyping algorithm for rapid and efficient identification of FH cases thereby enabling EHR-based surveillance of FH. We will deploy the phenotyping algorithm in the population-based setting of Olmsted County, Minnesota, to estimate prevalence and provide hitherto unavailable data on awareness, detection and control of FH. We will develop CDS to help care providers manage FH patients and an FH-specific decision aid to facilitate shared decision making related to lipid-lowering therapy and screening of family members. To accomplish these goals, we will leverage the following resources: a) the electronic phenotyping expertise available in the electronic Medical Records and Genomics (eMERGE) network; b) the Rochester Epidemiology Project (REP), that links medical records of Olmsted County MN residents thereby capturing nearly all health care delivered to residents of the community; and c) expertise in developing and deploying CDS in the EHR and in creating decision aids for disclosing cardiovascular risk and the benefits of lipid-lowering drugs. Our specific aims are: Aim 1. Develop and validate an electronic phenotyping algorithm to rapidly identify FH cases from the EHR. Aim 2. Conduct an e- epidemiology study to obtain hitherto unknown data regarding prevalence, awareness, detection, control of FH in a population-based setting in the US. Aim 3. a) Develop EHR-based tools to help care providers manage FH and facilitate shared decision making and cascade screening and b) assess outcomes after implementation of CDS and decision aid. The proposed research will enable rapid identification of FH in EHRs, provide hitherto unavailable data on the burden of FH in the community, facilitate EHR-based strategies for early detection, increase awareness of FH among care providers, provide guidance for management of FH at point of care and help both patients and providers make informed decisions about drug therapy and screening of family members. These are critical steps for early detection and treatment of FH to reduce the burden of premature ASCVD due to this condition. PROJECT NARRATIVE Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by high cholesterol levels and increased risk of heart attack or sudden cardiac death. The proposed research will develop electronic health record (EHR)-based strategies to prevent adverse outcomes such as heart attack in FH patients. These include methods to rapidly identify FH patients, estimate prevalence of FH and develop clinical decision support to help care providers manage FH patients. The proposed work will have a significant impact on clinical management of FH patients.",EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia,9389934,R01HL135879,"['Academy', 'Address', 'Algorithms', 'Atherosclerosis', 'Awareness', 'Benefits and Risks', 'Caring', 'Cholesterol', 'Clinic', 'Clinical', 'Clinical Management', 'Communities', 'Computerized Medical Record', 'County', 'Data', 'Data Set', 'Decision Aid', 'Decision Making', 'Detection', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Early treatment', 'Electronic Health Record', 'Epidemiology', 'Europe', 'Familial Hypercholesterolemia', 'Family', 'Family history of', 'Family member', 'Genetic screening method', 'Genomics', 'Goals', 'Healthcare', 'Healthcare Systems', 'Hereditary Disease', 'Heritability', 'Individual', 'Institutes', 'LDL Cholesterol Lipoproteins', 'Label', 'Laboratories', 'Learning', 'Link', 'Lipids', 'Low-Density Lipoproteins', 'Manuals', 'Medical Records', 'Medicine', 'Methods', 'Minnesota', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phenotype', 'Plasma', 'Population Study', 'Preclinical Drug Evaluation', 'Predictive Value', 'Prevalence', 'Prevalence Study', 'Primary Prevention', 'Provider', 'Public Health', 'Recommendation', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk', 'Structure', 'Test Result', 'Time', 'Work', 'World Health Organization', 'Xanthomas', 'adverse outcome', 'base', 'cardiovascular risk factor', 'case-based', 'epidemiology study', 'evaluation/testing', 'implementation science', 'improved', 'improved outcome', 'inhibitor/antagonist', 'innovation', 'lifetime risk', 'mortality', 'novel therapeutics', 'point of care', 'population based', 'precision medicine', 'premature', 'prevent', 'programs', 'screening', 'shared decision making', 'sudden cardiac death', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,R01,2017,520081,0.03351939063687697
"Interactive machine learning methods for clinical natural language processing DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3. Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,9132834,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face', 'Goals', 'Grant', 'Human', 'Hybrids', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'learning strategy', 'model building', 'model development', 'novel', 'open source', 'real world application', 'statistics', 'success', 'tool', 'usability']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2016,463405,0.025981778309663785
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,9115996,R01LM011934,"['Accounting', 'Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'novel', 'open source', 'profiles in patients', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2016,387966,0.03795178485971105
"Natural language processing for clinical and translational research DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.",Natural language processing for clinical and translational research,9033918,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2016,562809,0.041840380748110056
"A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications DESCRIPTION (provided by applicant): Electronic Health Records (EHRs) can improve the quality of healthcare delivery in the United States, by providing automated best-practice reminders to clinicians and patients. However such functionality is currently limited to narrow areas of clinical practice, as existing decision support systems can process only structured data, due to lack of a suitable framework and concerns about accuracy and portability. Preliminary work by the PI has shown that rule-based approach can be used to develop broad-domain reminder systems that can utilize free-text in addition to the structured data. The PI has developed prototype systems for cervical and colorectal cancer prevention. These systems consist of rule-based composite models of national guidelines, and rule-based Natural Language Processing (NLP) parsers. The NLP parsers extract the patient variables required for applying the guidelines. However further research is needed to extend the systems and to ensure their accuracy for clinical deployment. In the mentored phase, the PI will collaborate with clinicians to extend and iteratively optimize and validate the systems, and will make them available in open-source so that they can be adapted for deployment at other institutions (aim 1 - K99). In the independent phase, the PI will research methods to facilitate rapid development, deployment and cross- institutional portability of similar systems. Specifically, the PI will develp a hybrid design for the parsers and investigate domain adaptation and active learning methods, for reducing the manual effort for development and adaptation of the NLP parsers (aim 2 - R00). To enable other researchers to reuse the developed methodologies and software resources, a toolkit will be developed that will support the construction and deployment of similar systems (aim 3 - R00). The toolkit will consist of user-friendly tools and templates to replicate the processes engineered in the case studies, and will build on the SHARPn data normalization tooling and other open-source tools. The independent phase will be in collaboration with Intermountain Healthcare. The PI's career goal is to become a scientific leader in clinical informatics with a focus on optimizing clinical decision making. The PI has strong background in clinical medicine and medical informatics, and will receive mentoring from Drs. Hongfang Liu, Christopher Chute, Robert Greenes and Rajeev Chaudhry, who have complimentary areas of expertise. The mentored (K99) phase will be for 2 years at Mayo Clinic Rochester, wherein the PI will undertake courses on decision support and will get mentored training in NLP and health information standards. This will prepare the PI for independent research in R00 phase on portability and tooling. Completion of the proposed work will enable the PI to seek further funding for piloting clinical deployment of the developed systems, measuring their clinical impact, and for scaling the approach to other clinical domains and institutions. The career grant will enable the PI to establish himself as an independent investigator and to make significant contributions towards advancing clinical decision support for improving care delivery. PUBLIC HEALTH RELEVANCE STATEMENT The potential of Electronic Health Records (EHRs) to improve care delivery by providing best-practice reminders is unrealized, because reminder systems currently operate in narrow areas of clinical practice, as they can process only structured data. The proposed framework will enable construction of reminder systems that can encompass broader areas of practice, due to their capability to utilize free-text as well as structured EHR data. This pioneering research directly impacts public health by improving the quality of care through enhanced reminder functionality in the EHRs.",A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications,9187058,R00LM011575,"['Active Learning', 'Address', 'Area', 'Caregivers', 'Caring', 'Case Study', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Collaborations', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Decision Support Systems', 'Development', 'Electronic Health Record', 'Engineering', 'Ensure', 'Fostering', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Care Costs', 'Healthcare', 'Hybrids', 'Institution', 'Language', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Manuals', 'Measures', 'Medical Informatics', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Phase', 'Process', 'Public Health', 'Quality of Care', 'Reminder Systems', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Structure', 'System', 'Text', 'Training', 'United States', 'Validation', 'Work', 'base', 'care delivery', 'career', 'clinical application', 'clinical decision-making', 'clinical practice', 'colorectal cancer prevention', 'colorectal cancer screening', 'design', 'health care delivery', 'improved', 'learning strategy', 'open source', 'portability', 'prevent', 'prototype', 'public health relevance', 'tool', 'user-friendly']",NLM,MASSACHUSETTS GENERAL HOSPITAL,R00,2016,251021,0.0423270050500431
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility.         PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.        ",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9076555,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'System', 'Time', 'Training', 'cohort', 'computer science', 'crowdsourcing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2016,316000,0.015580595706349728
"Automated Detection of Anomalous Accesses to Electronic Health Records DESCRIPTION (provided by applicant): Health information technology (HIT) can lower costs, strengthen productivity, and promote safety. To realize such benefits on a large scale, healthcare organizations (HCOs) are adopting electronic health records (EHRs) to provide various capabilities. Yet, as EHRs and the healthcare workforce grow in diversity, so does their complexity. This is a concern because evidence suggests complex HIT can interrupt care delivery, contribute to medical errors, and expose patient data to privacy breaches. Moreover, such events tend to be discovered only after they transpire en masse, leading to negative media coverage, loss of patients' trust, and sanctions. Federal regulations now enable patients to receive accountings of who accessed their medical records during treatment, payment, and operations related activities. Yet, for patients to make sense of such accountings, they need to be provided with explanations regarding the extent to which accesses are normal in the context of routine HCO activities. We believe that relating specific accesses to patterns of healthcare operations can help explain how medical records are utilized. Unfortunately, many of the aforementioned problems manifest because EHR utilization patterns rarely guide the design and refinement of healthcare management practices. Thus, the overarching objective of our research is to develop novel strategies to automatically learn HCO behavior based on EHR usage. The past several years has witnessed a flurry of activity in this field, but it remains in is infancy and has only scratched the surface of care patterns and the types of anomalies that can be detected. Through this project, we propose to develop anomaly detection methods that integrate the semantics of healthcare operations and allow for the detection of workflows over time. This will enable HCOs and patients to audit in a meaningful way. Moreover, we believe the innovation and dissemination of such data mining strategies will enable HCOs to detect anomalous events that indicate system misuse and patients who require special attention, but also effectively audit business practices and discover inefficient workflows. The specific aims of this project are (1) to develop machine learning approaches, based on intrasession utilization patterns, to streamline EHR interface configuration and detect anomalous sessions, (2) to design a data mining framework, based on intersession EHR access patterns, to characterize HCO departmental interactions in patient treatment and detect anomalous events, and (3) to infer patient management pathways to consolidate redundant processes and detect deviations from anticipated workflows. In support of these goals, we will evaluate, compare, and contrast the workflows and anomalies in the EHR systems of two large medical centers. Additionally, we will ensure that our methods are integrated into an open source software system that can assist HCOs to extract, transform, and load (ETL) access data from EHRs, analyze such data for anomalies, and visualize the results in interfaces that enable review by healthcare administrators and patients. In doing so, we will be able to compare and contrast behavior of the workflows and multiple institutions and develop methods that appropriately generalize across EHR systems. As electronic health record systems, and healthcare organizations, grow in diversity, so do their complexity, which can lead to inefficient documentation, management of patients, and expose patient data to privacy breaches. In this research, we will develop technologies, to be disseminated through an open source software suite, to learn patterns associated with healthcare operations, upon which anomaly detection techniques can be based. The specific goals of this project are to 1) model HCO users' intrasession behaviors when interacting with an EHR, 2) learn interdepartmental relations based on the accesses of common patients, and 3) infer patient management pathways based on the sequence of accesses to a patient's record.",Automated Detection of Anomalous Accesses to Electronic Health Records,9143798,R01LM010207,"['Accounting', 'Administrator', 'Admission activity', 'Adopted', 'Architecture', 'Attention', 'Behavior', 'Businesses', 'Caring', 'Case Manager', 'Case Study', 'Clinical', 'Collaborations', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Security', 'Detection', 'Documentation', 'Electronic Health Record', 'Employee', 'Engineering', 'Ensure', 'Entropy', 'Event', 'Goals', 'Grant', 'Graph', 'Healthcare', 'Healthcare Systems', 'Inpatients', 'Institution', 'Interdepartmental Relations', 'Lead', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Errors', 'Medical Records', 'Medical center', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Movement', 'Neonatal Intensive Care', 'Neonatology', 'Pathway interactions', 'Patient Care', 'Patients', 'Pattern', 'Patterns of Care', 'Phase', 'Practice Management', 'Primary Health Care', 'Privacy', 'Process', 'Productivity', 'Provider', 'Recording of previous events', 'Regulation', 'Research', 'Research Personnel', 'Safety', 'Semantics', 'Surface', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trust', 'Variant', 'Work', 'base', 'care delivery', 'comparative', 'cost', 'data access', 'data mining', 'design', 'follow-up', 'health information technology', 'infancy', 'innovation', 'novel strategies', 'open source', 'operation', 'organizational structure', 'payment', 'programs', 'software systems', 'ward', 'web portal']",NLM,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2016,341475,0.06533299742828878
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics. PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,9134798,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Cost Analysis', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Electronics', 'Elements', 'Ethics', 'Evaluation', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Methylphenidate', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Reading', 'Recommendation', 'Records', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'base', 'biobank', 'clinical care', 'clinically actionable', 'cohort', 'data modeling', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'learning strategy', 'member', 'next generation sequencing', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'response', 'senior faculty', 'skills', 'success', 'support tools', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2016,855289,0.017226561382087276
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics. PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,9247889,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Cost Analysis', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Electronics', 'Elements', 'Ethics', 'Evaluation', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Methylphenidate', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Reading', 'Recommendation', 'Records', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'base', 'biobank', 'clinical care', 'clinically actionable', 'cohort', 'data modeling', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'learning strategy', 'member', 'next generation sequencing', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'response', 'senior faculty', 'skills', 'success', 'support tools', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2016,54784,0.017226561382087276
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics. PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,9358502,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Cost Analysis', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Electronics', 'Elements', 'Ethics', 'Evaluation', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Methylphenidate', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Reading', 'Recommendation', 'Records', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'base', 'biobank', 'clinical care', 'clinically actionable', 'cohort', 'data modeling', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'learning strategy', 'member', 'next generation sequencing', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'response', 'senior faculty', 'skills', 'success', 'support tools', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2016,62400,0.017226561382087276
"Use Frailty Status to Predict Postoperative Outcomes in Elderly Patient Frailty is increasingly recognized as a leading indicator of poor health outcomes, even death, as well as a barometer of how well patients respond to treatment. To truly provide patient-centered care, providers should be aware of each patient's frailty status and incorporate it into clinical decision making. Providers can now offer a number of invasive and aggressive procedures for cardiovascular disease, which involve risk, and can be painful. The treatment intensity need to match the expected patient outcome, yet providers do not have a reliable method to estimate prognosis for frail patients. In the study proposed here, we will use a novel approach that leverages the electronic health record (EHR) in identifying patient frailty status, with the goal of supporting retrospective clinical studies and prospective clinical decision making. Our preliminary studies have demonstrated the availability of frailty-related findings in EHR, the feasibility of extracting frailty findings, and the feasibility of using EHR-extracted frailty for outcome prediction. The specific aims of the project are to 1) Create a frailty ontology building on existing functional status and quality of life measurements; 2) Develop ontology guided, natural language processing (NLP) methods for extracting frailty descriptions and measurements; 3) Develop a model to aggregate NLP-extracted frailty findings to generate a patient-level frailty score; 4) Examine the all-cause mortality and all-cause hospital readmission one year after major cardiac procedures in heart failure patients with different frailty scores and assess the impact of this information on surgical decision making. Frailty is an important, but often overlooked, determinant of health outcomes in older adults. Providers can now offer invasive and aggressive treatments for various conditions, but these interventions involve risk and careful patient selection that balances risk and benefit is imperative. We will develop automated methods to extract frailty information from clinical records and generate an aggregated frailty score, which will enable retrospective analyses of EHR data for risk prediction and support prospective clinical decisional making.",Use Frailty Status to Predict Postoperative Outcomes in Elderly Patient,9354376,R56AG052536,"['Address', 'Affect', 'American', 'Atrial Fibrillation', 'Benefits and Risks', 'Cardiac', 'Cardiac Surgery procedures', 'Cardiovascular Diseases', 'Caregivers', 'Cessation of life', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collection', 'Comorbidity', 'Critical Care', 'Data', 'Data Sources', 'Databases', 'Decision Making', 'Devices', 'Dimensions', 'Disease', 'Documentation', 'Elderly', 'Electronic Health Record', 'Equilibrium', 'Fatigue', 'Goals', 'Health', 'Heart failure', 'Hospitalization', 'Implantable Defibrillators', 'Informatics', 'Intervention', 'Investigation', 'Life', 'Life Expectancy', 'Malnutrition', 'Measurement', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Modeling', 'Natural Language Processing', 'Ontology', 'Operative Surgical Procedures', 'Outcome', 'Pain', 'Patient Care', 'Patient Selection', 'Patient-Centered Care', 'Patient-Focused Outcomes', 'Patients', 'Perioperative', 'Population', 'Postoperative Period', 'Procedures', 'Provider', 'Quality of life', 'Records', 'Research Personnel', 'Risk', 'Technology', 'Veterans', 'base', 'clinical decision-making', 'cohort', 'comparative effectiveness', 'exercise capacity', 'frailty', 'functional status', 'hospital readmission', 'indexing', 'individualized medicine', 'interest', 'mortality', 'novel strategies', 'older patient', 'outcome forecast', 'outcome prediction', 'patient population', 'predictive modeling', 'prospective', 'treatment choice', 'treatment planning', 'trend']",NIA,GEORGE WASHINGTON UNIVERSITY,R56,2016,616197,0.025267316134234146
"Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients Project Summary Epilepsy is one of the leading neurological disorders in the United States, affecting more than 479,000 children and over 2 million adults. Approximately 30% of epileptic patients have poor seizure control despite antiepileptic medications and are potential candidates for neurosurgical intervention. Early identification and referral of children who are potential surgical candidates is complex and while relevant guidelines exist, there is no standard process to efficiently identify those patients meeting criteria for neurosurgical intervention. Given the large corpus of note-based data available in the electronic health record (EHR), it is challenging for providers to efficiently retain and process all the pertinent patient information. Natural Language Processing (NLP) and machine learning techniques have been successfully used to evaluate clinical notes and make recommendations in the research setting. However, NLP techniques are rarely integrated into practice to provide real-time clinical decision support. We developed and retrospectively evaluated a NLP system to help identify those patients who meet neurosurgical criteria and therefore enable surgical consults and evaluations to occur sooner. Knowing clinical decision support can improve outcomes of care, our proposed research will implement NLP into clinical practice and develop a decision support mechanism to improve the time to surgery for eligible patients. The objective of this project is to implement NLP directly into clinical care and determine the most effective decision support mechanism for provider adherence to epilepsy surgical consult recommendations. The long- term goal of this project is to reduce the time to initial surgery evaluation for patients with intractable epilepsy by integrating NLP-classification criteria into clinical practice. This project is one of the first in the field to study the integration of NLP recommendations into clinical care. We will use a human factors engineering framework to design and to analyze two different alerting methodologies for the best-fit for clinical workflow to produce the optimum provider adherence while reducing alert fatigue. Epilepsy progress notes can be classified across hospitals, and if successful, the system will be implemented in additional pediatric institutions around the United States. Project Narrative Epilepsy affects more than 479,000 children; 30% of those are not controlled with medication and may require surgery. We developed a novel Natural Language Processing (NLP) algorithm that can be integrated into neurology practice to detect patients who may be eligible for epilepsy surgical consults. Once implemented, this research can help to drive successful implementations of NLP and identify and use ideal alerting mechanisms in neurological care.",Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients,9222109,R21HS024977,[' '],AHRQ,CINCINNATI CHILDRENS HOSP MED CTR,R21,2016,137233,0.00010923999066842528
"Patient Medical History Representation, Extraction, and Inference from EHR Data DESCRIPTION (provided by applicant): The significance of developing tools for automatically harvesting temporal constraints of clinical events from Electronic Health Records (EHR) cannot be overestimated. Efficient analysis of the temporal aspects in EHR data could boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine.     One big challenge we are facing is to automatically untangle and linearize the temporal constraints of clinical events embedded in highly diverse large-scale EHR data. Barriers to temporal data modeling, normalization, extraction, and reasoning have precluded the efficient use of EHR data sources for event history evaluation and trending analysis: (1) The current federally-supported EHR data normalization tools do not focus on the time aspect of unstructured data yet; (2) Existing time models focus only on structured data with absolute time, lack of supporting reasoning systems, or only offer application-specific partial solutions which cannot be adopted by the complex EHR data; (3) Current temporal information extraction approaches are either difficult to be adopted to EHR data, not scalable, or only offers application-specific partial solution.     This proposed project fills in the current gaps among ontologies, Natural Language Processing (NLP), and EHR-based clinical research for temporal data representation, normalization, extractions, and reasoning. We propose to develop novel approaches for automatic temporal data representation, normalization and reasoning for large, diverse, and heterogeneous EHR data and prepare the integrated data for further analysis. We will build new reasoning and extraction capacities on our TIMER (Temporal Information Modeling, Extracting, and Reasoning) framework to provide an end-to-end, open-source, standard-conforming software package. TIMER will be built on strong prior work by our team. We will develop new features in our CNTRO (Clinical Narrative Temporal Relation Ontology) for semantically defining the time domain and representing temporal data in complex EHR data. On top of the new developed CNTRO semantics, we will implement temporal relation reasoning capacities to automatically normalize temporal expressions, compute and infer temporal relations, and resolve ambiguities. We will leverage existing NLP tools and work on top of these tools to develop new extraction approaches to fill in the current gaps between NLP approaches and ontology-based reasoning approaches. We will adapt the SHARPn EHR data normalization pipeline and cTAKES for extracting and normalizing clinical event mentions from clinical narratives. We will explore an innovative approach for temporal relation extraction and event coreference, and make it work with the TIMER framework. We will evaluate the system using Diabetes Mellitus (DM) and colorectal cancer (CRC) patient cohorts from two insititutions. Each component will be tested separately first followed by an evaluation of the whole framework. Results such as precision, recall, and f-measure will be reported. Project Narrative The significance of developing capabilities for automatically harvesting temporal constraints for clinical events from Electronic Health Records (EHR) cannot be overestimated. A substantial portion of the information in the EHR is historical in nature. Patient medical history can be long, especially in complex patients. The proposed work, by offering an end-to-end open-source framework for automatically extracting, normalizing, and reasoning clinically-important time-relevant information from large-scale EHR data, can boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine; as well as facilitate clinical practice for early disease detection, post-treatment care, and patient-clinician communication.","Patient Medical History Representation, Extraction, and Inference from EHR Data",9115724,R01LM011829,"['Address', 'Adopted', 'Aftercare', 'Archives', 'Automated Annotation', 'Big Data', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Colorectal Cancer', 'Communication', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Data Reporting', 'Data Set', 'Data Sources', 'Databases', 'Decision Support Systems', 'Detection', 'Diabetes Mellitus', 'Disease', 'Disease Progression', 'Electronic Health Record', 'Evaluation', 'Event', 'Goals', 'Gold', 'Harvest', 'Human', 'Institutes', 'Maps', 'Measures', 'Medical History', 'Medical Records', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patient Care', 'Patients', 'Performance', 'Recording of previous events', 'Registries', 'Reporting', 'Resolution', 'Semantics', 'Structure', 'System', 'Testing', 'Time', 'Translational Research', 'Work', 'application programming interface', 'base', 'clinical practice', 'cohort', 'colon cancer patients', 'data modeling', 'data structure', 'information model', 'innovation', 'novel strategies', 'open source', 'personalized medicine', 'tool', 'trend analysis']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2016,335217,0.058406506835919574
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,9109047,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'ethnic difference', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2016,430717,0.0014902703511776856
"Secondary use of EMRs for surgical complication surveillance DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.",Secondary use of EMRs for surgical complication surveillance,9050675,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'abstracting', 'base', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'meetings', 'patient safety', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2016,300000,0.03231005138540235
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing ﻿    DESCRIPTION (provided by applicant): The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potentials, but also equally growing concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potentials for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of the research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data, and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious ad costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for much faster de- identification than manual approaches. Clinacuity, Inc. proposes to develop a new system to automatically de-identify clinical notes found in the EHR, to then improve the availability of clinical text for secondary uses, as well as ameliorate the protection of patient data confidentiality. To establish the merit and feasibility of such a system, we will work on the following objectives: 1) create a reference standard for training and testing the text de-identification application, a reference standard that will include a random sample of clinical narratives with protected health information annotated by domain experts; 2) develop a prototype to automatically de-identify clinical text in near real-time, a prototype that will implement a novel stepwise hybrid approach to maximize sensitivity first (our priority for de-identification), and then filter out false positives to enhance positive predictive value, and also replace PHI with realistic substitutes for improved protection; and 3) test the prototype with the aforementioned reference standard, using a cross-validation approach for training and testing, and also train and test the prototype with a reference standard from another healthcare organization. Commercial application: To strengthen patient information confidentiality protection, the HITECH Act heightened financial penalties incurred for breaches of PHI, even introducing criminal penalties. These new severe consequences for violation of patient information confidentiality render protection requirements even more obvious, and automatic high-accuracy clinical text de-identification, as offered by the system Clinacuity proposes, will strongly help healthcare and clinical research organizations avoid such consequences. This system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will also ease research data sharing, and help healthcare organizations protect patient data confidentiality. PUBLIC HEALTH RELEVANCE: The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical daa becoming available in electronic format, with tremendous potentials, but also equally growing concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfil the potentials for high quality healthcare and effective clinical research. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data, and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes that have been dictated and transcribed or directly typed in, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for much faster de-identification than manual approaches. The overall goal of this project is to develop a new system to automatically de-identify clinical narrative text in he Electronic Health Record, to then improve the availability of clinical text for secondary uses, as well as ameliorate the protection of patient data confidentiality.",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,8982010,R41GM116479,"['Abbreviations', 'Adoption', 'Affect', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Confidentiality of Patient Information', 'Consent', 'Data', 'Direct Costs', 'Electronic Health Record', 'Electronics', 'Enrollment', 'Eponyms', 'Gilbert Disease', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Hybrids', 'Improve Access', 'Incentives', 'Informed Consent', 'Manuals', 'Measurement', 'Measures', 'Methods', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Population', 'Predictive Value', 'Privacy', 'Reference Standards', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrospective Studies', 'Risk', 'Sampling', 'Structure', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'United States National Institutes of Health', 'Validation', 'Vision', 'Work', 'base', 'commercial application', 'common rule', 'data sharing', 'flexibility', 'health care quality', 'improved', 'novel', 'patient privacy', 'payment', 'prototype', 'statistics', 'text searching']",NIGMS,"CLINACUITY,INC.",R41,2016,223924,0.03193110659181811
"Semi-supervised learning with electronic medical records Project Summary/Abstract The implementation of electronic medical record (EMR) systems in routine healthcare has resulted in a rich and inexpensive source of data for translational research. When linked with specimen biobanks, these extensive databases offer a unique opportunity to accelerate the goals of disease genomics as they contain large amounts of detailed clinical and genetic data collected for the purposes of medical care [4; 6; 7; 8; 9]. However, the statistical methods to analyze EMR data are limited and thus the focus of this proposal.  In particular, extracting accurate disease phenotype information is a major challenge impeding EMR-based research [10]. Currently, ICD9 codes are used to conﬁrm presence or absence of a disease in cohorts derived from EMRs. These codes are extremely variable and therefore have a signiﬁcant impact on the statistical power of genetic studies [11; 12]. An alternative approach is to develop a highly accurate algorithm to classify disease status. But due to the laborious medical record review required to obtain validated phenotype information for classiﬁer estimation, phenotypes are only available for a small training set nested in a large cohort. In contrast, predictors of phenotype are available for all observations. To improve accuracy and efﬁciency in model estimation and evaluation, it is therefore of interest to develop semi-supervised learning (SSL) methods that utilize the so- called unlabeled data or observations without conﬁrmed phenotype status in addition to the labeled training set.  Although a great body of literature on SSL exists, nearly all methods concern estimation of classiﬁers or prediction rules when the labeled training set is a simple random sample from the large unlabeled data set [13; 14; 15; 16; 17; 18; 19; 20; 21]. Despite the practical importance of evaluating the prediction performance of an estimated model, no SSL procedures currently exist to improve the estimation of model performance parame- ters. Additionally, the simple random sampling assumption is restrictive and the development of semi-supervised (SS) methods that accommodate more ﬂexible sampling schemes in the context of both model estimation and evaluation is needed.  In this proposal, these limitations are addressed through formulation of an efﬁcient method to estimate various prediction performance measures including the ROC curve within the traditional SS framework of simple random sampling. The stratiﬁed random sampling design in the SS setting is also considered and methods to estimate a classiﬁer and its accuracy are developed. These procedures will be applied to EMR-based studies of bipolar disorder and depression. The success of this work will thus improve efﬁciency in analyzing EMR data and expedite the use of EMRs in clinical and genetic research in neuropsychiatry. Project Narrative  The use of electronic medical records (EMRs) in routine healthcare has generated a rich source of data for in-depth study of disease risk factors. However, EMR data typically consists of a very small number of expensive observations with information on disease status and a large amount of automatically extracted data concerning risk factors such as laboratory results and previous health history. Statistical methods that accommodate this data structure are limited and thus the focus of this proposal.",Semi-supervised learning with electronic medical records,9192096,F31GM119263,"['Address', 'Algorithms', 'Bipolar Depression', 'Bipolar Disorder', 'Caring', 'Clinical', 'Clinical Research', 'Code', 'Computerized Medical Record', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Evaluation', 'Formulation', 'Genes', 'Genetic Research', 'Genetic study', 'Genomics', 'Goals', 'Health', 'Healthcare', 'International', 'Label', 'Laboratories', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Genetics', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Patients', 'Performance', 'Phenotype', 'Procedures', 'ROC Curve', 'Recording of previous events', 'Research', 'Rest', 'Risk Factors', 'Sampling', 'Sampling Biases', 'Scheme', 'Specimen', 'Statistical Methods', 'System', 'Time', 'Training', 'Translational Research', 'Work', 'abstracting', 'base', 'biobank', 'clinical care', 'clinical practice', 'clinically relevant', 'cohort', 'cost effective', 'data structure', 'design', 'disease phenotype', 'disorder risk', 'experience', 'improved', 'interest', 'learning strategy', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'patient population', 'phenome', 'repository', 'success']",NIGMS,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2016,34098,0.02230609016772423
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics ﻿    DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities.         PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.            ",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9094161,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Award', 'Benefits and Risks', 'CCL4 gene', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Intervention', 'Learning', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'interest', 'novel', 'open source', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2016,652772,0.013963725226351946
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9146765,R01LM010090,"['Apache', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Imagery', 'Informatics', 'Information Retrieval', 'International', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Outcomes Research', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Process', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translating', 'Translational Research', 'Trees', 'Vision', 'Work', 'abstracting', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'experience', 'individual patient', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'symptom treatment', 'syntax', 'tool', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2016,643863,0.019477782709814727
"SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs)  DESCRIPTION (provided by applicant): Smart algorithms that effectively analyze patient care data can enhance clinical communication to save lives. In 2000, the Institute of Medicine estimated 98,000 preventable patient deaths occur annually in US hospitals due to miscommunication [1]. Electronic health records (EHRs) were expected to facilitate accurate communication within the care team and provide data to enable automated clinical decision support systems. Unfortunately, miscommunication remains a significant cause of patient deaths [2]. Providers are now required to demonstrate meaningful use of EHR systems to improve quality of care and patient outcomes. Despite this, providers continue to report that EHR systems are cumbersome and interfere with care-team communication. Information entered into an EHR is rarely used by nurses due to the time and difficulty involved in its retrieval. As a result, nurses continue to verbally convey critical patient care information to the next nurse during shift changes. Verbal report or hand-off, where critical patient information is exchanged in only minutes, is inefficient. Worse, it is highly susceptible to communication errors. Broader Impacts: Research: 1) Increase patient safety; 2) Provide preliminary data to expand this work to include physician-physician and physician-RN communication and decision-making in the EHR; 3) Share our discoveries to inform other industries who may also benefit from this technology. Education: 1) Contribute to curriculum enhancements whereby RN students learn strategies to recognize and effectively communicate CEs; 2) As part of curriculum enhancements, include healthcare applications for computer and information science students; 3) Disseminate findings via academic publications, professional meetings, a project website and social media. Mentoring: 1) Mentor budding scientists in the roles of research assistants (RAs) and post doctoral fellows studying nursing and computer science to forge collaborative interdisciplinary relationships for ongoing research; 2) Interest and recruit underrepresented students in STEM and careers in healthcare. RELEVANCE (See instructions): The electronic health record (EHR) has been thought to be a tool to decrease patient deaths related to miscommunication. However, the current EHR falls short of this goal. We propose to develop and test an algorithm that will augment the EHR to more effectively assist nurses in decision-making and communication, ultimately increasing patient safety.",SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs) ,9110716,R01EB020395,"['Agreement', 'Algorithms', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Decision Support Systems', 'Communication', 'Communication Tools', 'Computer Simulation', 'Conscious', 'Data', 'Data Reporting', 'Decision Making', 'Discipline of Nursing', 'Documentation', 'Education', 'Effectiveness', 'Electronic Health Record', 'Electronics', 'Event', 'Fever', 'Goals', 'Hand', 'Health', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Imagery', 'Industry', 'Information Sciences', 'Institute of Medicine (U.S.)', 'Instruction', 'Lead', 'Link', 'Machine Learning', 'Measures', 'Mentors', 'Nurses', 'Outcome', 'Output', 'Pain', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Physicians', 'Postdoctoral Fellow', 'Provider', 'Publications', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Research', 'Research Assistant', 'Research Personnel', 'Retrieval', 'Role', 'STEM career', 'Sampling', 'Scientist', 'Severities', 'Students', 'Symptoms', 'System', 'Technology', 'Testing', 'Time', 'Underrepresented Students', 'Validity and Reliability', 'Work', 'computer science', 'curriculum enhancement', 'falls', 'forging', 'high risk', 'improved', 'interest', 'learning strategy', 'light weight', 'meetings', 'patient safety', 'prototype', 'respiratory', 'social media', 'tool', 'usability', 'web site']",NIBIB,UNIVERSITY OF ARIZONA,R01,2016,245338,0.05588560328830962
"SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs)  DESCRIPTION (provided by applicant): Smart algorithms that effectively analyze patient care data can enhance clinical communication to save lives. In 2000, the Institute of Medicine estimated 98,000 preventable patient deaths occur annually in US hospitals due to miscommunication [1]. Electronic health records (EHRs) were expected to facilitate accurate communication within the care team and provide data to enable automated clinical decision support systems. Unfortunately, miscommunication remains a significant cause of patient deaths [2]. Providers are now required to demonstrate meaningful use of EHR systems to improve quality of care and patient outcomes. Despite this, providers continue to report that EHR systems are cumbersome and interfere with care-team communication. Information entered into an EHR is rarely used by nurses due to the time and difficulty involved in its retrieval. As a result, nurses continue to verbally convey critical patient care information to the next nurse during shift changes. Verbal report or hand-off, where critical patient information is exchanged in only minutes, is inefficient. Worse, it is highly susceptible to communication errors. Broader Impacts: Research: 1) Increase patient safety; 2) Provide preliminary data to expand this work to include physician-physician and physician-RN communication and decision-making in the EHR; 3) Share our discoveries to inform other industries who may also benefit from this technology. Education: 1) Contribute to curriculum enhancements whereby RN students learn strategies to recognize and effectively communicate CEs; 2) As part of curriculum enhancements, include healthcare applications for computer and information science students; 3) Disseminate findings via academic publications, professional meetings, a project website and social media. Mentoring: 1) Mentor budding scientists in the roles of research assistants (RAs) and post doctoral fellows studying nursing and computer science to forge collaborative interdisciplinary relationships for ongoing research; 2) Interest and recruit underrepresented students in STEM and careers in healthcare. RELEVANCE (See instructions): The electronic health record (EHR) has been thought to be a tool to decrease patient deaths related to miscommunication. However, the current EHR falls short of this goal. We propose to develop and test an algorithm that will augment the EHR to more effectively assist nurses in decision-making and communication, ultimately increasing patient safety.",SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs) ,9349080,R01EB020395,"['Agreement', 'Algorithms', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Decision Support Systems', 'Communication', 'Communication Tools', 'Computer Simulation', 'Conscious', 'Data', 'Data Reporting', 'Decision Making', 'Discipline of Nursing', 'Documentation', 'Education', 'Effectiveness', 'Electronic Health Record', 'Electronics', 'Event', 'Fever', 'Goals', 'Hand', 'Health', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Imagery', 'Industry', 'Information Sciences', 'Institute of Medicine (U.S.)', 'Instruction', 'Lead', 'Link', 'Machine Learning', 'Measures', 'Mentors', 'Nurses', 'Outcome', 'Output', 'Pain', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Physicians', 'Postdoctoral Fellow', 'Provider', 'Publications', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Research', 'Research Assistant', 'Research Personnel', 'Retrieval', 'Role', 'STEM career', 'Sampling', 'Scientist', 'Severities', 'Students', 'Symptoms', 'System', 'Technology', 'Testing', 'Time', 'Underrepresented Students', 'Validity and Reliability', 'Work', 'computer science', 'curriculum enhancement', 'falls', 'forging', 'high risk', 'improved', 'interest', 'learning strategy', 'light weight', 'meetings', 'patient safety', 'prototype', 'respiratory', 'social media', 'tool', 'usability', 'web site']",NIBIB,UNIVERSITY OF ARIZONA,R01,2016,25000,0.05588560328830962
"Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes ﻿    DESCRIPTION (provided by applicant): The applicant and principal investigator (PI) is a board-certified cardiologist, Assistant Professor of Medicine and Senior Associate Consultant in the Mayo Clinic Cardiovascular Division. [The long-term goal of the PI is to become an independent clinician-investigator in translational informatics focused on the development, validation and deployment of electronic tools to the point-of-care to improve outcomes for patients with cardiovascular disease. The PI intends to leverage the EHR to conduct population-based studies using electronic algorithms that include NLP and by application of state-of-the-art informatics approaches to deliver the knowledge at the point-of-care in support of patient-centered decision-making for patients with peripheral arterial disease (PAD). The electronic tools developed will be portable to different institutions and other health conditions. The PI will be uniquely positioned as a cardiologist within a large academic health system working with novel NLP techniques at the interface of the EHR and CDS to rapidly translate knowledge acquired in community-based epidemiologic investigations to patient care.] The PI has proposed a research career development plan to acquire expertise in translational informatics including electronic phenotyping, and automated calculation and visual display of prognostic scores using a novel application integrated with clinical decision support (CDS). The PI will complete comprehensive coursework in informatics science and gain additional patient-oriented and epidemiologic research experience. The primary mentor will be Iftikhar Kullo, M.D., cardiologist and PI of the Mayo Clinic electronic MEdical Records and GEnomics (eMERGE) network and a leader in the development and application of electronic phenotyping for genomic studies of PAD. The mentoring team also includes Dr. Christopher Chute, M.D., Head of Medical Informatics at Mayo Clinic, an expert in informatics and epidemiology, and co-PI of the Mayo eMERGE grant and co-investigator of the Rochester Epidemiology Project; Hongfang Liu, Ph.D., an expert in natural language processing (NLP) and its application for EHR-based phenotyping; and Kent Bailey, Ph.D., senior biostatistician with expertise in development of prognostic risk scores. The specific aims of the proposal are: Aim 1 - Apply phenotyping algorithms that include NLP of clinical notes to [a] identify PAD cases and controls without PAD in the community from 1998-2011; and [b] ascertain adverse cardiovascular and limb outcomes through December 2013. Aim 2 - Create multivariable prognostic risk models and scores for adverse outcomes in PAD cases from the community. We will estimate the relative risk of death in PAD patients compared to controls without PAD. Aim 3 - Develop and evaluate usefulness of a novel electronic application within the Mayo EHR to retrieve relevant data elements, calculate, and display individualized prognostic scores. On completion of the proposed investigations the deliverables will be new knowledge and an e-health prognostication tool for PAD patients to be disseminated to stakeholders including clinicians, patients, and researchers portable to other institutions and other cardiovascular diseases. PUBLIC HEALTH RELEVANCE: The applicant, a board certified cardiovascular specialist, proposes to acquire training in translational informatics science which will enable application of novel electronic algorithms to electronic health records to more efficiently conduct community-based studies of peripheral arterial disease, a prevalent but undertreated health condition. These studies will concurrently develop novel electronic tools which will inform patients and providers of risk, facilitate shared-decision making at the point-of-care, and promote compliance to guideline recommended strategies. These tools will be portable to other institutions and to other cardiovascular diseases.",Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes,9031137,K01HL124045,"['Address', 'Adverse event', 'Algorithms', 'Amputation', 'Atherosclerosis', 'Atrial Fibrillation', 'Attention', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Investigator', 'Communities', 'Computerized Medical Record', 'Coronary heart disease', 'Data', 'Data Element', 'Decision Making', 'Dependence', 'Development', 'Development Plans', 'Diagnosis', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Electronic Health Record', 'Electronics', 'Epidemiologic Studies', 'Epidemiology', 'Genomics', 'Goals', 'Government', 'Grant', 'Guidelines', 'Head', 'Health', 'Health system', 'Healthcare', 'Heart failure', 'High Prevalence', 'Image', 'Informatics', 'Institution', 'Investigation', 'Ischemia', 'Knowledge', 'Laboratories', 'Limb structure', 'Link', 'Manuals', 'Medical Informatics', 'Medicine', 'Mentors', 'Methodology', 'Mining', 'Modeling', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Peripheral arterial disease', 'Pharmaceutical Preparations', 'Phenotype', 'Positioning Attribute', 'Principal Investigator', 'Provider', 'Relative Risks', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Safety', 'Science', 'Site', 'Specialist', 'Stratification', 'Stroke', 'System', 'Techniques', 'Time', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Visual', 'Work', 'adverse outcome', 'base', 'career development', 'case control', 'clinical decision-making', 'cost', 'demographics', 'didactic education', 'eHealth', 'experience', 'health information technology', 'high risk', 'improved', 'improved outcome', 'investigator training', 'mortality', 'multidisciplinary', 'novel', 'patient oriented', 'physical symptom', 'point of care', 'population based', 'predictive modeling', 'professor', 'prognostic', 'repository', 'research and development', 'shared decision making', 'skills', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,K01,2016,170856,0.051582619427197725
"Development and Evaluation of a Learning Electronic Medical Record System ﻿    DESCRIPTION (provided by applicant):  The goal of this project to develop and evaluate a learning electronic medical records (L-EMR) system that draws a physician's attention to the right data, at the right time. It learns how to do so by analyzing patterns of patient data access f many physicians in many past cases in the EMR, and learning which EMR data to highlight that are relevant for making clinical decisions in a given patient.      The hypothesis underlying this research is that the L-EMR system will have sufficiently high precision and recall in highlighting relevant data, decrease the average time to assess an intensive care unit (ICU) patient case, and be judged by critical care medicine (CCM) physicians to be clinically useful.    The first aim of this project is develop a highly-usable L-EMR user interface. The L-EMR user interface will include zoomable time-series displays of lab-results, med-orders, and vital signs. Usability studies of the L-EMR user interface will guide revisions and enhancements.      The second aim of the project is to train statistical models that can be applied to a patient case to predict relevant lab-results, med-orders, and vital signs. We will enlist CCM physicians to review a set of retrospective ICU patient cases on a focused set of clinical conditions. Participants will review these cases as if they were active patients, identifying relevant lab- results, med-orders, and vital signs. We will train and evaluate statistical models to predict relevant data, and identify the best performing algorithm to include in the L-EMR system.      The third aim of the project is to evaluate the L-EMR system. We will recruit CCM physicians to evaluate an L-EMR system based on user interfaces from Aim 1 and statistical models trained using the best performing algorithm in Aim 2 to highlight relevant data items. We will measure the precision and recall of the data-highlighting functionality for assessing patient cases and making clinical decisions (e.g., lab and medication orders), the time required to assess cases with and without the highlighting, and physicians' assessments of the strengths and weaknesses of the L-EMR system.    If the results of these experiments are positive, as anticipated, this project will introduce a computational method that has significant potential to improve future EMR systems and enhance patient care. Narrative The purpose of this research is to develop and evaluate a learning electronic medical records (EMR) system that draws a physician's attention to the right data, at the right time. The system works by analyzing patterns of EMR usage of physicians, and learning which EMR data to highlight that are relevant in a given patient. The main idea underlying the approach is that patterns of past EMR usage patterns can be exploited to selectively highlight clinically useful patient data.",Development and Evaluation of a Learning Electronic Medical Record System,9144440,R01LM012095,"['Address', 'Adult', 'Algorithms', 'American', 'Attention', 'Bayesian Modeling', 'Blood', 'Caring', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Computerized Medical Record', 'Computing Methodologies', 'Critical Care', 'Critical Illness', 'Data', 'Data Display', 'Data Set', 'Development', 'Educational workshop', 'Evaluation', 'Face', 'Future', 'Gastrointestinal Hemorrhage', 'Goals', 'Healthcare Systems', 'Heart Rate', 'Hemoglobin', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Intensive Care Units', 'Intravenous', 'Laboratories', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Physiological', 'Provider', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Series', 'Statistical Models', 'System', 'Test Result', 'Time', 'Training', 'Work', 'base', 'clinical decision-making', 'computer human interaction', 'data access', 'design', 'follow-up', 'improved', 'prototype', 'research clinical testing', 'research study', 'stem', 'trend', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2016,424376,0.038107506533143216
"Data-Mining Clinical Decision Support from Electronic Health Records ﻿    DESCRIPTION (provided by applicant)    Data-Mining Clinical Decision Support from Electronic Health Records Public Health Motivation: National healthcare quality is compromised by undesirable variability, reflected in different locales having anywhere from 20-80% compliance with evidence-based guidelines. Much of this is due to uncertainty, with half of clinical practice guidelines lacking adequate evidence to confirm their efficacy. This is unsurprising when clinical trials cost >$15 million to answer individual clinical questions. The result is medical practice routinely driven by individual opinio and anecdotal experience. While Big Data has revolutionized how society processes internet scale information, the status quo in clinical decision making remains the manual interpretation of literature and isolated decision aids. The adoption of electronic health records (EHR) creates a new opportunity to answer a ""grand challenge in clinical decision support (CDS)."" In a learning health system, we could automatically adapt knowledge from the collective expertise embedded in the EHR practices of real clinicians and close the loop by disseminating that knowledge back as executable decision support. Candidate Goals and Objectives: The unifying goal of this BD2K K01 proposal is the mentored career development of Jonathan H. Chen, MD, PhD. This proposal will accelerate his transition into an independent physician scientist, towards his long-term goals to produce Big Data technologies that answer such grand challenges in clinical decision support. His near-term objective is developing methods to translate EHR data into useful knowledge in the form of patient- specific, point-of-care clinical order recommendations for acute medical hospitalizations. His doctoral background in computer science gives him the technical capability to achieve these objectives, while his medical training will ensure clinically meaningful results. His preliminary work to build an order recommender, analogous to commercial product recommenders, demonstrates the proposal's overall feasibility. Institutional Environment and Career Development: The research facilities and training opportunities at Stanford University provide the ideal environment to achieve these objectives, with established and growing Centers for Biomedical Informatics Research, the Biomedical Data Science Initiative, and the first Clinical Informatics Fellowship accredited in the nation. Prof. Russ Altman, Director of the Biomedical Informatics Training Program, will lead a collaborative team of mentors with expertise in clinical decision support (Mary Goldstein), implementation science (Steven Asch), data-mining electronic health records (Nigam Shah), statistical learning algorithms (Lester Mackey), and healthcare statistics (Michael Baiocchi). Combined with respective didactic training, this mentorship will enable Dr. Chen to achieve his objectives through a series of research aims. Research Aims: The overriding hypothesis of the proposal is that clinical knowledge reflected in clinical order patterns from historical EHR data can improve medical decision making when adapted into functional clinical decision support. The specific aims each address components of this concept, as they seek to: (1) Develop the algorithms to learn clinical order patterns from historical EHR data, building on a preliminary recommender system; (2) Assess how underlying clinician proficiency affects the quality of those learned clinical order patterns through observational data inference against external standards; and (3) Determine the impact of automatically learned clinical decision support (CDS) on (simulated) clinical workflows through a randomized controlled crossover trial of human-computer interfaces with real clinicians. Expected Results and General Significance: By the completion of the proposed work, Dr. Chen will answer the grand challenge in clinical decision support (CDS) by automating much of the CDS production process, and have direct translational impact with a prototype system. This will advance the field with new paradigms of generating and disseminating clinical knowledge, which can then improve the consistency and quality of healthcare delivery. Additional benefits will include methods to identify and monitor areas of high practice variability for targeted optimization and improve predictive models that inform precision medicine. With this applied research experience and career development, Dr. Chen can compete for R01 funding and become an independent physician scientist developing Big Data approaches to solve national healthcare problems in clinical decision making. PUBLIC HEALTH RELEVANCE    National healthcare quality is compromised by undesirable practice variability and medical uncertainty, with most medical practice routinely driven by individual opinions and anecdotal experience. With methods analogous to commercial product recommender systems, the proposed project will automatically learn patterns in raw clinical transaction data to capture the undocumented knowledge of real-world clinicians, and close the loop in a learning health system by disseminating that knowledge back as clinical decision support to improve patient care.",Data-Mining Clinical Decision Support from Electronic Health Records,9147612,K01ES026837,"['Achievement', 'Acute', 'Address', 'Admission activity', 'Adoption', 'Affect', 'Algorithms', 'Applied Research', 'Area', 'Back', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Caring', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Practice Guideline', 'Clinical Trials', 'Cross-Over Trials', 'Crowding', 'Data', 'Data Science', 'Decision Aid', 'Decision Making', 'Diagnosis', 'Doctor of Philosophy', 'Educational process of instructing', 'Electronic Health Record', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Failure', 'Fellowship', 'Funding', 'Future', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hospitalization', 'Hospitals', 'Image', 'Individual', 'Institute of Medicine (U.S.)', 'Internet', 'Investments', 'Knowledge', 'Lead', 'Learning', 'Literature', 'Locales', 'Machine Learning', 'Manuals', 'Medical', 'Medical Residency', 'Mentors', 'Mentorship', 'Methods', 'Monitor', 'Motivation', 'Noise', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Production', 'Public Health', 'Quality of Care', 'Randomized', 'Recommendation', 'Research', 'Research Training', 'Scientist', 'Series', 'Services', 'Societies', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Training', 'Training Programs', 'Translating', 'Uncertainty', 'Universities', 'User-Computer Interface', 'Validation', 'Weight', 'Work', 'base', 'biomedical informatics', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'cost', 'data mining', 'design', 'evidence based guidelines', 'experience', 'health care delivery', 'health care quality', 'implementation science', 'improved', 'informatics training', 'point of care', 'precision medicine', 'predictive modeling', 'prototype', 'research facility', 'statistics', 'support tools', 'training opportunity']",NIEHS,STANFORD UNIVERSITY,K01,2016,178606,0.025559366617838974
"Encoding and Processing Patient Allergy Information in EHRs DESCRIPTION (provided by applicant): Allergies affect one in five Americans and are the 5th leading chronic disease in the U.S. Each year, allergies account for more than 17 million outpatient office visits. Although documenting and exchanging allergy information in electronic health records (EHRs) is becoming increasingly important, we still face multiple challenges. These include: lack of well-adopted standard terminologies for representing allergies, frequent entry of allergy information as free-text, and no existing process for reconciling allergy information. In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) conduct analyses on standard terminologies and a large allergy repository to build a comprehensive knowledge base for representing allergy information; 2) design, develop and evaluate a natural language processing (NLP) module for extracting and encoding free-text allergy information and integrate it with an existing NLP system; 3) measure the feasibility and efficiency of the proposed NLP system for the new process of allergy reconciliation; and 4) distribute our methods and tool, so they are widely available to other researchers and healthcare institutions for non-commercial use. PUBLIC HEALTH RELEVANCE: Managing allergy information within the electronic health record (EHR) is vital to ensuring patient safety. The goal of this study is to propose a comprehensive solution to assess existing terminology standards and knowledge bases for representing allergy information, develop and evaluate a natural language processing (NLP) system for extracting and encoding allergy information from free-text clinical documents, and finally measure the feasibility of using NLP output to facilitate the allergy reconciliation proces.",Encoding and Processing Patient Allergy Information in EHRs,9142280,R01HS022728,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,491053,0.010627888304786772
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9146893,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,249994,0.004696198921118965
"NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department Summary Timely identification of relevant or “need to know” clinical information about a patient’s history in the acute care setting can be critical for patient safety and medical decision-making. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. Currently, it is estimated that over 50% of the EHR is free-text. EHR search tools today are often inefficient, simplistic, and unable to rank or evoke the relevance of information for a particular problem or complaint. This is compounded by the fact that EHRs are amassing clinical information at an exponential rate. While the benefits of having a wealth of information at a provider’s fingertips seem obvious, the time and energy cost of culling through enormous amounts of data creates new issues of decreased efficiency and information overload for providers seeking to identify the most pertinent and relevant information about their patients. In the emergency department, where patients can present with life threatening conditions, timely unlocking of clinically relevant information for a patient’s problem or complaint at the point of care can be critical to medical decision-making and patient safety. In this study, we plan to address this challenge through the development of a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information based on the patients presenting complaint. We will accomplish this through the following specific aims: 1) identify and define complaint- specific information elements within a patient’s history and 2) develop and test an NLP-based information retrieval tool. Narrative Patient safety hinges on having right information about the right patient and the right time. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. The purpose of this study is to develop and evaluate a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information from EHRs that providers rely upon to make medical decisions for their patients. This study comes at an important time where data in the EHRs is increasing at an exponential rate, creating a new problem for clinicians, that of finding all the relevant information for patient’s particular problem. This is particularly true in the emergency department setting where providers have limited if any prior relationship and often have to make quick decisions for patients with life threatening conditions. This search tool will provide a snapshot of clinically relevant information that the providers can view alongside the structured information already in the EHR. We believe this approach has the potential to increase clinician efficiency, decrease healthcare costs by avoiding duplicate diagnostic tests, and provide clinicians with the tools they need to make well-informed medical decisions, thereby improving patient safety and reducing suffering.",NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department,9245525,R21HS024541,[' '],AHRQ,UNIVERSITY OF COLORADO DENVER,R21,2016,148922,0.03705509281247727
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,9144757,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2016,249135,0.016391971940315025
"Automated Problem and Allergy Lists Enrichment Based on High Accuracy Information Extraction from the Electronic Health Record ﻿    DESCRIPTION (provided by applicant): Medical errors are recognized as the cause of numerous deaths, and even if some are difficult to avoid, many are preventable. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information in the electronic health record (EHR). Unfortunately, a substantial proportion of the information available in the EHR is only mentioned in narrative clinical documents. Electronic lists of problems and allergies are available in most EHRs, but they require manual management by their users, to add new problems, modify existing ones, and the removal of the ones that are irrelevant. Consequently, these electronic lists are often incomplete, inaccurate, and out of date. Clinacuity, Inc. proposed a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the EHR of patients suffering from cancer, and established its feasibility. To advance this new system from a prototype to an accurate, adaptable, and robust system, integrated into the commercial EHR system used in our implementation and testing site (Huntsman Cancer Institute and University of Utah Hospital, Salt Lake City, Utah), and ready for commercialization efforts, we will work on the following aims: 1) enhance the NLP system performance, scalability, and quality, 2) develop an advanced visualization interface for local adaptation of the NLP system, and 3) integrate the NLP system with a commercial EHR system. A large and varied reference standard for training and testing the information extraction application will also be developed, a reference standard including a random sample of de-identified clinical narratives from patients treated at the Huntsman Cancer Institute and at the University of Utah Hospital (Salt Lake City, Utah), with problems and allergies annotated by domain experts. Commercial application: The system Clinacuity proposes will not only help healthcare providers maintain complete and timely lists of problems and allergies, providing them with an efficient overview of a patient, but also help healthcare organizations attain meaningful use requirements. The proposed system has potential commercial applications in inpatient and outpatient settings, increasing the efficiency of busy healthcare providers by saving time, and aiding healthcare organizations in demonstrating ""meaningful use"" and obtaining Centers for Medicare & Medicaid Services incentive payments. Clinacuity will further extend the commercial potential of the system and its output, using modular design principles allowing utilization of each module independently, and enhancing its local adaptability for easier deployment.         PUBLIC HEALTH RELEVANCE: Medical errors cause numerous deaths, and even if some are difficult to avoid, many could be prevented. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information such as entries in electronic lists of problems and allergies. Such lists are available in most electronic health records, but they require manual management and are often incomplete, inaccurate, and out of date. On the other hand, clinical text reports contain the majority of the patient information, including problems and allergies. The overall goal of this project is to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the electronic health record.           ",Automated Problem and Allergy Lists Enrichment Based on High Accuracy Information Extraction from the Electronic Health Record,9138574,R42CA180190,"['Adverse drug event', 'Cancer Patient', 'Caring', 'Cessation of life', 'Cities', 'Clinical', 'Code', 'Communications Media', 'Complex', 'Development', 'Disease', 'Electronic Health Record', 'Electronics', 'Ensure', 'Environment', 'Excision', 'Goals', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'Hospitals', 'Huntsman Cancer Institute at the University of Utah', 'Hybrids', 'Hypersensitivity', 'Imagery', 'Incentives', 'Injury', 'Inpatients', 'Institutes', 'Laboratories', 'Malignant Neoplasms', 'Manuals', 'Marketing', 'Medical', 'Medical Errors', 'Medication Errors', 'Methods', 'Natural Language Processing', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Process', 'Reference Standards', 'Reporting', 'Risk', 'Sampling', 'Secure', 'Site', 'Sodium Chloride', 'Speed', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Universities', 'Utah', 'Work', 'base', 'commercial application', 'commercialization', 'computerized physician order entry', 'design', 'improved', 'payment', 'prevent', 'processing speed', 'prototype', 'public health relevance', 'software development', 'standard measure', 'usability', 'web services']",NCI,"CLINACUITY,INC.",R42,2016,684880,0.06384527246241604
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,9068953,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Health', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'genomic data', 'improved', 'large-scale database', 'novel', 'open source', 'personalized medicine', 'rapid growth', 'rare variant', 'response', 'study population', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2016,536892,0.009438619806228447
"Interactive machine learning methods for clinical natural language processing DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3. Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,8936515,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face', 'Goals', 'Grant', 'Human', 'Hybrids', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Solutions', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'model building', 'model development', 'novel', 'open source', 'statistics', 'success', 'tool', 'usability']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2015,448348,0.025981778309663785
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,8928647,R01LM011934,"['Accounting', 'Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'novel', 'open source', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2015,376327,0.03795178485971105
"A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications DESCRIPTION (provided by applicant): Electronic Health Records (EHRs) can improve the quality of healthcare delivery in the United States, by providing automated best-practice reminders to clinicians and patients. However such functionality is currently limited to narrow areas of clinical practice, as existing decision support systems can process only structured data, due to lack of a suitable framework and concerns about accuracy and portability. Preliminary work by the PI has shown that rule-based approach can be used to develop broad-domain reminder systems that can utilize free-text in addition to the structured data. The PI has developed prototype systems for cervical and colorectal cancer prevention. These systems consist of rule-based composite models of national guidelines, and rule-based Natural Language Processing (NLP) parsers. The NLP parsers extract the patient variables required for applying the guidelines. However further research is needed to extend the systems and to ensure their accuracy for clinical deployment. In the mentored phase, the PI will collaborate with clinicians to extend and iteratively optimize and validate the systems, and will make them available in open-source so that they can be adapted for deployment at other institutions (aim 1 - K99). In the independent phase, the PI will research methods to facilitate rapid development, deployment and cross- institutional portability of similar systems. Specifically, the PI will develp a hybrid design for the parsers and investigate domain adaptation and active learning methods, for reducing the manual effort for development and adaptation of the NLP parsers (aim 2 - R00). To enable other researchers to reuse the developed methodologies and software resources, a toolkit will be developed that will support the construction and deployment of similar systems (aim 3 - R00). The toolkit will consist of user-friendly tools and templates to replicate the processes engineered in the case studies, and will build on the SHARPn data normalization tooling and other open-source tools. The independent phase will be in collaboration with Intermountain Healthcare. The PI's career goal is to become a scientific leader in clinical informatics with a focus on optimizing clinical decision making. The PI has strong background in clinical medicine and medical informatics, and will receive mentoring from Drs. Hongfang Liu, Christopher Chute, Robert Greenes and Rajeev Chaudhry, who have complimentary areas of expertise. The mentored (K99) phase will be for 2 years at Mayo Clinic Rochester, wherein the PI will undertake courses on decision support and will get mentored training in NLP and health information standards. This will prepare the PI for independent research in R00 phase on portability and tooling. Completion of the proposed work will enable the PI to seek further funding for piloting clinical deployment of the developed systems, measuring their clinical impact, and for scaling the approach to other clinical domains and institutions. The career grant will enable the PI to establish himself as an independent investigator and to make significant contributions towards advancing clinical decision support for improving care delivery. PUBLIC HEALTH RELEVANCE STATEMENT The potential of Electronic Health Records (EHRs) to improve care delivery by providing best-practice reminders is unrealized, because reminder systems currently operate in narrow areas of clinical practice, as they can process only structured data. The proposed framework will enable construction of reminder systems that can encompass broader areas of practice, due to their capability to utilize free-text as well as structured EHR data. This pioneering research directly impacts public health by improving the quality of care through enhanced reminder functionality in the EHRs.",A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications,8978947,K99LM011575,"['Active Learning', 'Address', 'Area', 'Caregivers', 'Caring', 'Case Study', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Collaborations', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Decision Support Systems', 'Development', 'Electronic Health Record', 'Engineering', 'Ensure', 'Fostering', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Care Costs', 'Healthcare', 'Hybrids', 'Institution', 'Language', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Manuals', 'Measures', 'Medical Informatics', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Phase', 'Process', 'Public Health', 'Quality of Care', 'Reminder Systems', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Solutions', 'Structure', 'System', 'Text', 'Training', 'United States', 'Validation', 'Work', 'base', 'care delivery', 'career', 'clinical application', 'clinical decision-making', 'clinical practice', 'colorectal cancer prevention', 'colorectal cancer screening', 'design', 'health care delivery', 'improved', 'open source', 'portability', 'prevent', 'prototype', 'public health relevance', 'tool', 'user-friendly']",NLM,MASSACHUSETTS GENERAL HOSPITAL,K99,2015,83160,0.0423270050500431
"Natural language processing for clinical and translational research DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.",Natural language processing for clinical and translational research,8826771,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2015,571551,0.041840380748110056
"IGF::OT::IGF EXPANDING SEER TO INCLUDE MOLECULAR PROFILING IN NON-SMALL CELL LUNG CANCER (NSCLC) This study seeks to leverage electronic pathology (ePath) reports through NLP and machine learning methods to automate the annotation of NSCLC lung cases with results from EGFR and ALK gene mutation testing.  Objectives:  1)	Develop and implement machine-learned predictive NLP models to automatically process ePath reports to ascertain the use of and reported results of EGFR and ALK testing in stage IV non-squamous NSCLC cases.   2)	Conduct a multiphase validation study of the NLP algorithms initially involving cases included in the Kentucky SEER registry, and posteriorly validating the algorithms for cases in the Seattle_Puget Sound SEER registry. 3)	Develop and evaluate an open source, distributable software implementation of the NLP algorithms, an accompanying application programming interface (API), and documentation that can be integrated into SEER*DMS and other registry software applications. n/a",IGF::OT::IGF EXPANDING SEER TO INCLUDE MOLECULAR PROFILING IN NON-SMALL CELL LUNG CANCER (NSCLC),9161889,61201300013I,"['ALK gene', 'Algorithms', 'Automated Annotation', 'Computer software', 'Documentation', 'EGFR gene', 'Electronics', 'Epidermal Growth Factor Receptor', 'Gene Mutation', 'Kentucky', 'Lung', 'Machine Learning', 'Methods', 'Modeling', 'Molecular Profiling', 'Non-Small-Cell Lung Carcinoma', 'Pathology Report', 'Process', 'Registries', 'Reporting', 'Staging', 'Testing', 'c-erbB-1 Proto-Oncogenes', 'open source', 'programs', 'sound', 'validation studies']",NCI,UNIVERSITY OF KENTUCKY,N01,2015,87813,-0.04194369884728455
"Automated Detection of Anomalous Accesses to Electronic Health Records DESCRIPTION (provided by applicant): Health information technology (HIT) can lower costs, strengthen productivity, and promote safety. To realize such benefits on a large scale, healthcare organizations (HCOs) are adopting electronic health records (EHRs) to provide various capabilities. Yet, as EHRs and the healthcare workforce grow in diversity, so does their complexity. This is a concern because evidence suggests complex HIT can interrupt care delivery, contribute to medical errors, and expose patient data to privacy breaches. Moreover, such events tend to be discovered only after they transpire en masse, leading to negative media coverage, loss of patients' trust, and sanctions. Federal regulations now enable patients to receive accountings of who accessed their medical records during treatment, payment, and operations related activities. Yet, for patients to make sense of such accountings, they need to be provided with explanations regarding the extent to which accesses are normal in the context of routine HCO activities. We believe that relating specific accesses to patterns of healthcare operations can help explain how medical records are utilized. Unfortunately, many of the aforementioned problems manifest because EHR utilization patterns rarely guide the design and refinement of healthcare management practices. Thus, the overarching objective of our research is to develop novel strategies to automatically learn HCO behavior based on EHR usage. The past several years has witnessed a flurry of activity in this field, but it remains in is infancy and has only scratched the surface of care patterns and the types of anomalies that can be detected. Through this project, we propose to develop anomaly detection methods that integrate the semantics of healthcare operations and allow for the detection of workflows over time. This will enable HCOs and patients to audit in a meaningful way. Moreover, we believe the innovation and dissemination of such data mining strategies will enable HCOs to detect anomalous events that indicate system misuse and patients who require special attention, but also effectively audit business practices and discover inefficient workflows. The specific aims of this project are (1) to develop machine learning approaches, based on intrasession utilization patterns, to streamline EHR interface configuration and detect anomalous sessions, (2) to design a data mining framework, based on intersession EHR access patterns, to characterize HCO departmental interactions in patient treatment and detect anomalous events, and (3) to infer patient management pathways to consolidate redundant processes and detect deviations from anticipated workflows. In support of these goals, we will evaluate, compare, and contrast the workflows and anomalies in the EHR systems of two large medical centers. Additionally, we will ensure that our methods are integrated into an open source software system that can assist HCOs to extract, transform, and load (ETL) access data from EHRs, analyze such data for anomalies, and visualize the results in interfaces that enable review by healthcare administrators and patients. In doing so, we will be able to compare and contrast behavior of the workflows and multiple institutions and develop methods that appropriately generalize across EHR systems. As electronic health record systems, and healthcare organizations, grow in diversity, so do their complexity, which can lead to inefficient documentation, management of patients, and expose patient data to privacy breaches. In this research, we will develop technologies, to be disseminated through an open source software suite, to learn patterns associated with healthcare operations, upon which anomaly detection techniques can be based. The specific goals of this project are to 1) model HCO users' intrasession behaviors when interacting with an EHR, 2) learn interdepartmental relations based on the accesses of common patients, and 3) infer patient management pathways based on the sequence of accesses to a patient's record.",Automated Detection of Anomalous Accesses to Electronic Health Records,8882547,R01LM010207,"['Accounting', 'Administrator', 'Admission activity', 'Adopted', 'Architecture', 'Attention', 'Behavior', 'Businesses', 'Caring', 'Case Manager', 'Case Study', 'Clinical', 'Collaborations', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Security', 'Detection', 'Documentation', 'Electronic Health Record', 'Employee', 'Engineering', 'Ensure', 'Entropy', 'Event', 'Goals', 'Grant', 'Graph', 'Healthcare', 'Healthcare Systems', 'Inpatients', 'Institution', 'Interdepartmental Relations', 'Internet', 'Lead', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Errors', 'Medical Records', 'Medical center', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Movement', 'Neonatal Intensive Care', 'Neonatology', 'Pathway interactions', 'Patient Care', 'Patients', 'Pattern', 'Patterns of Care', 'Phase', 'Practice Management', 'Primary Health Care', 'Privacy', 'Process', 'Productivity', 'Provider', 'Recording of previous events', 'Regulation', 'Research', 'Research Personnel', 'Safety', 'Semantics', 'Surface', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trust', 'Variant', 'Work', 'base', 'care delivery', 'comparative', 'cost', 'data mining', 'design', 'follow-up', 'health information technology', 'infancy', 'innovation', 'novel strategies', 'open source', 'operation', 'organizational structure', 'payment', 'programs', 'software systems', 'ward']",NLM,VANDERBILT UNIVERSITY,R01,2015,80,0.06533299742828878
"Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy ﻿    DESCRIPTION (provided by applicant): In May 2012 Cincinnati Children's (CCHMC) joined eMERGE II with our Boston Children's partner. Since then we have developed algorithms for the electronic health record (EHR), led the Pediatric Workgroup, developed pharmacogenomics, evaluated the preferences of parents and caregivers to advance genomic medicine and assimilated technical advances into our EHR. The eMERGE effort has become the basic fabric of the institutional initiative to incorporate the extraordinary advances of genetics, genomics and the electronic medical record into healthcare. In addition, we bring a comprehensive EHR (EPIC), operating in every venue for healthcare delivery at CCHMC; a deidentified i2b2 data warehouse of 1.2 million patient records; and a Biobank with 150,000 consents that allow return of results to 38,000 patients and guardians who have provided 58,000 DNA samples, all with consent to return results and i2b2 EHR records. Now, we present our plan to join the eMERGE III network with 17 proposed initiatives. Our eMERGE effort is designed to move an entire institution with our eMERGE III partners into a genomic-EHR era of healthcare implementation and discovery. Our effort is divided into Genomics , Aim 1, where we hope to help the eMERGE III Steering Committee identify the 100 or so genes for the eMERGE III Targeted Gene Panel (eTGP), select our 2,000 CCHMC patients to be sequenced (of the 38,000 in our Biobank), review 4,000 targeted gene panels from clinical care at CCHMC for somatic mosaicism and reinterpretation, and further develop and disseminate a software workflow suite for sequence analysis (CASSI). For Phenotypes, Aim 2, we will extend our work generating EHR phenotype algorithms using heuristic and machine learning methods with a comprehensive set of EHR features derived from data driven algorithms to describe phenotypic pleiotropy of eTGP gene variants. We will develop working collaborations with Patients Care Outcomes Research Institute (PCORI) and the Million Veterans Program by applying eMERGE developed EHR algorithms to these large electronic data warehouses. For Implementation and Evaluation, Aim 3, we will develop tools to evaluate adolescent return of results preferences, examine the ethical and legal obligations and potential to reanalyze results, analyze the cost of tacrolimus management of kidney transplant with and without CYP3A5 testing, develop clinical decision support for phenotyping, test ordering, and returning eTGP results. Our success in these eMERGE III studies will be enhanced by the ongoing institutional investments made in the CCHMC BioBank, the comprehensive EHR (EPIC), and the i2b2 deidentified medical record data warehouse, and hundreds of Faculty and senior staff who make genomics or informatics an active focus of their research. We present a comprehensive program addressing all of the salient elements presented in the RFP for eMERGE III (HG-14-025) to enhance our collaborative productivity within the eMERGE Network in ways that ultimately improve our healthcare systems through discovery, implementation, and advanced applications of genomics and informatics.         PUBLIC HEALTH RELEVANCE: The Cincinnati Children's Hospital Medical Center (CCHMC) proposes to contribute as a funded site in the eMERGE III Network by pursuing a series of initiatives designed to advance genomics and the electronic health record (EHR). We propose 100 genes to evaluate by sequencing in 2,000 patients consented for return of results, to advance genomics by evaluating fee for service for DNA sequence-based genetic testing at CCHMC, to develop the reliable identification many diseases in the EHR and along with genome wide genetic testing, to explore genetic variants for alleged and unknown clinical manifestations, to extend the work of eMERGE to other large collections of EHR and genetic data, to evaluate cost benefits of genetic testing, to characterize the genetics of pain, to explor the preferences of adolescents concerning the genetic results they prefer to learn, and to explore the legal, ethical, and social issues that surround the re-interpretation of genetic variants. All of this work will be pursued with our eMERGE III Network colleagues the goal of improving the quality of the health care we deliver.                ",Better Outcomes for Children: Promoting Excellence in Healthcare Genomics to Inform Policy,8967443,U01HG008666,"['Abdominal Aortic Aneurysm', 'Address', 'Administrator', 'Adolescent', 'Algorithms', 'Appendicitis', 'Archives', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Boston', 'Budgets', 'CYP3A5 gene', 'Caregivers', 'Caring', 'Child', 'Childhood', 'Chronic Obstructive Airway Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computerized Medical Record', 'Computers', 'Consent', 'Cost Analysis', 'Costs and Benefits', 'DNA', 'DNA Sequence', 'DNA Sequence Analysis', 'Data', 'Decision Making', 'Disease', 'Dose', 'Effectiveness', 'Ehlers-Danlos Syndrome', 'Electronic Health Record', 'Electronics', 'Elements', 'Ethics', 'Evaluation', 'Faculty', 'Familial Hypercholesterolemia', 'Fee-for-Service Plans', 'Fibromyalgia', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic screening method', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Investments', 'Kidney Transplantation', 'Learning', 'Legal Obligations', 'Letters', 'Machine Learning', 'Malignant hyperpyrexia due to anesthesia', 'Medical Records', 'Medical center', 'Medicine', 'Methods', 'Methylphenidate', 'Migraine', 'Modification', 'Mosaicism', 'Narcotic Addiction', 'Natural Language Processing', 'Neonatal Abstinence Syndrome', 'Outcome', 'Outcomes Research', 'Outpatients', 'PTEN gene', 'Pain', 'Parents', 'Patient Care', 'Patients', 'Pediatric Hospitals', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Policies', 'Positioning Attribute', 'Primary Ciliary Dyskinesias', 'Process', 'Productivity', 'Pyloric Stenosis', 'Reading', 'Recommendation', 'Records', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Running', 'Sampling', 'Scientist', 'Sequence Analysis', 'Series', 'Site', 'Tacrolimus', 'Testing', 'Textiles', 'Time', 'Tonsillectomy', 'Translating', 'Variant', 'Veterans', 'Work', 'base', 'biobank', 'clinical care', 'clinically actionable', 'cohort', 'data modeling', 'design', 'economic impact', 'electronic data', 'ethical legal social implication', 'follow-up', 'gene panel', 'genetic information', 'genetic variant', 'genome-wide', 'genomic variation', 'health care delivery', 'health care quality', 'heuristics', 'improved', 'interest', 'member', 'next generation sequencing', 'pleiotropism', 'preference', 'primary pulmonary hypertension', 'programs', 'public health relevance', 'response', 'skills', 'success', 'tool', 'tool development']",NHGRI,CINCINNATI CHILDRENS HOSP MED CTR,U01,2015,855289,0.017226561382087276
"An Information Fusion Approach to Longitudinal Health Records DESCRIPTION (provided by applicant): Our goal is to leverage an information fusion approach to integrate structured and unstructured information to generate a longitudinal health record (LHR) for accelerating the pace at which patients can be recruited into clinical trials. Because electronic health records (EHR) contain clinical summaries of a patient's clinical history, one would assume that they could be easily leveraged to automatically screen and identify potentially eligible patients. However most EHRs are not well designed to support screening of eligible patients and are composed of multiple data sources that are often redundant or inconsistent, stored in uncoordinated unstructured clinical narratives and structured data. These characteristics make EHRs difficult to use for matching patients against the complex event and temporal criteria of clinical trials protocols. This research proposes that an improved LHR, which contains a comprehensive clinical summary of a patient, can improve patient screening. We propose using a method of information fusion to generate this LHR, which merges information from multiple data sources, that addresses both the meaning and temporal nature of data, such that the resulting information is more accurate than would be possible if these sources were used individually.         The specific aims are to: 1) characterize the barriers of using EHR sources for screening in terms of data redundancy, inconsistency, lack of structure, and temporal imprecision; 2) automatically extract information from unstructured EHR sources necessary for screening patients against clinical trials eligibility criteria using natural language processing; 3) developan LHR appropriate for screening patients against eligibility criteria using information fusion methods based on semantic and temporal information; and 4) evaluate the accuracy of an LHR formed through information fusion for screening patients against clinical trials eligibility critera.         The respective hypotheses to be tested are: 1) Different parts of the EHR will contain variable amounts of redundancy, inconsistency, and temporal imprecision. Some sources will be more valuable for matching patients than others to clinical trials eligibility criteria. 2) Including th information contained in the unstructured notes will reduce the false positive rate of identifying potentially eligible patients over leveraging only the structured data in the EHR. 3) By using information fusion methods based on leveraging semantic and temporal information on a combination of structured and unstructured data, we will be able to accurately summarize the information contained in uncoordinated EHR data sources into an LHR that can be used for screening patients for clinical trials. 4) The use of information fusion to generate a longitudinal health record will increase the sensitivity and specificity of electronic clinical trial screening ver using a traditional EHR.         With an LHR formed through information fusion for screening patients for clinical trials eligibilit, we will be able to not only reduce the amount of staff effort required to recruit a patient into a clinical trial, but also accelerate the pace at which clinical trials can be conducted. Narrative This project is focused on generating a longitudinal health record for accelerating the pace at which patients can be recruited into clinical trials. Accelerating the pace at which patients are recruited into clinical trials has the potential for improving the speed at which new treatments are made available to the public.",An Information Fusion Approach to Longitudinal Health Records,8906937,R01LM011116,"['Address', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Complex', 'Consultations', 'Data', 'Data Sources', 'Electronic Health Record', 'Electronics', 'Eligibility Determination', 'Enrollment', 'Event', 'Failure', 'Goals', 'Hour', 'Laboratories', 'Manuals', 'Measures', 'Methods', 'Natural Language Processing', 'Nature', 'Patient Recruitments', 'Patients', 'Process', 'Randomized', 'Randomized Controlled Trials', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Semantics', 'Sensitivity and Specificity', 'Series', 'Source', 'Speed', 'Structure', 'Techniques', 'Testing', 'Time', 'TimeLine', 'Work', 'base', 'cohort', 'cost', 'design', 'falls', 'health record', 'improved', 'screening']",NLM,OHIO STATE UNIVERSITY,R01,2015,297811,0.03153867919077024
"Patient Medical History Representation, Extraction, and Inference from EHR Data DESCRIPTION (provided by applicant): The significance of developing tools for automatically harvesting temporal constraints of clinical events from Electronic Health Records (EHR) cannot be overestimated. Efficient analysis of the temporal aspects in EHR data could boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine.     One big challenge we are facing is to automatically untangle and linearize the temporal constraints of clinical events embedded in highly diverse large-scale EHR data. Barriers to temporal data modeling, normalization, extraction, and reasoning have precluded the efficient use of EHR data sources for event history evaluation and trending analysis: (1) The current federally-supported EHR data normalization tools do not focus on the time aspect of unstructured data yet; (2) Existing time models focus only on structured data with absolute time, lack of supporting reasoning systems, or only offer application-specific partial solutions which cannot be adopted by the complex EHR data; (3) Current temporal information extraction approaches are either difficult to be adopted to EHR data, not scalable, or only offers application-specific partial solution.     This proposed project fills in the current gaps among ontologies, Natural Language Processing (NLP), and EHR-based clinical research for temporal data representation, normalization, extractions, and reasoning. We propose to develop novel approaches for automatic temporal data representation, normalization and reasoning for large, diverse, and heterogeneous EHR data and prepare the integrated data for further analysis. We will build new reasoning and extraction capacities on our TIMER (Temporal Information Modeling, Extracting, and Reasoning) framework to provide an end-to-end, open-source, standard-conforming software package. TIMER will be built on strong prior work by our team. We will develop new features in our CNTRO (Clinical Narrative Temporal Relation Ontology) for semantically defining the time domain and representing temporal data in complex EHR data. On top of the new developed CNTRO semantics, we will implement temporal relation reasoning capacities to automatically normalize temporal expressions, compute and infer temporal relations, and resolve ambiguities. We will leverage existing NLP tools and work on top of these tools to develop new extraction approaches to fill in the current gaps between NLP approaches and ontology-based reasoning approaches. We will adapt the SHARPn EHR data normalization pipeline and cTAKES for extracting and normalizing clinical event mentions from clinical narratives. We will explore an innovative approach for temporal relation extraction and event coreference, and make it work with the TIMER framework. We will evaluate the system using Diabetes Mellitus (DM) and colorectal cancer (CRC) patient cohorts from two insititutions. Each component will be tested separately first followed by an evaluation of the whole framework. Results such as precision, recall, and f-measure will be reported. Project Narrative The significance of developing capabilities for automatically harvesting temporal constraints for clinical events from Electronic Health Records (EHR) cannot be overestimated. A substantial portion of the information in the EHR is historical in nature. Patient medical history can be long, especially in complex patients. The proposed work, by offering an end-to-end open-source framework for automatically extracting, normalizing, and reasoning clinically-important time-relevant information from large-scale EHR data, can boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine; as well as facilitate clinical practice for early disease detection, post-treatment care, and patient-clinician communication.","Patient Medical History Representation, Extraction, and Inference from EHR Data",8911361,R01LM011829,"['Address', 'Adopted', 'Aftercare', 'Archives', 'Automated Annotation', 'Big Data', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Colorectal Cancer', 'Communication', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Support Systems', 'Detection', 'Diabetes Mellitus', 'Disease', 'Disease Progression', 'Electronic Health Record', 'Evaluation', 'Event', 'Goals', 'Gold', 'Harvest', 'Human', 'Institutes', 'Maps', 'Measures', 'Medical History', 'Medical Records', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patient Care', 'Patients', 'Performance', 'Recording of previous events', 'Registries', 'Reporting', 'Resolution', 'Semantics', 'Solutions', 'Structure', 'System', 'Testing', 'Time', 'Translational Research', 'Work', 'base', 'clinical practice', 'cohort', 'colon cancer patients', 'data modeling', 'data structure', 'information model', 'innovation', 'novel strategies', 'open source', 'personalized medicine', 'programs', 'tool', 'trend']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2015,325159,0.058406506835919574
"Secondary use of EMRs for surgical complication surveillance     DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care.         PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.                ",Secondary use of EMRs for surgical complication surveillance,8798027,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'abstracting', 'base', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'meetings', 'patient safety', 'public health relevance', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2015,299888,0.03231005138540235
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,8882546,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial/ethnic difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2015,417794,0.0014902703511776856
"SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs)  DESCRIPTION (provided by applicant): Smart algorithms that effectively analyze patient care data can enhance clinical communication to save lives. In 2000, the Institute of Medicine estimated 98,000 preventable patient deaths occur annually in US hospitals due to miscommunication [1]. Electronic health records (EHRs) were expected to facilitate accurate communication within the care team and provide data to enable automated clinical decision support systems. Unfortunately, miscommunication remains a significant cause of patient deaths [2]. Providers are now required to demonstrate meaningful use of EHR systems to improve quality of care and patient outcomes. Despite this, providers continue to report that EHR systems are cumbersome and interfere with care-team communication. Information entered into an EHR is rarely used by nurses due to the time and difficulty involved in its retrieval. As a result, nurses continue to verbally convey critical patient care information to the next nurse during shift changes. Verbal report or hand-off, where critical patient information is exchanged in only minutes, is inefficient. Worse, it is highly susceptible to communication errors. Broader Impacts: Research: 1) Increase patient safety; 2) Provide preliminary data to expand this work to include physician-physician and physician-RN communication and decision-making in the EHR; 3) Share our discoveries to inform other industries who may also benefit from this technology. Education: 1) Contribute to curriculum enhancements whereby RN students learn strategies to recognize and effectively communicate CEs; 2) As part of curriculum enhancements, include healthcare applications for computer and information science students; 3) Disseminate findings via academic publications, professional meetings, a project website and social media. Mentoring: 1) Mentor budding scientists in the roles of research assistants (RAs) and post doctoral fellows studying nursing and computer science to forge collaborative interdisciplinary relationships for ongoing research; 2) Interest and recruit underrepresented students in STEM and careers in healthcare. RELEVANCE (See instructions): The electronic health record (EHR) has been thought to be a tool to decrease patient deaths related to miscommunication. However, the current EHR falls short of this goal. We propose to develop and test an algorithm that will augment the EHR to more effectively assist nurses in decision-making and communication, ultimately increasing patient safety.",SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs) ,8932003,R01EB020395,"['Agreement', 'Algorithms', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Decision Support Systems', 'Communication', 'Communication Tools', 'Computer Simulation', 'Conscious', 'Data', 'Data Reporting', 'Decision Making', 'Discipline of Nursing', 'Documentation', 'Education', 'Educational Curriculum', 'Effectiveness', 'Electronic Health Record', 'Electronics', 'Event', 'Fever', 'Goals', 'Hand', 'Health', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Imagery', 'Industry', 'Information Sciences', 'Institute of Medicine (U.S.)', 'Instruction', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mentors', 'Nurses', 'Outcome', 'Output', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Postdoctoral Fellow', 'Provider', 'Publications', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'STEM career', 'Sampling', 'Scientist', 'Severities', 'Students', 'Symptoms', 'System', 'Technology', 'Testing', 'Time', 'Underrepresented Students', 'Validity and Reliability', 'Work', 'computer science', 'falls', 'forging', 'high risk', 'improved', 'interest', 'light weight', 'meetings', 'patient safety', 'prototype', 'respiratory', 'social', 'tool', 'usability', 'web site']",NIBIB,UNIVERSITY OF ARIZONA,R01,2015,248507,0.05588560328830962
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing.             Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,8927274,R01LM010090,"['Apache Indians', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Imagery', 'Individual', 'Informatics', 'Information Retrieval', 'International', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Outcomes Research', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Process', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Solutions', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translating', 'Translational Research', 'Trees', 'Vision', 'Work', 'abstracting', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'experience', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'syntax', 'tool', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2015,720481,0.019477782709814727
"Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes ﻿    DESCRIPTION (provided by applicant): The applicant and principal investigator (PI) is a board-certified cardiologist, Assistant Professor of Medicine and Senior Associate Consultant in the Mayo Clinic Cardiovascular Division. [The long-term goal of the PI is to become an independent clinician-investigator in translational informatics focused on the development, validation and deployment of electronic tools to the point-of-care to improve outcomes for patients with cardiovascular disease. The PI intends to leverage the EHR to conduct population-based studies using electronic algorithms that include NLP and by application of state-of-the-art informatics approaches to deliver the knowledge at the point-of-care in support of patient-centered decision-making for patients with peripheral arterial disease (PAD). The electronic tools developed will be portable to different institutions and other health conditions. The PI will be uniquely positioned as a cardiologist within a large academic health system working with novel NLP techniques at the interface of the EHR and CDS to rapidly translate knowledge acquired in community-based epidemiologic investigations to patient care.] The PI has proposed a research career development plan to acquire expertise in translational informatics including electronic phenotyping, and automated calculation and visual display of prognostic scores using a novel application integrated with clinical decision support (CDS). The PI will complete comprehensive coursework in informatics science and gain additional patient-oriented and epidemiologic research experience. The primary mentor will be Iftikhar Kullo, M.D., cardiologist and PI of the Mayo Clinic electronic MEdical Records and GEnomics (eMERGE) network and a leader in the development and application of electronic phenotyping for genomic studies of PAD. The mentoring team also includes Dr. Christopher Chute, M.D., Head of Medical Informatics at Mayo Clinic, an expert in informatics and epidemiology, and co-PI of the Mayo eMERGE grant and co-investigator of the Rochester Epidemiology Project; Hongfang Liu, Ph.D., an expert in natural language processing (NLP) and its application for EHR-based phenotyping; and Kent Bailey, Ph.D., senior biostatistician with expertise in development of prognostic risk scores. The specific aims of the proposal are: Aim 1 - Apply phenotyping algorithms that include NLP of clinical notes to [a] identify PAD cases and controls without PAD in the community from 1998-2011; and [b] ascertain adverse cardiovascular and limb outcomes through December 2013. Aim 2 - Create multivariable prognostic risk models and scores for adverse outcomes in PAD cases from the community. We will estimate the relative risk of death in PAD patients compared to controls without PAD. Aim 3 - Develop and evaluate usefulness of a novel electronic application within the Mayo EHR to retrieve relevant data elements, calculate, and display individualized prognostic scores. On completion of the proposed investigations the deliverables will be new knowledge and an e-health prognostication tool for PAD patients to be disseminated to stakeholders including clinicians, patients, and researchers portable to other institutions and other cardiovascular diseases.         PUBLIC HEALTH RELEVANCE: The applicant, a board certified cardiovascular specialist, proposes to acquire training in translational informatics science which will enable application of novel electronic algorithms to electronic health records to more efficiently conduct community-based studies of peripheral arterial disease, a prevalent but undertreated health condition. These studies will concurrently develop novel electronic tools which will inform patients and providers of risk, facilitate shared-decision making at the point-of-care, and promote compliance to guideline recommended strategies. These tools will be portable to other institutions and to other cardiovascular diseases.            ",Novel Informatics Approaches for Ascertainment of PAD Status and Adverse Outcomes,8891601,K01HL124045,"['Address', 'Adverse event', 'Algorithms', 'Amputation', 'Atherosclerosis', 'Atrial Fibrillation', 'Attention', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Investigator', 'Communities', 'Computerized Medical Record', 'Coronary heart disease', 'Data', 'Data Element', 'Decision Making', 'Dependence', 'Development', 'Development Plans', 'Diagnosis', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Electronic Health Record', 'Electronics', 'Epidemiologic Studies', 'Epidemiology', 'Genomics', 'Goals', 'Government', 'Grant', 'Guidelines', 'Head', 'Health', 'Health system', 'Healthcare', 'Heart failure', 'High Prevalence', 'Image', 'Informatics', 'Institution', 'Investigation', 'Ischemia', 'Knowledge', 'Laboratories', 'Limb structure', 'Link', 'Manuals', 'Medical Informatics', 'Medicine', 'Mentors', 'Methodology', 'Mining', 'Modeling', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patients', 'Peripheral arterial disease', 'Pharmaceutical Preparations', 'Phenotype', 'Positioning Attribute', 'Principal Investigator', 'Provider', 'Relative Risks', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Safety', 'Science', 'Site', 'Specialist', 'Stratification', 'Stroke', 'Symptoms', 'System', 'Techniques', 'Time', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Visual', 'Work', 'adverse outcome', 'base', 'career development', 'case control', 'clinical decision-making', 'cost', 'demographics', 'didactic education', 'eHealth', 'experience', 'health information technology', 'high risk', 'improved', 'investigator training', 'mortality', 'multidisciplinary', 'novel', 'patient oriented', 'point of care', 'population based', 'predictive modeling', 'professor', 'prognostic', 'public health relevance', 'repository', 'research and development', 'shared decision making', 'skills', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,K01,2015,136485,0.051582619427197725
"Development and Evaluation of a Learning Electronic Medical Record System ﻿    DESCRIPTION (provided by applicant):  The goal of this project to develop and evaluate a learning electronic medical records (L-EMR) system that draws a physician's attention to the right data, at the right time. It learns how to do so by analyzing patterns of patient data access f many physicians in many past cases in the EMR, and learning which EMR data to highlight that are relevant for making clinical decisions in a given patient.      The hypothesis underlying this research is that the L-EMR system will have sufficiently high precision and recall in highlighting relevant data, decrease the average time to assess an intensive care unit (ICU) patient case, and be judged by critical care medicine (CCM) physicians to be clinically useful.    The first aim of this project is develop a highly-usable L-EMR user interface. The L-EMR user interface will include zoomable time-series displays of lab-results, med-orders, and vital signs. Usability studies of the L-EMR user interface will guide revisions and enhancements.      The second aim of the project is to train statistical models that can be applied to a patient case to predict relevant lab-results, med-orders, and vital signs. We will enlist CCM physicians to review a set of retrospective ICU patient cases on a focused set of clinical conditions. Participants will review these cases as if they were active patients, identifying relevant lab- results, med-orders, and vital signs. We will train and evaluate statistical models to predict relevant data, and identify the best performing algorithm to include in the L-EMR system.      The third aim of the project is to evaluate the L-EMR system. We will recruit CCM physicians to evaluate an L-EMR system based on user interfaces from Aim 1 and statistical models trained using the best performing algorithm in Aim 2 to highlight relevant data items. We will measure the precision and recall of the data-highlighting functionality for assessing patient cases and making clinical decisions (e.g., lab and medication orders), the time required to assess cases with and without the highlighting, and physicians' assessments of the strengths and weaknesses of the L-EMR system.    If the results of these experiments are positive, as anticipated, this project will introduce a computational method that has significant potential to improve future EMR systems and enhance patient care.             Narrative The purpose of this research is to develop and evaluate a learning electronic medical records (EMR) system that draws a physician's attention to the right data, at the right time. The system works by analyzing patterns of EMR usage of physicians, and learning which EMR data to highlight that are relevant in a given patient. The main idea underlying the approach is that patterns of past EMR usage patterns can be exploited to selectively highlight clinically useful patient data.",Development and Evaluation of a Learning Electronic Medical Record System,9030245,R01LM012095,"['Address', 'Adult', 'Algorithms', 'American', 'Attention', 'Bayesian Modeling', 'Blood', 'Caring', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Computerized Medical Record', 'Computing Methodologies', 'Critical Care', 'Critical Illness', 'Data', 'Data Display', 'Data Set', 'Development', 'Educational workshop', 'Evaluation', 'Face', 'Future', 'Goals', 'Healthcare Systems', 'Heart Rate', 'Hemoglobin', 'Hemorrhage', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Intensive Care Units', 'Intravenous', 'Laboratories', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Physiological', 'Provider', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Series', 'Statistical Models', 'System', 'Test Result', 'Time', 'Training', 'Work', 'base', 'clinical decision-making', 'computer human interaction', 'design', 'follow-up', 'gastrointestinal', 'improved', 'prototype', 'research clinical testing', 'research study', 'stem', 'trend', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2015,447973,0.038107506533143216
"Data-Mining Clinical Decision Support from Electronic Health Records ﻿    DESCRIPTION (provided by applicant)    Data-Mining Clinical Decision Support from Electronic Health Records Public Health Motivation: National healthcare quality is compromised by undesirable variability, reflected in different locales having anywhere from 20-80% compliance with evidence-based guidelines. Much of this is due to uncertainty, with half of clinical practice guidelines lacking adequate evidence to confirm their efficacy. This is unsurprising when clinical trials cost >$15 million to answer individual clinical questions. The result is medical practice routinely driven by individual opinio and anecdotal experience. While Big Data has revolutionized how society processes internet scale information, the status quo in clinical decision making remains the manual interpretation of literature and isolated decision aids. The adoption of electronic health records (EHR) creates a new opportunity to answer a ""grand challenge in clinical decision support (CDS)."" In a learning health system, we could automatically adapt knowledge from the collective expertise embedded in the EHR practices of real clinicians and close the loop by disseminating that knowledge back as executable decision support. Candidate Goals and Objectives: The unifying goal of this BD2K K01 proposal is the mentored career development of Jonathan H. Chen, MD, PhD. This proposal will accelerate his transition into an independent physician scientist, towards his long-term goals to produce Big Data technologies that answer such grand challenges in clinical decision support. His near-term objective is developing methods to translate EHR data into useful knowledge in the form of patient- specific, point-of-care clinical order recommendations for acute medical hospitalizations. His doctoral background in computer science gives him the technical capability to achieve these objectives, while his medical training will ensure clinically meaningful results. His preliminary work to build an order recommender, analogous to commercial product recommenders, demonstrates the proposal's overall feasibility. Institutional Environment and Career Development: The research facilities and training opportunities at Stanford University provide the ideal environment to achieve these objectives, with established and growing Centers for Biomedical Informatics Research, the Biomedical Data Science Initiative, and the first Clinical Informatics Fellowship accredited in the nation. Prof. Russ Altman, Director of the Biomedical Informatics Training Program, will lead a collaborative team of mentors with expertise in clinical decision support (Mary Goldstein), implementation science (Steven Asch), data-mining electronic health records (Nigam Shah), statistical learning algorithms (Lester Mackey), and healthcare statistics (Michael Baiocchi). Combined with respective didactic training, this mentorship will enable Dr. Chen to achieve his objectives through a series of research aims. Research Aims: The overriding hypothesis of the proposal is that clinical knowledge reflected in clinical order patterns from historical EHR data can improve medical decision making when adapted into functional clinical decision support. The specific aims each address components of this concept, as they seek to: (1) Develop the algorithms to learn clinical order patterns from historical EHR data, building on a preliminary recommender system; (2) Assess how underlying clinician proficiency affects the quality of those learned clinical order patterns through observational data inference against external standards; and (3) Determine the impact of automatically learned clinical decision support (CDS) on (simulated) clinical workflows through a randomized controlled crossover trial of human-computer interfaces with real clinicians. Expected Results and General Significance: By the completion of the proposed work, Dr. Chen will answer the grand challenge in clinical decision support (CDS) by automating much of the CDS production process, and have direct translational impact with a prototype system. This will advance the field with new paradigms of generating and disseminating clinical knowledge, which can then improve the consistency and quality of healthcare delivery. Additional benefits will include methods to identify and monitor areas of high practice variability for targeted optimization and improve predictive models that inform precision medicine. With this applied research experience and career development, Dr. Chen can compete for R01 funding and become an independent physician scientist developing Big Data approaches to solve national healthcare problems in clinical decision making.         PUBLIC HEALTH RELEVANCE    National healthcare quality is compromised by undesirable practice variability and medical uncertainty, with most medical practice routinely driven by individual opinions and anecdotal experience. With methods analogous to commercial product recommender systems, the proposed project will automatically learn patterns in raw clinical transaction data to capture the undocumented knowledge of real-world clinicians, and close the loop in a learning health system by disseminating that knowledge back as clinical decision support to improve patient care.                ",Data-Mining Clinical Decision Support from Electronic Health Records,9044538,K01ES026837,"['Achievement', 'Acute', 'Address', 'Admission activity', 'Adoption', 'Affect', 'Algorithms', 'Applied Research', 'Area', 'Back', 'Big Data', 'Biomedical Research', 'Caring', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Practice Guideline', 'Clinical Trials', 'Cross-Over Trials', 'Crowding', 'Data', 'Decision Aid', 'Decision Making', 'Diagnosis', 'Doctor of Philosophy', 'Educational process of instructing', 'Electronic Health Record', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Failure', 'Fellowship', 'Funding', 'Future', 'Goals', 'Health system', 'Healthcare', 'Hospitalization', 'Hospitals', 'Image', 'Individual', 'Institute of Medicine (U.S.)', 'Internet', 'Investments', 'Knowledge', 'Lead', 'Learning', 'Literature', 'Locales', 'Machine Learning', 'Manuals', 'Medical', 'Medical Residency', 'Mentors', 'Mentorship', 'Methods', 'Monitor', 'Motivation', 'Noise', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Production', 'Public Health', 'Quality of Care', 'Randomized', 'Recommendation', 'Relative (related person)', 'Research', 'Research Training', 'Science', 'Scientist', 'Series', 'Services', 'Societies', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Training', 'Training Programs', 'Translating', 'Uncertainty', 'Universities', 'User-Computer Interface', 'Validation', 'Weight', 'Work', 'base', 'biomedical informatics', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'cost', 'data mining', 'design', 'evidence based guidelines', 'experience', 'health care delivery', 'health care quality', 'implementation science', 'improved', 'informatics training', 'point of care', 'precision medicine', 'predictive modeling', 'prototype', 'public health relevance', 'research facility', 'statistics', 'tool']",NIEHS,STANFORD UNIVERSITY,K01,2015,178606,0.025559366617838974
"Encoding and Processing Patient Allergy Information in EHRs DESCRIPTION (provided by applicant): Allergies affect one in five Americans and are the 5th leading chronic disease in the U.S. Each year, allergies account for more than 17 million outpatient office visits. Although documenting and exchanging allergy information in electronic health records (EHRs) is becoming increasingly important, we still face multiple challenges. These include: lack of well-adopted standard terminologies for representing allergies, frequent entry of allergy information as free-text, and no existing process for reconciling allergy information. In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) conduct analyses on standard terminologies and a large allergy repository to build a comprehensive knowledge base for representing allergy information; 2) design, develop and evaluate a natural language processing (NLP) module for extracting and encoding free-text allergy information and integrate it with an existing NLP system; 3) measure the feasibility and efficiency of the proposed NLP system for the new process of allergy reconciliation; and 4) distribute our methods and tool, so they are widely available to other researchers and healthcare institutions for non-commercial use. PUBLIC HEALTH RELEVANCE: Managing allergy information within the electronic health record (EHR) is vital to ensuring patient safety. The goal of this study is to propose a comprehensive solution to assess existing terminology standards and knowledge bases for representing allergy information, develop and evaluate a natural language processing (NLP) system for extracting and encoding allergy information from free-text clinical documents, and finally measure the feasibility of using NLP output to facilitate the allergy reconciliation proces.",Encoding and Processing Patient Allergy Information in EHRs,8920540,R01HS022728,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,488893,0.010627888304786772
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005).         PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.            ",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9004939,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,250000,0.004696198921118965
"Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach DESCRIPTION (provided by applicant): Physician progress notes contain information essential to patient care, including findings from history and physical exam, interpretation of tests, assessment and treatment plans. However in the transition from paper to electronic physician notes, many physicians spend more time creating them, which has led to the use of time-saving measures such as copy/paste and templates that have degraded note accuracy and quality. This threatens the usefulness of notes not only for their most important use-patient care-but also for research, quality improvement, and in supporting reimbursement. To address these problems, we propose a project with the following specific aims: 1. To refine and implement a new voice-generated enhanced electronic note system (VGEENS) integrating voice recognition with natural language processing and links to the electronic medical record (EMR) to improve note accuracy and timeliness. 2. To evaluate VGEENS using a randomized trial with 30 internal medicine physicians in each arm to assess electronic note accuracy, quality, timeliness, and user satisfaction. Intervention physicians will use VGEENS, while the control physicians will continue with note creation as they normally would. This novel approach has the potential to improve note accuracy while reducing delays in making progress notes in EMRs available to other clinicians. It leverages rapidly improving voice recognition and NLP technologies to permit physicians to use a natural, fast method-human voice-to convey their observation and thoughts into the EMR record. PUBLIC HEALTH RELEVANCE: Physician documentation of a patient visit contains information that is used in that patient's care. This information includes findings from a patient' history and physical exam, interpretation of necessary tests, the problem assessment and treatment plan. However, in the transition from paper to electronic physician notes, many physicians are spending more time creating these notes. This has led to use of time-saving measures that have degraded the accuracy and ease of use of patient notes. By the end of this project, we expect to have developed, used and evaluated a new method for creating electronic physician notes that both improve accuracy and timely availability of inpatient progress notes.","Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach",8928601,R21HS023631,[' '],AHRQ,UNIVERSITY OF WASHINGTON,R21,2015,139245,0.017982418754240875
"Outpatient Adverse Drug Effect Alerting System Using Admission H&P Notes ﻿    DESCRIPTION (provided by applicant): This project will create, test, implement, and evaluate a real-time Adverse Drug Effect (ADE) alerting system at Vanderbilt University Hospital (VUH). Utilizing information extracted from admission history and physical examination notes (H&Ps) stored in the electronic medical record (EMR), our system will detect adult inpatients' previously unrecognized symptomatic ADEs and alert appropriate care providers. The project will create an ADE knowledge base, mined from multiple publically available sources including MEDLINE, RxNorm, and the product labels for human prescription drugs. Next, we will apply natural language processing (NLP) to EMR-based H&P texts to identify mentions of patients' current medications and dosages. We will similarly detect EMR-based H&P documentation of patients' clinical manifestations (CMs - diseases, symptoms, findings, etc.), and then represent them using the Unified Medical Language System's (UMLS) Concept Unique Identifiers (CUIs). The system will then compare each patient's recognized medications and CMs against the aforementioned ADE knowledgebase, generating appropriate patient-specific alerts for potential adverse effects related to the patient's current medications. Each alert will identify the offendin medication, the suspected nature of the ADE, and evidence supporting the assertion. We will independently evaluate the accuracy of the ADE knowledgebase using expert manual review and comparison to multiple other sources. Before implementing the real-time ADE monitoring system, we will conduct a pilot implementation using retrospective EMR data from the Vanderbilt Synthetic Derivative (SD), a de-identified version of the Vanderbilt EMR. After successful testing and any necessary iterative improvements, we will implement the real-time detection system at VUH, initially monitoring newly admitted inpatients presenting to the Internal Medicine service. Using the methods described above, the system will detect potential ADEs each time a new inpatient H&P is generated, and alert appropriate clinicians via additions to an existing dashboard that the clinicians already utilize. We will survey clinicians immediately upon receipt of an ADE alert to determine if the alerting condition was already known or not, whether the alert seems plausible, and whether it requires intervention. Then we will compare discharge medications to admission medications to determine what actions occurred post-alert, independent of physician survey results. We will thus evaluate both the effectiveness of the system in improving ADE recognition and its perceived usefulness according to the physician-subjects. We hypothesize that our system will improve clinicians' awareness of ADEs in a manner applicable to any facility that stores admission H&Ps electronically. Addressing previously unrecognized ADEs has the potential to reduce costs and improve patient care.         PUBLIC HEALTH RELEVANCE: Unrecognized adverse drug effects (ADEs), a serious clinical problem, cause preventable hospitalizations, increase healthcare costs, and worsen health outcomes. To potentially improve quality of care, my dissertation project will develop and evaluate a novel system to detect adult inpatients' previously unrecognized symptomatic ADEs and alert appropriate care providers. Using automated natural language processing of clinician-generated electronic admission history and physical exam (H&P) notes and a locally-developed ADE knowledgebase derived from publically available sources, the system, once validated, could improve both recognition and treatment of ADEs in a generalizable manner - applicable in hospital environments using electronic medical records.            ",Outpatient Adverse Drug Effect Alerting System Using Admission H&P Notes,8796404,R36HS023485,[' '],AHRQ,VANDERBILT UNIVERSITY,R36,2015,41626,0.0015348605135704011
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,8928596,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2015,249655,0.016391971940315025
"NLP-enabled decision support for cervical cancer screening and surveillance DESCRIPTION (provided by applicant): Although cervical cancer is preventable, it still continues to be a leading cause of death. Following the evidence- based guidelines for cervical cancer prevention is challenging for healthcare providers, due to which many patients do not receive the optimal preventive care. Clinical decision support (CDS) systems can potentially improve the care delivery. However, the current CDS systems only identify patients overdue for screening, and do not suggest the optimal screening interval. Moreover they do not help with surveillance of patients with abnormal screening results. This is because the existing systems lack the capability to process free- text clinical reports that contain information needed for applying the guidelines. Hence there is a critical need for natural language processing (NLP)-enabled CDS systems that can utilize discrete as well as free-text patient information for enhancing the decision support. Our long-term goal is to improve healthcare delivery of cervical cancer prevention with guideline based reminders. The central hypothesis is that NLP- enabled CDS system will significantly improve the quality of care delivery for cervical cancer prevention. The rationale is that use of NLP will improve granularity of the guideline implementation, which will in-turn enhance care delivery. As preliminary work we have developed an NLP-enabled CDS system that automatically interprets the patient information from the electronic health record and applies the national guidelines to compute the optimal recommendation for screening and surveillance. We have performed validation of the system in a non-clinical setting.1 In this application we will proceed towards deployment of the system in the clinical setting, and will carry out studies for measuring the impact on the quality of care delivery. In ai one, we will validate the system in the clinical setting and will optimize its usability and workflw integration. In aim two, we will test the hypothesis that reminders from the NLP-enabled CDS system to primary care providers will improve the quality of care delivery, by performing a one year intervention control study across four sites of a primary care practice. In aim three, we will test the hypothesis that reminders to non-adherent high-risk patients will improve their surveillance rates, by performing a randomized intervention study for three months. In this study, care coordinators will utilize the CDS system for sending reminders to patients that are non-adherent and at high risk due to abnormal screenings. The main contribution of this project will be knowledge about the effectiveness of NLP in enhancing the impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This project is innovative because the CDS system will utilize NLP to generate screening reminders for normal patients and surveillance reminders for patients with abnormal findings. This is a major advancement over existing systems that can only identify patients for screening. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because it will yield knowledge about the effectiveness of natural language processing (NLP) to enhance impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This research will foster implementation of similar CDS systems across the nation for cervical cancer prevention and for other decision problems, which will improve the quality of healthcare delivery. Thus, the proposed research is relevant to AHRQ's mission to improve the quality, safety, efficiency, and effectiveness of health care for all Americans.",NLP-enabled decision support for cervical cancer screening and surveillance,8934087,R21HS022911,[' '],AHRQ,MAYO CLINIC ROCHESTER,R21,2015,145229,0.022069425250648964
"Construction of Relation Detection Framework Empowered by Topic Modeling     DESCRIPTION (provided by applicant): The availability of large volume of EHRs enhances the possibility for using them for health services, EBM and clinical research. However such functionality is currently limited to narrow areas of clinical practice, as relation detections between medical events among unstructured EHRs still pose a big challenge, consequently leading to the inaccuracy of patient cohort identification, especially for cross-institutional environment. Preliminary work by the PI has shown that topic modeling can discover data semantics, which can then be employed as significant cues for diverse relation detections among EHRs. Among them, co-referring, temporal relations and domain semantics are intertwined and positively correlated. Up to now, not much research is done to combine the three relations to build a better patient cohort identification system. Therefore, the PI proposes to develop a relation detection framework for EHRs empowered with topic modeling for more accurate patient cohort identification. In the mentored phase, the PI will implement the relation detection framework under the guidance of my mentor team and will make them available in open-source so that they can be adapted for deployment at other institutions (aim 1 - K99). In the independent phase, the PI will research methods to facilitate rapid development, deployment and cross-institutional portability of similar systems. Specifically, the PI will develo a hybrid design with ICD-9, RxNorm and MeSH ontologies for the data semantics discoveries from EHRs and MedLINE respectively and investigate categorization of data semantics aligning with medical ontologies (aim 2 - R00). To enable other researchers to reuse the developed methodologies and software resources and more importantly to make corrections or adjustments on data semantics, a toolkit will be developed that will support the construction and deployment of similar systems (aim 3 - R00). The independent phase will be in collaboration with both UTHealth and University of Maryland. The PI's career goal is to become a scientific leader in clinical informatics with a focus on relation detections among EHRs for efficient patient cohort identification. The PI has strong background in computational linguistics and rich experiences in medical clinical records processing and analyses, and will receive mentoring from Drs. Hongfang Liu, Christopher Chute, and Terry Therneau, who have complimentary areas of expertise. The mentored phase will be in Mayo Clinic Rochester where the PI will undertake courses in US healthcare system, health system engineering, clinical statistics and clinical epidemiology and will get mentored training in health informatics which is what he needs to continue to strengthen since he didn't get regular training in his PhD education. In the independent R00 phase, the PI will strive for making independent scientific contributions to the use of informatics for healthcare via the implementation of Aims 2 and 3 and via the independent collaborations internal and externally. Completion of the proposed work will enable the PI to seek further funding for piloting clinical deployment of the developed systems, measuring their clinical impact, and for scaling the approach to other clinical domains and institutions. The career grant will enable the PI to establish himself as an independent investigator and to make significant contributions towards advancing the construction of medical knowledge systems and clinical practices as well as clinical research.             PUBLIC HEALTH RELEVANCE STATEMENT The potential of Electronic Health Records (EHRs) to improve care delivery by providing accurate and real-time patient cohort identification is not fully realized, because corresponding relation detection systems for medical events among EHRs still need improvements. The proposed framework will enable patient cohort identification that can encompass broader areas of practice, due to their capability to utilize free-text as well as structured EHR data. This pioneering research directly impacts public health by improving the quality of care through enhanced relation detection functionality in the EHRs.",Construction of Relation Detection Framework Empowered by Topic Modeling,8804480,K99LM012021,"['Address', 'Area', 'Back', 'Categories', 'Chronic', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Engineering', 'Clinical Informatics', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Complement', 'Computational Linguistics', 'Computer software', 'Cues', 'Data', 'Dependency', 'Detection', 'Development', 'Diabetes Mellitus', 'Disease', 'Doctor of Philosophy', 'Education', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Environment', 'Event', 'Foundations', 'Funding', 'Future', 'Goals', 'Grant', 'Health Care Costs', 'Health Services', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hybrids', 'Hypertension', 'ICD-9', 'Informatics', 'Institution', 'International Classification of Disease Codes', 'Knowledge', 'Lead', 'Learning', 'Linguistics', 'Logic', 'Maryland', 'MeSH Thesaurus', 'Measures', 'Medical', 'Medical Informatics', 'Medical Research', 'Mental Depression', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nurses', 'Ontology', 'Patients', 'Performance', 'Phase', 'Physicians', 'Procedures', 'Process', 'Public Health', 'Public Health Informatics', 'Qualifier', 'Quality of Care', 'Records', 'Relative (related person)', 'Research', 'Research Methodology', 'Research Personnel', 'Research Training', 'Resolution', 'Resources', 'Risk Factors', 'Running', 'Schools', 'Semantics', 'Structure', 'System', 'Terminology', 'Text', 'Time', 'Training', 'Universities', 'Work', 'base', 'care delivery', 'career', 'clinical epidemiology', 'clinical practice', 'cohort', 'data mining', 'design', 'empowered', 'evidence base', 'experience', 'improved', 'informatics training', 'knowledge base', 'news', 'open source', 'operation', 'portability', 'public health relevance', 'statistics']",NLM,MAYO CLINIC ROCHESTER,K99,2015,93654,0.026394093304349657
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,8929257,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Health', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Population Study', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'improved', 'large-scale database', 'novel', 'open source', 'personalized medicine', 'rapid growth', 'rare variant', 'response', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY,R01,2015,598996,0.009438619806228447
"Interactive machine learning methods for clinical natural language processing     DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3.             Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,8818096,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face', 'Goals', 'Grant', 'Human', 'Hybrids', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Solutions', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'model development', 'novel', 'open source', 'statistics', 'success', 'tool', 'usability']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2014,558372,0.025981778309663785
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8722030,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2014,260727,0.01171561179007688
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification     DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts.         PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.                ",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,8811565,R01LM011934,"['Accounting', 'Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'novel', 'open source', 'public health relevance', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2014,460688,0.03795178485971105
"A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications  PROJECT SUMMARY Electronic Health Records (EHRs) can improve the quality of healthcare delivery in the United States, by providing automated best-practice reminders to clinicians and patients. However such functionality is currently limited to narrow areas of clinical practice, as existing decision support systems can process only structured data, due to lack of a suitable framework and concerns about accuracy and portability. Preliminary work by the PI has shown that rule-based approach can be used to develop broad-domain reminder systems that can utilize free-text in addition to the structured data. The PI has developed prototype systems for cervical and colorectal cancer prevention. These systems consist of rule-based composite models of national guidelines, and rule-based Natural Language Processing (NLP) parsers. The NLP parsers extract the patient variables required for applying the guidelines. However further research is needed to extend the systems and to ensure their accuracy for clinical deployment. In the mentored phase, the PI will collaborate with clinicians to extend and iteratively optimize and validate the systems, and will make them available in open-source so that they can be adapted for deployment at other institutions (aim 1 - K99). In the independent phase, the PI will research methods to facilitate rapid development, deployment and cross- institutional portability of similar systems. Specifically, the PI will develop a hybrid design for the parsers and investigate domain adaptation and active learning methods, for reducing the manual effort for development and adaptation of the NLP parsers (aim 2 - R00). To enable other researchers to reuse the developed methodologies and software resources, a toolkit will be developed that will support the construction and deployment of similar systems (aim 3 - R00). The toolkit will consist of user-friendly tools and templates to replicate the processes engineered in the case studies, and will build on the SHARPn data normalization tooling and other open-source tools. The independent phase will be in collaboration with Intermountain Healthcare. The PI's career goal is to become a scientific leader in clinical informatics with a focus on optimizing clinical decision making. The PI has strong background in clinical medicine and medical informatics, and will receive mentoring from Drs. Hongfang Liu, Christopher Chute, Robert Greenes and Rajeev Chaudhry, who have complimentary areas of expertise. The mentored (K99) phase will be for 2 years at Mayo Clinic Rochester, wherein the PI will undertake courses on decision support and will get mentored training in NLP and health information standards. This will prepare the PI for independent research in R00 phase on portability and tooling. Completion of the proposed work will enable the PI to seek further funding for piloting clinical deployment of the developed systems, measuring their clinical impact, and for scaling the approach to other clinical domains and institutions. The career grant will enable the PI to establish himself as an independent investigator and to make significant contributions towards advancing clinical decision support for improving care delivery.  PUBLIC HEALTH RELEVANCE STATEMENT The potential of Electronic Health Records (EHRs) to improve care delivery by providing best-practice reminders is unrealized, because reminder systems currently operate in narrow areas of clinical practice, as they can process only structured data. The proposed framework will enable construction of reminder systems that can encompass broader areas of practice, due to their capability to utilize free-text as well as structured EHR data. This pioneering research directly impacts public health by improving the quality of care through enhanced reminder functionality in the EHRs.",A Framework to Enhance Decision Support by Invoking NLP: Methods and Applications,8633838,K99LM011575,"['Active Learning', 'Address', 'Area', 'Caregivers', 'Caring', 'Case Study', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Collaborations', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Decision Support Systems', 'Development', 'Electronic Health Record', 'Engineering', 'Ensure', 'Fostering', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Care Costs', 'Healthcare', 'Hybrids', 'Institution', 'Language', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Manuals', 'Measures', 'Medical Informatics', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Phase', 'Process', 'Public Health', 'Quality of Care', 'Reminder Systems', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Solutions', 'Structure', 'System', 'Text', 'Training', 'United States', 'Validation', 'Work', 'base', 'care delivery', 'career', 'clinical application', 'clinical decision-making', 'clinical practice', 'colorectal cancer prevention', 'colorectal cancer screening', 'design', 'health care delivery', 'improved', 'open source', 'portability', 'prevent', 'prototype', 'public health relevance', 'tool', 'user-friendly']",NLM,MAYO CLINIC ROCHESTER,K99,2014,96232,0.04210940518719533
"Natural language processing for clinical and translational research DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.",Natural language processing for clinical and translational research,8920720,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease Association', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2014,160000,0.041840380748110056
"Natural language processing for clinical and translational research     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive.         PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.                ",Natural language processing for clinical and translational research,8640959,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease Association', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'public health relevance', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2014,580082,0.041840380748110056
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8729006,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Cloud Computing', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,554661,0.02123076526709777
"Automated Detection of Anomalous Accesses to Electronic Health Records  Health information technology (HIT) can lower costs, strengthen productivity, and promote safety. To realize such benefits on a large scale, healthcare organizations (HCOs) are adopting electronic health records (EHRs) to provide various capabilities. Yet, as EHRs and the healthcare workforce grow in diversity, so does their complexity. This is a concern because evidence suggests complex HIT can interrupt care delivery, contribute to medical errors, and expose patient data to privacy breaches. Moreover, such events tend to be discovered only after they transpire en masse, leading to negative media coverage, loss of patients' trust, and sanctions. Federal regulations now enable patients to receive accountings of who accessed their medical records during treatment, payment, and operations related activities. Yet, for patients to make sense of such accountings, they need to be provided with explanations regarding the extent to which accesses are normal in the context of routine HCO activities. We believe that relating specific accesses to patterns of healthcare operations can help explain how medical records are utilized. Unfortunately, many of the aforementioned problems manifest because EHR utilization patterns rarely guide the design and refinement of healthcare management practices. Thus, the overarching objective of our research is to develop novel strategies to automatically learn HCO behavior based on EHR usage. The past several years has witnessed a flurry of activity in this field, but it remains in its infancy and has only scratched the surface of care patterns and the types of anomalies that can be detected. Through this project, we propose to develop anomaly detection methods that integrate the semantics of healthcare operations and allow for the detection of workflows over time. This will enable HCOs and patients to audit in a meaningful way. Moreover, we believe the innovation and dissemination of such data mining strategies will enable HCOs to detect anomalous events that indicate system misuse and patients who require special attention, but also effectively audit business practices and discover inefficient workflows. The specific aims of this project are (1) to develop machine learning approaches, based on intrasession utilization patterns, to streamline EHR interface configuration and detect anomalous sessions, (2) to design a data mining framework, based on intersession EHR access patterns, to characterize HCO departmental interactions in patient treatment and detect anomalous events, and (3) to infer patient management pathways to consolidate redundant processes and detect deviations from anticipated workflows. In support of these goals, we will evaluate, compare, and contrast the workflows and anomalies in the EHR systems of two large medical centers. Additionally, we will ensure that our methods are integrated into an open source software system that can assist HCOs to extract, transform, and load (ETL) access data from EHRs, analyze such data for anomalies, and visualize the results in interfaces that enable review by healthcare administrators and patients. In doing so, we will be able to compare and contrast behavior of the workflows and multiple institutions and develop methods that appropriately generalize across EHR systems.  As electronic health record systems, and healthcare organizations, grow in diversity, so do their complexity, which can lead to inefficient documentation, management of patients, and expose patient data to privacy breaches. In this research, we will develop technologies, to be disseminated through an open source software suite, to learn patterns associated with healthcare operations, upon which anomaly detection techniques can be based. The specific goals of this project are to 1) model HCO users' intrasession behaviors when interacting with an EHR, 2) learn interdepartmental relations based on the accesses of common patients, and 3) infer patient management pathways based on the sequence of accesses to a patient's record.",Automated Detection of Anomalous Accesses to Electronic Health Records,8694383,R01LM010207,"['Accounting', 'Administrator', 'Admission activity', 'Adopted', 'Architecture', 'Attention', 'Behavior', 'Businesses', 'Caring', 'Case Manager', 'Case Study', 'Clinical', 'Collaborations', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Security', 'Detection', 'Documentation', 'Electronic Health Record', 'Employee', 'Engineering', 'Ensure', 'Entropy', 'Event', 'Goals', 'Grant', 'Graph', 'Healthcare', 'Healthcare Systems', 'Inpatients', 'Institution', 'Interdepartmental Relations', 'Internet', 'Lead', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Errors', 'Medical Records', 'Medical center', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Movement', 'Neonatal Intensive Care', 'Neonatology', 'Pathway interactions', 'Patient Care', 'Patients', 'Pattern', 'Patterns of Care', 'Phase', 'Practice Management', 'Primary Health Care', 'Privacy', 'Process', 'Productivity', 'Provider', 'Recording of previous events', 'Regulation', 'Research', 'Research Personnel', 'Safety', 'Semantics', 'Simulate', 'Surface', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trust', 'Variant', 'Work', 'base', 'care delivery', 'comparative', 'cost', 'data mining', 'design', 'follow-up', 'health information technology', 'infancy', 'innovation', 'novel strategies', 'open source', 'operation', 'organizational structure', 'payment', 'programs', 'software systems', 'ward']",NLM,VANDERBILT UNIVERSITY,R01,2014,392500,0.06481170926892102
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8714052,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,UNIVERSITY OF UTAH,R01,2014,579144,0.017581636758104324
"An Information Fusion Approach to Longitudinal Health Records     DESCRIPTION (provided by applicant): Our goal is to leverage an information fusion approach to integrate structured and unstructured information to generate a longitudinal health record (LHR) for accelerating the pace at which patients can be recruited into clinical trials. Because electronic health records (EHR) contain clinical summaries of a patient's clinical history, one would assume that they could be easily leveraged to automatically screen and identify potentially eligible patients. However most EHRs are not well designed to support screening of eligible patients and are composed of multiple data sources that are often redundant or inconsistent, stored in uncoordinated unstructured clinical narratives and structured data. These characteristics make EHRs difficult to use for matching patients against the complex event and temporal criteria of clinical trials protocols. This research proposes that an improved LHR, which contains a comprehensive clinical summary of a patient, can improve patient screening. We propose using a method of information fusion to generate this LHR, which merges information from multiple data sources, that addresses both the meaning and temporal nature of data, such that the resulting information is more accurate than would be possible if these sources were used individually.         The specific aims are to: 1) characterize the barriers of using EHR sources for screening in terms of data redundancy, inconsistency, lack of structure, and temporal imprecision; 2) automatically extract information from unstructured EHR sources necessary for screening patients against clinical trials eligibility criteria using natural language processing; 3) developan LHR appropriate for screening patients against eligibility criteria using information fusion methods based on semantic and temporal information; and 4) evaluate the accuracy of an LHR formed through information fusion for screening patients against clinical trials eligibility critera.         The respective hypotheses to be tested are: 1) Different parts of the EHR will contain variable amounts of redundancy, inconsistency, and temporal imprecision. Some sources will be more valuable for matching patients than others to clinical trials eligibility criteria. 2) Including th information contained in the unstructured notes will reduce the false positive rate of identifying potentially eligible patients over leveraging only the structured data in the EHR. 3) By using information fusion methods based on leveraging semantic and temporal information on a combination of structured and unstructured data, we will be able to accurately summarize the information contained in uncoordinated EHR data sources into an LHR that can be used for screening patients for clinical trials. 4) The use of information fusion to generate a longitudinal health record will increase the sensitivity and specificity of electronic clinical trial screening ver using a traditional EHR.         With an LHR formed through information fusion for screening patients for clinical trials eligibilit, we will be able to not only reduce the amount of staff effort required to recruit a patient into a clinical trial, but also accelerate the pace at which clinical trials can be conducted.                  Narrative This project is focused on generating a longitudinal health record for accelerating the pace at which patients can be recruited into clinical trials. Accelerating the pace at which patients are recruited into clinical trials has the potential for improving the speed at which new treatments are made available to the public.",An Information Fusion Approach to Longitudinal Health Records,8722624,R01LM011116,"['Address', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Complex', 'Consultations', 'Data', 'Data Sources', 'Electronic Health Record', 'Electronics', 'Eligibility Determination', 'Enrollment', 'Event', 'Failure', 'Goals', 'Hour', 'Laboratories', 'Manuals', 'Measures', 'Methods', 'Natural Language Processing', 'Nature', 'Patient Recruitments', 'Patients', 'Process', 'Randomized', 'Randomized Controlled Trials', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Semantics', 'Sensitivity and Specificity', 'Series', 'Source', 'Speed', 'Structure', 'Techniques', 'Testing', 'Time', 'TimeLine', 'Work', 'base', 'cohort', 'cost', 'design', 'falls', 'health record', 'improved', 'screening']",NLM,OHIO STATE UNIVERSITY,R01,2014,300537,0.03153867919077024
"Patient Medical History Representation, Extraction, and Inference from EHR Data     DESCRIPTION (provided by applicant): The significance of developing tools for automatically harvesting temporal constraints of clinical events from Electronic Health Records (EHR) cannot be overestimated. Efficient analysis of the temporal aspects in EHR data could boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine.     One big challenge we are facing is to automatically untangle and linearize the temporal constraints of clinical events embedded in highly diverse large-scale EHR data. Barriers to temporal data modeling, normalization, extraction, and reasoning have precluded the efficient use of EHR data sources for event history evaluation and trending analysis: (1) The current federally-supported EHR data normalization tools do not focus on the time aspect of unstructured data yet; (2) Existing time models focus only on structured data with absolute time, lack of supporting reasoning systems, or only offer application-specific partial solutions which cannot be adopted by the complex EHR data; (3) Current temporal information extraction approaches are either difficult to be adopted to EHR data, not scalable, or only offers application-specific partial solution.     This proposed project fills in the current gaps among ontologies, Natural Language Processing (NLP), and EHR-based clinical research for temporal data representation, normalization, extractions, and reasoning. We propose to develop novel approaches for automatic temporal data representation, normalization and reasoning for large, diverse, and heterogeneous EHR data and prepare the integrated data for further analysis. We will build new reasoning and extraction capacities on our TIMER (Temporal Information Modeling, Extracting, and Reasoning) framework to provide an end-to-end, open-source, standard-conforming software package. TIMER will be built on strong prior work by our team. We will develop new features in our CNTRO (Clinical Narrative Temporal Relation Ontology) for semantically defining the time domain and representing temporal data in complex EHR data. On top of the new developed CNTRO semantics, we will implement temporal relation reasoning capacities to automatically normalize temporal expressions, compute and infer temporal relations, and resolve ambiguities. We will leverage existing NLP tools and work on top of these tools to develop new extraction approaches to fill in the current gaps between NLP approaches and ontology-based reasoning approaches. We will adapt the SHARPn EHR data normalization pipeline and cTAKES for extracting and normalizing clinical event mentions from clinical narratives. We will explore an innovative approach for temporal relation extraction and event coreference, and make it work with the TIMER framework. We will evaluate the system using Diabetes Mellitus (DM) and colorectal cancer (CRC) patient cohorts from two insititutions. Each component will be tested separately first followed by an evaluation of the whole framework. Results such as precision, recall, and f-measure will be reported.                 Project Narrative The significance of developing capabilities for automatically harvesting temporal constraints for clinical events from Electronic Health Records (EHR) cannot be overestimated. A substantial portion of the information in the EHR is historical in nature. Patient medical history can be long, especially in complex patients. The proposed work, by offering an end-to-end open-source framework for automatically extracting, normalizing, and reasoning clinically-important time-relevant information from large-scale EHR data, can boost an array of clinical and translational research such as disease progression studies, decision support systems, and personalized medicine; as well as facilitate clinical practice for early disease detection, post-treatment care, and patient-clinician communication.","Patient Medical History Representation, Extraction, and Inference from EHR Data",8760594,R01LM011829,"['Address', 'Adopted', 'Aftercare', 'Archives', 'Automated Annotation', 'Big Data', 'Cancer Patient', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Colorectal Cancer', 'Communication', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Support Systems', 'Detection', 'Diabetes Mellitus', 'Disease', 'Disease Progression', 'Electronic Health Record', 'Evaluation', 'Event', 'Goals', 'Gold', 'Harvest', 'Human', 'Institutes', 'Maps', 'Measures', 'Medical History', 'Medical Records', 'Medicine', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patient Care', 'Patients', 'Performance', 'Recording of previous events', 'Registries', 'Reporting', 'Resolution', 'Semantics', 'Solutions', 'Structure', 'System', 'Testing', 'Time', 'Translational Research', 'Work', 'base', 'clinical practice', 'cohort', 'data modeling', 'data structure', 'information model', 'innovation', 'novel strategies', 'open source', 'programs', 'tool', 'trend']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2014,398307,0.058406506835919574
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,8660067,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial/ethnic difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2014,417795,0.0014902703511776856
"SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs)  ﻿    DESCRIPTION (provided by applicant): Smart algorithms that effectively analyze patient care data can enhance clinical communication to save lives. In 2000, the Institute of Medicine estimated 98,000 preventable patient deaths occur annually in US hospitals due to miscommunication [1]. Electronic health records (EHRs) were expected to facilitate accurate communication within the care team and provide data to enable automated clinical decision support systems. Unfortunately, miscommunication remains a significant cause of patient deaths [2]. Providers are now required to demonstrate meaningful use of EHR systems to improve quality of care and patient outcomes. Despite this, providers continue to report that EHR systems are cumbersome and interfere with care-team communication. Information entered into an EHR is rarely used by nurses due to the time and difficulty involved in its retrieval. As a result, nurses continue to verbally convey critical patient care information to the next nurse during shift changes. Verbal report or hand-off, where critical patient information is exchanged in only minutes, is inefficient. Worse, it is highly susceptible to communication errors. Broader Impacts: Research: 1) Increase patient safety; 2) Provide preliminary data to expand this work to include physician-physician and physician-RN communication and decision-making in the EHR; 3) Share our discoveries to inform other industries who may also benefit from this technology. Education: 1) Contribute to curriculum enhancements whereby RN students learn strategies to recognize and effectively communicate CEs; 2) As part of curriculum enhancements, include healthcare applications for computer and information science students; 3) Disseminate findings via academic publications, professional meetings, a project website and social media. Mentoring: 1) Mentor budding scientists in the roles of research assistants (RAs) and post doctoral fellows studying nursing and computer science to forge collaborative interdisciplinary relationships for ongoing research; 2) Interest and recruit underrepresented students in STEM and careers in healthcare. RELEVANCE (See instructions): The electronic health record (EHR) has been thought to be a top! to decrease patient deaths related to miscommunication. However, the current EHR falls short of this goal. We propose to develop and test an algorithm that will augment the EHR to more effectively assist nurses in decision-making and communication, ultimately increasing patient safety.                 n/a",SCH: Enhancing Nurse Decision-Making via Augmented Communication Tools (ACTs) ,8894220,R01EB020395,"['Agreement', 'Algorithms', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Decision Support Systems', 'Communication', 'Communication Tools', 'Computer Simulation', 'Conscious', 'Data', 'Data Reporting', 'Decision Making', 'Discipline of Nursing', 'Documentation', 'Education', 'Educational Curriculum', 'Effectiveness', 'Electronic Health Record', 'Electronics', 'Event', 'Fever', 'Goals', 'Hand', 'Health', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Imagery', 'Industry', 'Information Sciences', 'Institute of Medicine (U.S.)', 'Instruction', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mentors', 'Nurses', 'Outcome', 'Output', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Postdoctoral Fellow', 'Principal Investigator', 'Provider', 'Publications', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'STEM career', 'Sampling', 'Scientist', 'Severities', 'Students', 'Symptoms', 'System', 'Technology', 'Testing', 'Time', 'Validity and Reliability', 'Work', 'computer science', 'falls', 'forging', 'high risk', 'improved', 'interest', 'light weight', 'meetings', 'patient safety', 'prototype', 'respiratory', 'social', 'usability', 'web site']",NIBIB,UNIVERSITY OF ARIZONA,R01,2014,251572,0.056010087219850214
"Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership    DESCRIPTION (provided by applicant): The Seattle eMERGE project aims to bring personal genomics to practice settings by taking advantage of the extensive electronic medical record (EMR) and biorepository of Group Health Cooperative (GH), including a 33-year pharmacy database and longitudinal data on an aging population. Algorithms developed in eMERGE I will be used to combine genome-wide association studies with phenotypes mined from EMRs to discover new polymorphism-phenotype relationships. Target phenotypes are infectious disease susceptibility, specifically to Clostridium difficile diarrhea, shingles from varicella zoster virus, and fungal nail infection, responses to antihypertensive drugs, serotonin-specific reuptake inhibitors, and statins, including adverse events. A new algorithm will follow longitudinal glycemia and hematocrit trajectories, and a novel automated method will detect karyotype abnormalities for assessing correlation to myelodysplasia and leukemia. Data will also support phenotypes investigated at other eMERGE sites. To create a model for introducing genomics into clinical practice, successful needs assessment methods from eMERGE I will engage stakeholders in guiding development of prototype EMR user interfaces in a clinical decision support format. The test case will be human leukocyte antigen-typing for an adverse drug reaction and the setting will be the patient-centered medical home care model developed at GH. This proposal provides the eMERGE network and its collaborators with the Seattle team's unique expertise in using natural language processing (NLP) to extract information from EMRs, and assisting in adoption of NLP methods. To disseminate eMERGE results and foster collaborations, it takes advantage of leadership positions of the investigators, including partners within eMERGE, other consortia and the HMO Research network, especially the potential for developments supported by the NIH Director's Common Fund in biobanking and megaepidemiology. Completion of the aims will reveal new, medically useful markers, improve the linking of high-throughput genomic methods to EMR data, and develop policies and practices for bringing individualized evidence-based medicine to communities.       RELEVANCE (See instrucfions): To advance personalized medicine-treatment and preventive care based on individual traits; this project matches small differences in DNA to infectious disease susceptibility and response to statins, serotonin- specific reuptake inhibitors (SSRIs) and blood pressure medications. Methods to use these results in clinical care will be guided by focus groups of patients and caregivers in the patient-centered Group Health system.              n/a",Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership,8721471,U01HG006375,"['Address', 'Adoption', 'Adverse event', 'Algorithms', 'Anemia', 'Antihypertensive Agents', 'Bioethics', 'Blood Pressure', 'Bone Marrow Diseases', 'Caregivers', 'Caring', 'Chromosome abnormality', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Consent', 'DNA', 'Data', 'Development', 'Diabetes Mellitus', 'Diarrhea', 'Disease', 'Disease susceptibility', 'Dysmyelopoietic Syndromes', 'Ensure', 'Epidemiology', 'Evidence Based Medicine', 'Excision', 'Focus Groups', 'Fostering', 'Funding', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'HLA Antigens', 'Health', 'Health system', 'Healthcare', 'Hematocrit procedure', 'Herpes zoster disease', 'Herpesvirus Type 3', 'Incidental Findings', 'Individual', 'Infection', 'Karyotype', 'Knowledge', 'Leadership', 'Libraries', 'Link', 'Low-Density Lipoproteins', 'Medical', 'Medical Records', 'Medical center', 'Medicine', 'Mental Depression', 'Methods', 'Mining', 'Modeling', 'Nail plate', 'Natural Language Processing', 'Needs Assessment', 'Other Genetics', 'Outcome', 'Oxidoreductase', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phenotype', 'Policies', 'Population', 'Positioning Attribute', 'Predisposition', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Qualifying', 'Reaction', 'Research', 'Research Personnel', 'Resources', 'Serotonin', 'Single Nucleotide Polymorphism', 'Site', 'Solutions', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Variant', 'aging population', 'base', 'biobank', 'clinical application', 'clinical care', 'clinical practice', 'clinical research site', 'clinically relevant', 'comparative effectiveness', 'design', 'effectiveness research', 'ethical legal social implication', 'evidence base', 'experience', 'genetic technology', 'genome wide association study', 'improved', 'inhibitor/antagonist', 'leukemia', 'leukocyte antigen typing', 'longitudinal database', 'member', 'novel', 'patient home care', 'patient oriented', 'patient population', 'prototype', 'response', 'reuptake', 'skills', 'standard care', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2014,191107,0.025982980640413204
"Encoding and Processing Patient Allergy Information in EHRs     DESCRIPTION (provided by applicant): Allergies affect one in five Americans and are the 5th leading chronic disease in the U.S. Each year, allergies account for more than 17 million outpatient office visits. Although documenting and exchanging allergy information in electronic health records (EHRs) is becoming increasingly important, we still face multiple challenges. These include: lack of well-adopted standard terminologies for representing allergies, frequent entry of allergy information as free-text, and no existing process for reconciling allergy information. In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) conduct analyses on standard terminologies and a large allergy repository to build a comprehensive knowledge base for representing allergy information; 2) design, develop and evaluate a natural language processing (NLP) module for extracting and encoding free-text allergy information and integrate it with an existing NLP system; 3) measure the feasibility and efficiency of the proposed NLP system for the new process of allergy reconciliation; and 4) distribute our methods and tool, so they are widely available to other researchers and healthcare institutions for non-commercial use.         PUBLIC HEALTH RELEVANCE: Managing allergy information within the electronic health record (EHR) is vital to ensuring patient safety. The goal of this study is to propose a comprehensive solution to assess existing terminology standards and knowledge bases for representing allergy information, develop and evaluate a natural language processing (NLP) system for extracting and encoding allergy information from free-text clinical documents, and finally measure the feasibility of using NLP output to facilitate the allergy reconciliation proces.            ",Encoding and Processing Patient Allergy Information in EHRs,8741955,R01HS022728,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2014,489854,0.010627888304786772
"Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach     DESCRIPTION (provided by applicant): Physician progress notes contain information essential to patient care, including findings from history and physical exam, interpretation of tests, assessment and treatment plans. However in the transition from paper to electronic physician notes, many physicians spend more time creating them, which has led to the use of time-saving measures such as copy/paste and templates that have degraded note accuracy and quality. This threatens the usefulness of notes not only for their most important use-patient care-but also for research, quality improvement, and in supporting reimbursement. To address these problems, we propose a project with the following specific aims: 1. To refine and implement a new voice-generated enhanced electronic note system (VGEENS) integrating voice recognition with natural language processing and links to the electronic medical record (EMR) to improve note accuracy and timeliness. 2. To evaluate VGEENS using a randomized trial with 30 internal medicine physicians in each arm to assess electronic note accuracy, quality, timeliness, and user satisfaction. Intervention physicians will use VGEENS, while the control physicians will continue with note creation as they normally would. This novel approach has the potential to improve note accuracy while reducing delays in making progress notes in EMRs available to other clinicians. It leverages rapidly improving voice recognition and NLP technologies to permit physicians to use a natural, fast method-human voice-to convey their observation and thoughts into the EMR record.         PUBLIC HEALTH RELEVANCE: Physician documentation of a patient visit contains information that is used in that patient's care. This information includes findings from a patient' history and physical exam, interpretation of necessary tests, the problem assessment and treatment plan. However, in the transition from paper to electronic physician notes, many physicians are spending more time creating these notes. This has led to use of time-saving measures that have degraded the accuracy and ease of use of patient notes. By the end of this project, we expect to have developed, used and evaluated a new method for creating electronic physician notes that both improve accuracy and timely availability of inpatient progress notes.            ","Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach",8805997,R21HS023631,[' '],AHRQ,UNIVERSITY OF WASHINGTON,R21,2014,154347,0.017982418754240875
"Automated Dynamic Lists for Efficient Electronic Health Record Management DESCRIPTION (provided by APPLICANT): Medical errors are recognized as the cause of numerous deaths, and even if some are difficult to avoid, many are preventable. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information in the electronic health record (EHR). Unfortunately, a substantial proportion of the information available in the EHR is only mentioned in narrative clinical documents. Electronic lists of problems and allergies are available in most EHRs, but they require manual management by their users, to add new problems, modify existing ones, and the removal of the ones that are irrelevant. Consequently, these electronic lists are often incomplete, inaccurate, and out of date. Clinacuity, Inc. proposes to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the EHR of patients suffering from cancer. To establish the merit and feasibility of such a system, we will work on the following objectives: 1) create a reference standard for training and testing the information extraction application, a reference standard including a random sample of de-identified clinical narratives from patients treated at the Huntsman Cancer Institute Cancer Clinics (Salt Lake City, Utah), with problems and allergies annotated by domain experts; 2) develop a prototype to automatically extract medical problems and allergies, implementing a novel stepwise hybrid approach to maximize sensitivity first, and also enhance positive predictive value; and 3) test the prototype with the aforementioned reference standard, using a cross-validation approach for training and testing. Commercial application: The system Clinacuity proposes will not only help healthcare providers maintain complete and timely lists of problems and allergies, providing them with an efficient overview of a patient, but also help healthcare organizations attain meaningful use requirements. The proposed system has potential commercial applications in inpatient and outpatient settings, increasing the efficiency of busy healthcare providers by saving time, and aiding healthcare organizations in demonstrating ""meaningful use"" and obtaining Centers for Medicare and Medicaid Services incentive payments. Clinacuity will further extend the commercial potential of the system and it is output, using modular design principles allowing utilization of each module independently, and enhancing its local adaptability for easier deployment. PUBLIC HEALTH RELEVANCE: Medical errors cause numerous deaths, and even if some are difficult to avoid, many could be prevented. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information such as entries in electronic lists of problems and allergies. Such lists are available in most electronic health records, but they require manual management and are often incomplete, inaccurate, and out of date. On the other hand, clinical text reports contain the majority of the patient information, including problems and allergies. The overall goal of this project is to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the electronic health record.",Automated Dynamic Lists for Efficient Electronic Health Record Management,8926527,R41CA180190,"['Cancer Patient', 'Caring', 'Cessation of life', 'Cities', 'Clinic', 'Clinical', 'Code', 'Complex', 'Disease', 'Electronic Health Record', 'Electronics', 'Ensure', 'Event', 'Excision', 'Goals', 'Health', 'Health Personnel', 'Healthcare', 'Hybrids', 'Hypersensitivity', 'Incentives', 'Inpatients', 'Institutes', 'Laboratories', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Medical Errors', 'Medication Errors', 'Methods', 'Metric', 'Natural Language Processing', 'Outpatients', 'Output', 'Patients', 'Pharmaceutical Preparations', 'Predictive Value', 'Reference Standards', 'Reporting', 'Risk', 'Sampling', 'Sodium Chloride', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Work', 'base', 'commercial application', 'computerized physician order entry', 'design', 'improved', 'novel', 'payment', 'prevent', 'prototype']",NCI,"CLINACUITY,INC.",R41,2014,24960,0.04919459378265855
"Automated Dynamic Lists for Efficient Electronic Health Record Management     DESCRIPTION (provided by APPLICANT): Medical errors are recognized as the cause of numerous deaths, and even if some are difficult to avoid, many are preventable. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information in the electronic health record (EHR). Unfortunately, a substantial proportion of the information available in the EHR is only mentioned in narrative clinical documents. Electronic lists of problems and allergies are available in most EHRs, but they require manual management by their users, to add new problems, modify existing ones, and the removal of the ones that are irrelevant. Consequently, these electronic lists are often incomplete, inaccurate, and out of date. Clinacuity, Inc. proposes to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the EHR of patients suffering from cancer. To establish the merit and feasibility of such a system, we will work on the following objectives: 1) create a reference standard for training and testing the information extraction application, a reference standard including a random sample of de-identified clinical narratives from patients treated at the Huntsman Cancer Institute Cancer Clinics (Salt Lake City, Utah), with problems and allergies annotated by domain experts; 2) develop a prototype to automatically extract medical problems and allergies, implementing a novel stepwise hybrid approach to maximize sensitivity first, and also enhance positive predictive value; and 3) test the prototype with the aforementioned reference standard, using a cross-validation approach for training and testing. Commercial application: The system Clinacuity proposes will not only help healthcare providers maintain complete and timely lists of problems and allergies, providing them with an efficient overview of a patient, but also help healthcare organizations attain meaningful use requirements. The proposed system has potential commercial applications in inpatient and outpatient settings, increasing the efficiency of busy healthcare providers by saving time, and aiding healthcare organizations in demonstrating ""meaningful use"" and obtaining Centers for Medicare and Medicaid Services incentive payments. Clinacuity will further extend the commercial potential of the system and it is output, using modular design principles allowing utilization of each module independently, and enhancing its local adaptability for easier deployment.         PUBLIC HEALTH RELEVANCE: Medical errors cause numerous deaths, and even if some are difficult to avoid, many could be prevented. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information such as entries in electronic lists of problems and allergies. Such lists are available in most electronic health records, but they require manual management and are often incomplete, inaccurate, and out of date. On the other hand, clinical text reports contain the majority of the patient information, including problems and allergies. The overall goal of this project is to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the electronic health record.           ",Automated Dynamic Lists for Efficient Electronic Health Record Management,8830154,R41CA180190,"['Cancer Patient', 'Caring', 'Cessation of life', 'Cities', 'Clinic', 'Clinical', 'Code', 'Complex', 'Disease', 'Electronic Health Record', 'Electronics', 'Ensure', 'Event', 'Excision', 'Goals', 'Health Personnel', 'Healthcare', 'Hybrids', 'Hypersensitivity', 'Incentives', 'Inpatients', 'Institutes', 'Laboratories', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Medical Errors', 'Medication Errors', 'Methods', 'Metric', 'Natural Language Processing', 'Outpatients', 'Output', 'Patients', 'Pharmaceutical Preparations', 'Predictive Value', 'Reference Standards', 'Reporting', 'Risk', 'Sampling', 'Sodium Chloride', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Work', 'base', 'commercial application', 'computerized physician order entry', 'design', 'improved', 'novel', 'payment', 'prevent', 'prototype', 'public health relevance']",NCI,"CLINACUITY,INC.",R41,2014,119730,0.04919459378265855
"Learning from patient safety events: A case base tool kit     DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture.         PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.            ",Learning from patient safety events: A case base tool kit,8818528,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2014,249655,0.016391971940315025
"NLP-enabled decision support for cervical cancer screening and surveillance     DESCRIPTION (provided by applicant): Although cervical cancer is preventable, it still continues to be a leading cause of death. Following the evidence- based guidelines for cervical cancer prevention is challenging for healthcare providers, due to which many patients do not receive the optimal preventive care. Clinical decision support (CDS) systems can potentially improve the care delivery. However, the current CDS systems only identify patients overdue for screening, and do not suggest the optimal screening interval. Moreover they do not help with surveillance of patients with abnormal screening results. This is because the existing systems lack the capability to process free- text clinical reports that contain information needed for applying the guidelines. Hence there is a critical need for natural language processing (NLP)-enabled CDS systems that can utilize discrete as well as free-text patient information for enhancing the decision support. Our long-term goal is to improve healthcare delivery of cervical cancer prevention with guideline based reminders. The central hypothesis is that NLP- enabled CDS system will significantly improve the quality of care delivery for cervical cancer prevention. The rationale is that use of NLP will improve granularity of the guideline implementation, which will in-turn enhance care delivery. As preliminary work we have developed an NLP-enabled CDS system that automatically interprets the patient information from the electronic health record and applies the national guidelines to compute the optimal recommendation for screening and surveillance. We have performed validation of the system in a non-clinical setting.1 In this application we will proceed towards deployment of the system in the clinical setting, and will carry out studies for measuring the impact on the quality of care delivery. In ai one, we will validate the system in the clinical setting and will optimize its usability and workflw integration. In aim two, we will test the hypothesis that reminders from the NLP-enabled CDS system to primary care providers will improve the quality of care delivery, by performing a one year intervention control study across four sites of a primary care practice. In aim three, we will test the hypothesis that reminders to non-adherent high-risk patients will improve their surveillance rates, by performing a randomized intervention study for three months. In this study, care coordinators will utilize the CDS system for sending reminders to patients that are non-adherent and at high risk due to abnormal screenings. The main contribution of this project will be knowledge about the effectiveness of NLP in enhancing the impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This project is innovative because the CDS system will utilize NLP to generate screening reminders for normal patients and surveillance reminders for patients with abnormal findings. This is a major advancement over existing systems that can only identify patients for screening.         PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because it will yield knowledge about the effectiveness of natural language processing (NLP) to enhance impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This research will foster implementation of similar CDS systems across the nation for cervical cancer prevention and for other decision problems, which will improve the quality of healthcare delivery. Thus, the proposed research is relevant to AHRQ's mission to improve the quality, safety, efficiency, and effectiveness of health care for all Americans.            ",NLP-enabled decision support for cervical cancer screening and surveillance,8678798,R21HS022911,[' '],AHRQ,MAYO CLINIC ROCHESTER,R21,2014,145229,0.022069425250648964
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers.         PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.            ",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,8629996,R01GM103859,"['Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Medicine', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Population Study', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'improved', 'large-scale database', 'novel', 'open source', 'public health relevance', 'rapid growth', 'rare variant', 'response', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY,R01,2014,648591,0.009438619806228447
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8532984,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2013,247288,0.01171561179007688
"Natural language processing for clinical and translational research     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive.         PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.                ",Natural language processing for clinical and translational research,8505753,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease Association', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'public health relevance', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2013,630706,0.041840380748110056
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8531347,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2013,512888,0.02123076526709777
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.       PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.         ","Annotation, development and evaluation for clinical information extraction",8501543,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'clinical care', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,370221,0.037569989993941934
"An Information Fusion Approach to Longitudinal Health Records     DESCRIPTION (provided by applicant): Our goal is to leverage an information fusion approach to integrate structured and unstructured information to generate a longitudinal health record (LHR) for accelerating the pace at which patients can be recruited into clinical trials. Because electronic health records (EHR) contain clinical summaries of a patient's clinical history, one would assume that they could be easily leveraged to automatically screen and identify potentially eligible patients. However most EHRs are not well designed to support screening of eligible patients and are composed of multiple data sources that are often redundant or inconsistent, stored in uncoordinated unstructured clinical narratives and structured data. These characteristics make EHRs difficult to use for matching patients against the complex event and temporal criteria of clinical trials protocols. This research proposes that an improved LHR, which contains a comprehensive clinical summary of a patient, can improve patient screening. We propose using a method of information fusion to generate this LHR, which merges information from multiple data sources, that addresses both the meaning and temporal nature of data, such that the resulting information is more accurate than would be possible if these sources were used individually.         The specific aims are to: 1) characterize the barriers of using EHR sources for screening in terms of data redundancy, inconsistency, lack of structure, and temporal imprecision; 2) automatically extract information from unstructured EHR sources necessary for screening patients against clinical trials eligibility criteria using natural language processing; 3) developan LHR appropriate for screening patients against eligibility criteria using information fusion methods based on semantic and temporal information; and 4) evaluate the accuracy of an LHR formed through information fusion for screening patients against clinical trials eligibility critera.         The respective hypotheses to be tested are: 1) Different parts of the EHR will contain variable amounts of redundancy, inconsistency, and temporal imprecision. Some sources will be more valuable for matching patients than others to clinical trials eligibility criteria. 2) Including th information contained in the unstructured notes will reduce the false positive rate of identifying potentially eligible patients over leveraging only the structured data in the EHR. 3) By using information fusion methods based on leveraging semantic and temporal information on a combination of structured and unstructured data, we will be able to accurately summarize the information contained in uncoordinated EHR data sources into an LHR that can be used for screening patients for clinical trials. 4) The use of information fusion to generate a longitudinal health record will increase the sensitivity and specificity of electronic clinical trial screening ver using a traditional EHR.         With an LHR formed through information fusion for screening patients for clinical trials eligibilit, we will be able to not only reduce the amount of staff effort required to recruit a patient into a clinical trial, but also accelerate the pace at which clinical trials can be conducted.                  Narrative This project is focused on generating a longitudinal health record for accelerating the pace at which patients can be recruited into clinical trials. Accelerating the pace at which patients are recruited into clinical trials has the potential for improving the speed at which new treatments are made available to the public.",An Information Fusion Approach to Longitudinal Health Records,8532982,R01LM011116,"['Address', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Complex', 'Consultations', 'Data', 'Data Sources', 'Electronic Health Record', 'Electronics', 'Eligibility Determination', 'Enrollment', 'Event', 'Failure', 'Goals', 'Hour', 'Laboratories', 'Manuals', 'Measures', 'Methods', 'Natural Language Processing', 'Nature', 'Patient Recruitments', 'Patients', 'Process', 'Randomized', 'Randomized Controlled Trials', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Semantics', 'Sensitivity and Specificity', 'Series', 'Source', 'Speed', 'Structure', 'Techniques', 'Testing', 'Time', 'TimeLine', 'Work', 'base', 'cohort', 'cost', 'design', 'falls', 'health record', 'improved', 'screening']",NLM,OHIO STATE UNIVERSITY,R01,2013,312584,0.03153867919077024
"EHR-based patient safety: Automated error detection in neonatal intensive care     DESCRIPTION (provided by applicant): In the field of neonatal patient safety, the paucity of systematic research is a critical barrier to progress. Notably missing are studies that meticulously investigate Electronic Health Records (EHR) and information technology in detecting neonatal intensive care-related errors. The expert panel at the National Institute of Child Health and Human Development (NICHHD) identified multiple gaps in the current knowledge of neonatal patient safety research. The proposed work is a well focused response to three dimensions of the Funding Opportunity Announcement:  1.Develop prospective and retrospective study designs to collect data on patient safety and adverse events.  2.Study the strength and limitations of current methods of error reporting systems.  3.Study the usefulness of commercial IT systems and EHRs in reducing medical errors.  In our study we seek to shift patient safety research toward an automated and computerized approach to achieve a more comprehensive patient safety paradigm. We will develop novel Electronic Health Record (EHR) content-based automated algorithms that are new to patient safety research to 1) detect errors (Aim 1) and 2) categorize subsequent harm (Aim 2). State of the art information extraction and statistical classification techniques from the field of clinical Natural Language Processing (NLP) will be adapted to the patient safety research tasks.  In Aim 1 we will fill the gap in the literatre by implementing a focused manual review of 700 charts (one full year of patient admissions at our institution) in one of the largest Neonatal Intensive Care Units (NICU) in the nation. Using a trigger tool, we will identify errors occurring in three specified categories - laboratory test errrs, medication/fluid errors, and airway management errors. We will develop novel algorithms for automated EHR-based detection of the errors and evaluate the performance of the new algorithms against the performance of both trigger tool review by human chart reviewers (current gold standard) and the voluntary incident reporting system (accepted standard). In Aim 2, we will study the utility of novel EHR-based information extraction and statistical algorithms for the automated categorization of errors according to the resulting level of harm.  Our proposed work has the potential to accomplish a paradigm shift in the methods of neonatal patient safety research and practice. The study is a fundamental step to automating patient safety monitoring on a large scale and improving error identification and patient safety in NICUs for millions of children every year.          We are developing an automated error detection technique to improve the safety of newborn babies during hospital care. Our work is the first known attempt to use text analysis in the electronic health records on a large scale to reduce the cost while at the same time increase the speed and comprehensiveness of error detection in the clinical care of newborns.            ",EHR-based patient safety: Automated error detection in neonatal intensive care,8517787,R21HD072883,"['Adverse event', 'Algorithms', 'Caring', 'Categories', 'Cessation of life', 'Characteristics', 'Child', 'Child health care', 'Classification', 'Clinical', 'Data', 'Detection', 'Dimensions', 'Electronic Health Record', 'Foundations', 'Funding Opportunities', 'Gold', 'Hospitals', 'Hour', 'Human', 'Human Development', 'Information Systems', 'Information Technology', 'Institutes', 'Institution', 'Intervention', 'Knowledge', 'Laboratories', 'Liquid substance', 'Literature', 'Manuals', 'Medical Errors', 'Medication Errors', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Neonatal', 'Neonatal Intensive Care', 'Neonatal Intensive Care Units', 'Newborn Infant', 'Patient Admission', 'Patient Monitoring', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Prevention', 'Process', 'Prospective Studies', 'Reporting', 'Research', 'Research Design', 'Retrospective Studies', 'Risk', 'Safety', 'Solid', 'Specific qualifier value', 'Speed', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Work', 'base', 'clinical care', 'computerized', 'cost', 'design', 'experience', 'improved', 'indexing', 'innovation', 'novel', 'patient safety', 'phrases', 'response', 'tool']",NICHD,CINCINNATI CHILDRENS HOSP MED CTR,R21,2013,254095,0.04868220365015502
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8536940,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2013,292186,0.02110221778180246
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,8438732,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial/ethnic difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2013,440117,0.0014902703511776856
"Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership    DESCRIPTION (provided by applicant): The Seattle eMERGE project aims to bring personal genomics to practice settings by taking advantage of the extensive electronic medical record (EMR) and biorepository of Group Health Cooperative (GH), including a 33-year pharmacy database and longitudinal data on an aging population. Algorithms developed in eMERGE I will be used to combine genome-wide association studies with phenotypes mined from EMRs to discover new polymorphism-phenotype relationships. Target phenotypes are infectious disease susceptibility, specifically to Clostridium difficile diarrhea, shingles from varicella zoster virus, and fungal nail infection, responses to antihypertensive drugs, serotonin-specific reuptake inhibitors, and statins, including adverse events. A new algorithm will follow longitudinal glycemia and hematocrit trajectories, and a novel automated method will detect karyotype abnormalities for assessing correlation to myelodysplasia and leukemia. Data will also support phenotypes investigated at other eMERGE sites. To create a model for introducing genomics into clinical practice, successful needs assessment methods from eMERGE I will engage stakeholders in guiding development of prototype EMR user interfaces in a clinical decision support format. The test case will be human leukocyte antigen-typing for an adverse drug reaction and the setting will be the patient-centered medical home care model developed at GH. This proposal provides the eMERGE network and its collaborators with the Seattle team's unique expertise in using natural language processing (NLP) to extract information from EMRs, and assisting in adoption of NLP methods. To disseminate eMERGE results and foster collaborations, it takes advantage of leadership positions of the investigators, including partners within eMERGE, other consortia and the HMO Research network, especially the potential for developments supported by the NIH Director's Common Fund in biobanking and megaepidemiology. Completion of the aims will reveal new, medically useful markers, improve the linking of high-throughput genomic methods to EMR data, and develop policies and practices for bringing individualized evidence-based medicine to communities.       RELEVANCE (See instrucfions): To advance personalized medicine-treatment and preventive care based on individual traits; this project matches small differences in DNA to infectious disease susceptibility and response to statins, serotonin- specific reuptake inhibitors (SSRIs) and blood pressure medications. Methods to use these results in clinical care will be guided by focus groups of patients and caregivers in the patient-centered Group Health system.              n/a",Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership,8728454,U01HG006375,"['Address', 'Adoption', 'Adverse event', 'Algorithms', 'Anemia', 'Antihypertensive Agents', 'Bioethics', 'Blood Pressure', 'Bone Marrow Diseases', 'Caregivers', 'Caring', 'Chromosome abnormality', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Consent', 'DNA', 'Data', 'Development', 'Diabetes Mellitus', 'Diarrhea', 'Disease', 'Disease susceptibility', 'Dysmyelopoietic Syndromes', 'Ensure', 'Epidemiology', 'Evidence Based Medicine', 'Excision', 'Focus Groups', 'Fostering', 'Funding', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'HLA Antigens', 'Health', 'Health system', 'Healthcare', 'Hematocrit procedure', 'Herpes zoster disease', 'Herpesvirus Type 3', 'Incidental Findings', 'Individual', 'Infection', 'Karyotype', 'Knowledge', 'Leadership', 'Libraries', 'Link', 'Low-Density Lipoproteins', 'Medical', 'Medical Records', 'Medical center', 'Medicine', 'Mental Depression', 'Methods', 'Mining', 'Modeling', 'Nail plate', 'Natural Language Processing', 'Needs Assessment', 'Other Genetics', 'Outcome', 'Oxidoreductase', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phenotype', 'Policies', 'Population', 'Positioning Attribute', 'Predisposition', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Qualifying', 'Reaction', 'Research', 'Research Personnel', 'Resources', 'Serotonin', 'Single Nucleotide Polymorphism', 'Site', 'Solutions', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Variant', 'aging population', 'base', 'biobank', 'clinical application', 'clinical care', 'clinical practice', 'clinical research site', 'clinically relevant', 'comparative effectiveness', 'design', 'effectiveness research', 'ethical legal social implication', 'evidence base', 'experience', 'genetic technology', 'genome wide association study', 'improved', 'inhibitor/antagonist', 'leukemia', 'leukocyte antigen typing', 'longitudinal database', 'member', 'novel', 'patient home care', 'patient oriented', 'patient population', 'prototype', 'response', 'reuptake', 'skills', 'standard care', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2013,197687,0.025982980640413204
"Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership    DESCRIPTION (provided by applicant): The Seattle eMERGE project aims to bring personal genomics to practice settings by taking advantage of the extensive electronic medical record (EMR) and biorepository of Group Health Cooperative (GH), including a 33-year pharmacy database and longitudinal data on an aging population. Algorithms developed in eMERGE I will be used to combine genome-wide association studies with phenotypes mined from EMRs to discover new polymorphism-phenotype relationships. Target phenotypes are infectious disease susceptibility, specifically to Clostridium difficile diarrhea, shingles from varicella zoster virus, and fungal nail infection, responses to antihypertensive drugs, serotonin-specific reuptake inhibitors, and statins, including adverse events. A new algorithm will follow longitudinal glycemia and hematocrit trajectories, and a novel automated method will detect karyotype abnormalities for assessing correlation to myelodysplasia and leukemia. Data will also support phenotypes investigated at other eMERGE sites. To create a model for introducing genomics into clinical practice, successful needs assessment methods from eMERGE I will engage stakeholders in guiding development of prototype EMR user interfaces in a clinical decision support format. The test case will be human leukocyte antigen-typing for an adverse drug reaction and the setting will be the patient-centered medical home care model developed at GH. This proposal provides the eMERGE network and its collaborators with the Seattle team's unique expertise in using natural language processing (NLP) to extract information from EMRs, and assisting in adoption of NLP methods. To disseminate eMERGE results and foster collaborations, it takes advantage of leadership positions of the investigators, including partners within eMERGE, other consortia and the HMO Research network, especially the potential for developments supported by the NIH Director's Common Fund in biobanking and megaepidemiology. Completion of the aims will reveal new, medically useful markers, improve the linking of high-throughput genomic methods to EMR data, and develop policies and practices for bringing individualized evidence-based medicine to communities.       RELEVANCE (See instrucfions): To advance personalized medicine-treatment and preventive care based on individual traits; this project matches small differences in DNA to infectious disease susceptibility and response to statins, serotonin- specific reuptake inhibitors (SSRIs) and blood pressure medications. Methods to use these results in clinical care will be guided by focus groups of patients and caregivers in the patient-centered Group Health system.              n/a",Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership,8517791,U01HG006375,"['Address', 'Adoption', 'Adverse event', 'Algorithms', 'Anemia', 'Antihypertensive Agents', 'Bioethics', 'Blood Pressure', 'Bone Marrow Diseases', 'Caregivers', 'Caring', 'Chromosome abnormality', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Consent', 'DNA', 'Data', 'Development', 'Diabetes Mellitus', 'Diarrhea', 'Disease', 'Disease susceptibility', 'Dysmyelopoietic Syndromes', 'Ensure', 'Epidemiology', 'Evidence Based Medicine', 'Excision', 'Focus Groups', 'Fostering', 'Funding', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'HLA Antigens', 'Health', 'Health system', 'Healthcare', 'Hematocrit procedure', 'Herpes zoster disease', 'Herpesvirus Type 3', 'Incidental Findings', 'Individual', 'Infection', 'Karyotype', 'Knowledge', 'Leadership', 'Libraries', 'Link', 'Low-Density Lipoproteins', 'Medical', 'Medical Records', 'Medical center', 'Medicine', 'Mental Depression', 'Methods', 'Mining', 'Modeling', 'Nail plate', 'Natural Language Processing', 'Needs Assessment', 'Other Genetics', 'Outcome', 'Oxidoreductase', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phenotype', 'Policies', 'Population', 'Positioning Attribute', 'Predisposition', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Qualifying', 'Reaction', 'Research', 'Research Personnel', 'Resources', 'Serotonin', 'Single Nucleotide Polymorphism', 'Site', 'Solutions', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Variant', 'aging population', 'base', 'biobank', 'clinical application', 'clinical care', 'clinical practice', 'clinical research site', 'clinically relevant', 'comparative effectiveness', 'design', 'effectiveness research', 'ethical legal social implication', 'evidence base', 'experience', 'genetic technology', 'genome wide association study', 'improved', 'inhibitor/antagonist', 'leukemia', 'leukocyte antigen typing', 'longitudinal database', 'member', 'novel', 'patient home care', 'patient oriented', 'patient population', 'prototype', 'response', 'reuptake', 'skills', 'standard care', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2013,935039,0.025982980640413204
"Annotation, development and evaluation for clinical information extraction (transfer) Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible. In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction (transfer)",8868500,R01GM090187,[' '],NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2013,297936,0.03854244168622661
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann No abstract available  Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8520393,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,19641,0.011256493760820633
"Automated Dynamic Lists for Efficient Electronic Health Record Management     DESCRIPTION (provided by APPLICANT): Medical errors are recognized as the cause of numerous deaths, and even if some are difficult to avoid, many are preventable. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information in the electronic health record (EHR). Unfortunately, a substantial proportion of the information available in the EHR is only mentioned in narrative clinical documents. Electronic lists of problems and allergies are available in most EHRs, but they require manual management by their users, to add new problems, modify existing ones, and the removal of the ones that are irrelevant. Consequently, these electronic lists are often incomplete, inaccurate, and out of date. Clinacuity, Inc. proposes to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the EHR of patients suffering from cancer. To establish the merit and feasibility of such a system, we will work on the following objectives: 1) create a reference standard for training and testing the information extraction application, a reference standard including a random sample of de-identified clinical narratives from patients treated at the Huntsman Cancer Institute Cancer Clinics (Salt Lake City, Utah), with problems and allergies annotated by domain experts; 2) develop a prototype to automatically extract medical problems and allergies, implementing a novel stepwise hybrid approach to maximize sensitivity first, and also enhance positive predictive value; and 3) test the prototype with the aforementioned reference standard, using a cross-validation approach for training and testing. Commercial application: The system Clinacuity proposes will not only help healthcare providers maintain complete and timely lists of problems and allergies, providing them with an efficient overview of a patient, but also help healthcare organizations attain meaningful use requirements. The proposed system has potential commercial applications in inpatient and outpatient settings, increasing the efficiency of busy healthcare providers by saving time, and aiding healthcare organizations in demonstrating ""meaningful use"" and obtaining Centers for Medicare and Medicaid Services incentive payments. Clinacuity will further extend the commercial potential of the system and it is output, using modular design principles allowing utilization of each module independently, and enhancing its local adaptability for easier deployment.         PUBLIC HEALTH RELEVANCE: Medical errors cause numerous deaths, and even if some are difficult to avoid, many could be prevented. Computerized physician order-entry systems with decision support have been proposed to reduce this risk of medication errors, but these systems rely on structured and coded information such as entries in electronic lists of problems and allergies. Such lists are available in most electronic health records, but they require manual management and are often incomplete, inaccurate, and out of date. On the other hand, clinical text reports contain the majority of the patient information, including problems and allergies. The overall goal of this project is to develop a new system to automatically extract structured and coded medical problems and allergies from clinical narrative text in the electronic health record.           ",Automated Dynamic Lists for Efficient Electronic Health Record Management,8590856,R41CA180190,"['Cancer Patient', 'Caring', 'Cessation of life', 'Cities', 'Clinic', 'Clinical', 'Code', 'Complex', 'Disease', 'Electronic Health Record', 'Electronics', 'Ensure', 'Event', 'Excision', 'Goals', 'Health Personnel', 'Healthcare', 'Hybrids', 'Hypersensitivity', 'Incentives', 'Inpatients', 'Institutes', 'Laboratories', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Medical Errors', 'Medication Errors', 'Methods', 'Metric', 'Natural Language Processing', 'Outpatients', 'Output', 'Patients', 'Pharmaceutical Preparations', 'Predictive Value', 'Reference Standards', 'Reporting', 'Risk', 'Sampling', 'Sodium Chloride', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Work', 'base', 'commercial application', 'computerized physician order entry', 'design', 'improved', 'novel', 'payment', 'prevent', 'prototype', 'public health relevance']",NCI,"CLINACUITY,INC.",R41,2013,195683,0.04919459378265855
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8345041,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2012,318849,0.01171561179007688
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,8305149,R01LM010681,"['Abbreviations', 'Algorithms', 'Architecture', 'Caring', 'Cessation of life', 'Clinical', 'Communication', 'Computer Systems', 'Computer software', 'Computers', 'Confusion', 'Coronary Arteriosclerosis', 'Databases', 'Detection', 'Disease', 'Documentation', 'Effectiveness', 'Electronics', 'Equipment and supply inventories', 'Frequencies', 'Health Personnel', 'Healthcare', 'Individual', 'Informatics', 'Joints', 'Machine Learning', 'Manuals', 'Medical Records', 'Methods', 'Names', 'Natural Language Processing', 'Nitroglycerin', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Provider', 'Reader', 'Records', 'Serious Adverse Event', 'Solutions', 'System', 'Technology', 'Text', 'Time', 'Work', 'Writing', 'acronyms', 'base', 'health care delivery', 'health record', 'improved', 'innovation', 'insight', 'knowledge base', 'novel', 'phrases', 'satisfaction']",NLM,VANDERBILT UNIVERSITY,R01,2012,129035,0.02705034682262247
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",8318253,R01LM010016,"['Adverse event', 'Affect', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Evaluation', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2012,327503,0.014404018369623775
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8333324,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2012,545129,0.02123076526709777
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8333306,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,592423,0.017581636758104324
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",8288078,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'clinical care', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,663130,0.039624202622556545
"Contextual ASR to Support EHR Adoption    DESCRIPTION (provided by applicant): The adoption of electronic health record (EHR) systems is a national healthcare priority. However studies show massive physician productivity drop of up to 25-40% upon transition to EHR. The majority of workflow delay is based on the need to perform manual operations to fill structured forms within the EHR, as opposed to simple unstructured narratives used in traditional written notes and transcriptions. Vanguard Medical Technologies (VMT), under NIH grant 1R43LM010750, proved feasibility for DocTalk, a real-time, speech-driven, open-source augmented, small practice encounter recording system that processes voice to text to structured medical data to EHR input, utilizing integrated automated speech recognition (ASR) and natural language processing (NLP) in the cloud. While NLP accuracy in Phase I was high, voice accuracy prior to physician review was inadequate. Fortunately, the tight integration of ASR and NLP combined with the formal structure of physician notes offers unique context based approaches to address the challenge. Current speech recognition methods use a single general-purpose medical lexicon to train a recognizer when identifying words. Medical context-specific probabilities are ignored. The four Specific Aims of this Phase I SBIR project are to: 1. Create a textual corpus for each section of a patient encounter note by processing 1 million text based narrative structured encounter notes 2. Build a family of Section-Specific Statistical Language Models (SS-SLMs) specialized in recognizing speech pertaining to each specific section of a patient encounter note, using industry standard open source statistical language modeling tools. 3. Use NLP techniques to infer patterns of language usage from text of each section, a. To detect section boundaries to be used as trigger words for invoking SS-SLMs b. To determine characteristic word distributions of each section 4. Assess improvement in accuracy per section due to use of SS-SLMs, with the goal of 50% overall reduction of errors compared to non-section-specific SLMs in the same medical dictation system.      PUBLIC HEALTH RELEVANCE: Successful completion of this innovative proposed program of NLP-enhanced context based ASR, will provide the accuracy required to deploy an integrated, interactive, intuitive, low-cost data entry system for small practice primary care physicians. The augmented DocTalk system will enable physicians to increase usable information, avoid third-party transcription errors, and mitigate workflow delays. Increased small practice EHR adoption directly addresses national healthcare goals.              Successful completion of this innovative proposed program of NLP-enhanced context based ASR, will provide the accuracy required to deploy an integrated, interactive, intuitive, low-cost data entry system for small practice primary care physicians. The augmented DocTalk system will enable physicians to increase usable information, avoid third-party transcription errors, and mitigate workflow delays. Increased small practice EHR adoption directly addresses national healthcare goals.            ",Contextual ASR to Support EHR Adoption,8253003,R43TR000179,"['Address', 'Adoption', 'Characteristics', 'Code', 'Data', 'Documentation', 'Drops', 'Electronic Health Record', 'Electronics', 'Family', 'Genetic Transcription', 'Goals', 'Grant', 'Healthcare', 'Industry', 'Language', 'Libraries', 'Manuals', 'Medical', 'Medical Records', 'Medical Technology', 'Methods', 'Modeling', 'Natural Language Processing', 'Patients', 'Pattern', 'Phase', 'Physicians', 'Positioning Attribute', 'Primary Care Physician', 'Probability', 'Process', 'Productivity', 'Research Infrastructure', 'Safety', 'Small Business Innovation Research Grant', 'Solutions', 'Speech', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Variant', 'Voice', 'Writing', 'base', 'cost', 'innovation', 'open source', 'operation', 'programs', 'speech recognition', 'tool', 'voice recognition']",NCATS,"HEALTH FIDELITY, INC.",R43,2012,150000,0.025008183875989628
"Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2)    DESCRIPTION (provided by applicant): The eTfor2 project will develop and evaluate open-source programs and knowledge representations to better characterize patients for translational and clinical research studies. The project addresses National Library of Medicine (NLM) RFA initiatives for: (a) information & knowledge processing, including natural language processing and text summarization, (b) approaches for linking phenomic and genomic information, and (c) integration of information from heterogeneous sources. Translational studies correlate clinical patient descriptors (phenome) with results of genomic investigations, e.g., genome-wide association studies (GWAS). Standard methods for defining phenotypes require costly, labor-intensive cohort enrollments to identify patients with diseases and appropriate controls. Recently, translational and clinical researchers have used electronic medical record (EMR) data as an alternative to identifying patient characteristics. However, EMR case extraction requires substantial manual review and ""tuning"" for case selection, due to the inaccuracies inherent in ICD9 billing codes. While relevant and useful natural language processing (NLP) approaches to facilitate EMR text extraction have proliferated, the target patient descriptors these approaches employ typically remain non-standard and locally defined, and vary from disease to disease, project to project and institution to institution. At best, such NLP applications use standard terminology descriptors such as SNOMED-CT as EMR extraction targets. Yet, there is no generally utilized ""standard"" knowledge base that links such ""extractable"" descriptors to an academic-quality knowledge source detailing what findings have been reliably reported to occur in each disease. To facilitate translational and clinical research, the eTfor2 project will make available an open-source, evidence-based, electronic clinical knowledge base (KB) and related NLP tools enabling researchers at any site to extract a standard ""target"" set of EMR-based phenomic descriptors at both the finding and disease levels. It will further include diagnostic decision support logic to confirm the degree of support for patients' diagnoses in their EMR records. The eTfor2 project will decrease effort required to harvest EMR patient descriptors for clinical and translational studies, and enable new translational work that identifies genomic associations at both finding and disease levels. The eTfor2 resources should improve the quality and cross-institutional validity of EMR-based translational and clinical studies.           Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2) Project Narrative When completed successfully, the eTfor2 project will enable researchers at disparate institutions to extract from their respective EMR systems a shared ""target"" set of common phenomic descriptors, in a standard, reproducible manner. Doing so should improve the quality and cross-institutional validity of EMR-based translational and clinical studies.",Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2),8318247,R01LM010828,"['18 year old', 'Abdomen', 'Abdominal Pain', 'Address', 'Adult', 'Algorithms', 'Automated Abstracting', 'Biopsy', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Clinical Research', 'Code', 'Cohort Studies', 'Companions', 'Computer-Assisted Diagnosis', 'Computerized Medical Record', 'Core Facility', 'DNA', 'DNA Databases', 'Data', 'Data Analyses', 'Descriptor', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Electronics', 'Enrollment', 'Epigastrium', 'Evaluation Studies', 'Exhibits', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Gold', 'Harvest', 'Human', 'Image', 'Individual', 'Institution', 'Intellectual Property', 'Internal Medicine', 'Internist', 'Intra-abdominal', 'Investigation', 'Knowledge', 'Laboratories', 'Licensing', 'Link', 'Literature', 'Logic', 'Manuals', 'Maps', 'Methods', 'Metric', 'Names', 'Natural Language Processing', 'Negative Finding', 'Normal Range', 'Outcome', 'Pain', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Physical Examination', 'Process', 'Proliferating', 'Property Rights', 'Proteomics', 'Publishing', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'SNOMED Clinical Terms', 'Sampling', 'Side', 'Site', 'Source', 'Specific qualifier value', 'Splenomegaly', 'Supplementation', 'Symptoms', 'System', 'Systematized Nomenclature of Medicine', 'Terminology', 'Testing', 'Text', 'Time', 'Translational Research', 'United States National Library of Medicine', 'Universities', 'Visceromegaly', 'Vocabulary', 'Work', 'base', 'case control', 'clinical phenotype', 'cohort', 'evidence base', 'genome wide association study', 'improved', 'information organization', 'innovation', 'interest', 'knowledge base', 'meetings', 'member', 'open source', 'phenome', 'phenomics', 'programs', 'research study', 'success', 'theories', 'tool', 'translational study']",NLM,VANDERBILT UNIVERSITY,R01,2012,366912,0.02867487427678448
"An Information Fusion Approach to Longitudinal Health Records     DESCRIPTION (provided by applicant): Our goal is to leverage an information fusion approach to integrate structured and unstructured information to generate a longitudinal health record (LHR) for accelerating the pace at which patients can be recruited into clinical trials. Because electronic health records (EHR) contain clinical summaries of a patient's clinical history, one would assume that they could be easily leveraged to automatically screen and identify potentially eligible patients. However most EHRs are not well designed to support screening of eligible patients and are composed of multiple data sources that are often redundant or inconsistent, stored in uncoordinated unstructured clinical narratives and structured data. These characteristics make EHRs difficult to use for matching patients against the complex event and temporal criteria of clinical trials protocols. This research proposes that an improved LHR, which contains a comprehensive clinical summary of a patient, can improve patient screening. We propose using a method of information fusion to generate this LHR, which merges information from multiple data sources, that addresses both the meaning and temporal nature of data, such that the resulting information is more accurate than would be possible if these sources were used individually.         The specific aims are to: 1) characterize the barriers of using EHR sources for screening in terms of data redundancy, inconsistency, lack of structure, and temporal imprecision; 2) automatically extract information from unstructured EHR sources necessary for screening patients against clinical trials eligibility criteria using natural language processing; 3) developan LHR appropriate for screening patients against eligibility criteria using information fusion methods based on semantic and temporal information; and 4) evaluate the accuracy of an LHR formed through information fusion for screening patients against clinical trials eligibility critera.         The respective hypotheses to be tested are: 1) Different parts of the EHR will contain variable amounts of redundancy, inconsistency, and temporal imprecision. Some sources will be more valuable for matching patients than others to clinical trials eligibility criteria. 2) Including th information contained in the unstructured notes will reduce the false positive rate of identifying potentially eligible patients over leveraging only the structured data in the EHR. 3) By using information fusion methods based on leveraging semantic and temporal information on a combination of structured and unstructured data, we will be able to accurately summarize the information contained in uncoordinated EHR data sources into an LHR that can be used for screening patients for clinical trials. 4) The use of information fusion to generate a longitudinal health record will increase the sensitivity and specificity of electronic clinical trial screening ver using a traditional EHR.         With an LHR formed through information fusion for screening patients for clinical trials eligibilit, we will be able to not only reduce the amount of staff effort required to recruit a patient into a clinical trial, but also accelerate the pace at which clinical trials can be conducted.                  Narrative This project is focused on generating a longitudinal health record for accelerating the pace at which patients can be recruited into clinical trials. Accelerating the pace at which patients are recruited into clinical trials has the potential for improving the speed at which new treatments are made available to the public.",An Information Fusion Approach to Longitudinal Health Records,8373437,R01LM011116,"['Address', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Complex', 'Consultations', 'Data', 'Data Sources', 'Electronic Health Record', 'Electronics', 'Eligibility Determination', 'Enrollment', 'Event', 'Failure', 'Goals', 'Hour', 'Laboratories', 'Manuals', 'Measures', 'Methods', 'Natural Language Processing', 'Nature', 'Patient Recruitments', 'Patients', 'Process', 'Randomized', 'Randomized Controlled Trials', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Series', 'Source', 'Speed', 'Structure', 'Techniques', 'Testing', 'Time', 'TimeLine', 'Work', 'base', 'cohort', 'cost', 'design', 'falls', 'health record', 'improved']",NLM,OHIO STATE UNIVERSITY,R01,2012,341342,0.03153867919077024
"Applying NLP to Free Text as an EHR Data Capture Method to Improve EHR Usability     DESCRIPTION (provided by applicant): This proposal aims to ensure the ability of ""NLP-Standalone-or-Hybrid Documentation,"" a method of EHR data capture involving Natural Language Processing and possibly also standard EHR data capture, to improve the usability of EHR by reducing documentation time, increasing documentation quality, and increasing clinician satisfaction. Problem to be Addressed. Limited usability of the Electronic Health Record (""EHR"") and lack of standardized terminology impedes EHR adoption and optimal use, and therefore hinders realization of a universally interoperable and evidence-based reportable health care system. Large amounts of time required for documentation, low clinician satisfaction, and incomplete documentation are problems plaguing EHR. Innovation. Current research has demonstrated that NLP may be used for EHR data capture. ZyDoc is furthering the state of research by assessing the capability of NLP-Standalone-or-Hybrid Documentation to improve EHR usability along several criteria. Long Term Goal. By enabling interoperability and improving EHR usability, through improving clinician satisfaction, improving documentation quality, and reducing data capture time, MediSapien will encourage widespread EHR adoption and optimal use with structured data. Phase I Summary. The purpose of the first Specific Aim of this grant proposal is to ensure that NLP- Standalone-or-Hybrid Documentation is capable of improving clinician satisfaction, efficiency, and documentation quality, relative to standard EHR data capture methods. The purpose of the second Specific Aim is to improve the accuracy of MediSapien's coding. These Specific Aims will ensure the technical feasibility of NLP-Standalone-or-Hybrid Documentation and MediSapien for improving EHR usability. Phase II Objectives. In Phase II, ZyDoc will complete product development, beta test MediSapien at two hospitals, and measure the product's impact on clinical outcomes or documentation results. Commercial Opportunity. ZyDoc will offer MediSapien as a modular component by partnering with vendors that combine MediSapien in their own solutions, enabling their clients to meet EHR meaningful use standards.        PUBLIC HEALTH RELEVANCE: Limited usability of the Electronic Health Record (""EHR"") and lack of standardized terminology impedes EHR adoption and meaningful use, and therefore hinders realization of a universally interoperable and evidence- based reportable health care system. This proposal aims to prove that EHR usability can be increased by applying NLP and other technologies to convert dictated and transcribed unstructured text to structured data and inserting it into the EHR. Achievement of this result will encourage optimal EHR use with searchable, structured data that will enable interoperability.                  Limited usability of the Electronic Health Record (""EHR"") and lack of standardized terminology impedes EHR adoption and meaningful use, and therefore hinders realization of a universally interoperable and evidence- based reportable health care system. This proposal aims to prove that EHR usability can be increased by applying NLP and other technologies to convert dictated and transcribed unstructured text to structured data and inserting it into the EHR. Achievement of this result will encourage optimal EHR use with searchable, structured data that will enable interoperability.                ",Applying NLP to Free Text as an EHR Data Capture Method to Improve EHR Usability,8314587,R43LM011165,"['Achievement', 'Address', 'Adoption', 'Algorithms', 'Applications Grants', 'Client', 'Clinical', 'Code', 'Computer Assisted', 'Data', 'Documentation', 'Electronic Health Record', 'Ensure', 'Genetic Transcription', 'Goals', 'Health', 'Healthcare Systems', 'Hospitals', 'Hybrids', 'ICD-10-CM', 'ICD-9-CM', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Logical Observation Identifiers Names and Codes', 'Maps', 'Measures', 'Medical Informatics', 'Methods', 'Mus', 'Natural Language Processing', 'Outcome', 'Output', 'Patients', 'Phase', 'Physicians', 'Plague', 'Process', 'Provider', 'Records', 'Relative (related person)', 'Research', 'Risk', 'Solutions', 'Speech', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Teaching Hospitals', 'Technology', 'Terminology', 'Testing', 'Text', 'Time', 'Vendor', 'base', 'clinical care', 'commercial application', 'evidence base', 'expectation', 'improved', 'innovation', 'interoperability', 'medical specialties', 'meetings', 'novel', 'product development', 'prospective', 'research study', 'satisfaction', 'usability']",NLM,"ZYDOC MEDICAL TRANSCRIPTION, LLC",R43,2012,150000,0.0689980229199636
"EHR-based patient safety: Automated error detection in neonatal intensive care     DESCRIPTION (provided by applicant): In the field of neonatal patient safety, the paucity of systematic research is a critical barrier to progress. Notably missing are studies that meticulously investigate Electronic Health Records (EHR) and information technology in detecting neonatal intensive care-related errors. The expert panel at the National Institute of Child Health and Human Development (NICHHD) identified multiple gaps in the current knowledge of neonatal patient safety research. The proposed work is a well focused response to three dimensions of the Funding Opportunity Announcement:  1.Develop prospective and retrospective study designs to collect data on patient safety and adverse events.  2.Study the strength and limitations of current methods of error reporting systems.  3.Study the usefulness of commercial IT systems and EHRs in reducing medical errors.  In our study we seek to shift patient safety research toward an automated and computerized approach to achieve a more comprehensive patient safety paradigm. We will develop novel Electronic Health Record (EHR) content-based automated algorithms that are new to patient safety research to 1) detect errors (Aim 1) and 2) categorize subsequent harm (Aim 2). State of the art information extraction and statistical classification techniques from the field of clinical Natural Language Processing (NLP) will be adapted to the patient safety research tasks.  In Aim 1 we will fill the gap in the literatre by implementing a focused manual review of 700 charts (one full year of patient admissions at our institution) in one of the largest Neonatal Intensive Care Units (NICU) in the nation. Using a trigger tool, we will identify errors occurring in three specified categories - laboratory test errrs, medication/fluid errors, and airway management errors. We will develop novel algorithms for automated EHR-based detection of the errors and evaluate the performance of the new algorithms against the performance of both trigger tool review by human chart reviewers (current gold standard) and the voluntary incident reporting system (accepted standard). In Aim 2, we will study the utility of novel EHR-based information extraction and statistical algorithms for the automated categorization of errors according to the resulting level of harm.  Our proposed work has the potential to accomplish a paradigm shift in the methods of neonatal patient safety research and practice. The study is a fundamental step to automating patient safety monitoring on a large scale and improving error identification and patient safety in NICUs for millions of children every year.        PUBLIC HEALTH RELEVANCE: We are developing an automated error detection technique to improve the safety of newborn babies during hospital care. Our work is the first known attempt to use text analysis in the electronic health records on a large scale to reduce the cost while at the same time increase the speed and comprehensiveness of error detection in the clinical care of newborns.              We are developing an automated error detection technique to improve the safety of newborn babies during hospital care. Our work is the first known attempt to use text analysis in the electronic health records on a large scale to reduce the cost while at the same time increase the speed and comprehensiveness of error detection in the clinical care of newborns.            ",EHR-based patient safety: Automated error detection in neonatal intensive care,8334934,R21HD072883,"['Adverse event', 'Algorithms', 'Caring', 'Categories', 'Cessation of life', 'Characteristics', 'Child', 'Child health care', 'Classification', 'Clinical', 'Data', 'Detection', 'Dimensions', 'Electronic Health Record', 'Foundations', 'Funding Opportunities', 'Gold', 'Hospitals', 'Hour', 'Human', 'Human Development', 'Information Systems', 'Information Technology', 'Institutes', 'Institution', 'Intervention', 'Knowledge', 'Laboratories', 'Liquid substance', 'Literature', 'Manuals', 'Medical Errors', 'Medication Errors', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Neonatal', 'Neonatal Intensive Care', 'Neonatal Intensive Care Units', 'Newborn Infant', 'Patient Admission', 'Patient Monitoring', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Prevention', 'Process', 'Prospective Studies', 'Reporting', 'Research', 'Research Design', 'Retrospective Studies', 'Risk', 'Safety', 'Solid', 'Specific qualifier value', 'Speed', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Work', 'base', 'clinical care', 'computerized', 'cost', 'design', 'experience', 'improved', 'indexing', 'innovation', 'novel', 'patient safety', 'phrases', 'response', 'tool']",NICHD,CINCINNATI CHILDRENS HOSP MED CTR,R21,2012,153000,0.04806035887001301
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8326648,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2012,318393,0.02110221778180246
"An in-silico method for epidemiological studies using Electronic Medical Records Observational epidemiological studies are effective methods for identifying  factors affecting the health and illness of populations, as well as for determining optimal  treatments for diseases, such as cancers. However, conventional epidemiological  research usually involves personnel-intensive effort (such as manual chart and public  records review) and can be very time consuming before conclusive results are obtained.  Recently, a large amount of detailed longitudinal clinical data has been accumulated at  hospitals' Electronic Medical Records (EMR) systems and it has become a valuable data  source for epidemiological studies. However, there are two obstacles that prevent the  wide usage of EMR data in epidemiological studies. First, most of the detailed clinical  information in EMRs is embedded in narrative text and it is very costly to extract that  information manually. Second, EMRs usually have data quality problems such as  selection bias and missing data, which require adaptation of conventional statistical  methods developed for randomized controlled trials.   In this study, we propose an in silico informatics-based approach for  observational epidemiological studies using EMR data. We hypothesize that existing  EMR data can be used for certain types of epidemiological studies in a very efficient  manner with the help of informatics methods. The informatics-based approach will  contain two major components. One is an NLP (Natural Language Processing) based  information extraction system that can automatically extract detailed clinical information  from EMR and another is a set of statistical and informatics methods that can be used to  analyze EMR-derived data. If the feasibility of this approach is proven, it will change the  standard paradigm of observational epidemiological research, because it has the  capability to answer an epidemiological question in a very short time at a very low cost.   The specific aim of this study is to develop an automated informatics approach to  extract both fine-grained cancer findings and general clinical information from EMRs and  use them to conduct cancer related epidemiological studies. We will perform both case-  control and cohort studies related to prevention and treatment of breast and colon  cancers using EMR data. The informatics approach will be validated on EMRs from two  major hospitals to demonstrate its generalizability. Epidemiological findings from our  study will be compared to reported findings for validation. Project Narrative  According to the American Cancer Society, about 7.6 million people died from various  types of cancer in the world during 2007. It is very important to identify risk factors of  cancers and to determine optimal treatments of cancers, and epidemiological study is  one of the methods to achieve it. This proposed study will use natural language  processing technologies to automatically extract fine-grained cancer information from  existing patient electronic medical records and use it to conduct cancer related  epidemiological studies, thus accelerating knowledge accumulation of cancer research.",An in-silico method for epidemiological studies using Electronic Medical Records,8298614,R01CA141307,"['Address', 'Affect', 'American Cancer Society', 'Breast Cancer Treatment', 'Case-Control Studies', 'Cereals', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Colon Carcinoma', 'Computer Simulation', 'Computerized Medical Record', 'Data', 'Data Collection', 'Data Quality', 'Data Sources', 'Databases', 'Discipline of Nursing', 'Disease', 'Documentation', 'Effectiveness', 'Epidemiologic Studies', 'Epidemiology', 'Ethics', 'Gold', 'Health', 'Healthcare Industry', 'Hospitals', 'Human Resources', 'Informatics', 'Knowledge', 'Language', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Medical Education', 'Methods', 'Natural Language Processing', 'Nature', 'New York', 'Observational Study', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Play', 'Population', 'Presbyterian Church', 'Prevention', 'Process', 'Quality of Care', 'Radiology Specialty', 'Randomized Clinical Trials', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Risk Factors', 'Role', 'Selection Bias', 'Statistical Methods', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Therapeutic Agents', 'Therapeutic procedure', 'Time', 'Translational Research', 'Universities', 'University Hospitals', 'Validation', 'anticancer research', 'base', 'cancer therapy', 'cancer type', 'clinical application', 'clinical practice', 'cost', 'efficacy testing', 'improved', 'malignant breast neoplasm', 'novel', 'prevent', 'prognostic indicator', 'public health research', 'statistics', 'treatment effect']",NCI,VANDERBILT UNIVERSITY,R01,2012,56569,0.024592213162192185
"Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach  Abstract The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The study design is prospective observational study. Scope is limited to cancer patients. There are three specific aims for this project. The first aim is to identify concepts that overlap between the electronic medical record's (EMR) clinical notes and the free text of clinical trial announcements. The PI will use the concepts to develop mapping frames that connect concepts in the text of trial announcements to those found in clinical notes in the medical record. When he has the mapping frames he will build the NLP module for the application. In the software development work he will utilize as many publicly available software components as possible. He will experiment with UIMA, GATE, MetaMap, Stanford Parser, NegEx algorithm and others. The PI will develop the tool around the National Library of Medicine's Unified Medical Language System knowledgebase. He will use Java for programming. The second aim is to create an algorithm that automatically generates questions to request information directly from the patient if the information is not available or accessible in the records. The third aim is to evaluate the in-vitro, laboratory performance of the application. For performance evaluation purposes the PI will recruit cancer care specialists to generate the gold standard lists of eligible clinical trials for study patients. He will publicly release the developed code at the end of the grant period. This K99/R00 project will serve the foundation for future R01 grant applications. The PI is fully committed to become faculty in the Clinical Research Informatics domain with a specialization in biomedical NLP. The support of the K99/R00 grant will enable him to acquire substantial formal training in Computational Linguistics while contributing to the body of knowledge of the Clinical Research Informatics field. The five-year grant support will ensure success in his endeavor. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, EMR based clinical trial recommendations directly to the patients. The results of this research will empower the patients and elevate their role in the decision making process.  Relevance The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, electronic medical record based clinical trial recommendations directly to the patients. The results of this research will empower patients and elevate their role in the decision making process.",Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach,8331381,R00LM010227,"['Adult', 'Age', 'Algorithms', 'Applications Grants', 'Biomedical Research', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Code', 'Commit', 'Complement', 'Computer software', 'Computerized Medical Record', 'Decision Making', 'Development', 'Elderly', 'Elements', 'Eligibility Determination', 'Enrollment', 'Ensure', 'Equation', 'Evaluation', 'Faculty', 'Foundations', 'Future', 'Goals', 'Gold', 'Grant', 'In Vitro', 'Informatics', 'Java', 'Knowledge', 'Laboratories', 'Linguistics', 'Malignant Neoplasms', 'Maps', 'Medical Records', 'Medicine', 'Methods', 'Minority', 'Modification', 'Natural Language Processing', 'Newly Diagnosed', 'Observational Study', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protocols documentation', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Role', 'Screening for cancer', 'Screening procedure', 'Specialist', 'Surgeon', 'Text', 'Training', 'Unified Medical Language System', 'United States National Library of Medicine', 'Work', 'abstracting', 'base', 'cancer care', 'empowered', 'ethnic minority population', 'information organization', 'knowledge base', 'novel', 'older patient', 'patient oriented', 'programs', 'prospective', 'research study', 'software development', 'success', 'tool']",NLM,CINCINNATI CHILDRENS HOSP MED CTR,R00,2012,238944,0.013935378511670406
"Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership    DESCRIPTION (provided by applicant): The Seattle eMERGE project aims to bring personal genomics to practice settings by taking advantage of the extensive electronic medical record (EMR) and biorepository of Group Health Cooperative (GH), including a 33-year pharmacy database and longitudinal data on an aging population. Algorithms developed in eMERGE I will be used to combine genome-wide association studies with phenotypes mined from EMRs to discover new polymorphism-phenotype relationships. Target phenotypes are infectious disease susceptibility, specifically to Clostridium difficile diarrhea, shingles from varicella zoster virus, and fungal nail infection, responses to antihypertensive drugs, serotonin-specific reuptake inhibitors, and statins, including adverse events. A new algorithm will follow longitudinal glycemia and hematocrit trajectories, and a novel automated method will detect karyotype abnormalities for assessing correlation to myelodysplasia and leukemia. Data will also support phenotypes investigated at other eMERGE sites. To create a model for introducing genomics into clinical practice, successful needs assessment methods from eMERGE I will engage stakeholders in guiding development of prototype EMR user interfaces in a clinical decision support format. The test case will be human leukocyte antigen-typing for an adverse drug reaction and the setting will be the patient-centered medical home care model developed at GH. This proposal provides the eMERGE network and its collaborators with the Seattle team's unique expertise in using natural language processing (NLP) to extract information from EMRs, and assisting in adoption of NLP methods. To disseminate eMERGE results and foster collaborations, it takes advantage of leadership positions of the investigators, including partners within eMERGE, other consortia and the HMO Research network, especially the potential for developments supported by the NIH Director's Common Fund in biobanking and megaepidemiology. Completion of the aims will reveal new, medically useful markers, improve the linking of high-throughput genomic methods to EMR data, and develop policies and practices for bringing individualized evidence-based medicine to communities.       RELEVANCE (See instrucfions): To advance personalized medicine-treatment and preventive care based on individual traits; this project matches small differences in DNA to infectious disease susceptibility and response to statins, serotonin- specific reuptake inhibitors (SSRIs) and blood pressure medications. Methods to use these results in clinical care will be guided by focus groups of patients and caregivers in the patient-centered Group Health system.              n/a",Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership,8493901,U01HG006375,"['Address', 'Adoption', 'Adverse event', 'Algorithms', 'Anemia', 'Antihypertensive Agents', 'Bioethics', 'Blood Pressure', 'Bone Marrow Diseases', 'Caregivers', 'Caring', 'Chromosome abnormality', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Consent', 'DNA', 'Data', 'Development', 'Diabetes Mellitus', 'Diarrhea', 'Disease', 'Disease susceptibility', 'Dysmyelopoietic Syndromes', 'Ensure', 'Epidemiology', 'Evidence Based Medicine', 'Excision', 'Focus Groups', 'Fostering', 'Funding', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'HLA Antigens', 'Health', 'Health system', 'Healthcare', 'Hematocrit procedure', 'Herpes zoster disease', 'Herpesvirus Type 3', 'Incidental Findings', 'Individual', 'Infection', 'Karyotype', 'Knowledge', 'Leadership', 'Libraries', 'Link', 'Low-Density Lipoproteins', 'Medical', 'Medical Records', 'Medical center', 'Medicine', 'Mental Depression', 'Methods', 'Mining', 'Modeling', 'Nail plate', 'Natural Language Processing', 'Needs Assessment', 'Other Genetics', 'Outcome', 'Oxidoreductase', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phenotype', 'Policies', 'Population', 'Positioning Attribute', 'Predisposition', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Qualifying', 'Reaction', 'Research', 'Research Personnel', 'Resources', 'Serotonin', 'Single Nucleotide Polymorphism', 'Site', 'Solutions', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Variant', 'aging population', 'base', 'biobank', 'clinical application', 'clinical care', 'clinical practice', 'clinical research site', 'clinically relevant', 'comparative effectiveness', 'design', 'effectiveness research', 'ethical legal social implication', 'evidence base', 'experience', 'genetic technology', 'genome wide association study', 'improved', 'inhibitor/antagonist', 'leukemia', 'leukocyte antigen typing', 'longitudinal database', 'member', 'novel', 'patient home care', 'patient oriented', 'patient population', 'prototype', 'response', 'reuptake', 'skills', 'standard care', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2012,1154966,0.025982980640413204
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,8589822,R01LM010681,[' '],NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2012,237877,0.02705034682262247
"An insilico method for epidemiological studies using Electonic Medical Records Observational epidemiological studies are effective methods for identifying  factors affecting the health and illness of populations, as well as for determining optimal  treatments for diseases, such as cancers. However, conventional epidemiological  research usually involves personnel-intensive effort (such as manual chart and public  records review) and can be very time consuming before conclusive results are obtained.  Recently, a large amount of detailed longitudinal clinical data has been accumulated at  hospitals' Electronic Medical Records (EMR) systems and it has become a valuable data  source for epidemiological studies. However, there are two obstacles that prevent the  wide usage of EMR data in epidemiological studies. First, most of the detailed clinical  information in EMRs is embedded in narrative text and it is very costly to extract that  information manually. Second, EMRs usually have data quality problems such as  selection bias and missing data, which require adaptation of conventional statistical  methods developed for randomized controlled trials.   In this study, we propose an in silico informatics-based approach for  observational epidemiological studies using EMR data. We hypothesize that existing  EMR data can be used for certain types of epidemiological studies in a very efficient  manner with the help of informatics methods. The informatics-based approach will  contain two major components. One is an NLP (Natural Language Processing) based  information extraction system that can automatically extract detailed clinical information  from EMR and another is a set of statistical and informatics methods that can be used to  analyze EMR-derived data. If the feasibility of this approach is proven, it will change the  standard paradigm of observational epidemiological research, because it has the  capability to answer an epidemiological question in a very short time at a very low cost.   The specific aim of this study is to develop an automated informatics approach to  extract both fine-grained cancer findings and general clinical information from EMRs and  use them to conduct cancer related epidemiological studies. We will perform both case-  control and cohort studies related to prevention and treatment of breast and colon  cancers using EMR data. The informatics approach will be validated on EMRs from two  major hospitals to demonstrate its generalizability. Epidemiological findings from our  study will be compared to reported findings for validation. Project Narrative  According to the American Cancer Society, about 7.6 million people died from various  types of cancer in the world during 2007. It is very important to identify risk factors of  cancers and to determine optimal treatments of cancers, and epidemiological study is  one of the methods to achieve it. This proposed study will use natural language  processing technologies to automatically extract fine-grained cancer information from  existing patient electronic medical records and use it to conduct cancer related  epidemiological studies, thus accelerating knowledge accumulation of cancer research.",An insilico method for epidemiological studies using Electonic Medical Records,8589201,R01CA141307,[' '],NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2012,195838,0.023501342289845382
"Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects    DESCRIPTION (provided by applicant):        	Epidemiologic analyses of health care data can provide critical evidence on the effectiveness and safety of therapeutics. This is particularly vital during the transition from the point of regulatory approval through the early marketing of new drugs, a time when physicians, regulators and payers are all struggling with incomplete data. Health plans pay for these drugs without knowing how their effectiveness and safety compares with established alternatives, as new compounds are tested against placebos rather than active agents, and tested only in selected patients. Non-randomized studies in large healthcare databases can provide fast and less costly evidence on drug effects. However, conventional adjustment methods that rely on a small number of investigator-specified confounders often fail and may produce biased results.     We propose and have preliminary evidence that employing modern medical informatics algorithms that structure and search databases to empirically identify thousands of new covariates. These will then enter established propensity score-based models and so make far more effective use of the information contained in health care databases and electronic medical records (EMRs), resulting in more valid causal interpretations of treatment effects. We will:    - Develop algorithms that make greater use of information contained in longitudinal claims and EMR databases by empirically identifying thousands of potential confounders. The performance of these approaches will be evaluated in 6 example studies encompassing recent drug safety and comparative effectiveness problems, and will be implemented in multiple large claims databases supplemented by such data as lab values and EMR information in subgroups.    -- Develop novel methods for confounding adjustment based on textual information found in EMRs.    -- Expand the newly developed mining algorithms into a framework that integrates distributed database networks with uneven information content, similar to the Sentinel Network recently initiated by FDA.                            This project is likely to produce groundbreaking results at the interface of medicine, biomedical informatics, and epidemiologic methods. After completion of this project a library of documented and validated algorithms will be available to significantly improve confounder control in a range of healthcare databases. The theoretical foundation and the ready-to-use algorithms will likely lead to a fundamental shift in how databases contribute to the fast and accurate assessment of newly-marketed medications.            Large healthcare databases are used to assess the safety and effectiveness of drugs. However, conventional adjustment methods that rely on a limited number of investigator-specific covariates often fail to produce unbiased results. We will develop algorithms that make greater use of information contained in longitudinal claims data and electronic medical records databases by empirically identifying thousands of potential confounders. This will result in improved causal inference on the comparative safety and effectiveness of newly marketed medications that is both less susceptible to investigator omissions and faster than conventional approaches.",Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects,8318797,R01LM010213,"['Address', 'Algorithms', 'Clinical', 'Code', 'Complex', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Distributed Databases', 'Effectiveness', 'Elderly woman', 'Epidemiologic Methods', 'Epidemiology', 'Etiology', 'Evaluation', 'Foundations', 'Head', 'Health Planning', 'Healthcare', 'Heterogeneity', 'Individual', 'Insurance', 'Internet', 'Knowledge', 'Lead', 'Libraries', 'Marketing', 'Medical Informatics', 'Medicare/Medicaid', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Placebos', 'Population', 'Pregnant Women', 'Process', 'Publications', 'Randomized', 'Research', 'Research Personnel', 'Safety', 'Scoring Method', 'Sentinel', 'Severity of illness', 'Solid', 'Solutions', 'Specific qualifier value', 'Speed', 'Structure', 'Subgroup', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'To specify', 'Training', 'base', 'biomedical informatics', 'comparative', 'comparative effectiveness', 'compare effectiveness', 'data mining', 'improved', 'indexing', 'lectures', 'novel', 'outcome forecast', 'patient privacy', 'programs', 'routine care', 'symposium', 'treatment effect']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2012,383445,0.011860340312399319
"Near Miss Narratives from the Fire Service: A Bayesian Analysis    DESCRIPTION (provided by applicant): This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source that has not yet been rigorously investigated. The proposal has 3 aims:  I. to use recently developed auto coding methods to characterize firefighter near miss narratives and classify these narratives into mechanisms of risk/injury. This analysis will apply the International Classification of External Cause of Injuries (ICECI) using Bayesian machine learning techniques to identify the various mechanisms captured in the near miss narratives and their relative prevalence.  II. To identify the correlation between each mechanism of risk/injury and each of the ""Contributing Factors"" listed on the NFFNMRS reporting form. The results will reveal any patterns and trends in the distribution of the contributing factors among the mechanisms, creating a deeper understanding of near miss circumstances, as well as a basis for improving the quality of future near miss data collection.  III. To use manual coding to identify actual injury incidents contained within a random sample of 1,000 near miss narratives and correlate these injuries with the ""Loss Potential"" categories on the NFFNMRS reporting form. The results will demonstrate how actual injuries are distributed within the reporting form's ""Loss Potential"" categories. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention. This study addresses a major gap in firefighter safety knowledge, i.e. the insufficient understanding of near miss events, and will have a high impact on efforts to improve the occupational health and safety of firefighters.         This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source, the NFFNMRS that has not yet been rigorously investigated. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention.         ",Near Miss Narratives from the Fire Service: A Bayesian Analysis,8325335,R03OH009984,[' '],NIOSH,DREXEL UNIVERSITY,R03,2012,76332,0.011494004442865224
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,8077875,R01LM010681,"['Abbreviations', 'Algorithms', 'Architecture', 'Caring', 'Cessation of life', 'Clinical', 'Communication', 'Computer Systems', 'Computer software', 'Computers', 'Confusion', 'Coronary Arteriosclerosis', 'Databases', 'Detection', 'Disease', 'Documentation', 'Effectiveness', 'Electronics', 'Equipment and supply inventories', 'Frequencies', 'Health Personnel', 'Healthcare', 'Individual', 'Informatics', 'Joints', 'Machine Learning', 'Manuals', 'Medical Records', 'Methods', 'Names', 'Natural Language Processing', 'Nitroglycerin', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Provider', 'Reader', 'Records', 'Serious Adverse Event', 'Solutions', 'System', 'Technology', 'Text', 'Time', 'Work', 'Writing', 'acronyms', 'base', 'health care delivery', 'health record', 'improved', 'innovation', 'insight', 'knowledge base', 'novel', 'phrases', 'satisfaction']",NLM,VANDERBILT UNIVERSITY,R01,2011,374000,0.02705034682262247
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",8105502,R01LM010016,"['Adverse event', 'Affect', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Evaluation', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2011,333575,0.014404018369623775
"Using medical records repositories to improve the alert system design    DESCRIPTION (provided by applicant):       Rapid and accurate alerting of concerning patient events and conditions remains an important problem in clinical practice. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into the monitoring and alerting systems. However, it is often time-consuming and costly to extract and codify such knowledge; hence such systems are typically built to cover only very specific conditions. In addition, it is difficult for an expert to foresee the performance of the deployed systems and their potential drawbacks, especially their false alarm rates. It is not uncommon that computer alerting systems are discarded or must undergo multiple costly modification cycles before they reach clinically acceptable levels of performance.    Electronic health record (EHR) repositories today provide an opportunity to test various theories and develop new computational solutions to various clinical problems. The objective of this project is to investigate methods for using the data in such repositories to assist in the development of alerting systems. The project goals include the building of an evidence-driven framework for the evaluation and optimization of alerting systems with the help of past data. The framework will be able to provide early feedback and future performance estimates of an alerting system before it is deployed, which is anticipated to decrease the expert effort required to design such a system and lead to a shorter alerting system design cycle. The evidence-driven framework will be tested and evaluated on multiple clinical conditions and compared to the performance of alerting rules currently deployed at the University of Pittsburgh Medical Center (UPMC). The project investigators consist of a multidisciplinary team with expertise in rule-based alerting in the hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, and knowledge based systems.           Project Narrative: There remain numerous opportunities to reduce medical errors by sending computer-based reminders and alerts to clinicians. This project investigates a novel combination of past patient data stored in electronic form and statistical machine-learning methods to help develop and refine computer-based alerts, which are expected to improve healthcare quality and reduce costs.",Using medical records repositories to improve the alert system design,8139263,R01LM010019,"['Amiodarone', 'Archives', 'Clinical', 'Clinical Data', 'Complex', 'Computers', 'Data', 'Databases', 'Detection', 'Development', 'Electronic Health Record', 'Electronics', 'Evaluation', 'Event', 'Expert Opinion', 'Feedback', 'Future', 'Goals', 'Gold', 'Heparin', 'Hospitals', 'Human', 'Information Systems', 'Knowledge', 'Knowledge acquisition', 'Label', 'Laboratories', 'Lead', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Medical Errors', 'Medical Informatics', 'Medical Records', 'Medical center', 'Medicine', 'Methods', 'Metric', 'Modeling', 'Modification', 'Monitor', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Pharmacy facility', 'Physicians', 'Research', 'Research Personnel', 'Solutions', 'Source', 'Statistical Methods', 'Stream', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Time', 'Toxic effect', 'Training', 'Uncertainty', 'Universities', 'base', 'biomedical informatics', 'clinical practice', 'cost', 'design', 'evaluation/testing', 'flexibility', 'health care quality', 'improved', 'knowledge base', 'multidisciplinary', 'novel', 'predictive modeling', 'repository', 'response', 'statistics', 'success', 'theories', 'treatment response']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,366808,0.029416682191504045
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8085051,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2011,588817,0.02123076526709777
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8055880,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2011,328942,0.008601118362087327
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",8133360,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'clinical care', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,664617,0.039624202622556545
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8022026,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,591195,0.017581636758104324
"An in-silico method for epidemiological studies using Electronic Medical Records DESCRIPTION: Observational epidemiological studies are effective methods for identifying factors affecting the health and illness of populations, as well as for determining optimal treatments for diseases, such as cancers. However, conventional epidemiological research usually involves personnel-intensive effort (such as manual chart and public records review) and can be very time consuming before conclusive results are obtained. Recently, a large amount of detailed longitudinal clinical data has been accumulated at hospitals' Electronic Medical Records (EMR) systems and it has become a valuable data source for epidemiological studies. However, there are two obstacles that prevent the wide usage of EMR data in epidemiological studies. First, most of the detailed clinical information in EMRs is embedded in narrative text and it is very costly to extract that information manually. Second, EMRs usually have data quality problems such as selection bias and missing data, which require adaptation of conventional statistical methods developed for randomized controlled trials.   In this study, we propose an in silico informatics-based approach for observational epidemiological studies using EMR data. We hypothesize that existing EMR data can be used for certain types of epidemiological studies in a very efficient manner with the help of informatics methods. The informatics-based approach will contain two major components. One is an NLP (Natural Language Processing) based information extraction system that can automatically extract detailed clinical information from EMR and another is a set of statistical and informatics methods that can be used to analyze EMR-derived data. If the feasibility of this approach is proven, it will change the standard paradigm of observational epidemiological research, because it has the capability to answer an epidemiological question in a very short time at a very low cost. The specific aim of this study is to develop an automated informatics approach to extract both fine-grained cancer findings and general clinical information from EMRs and use them to conduct cancer related epidemiological studies. We will perform both casecontrol and cohort studies related to prevention and treatment of breast and colon cancers using EMR data. The informatics approach will be validated on EMRs from two major hospitals to demonstrate its generalizability. Epidemiological findings from our study will be compared to reported findings for validation.  Project Narrative According to the American Cancer Society, about 7.6 million people died from various types of cancer in the world during 2007. It is very important to identify risk factors of cancers and to determine optimal treatments of cancers, and epidemiological study is one of the methods to achieve it. This proposed study will use natural language processing technologies to automatically extract fine-grained cancer information from existing patient electronic medical records and use it to conduct cancer related epidemiological studies, thus accelerating knowledge accumulation of cancer research.",An in-silico method for epidemiological studies using Electronic Medical Records,8110041,R01CA141307,"['Affect', 'American Cancer Society', 'Breast Cancer Treatment', 'Cereals', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Colon Carcinoma', 'Computer Simulation', 'Computerized Medical Record', 'Data', 'Data Quality', 'Data Sources', 'Disease', 'Epidemiologic Studies', 'Epidemiology', 'Health', 'Hospitals', 'Human Resources', 'Informatics', 'Knowledge', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Natural Language Processing', 'Patients', 'Population', 'Prevention', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research', 'Risk Factors', 'Selection Bias', 'Statistical Methods', 'System', 'Technology', 'Text', 'Time', 'Validation', 'anticancer research', 'base', 'cancer therapy', 'cancer type', 'cost', 'prevent']",NCI,VANDERBILT UNIVERSITY,R01,2011,252298,0.024729723801679353
"Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2)    DESCRIPTION (provided by applicant): The eTfor2 project will develop and evaluate open-source programs and knowledge representations to better characterize patients for translational and clinical research studies. The project addresses National Library of Medicine (NLM) RFA initiatives for: (a) information & knowledge processing, including natural language processing and text summarization, (b) approaches for linking phenomic and genomic information, and (c) integration of information from heterogeneous sources. Translational studies correlate clinical patient descriptors (phenome) with results of genomic investigations, e.g., genome-wide association studies (GWAS). Standard methods for defining phenotypes require costly, labor-intensive cohort enrollments to identify patients with diseases and appropriate controls. Recently, translational and clinical researchers have used electronic medical record (EMR) data as an alternative to identifying patient characteristics. However, EMR case extraction requires substantial manual review and ""tuning"" for case selection, due to the inaccuracies inherent in ICD9 billing codes. While relevant and useful natural language processing (NLP) approaches to facilitate EMR text extraction have proliferated, the target patient descriptors these approaches employ typically remain non-standard and locally defined, and vary from disease to disease, project to project and institution to institution. At best, such NLP applications use standard terminology descriptors such as SNOMED-CT as EMR extraction targets. Yet, there is no generally utilized ""standard"" knowledge base that links such ""extractable"" descriptors to an academic-quality knowledge source detailing what findings have been reliably reported to occur in each disease. To facilitate translational and clinical research, the eTfor2 project will make available an open-source, evidence-based, electronic clinical knowledge base (KB) and related NLP tools enabling researchers at any site to extract a standard ""target"" set of EMR-based phenomic descriptors at both the finding and disease levels. It will further include diagnostic decision support logic to confirm the degree of support for patients' diagnoses in their EMR records. The eTfor2 project will decrease effort required to harvest EMR patient descriptors for clinical and translational studies, and enable new translational work that identifies genomic associations at both finding and disease levels. The eTfor2 resources should improve the quality and cross-institutional validity of EMR-based translational and clinical studies.           Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2) Project Narrative When completed successfully, the eTfor2 project will enable researchers at disparate institutions to extract from their respective EMR systems a shared ""target"" set of common phenomic descriptors, in a standard, reproducible manner. Doing so should improve the quality and cross-institutional validity of EMR-based translational and clinical studies.",Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2),8145183,R01LM010828,"['18 year old', 'Abdomen', 'Abdominal Pain', 'Address', 'Adult', 'Algorithms', 'Automated Abstracting', 'Biopsy', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Clinical Research', 'Code', 'Cohort Studies', 'Companions', 'Computer-Assisted Diagnosis', 'Computerized Medical Record', 'Core Facility', 'DNA', 'DNA Databases', 'Data', 'Data Analyses', 'Descriptor', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Electronics', 'Enrollment', 'Epigastrium', 'Evaluation Studies', 'Exhibits', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Gold', 'Harvest', 'Human', 'Image', 'Individual', 'Institution', 'Intellectual Property', 'Internal Medicine', 'Internist', 'Intra-abdominal', 'Investigation', 'Knowledge', 'Laboratories', 'Licensing', 'Link', 'Literature', 'Logic', 'Manuals', 'Maps', 'Methods', 'Metric', 'Names', 'Natural Language Processing', 'Negative Finding', 'Normal Range', 'Outcome', 'Pain', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Physical Examination', 'Process', 'Proliferating', 'Property Rights', 'Proteomics', 'Publishing', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'SNOMED Clinical Terms', 'Sampling', 'Side', 'Site', 'Source', 'Specific qualifier value', 'Splenomegaly', 'Supplementation', 'Symptoms', 'System', 'Systematized Nomenclature of Medicine', 'Terminology', 'Testing', 'Text', 'Time', 'Translational Research', 'United States National Library of Medicine', 'Universities', 'Visceromegaly', 'Vocabulary', 'Work', 'base', 'case control', 'clinical phenotype', 'cohort', 'evidence base', 'genome wide association study', 'improved', 'information organization', 'innovation', 'interest', 'knowledge base', 'meetings', 'member', 'open source', 'phenome', 'phenomics', 'programs', 'research study', 'success', 'theories', 'tool', 'translational study']",NLM,VANDERBILT UNIVERSITY,R01,2011,374400,0.02867487427678448
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,8306467,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2011,13000,0.019752062475471188
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8182025,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2011,325163,0.02110221778180246
"Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership    DESCRIPTION (provided by applicant): The Seattle eMERGE project aims to bring personal genomics to practice settings by taking advantage of the extensive electronic medical record (EMR) and biorepository of Group Health Cooperative (GH), including a 33-year pharmacy database and longitudinal data on an aging population. Algorithms developed in eMERGE I will be used to combine genome-wide association studies with phenotypes mined from EMRs to discover new polymorphism-phenotype relationships. Target phenotypes are infectious disease susceptibility, specifically to Clostridium difficile diarrhea, shingles from varicella zoster virus, and fungal nail infection, responses to antihypertensive drugs, serotonin-specific reuptake inhibitors, and statins, including adverse events. A new algorithm will follow longitudinal glycemia and hematocrit trajectories, and a novel automated method will detect karyotype abnormalities for assessing correlation to myelodysplasia and leukemia. Data will also support phenotypes investigated at other eMERGE sites. To create a model for introducing genomics into clinical practice, successful needs assessment methods from eMERGE I will engage stakeholders in guiding development of prototype EMR user interfaces in a clinical decision support format. The test case will be human leukocyte antigen-typing for an adverse drug reaction and the setting will be the patient-centered medical home care model developed at GH. This proposal provides the eMERGE network and its collaborators with the Seattle team's unique expertise in using natural language processing (NLP) to extract information from EMRs, and assisting in adoption of NLP methods. To disseminate eMERGE results and foster collaborations, it takes advantage of leadership positions of the investigators, including partners within eMERGE, other consortia and the HMO Research network, especially the potential for developments supported by the NIH Director's Common Fund in biobanking and megaepidemiology. Completion of the aims will reveal new, medically useful markers, improve the linking of high-throughput genomic methods to EMR data, and develop policies and practices for bringing individualized evidence-based medicine to communities.       RELEVANCE (See instrucfions): To advance personalized medicine-treatment and preventive care based on individual traits; this project matches small differences in DNA to infectious disease susceptibility and response to statins, serotonin- specific reuptake inhibitors (SSRIs) and blood pressure medications. Methods to use these results in clinical care will be guided by focus groups of patients and caregivers in the patient-centered Group Health system.              n/a",Genetic Discovery and  Application in a Clinical Setting Continuing a Partnership,8192387,U01HG006375,"['Address', 'Adoption', 'Adverse event', 'Algorithms', 'Anemia', 'Antihypertensive Agents', 'Bioethics', 'Blood Pressure', 'Bone Marrow Diseases', 'Caregivers', 'Caring', 'Chromosome abnormality', 'Clinical', 'Clinical Research', 'Clostridium difficile', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Consent', 'DNA', 'Data', 'Development', 'Diabetes Mellitus', 'Diarrhea', 'Disease', 'Disease susceptibility', 'Dysmyelopoietic Syndromes', 'Ensure', 'Epidemiology', 'Evidence Based Medicine', 'Excision', 'Focus Groups', 'Fostering', 'Funding', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'HLA Antigens', 'Health', 'Health system', 'Healthcare', 'Hematocrit procedure', 'Herpes zoster disease', 'Herpesvirus Type 3', 'Incidental Findings', 'Individual', 'Infection', 'Karyotype', 'Knowledge', 'Leadership', 'Libraries', 'Link', 'Low-Density Lipoproteins', 'Medical', 'Medical Records', 'Medical center', 'Medicine', 'Mental Depression', 'Methods', 'Mining', 'Modeling', 'Nail plate', 'Natural Language Processing', 'Needs Assessment', 'Other Genetics', 'Outcome', 'Oxidoreductase', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phenotype', 'Policies', 'Population', 'Positioning Attribute', 'Predisposition', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Qualifying', 'Reaction', 'Research', 'Research Personnel', 'Resources', 'Serotonin', 'Single Nucleotide Polymorphism', 'Site', 'Solutions', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Variant', 'aging population', 'base', 'biobank', 'clinical application', 'clinical care', 'clinical practice', 'clinical research site', 'clinically relevant', 'comparative effectiveness', 'design', 'effectiveness research', 'ethical legal social implication', 'evidence base', 'experience', 'genetic technology', 'genome wide association study', 'improved', 'inhibitor/antagonist', 'leukemia', 'leukocyte antigen typing', 'longitudinal database', 'member', 'novel', 'patient home care', 'patient oriented', 'patient population', 'prototype', 'response', 'reuptake', 'skills', 'standard care', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2011,818798,0.025982980640413204
"Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects    DESCRIPTION (provided by applicant):        	Epidemiologic analyses of health care data can provide critical evidence on the effectiveness and safety of therapeutics. This is particularly vital during the transition from the point of regulatory approval through the early marketing of new drugs, a time when physicians, regulators and payers are all struggling with incomplete data. Health plans pay for these drugs without knowing how their effectiveness and safety compares with established alternatives, as new compounds are tested against placebos rather than active agents, and tested only in selected patients. Non-randomized studies in large healthcare databases can provide fast and less costly evidence on drug effects. However, conventional adjustment methods that rely on a small number of investigator-specified confounders often fail and may produce biased results.     We propose and have preliminary evidence that employing modern medical informatics algorithms that structure and search databases to empirically identify thousands of new covariates. These will then enter established propensity score-based models and so make far more effective use of the information contained in health care databases and electronic medical records (EMRs), resulting in more valid causal interpretations of treatment effects. We will:    - Develop algorithms that make greater use of information contained in longitudinal claims and EMR databases by empirically identifying thousands of potential confounders. The performance of these approaches will be evaluated in 6 example studies encompassing recent drug safety and comparative effectiveness problems, and will be implemented in multiple large claims databases supplemented by such data as lab values and EMR information in subgroups.    -- Develop novel methods for confounding adjustment based on textual information found in EMRs.    -- Expand the newly developed mining algorithms into a framework that integrates distributed database networks with uneven information content, similar to the Sentinel Network recently initiated by FDA.                            This project is likely to produce groundbreaking results at the interface of medicine, biomedical informatics, and epidemiologic methods. After completion of this project a library of documented and validated algorithms will be available to significantly improve confounder control in a range of healthcare databases. The theoretical foundation and the ready-to-use algorithms will likely lead to a fundamental shift in how databases contribute to the fast and accurate assessment of newly-marketed medications.            Large healthcare databases are used to assess the safety and effectiveness of drugs. However, conventional adjustment methods that rely on a limited number of investigator-specific covariates often fail to produce unbiased results. We will develop algorithms that make greater use of information contained in longitudinal claims data and electronic medical records databases by empirically identifying thousands of potential confounders. This will result in improved causal inference on the comparative safety and effectiveness of newly marketed medications that is both less susceptible to investigator omissions and faster than conventional approaches.",Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects,8143550,R01LM010213,"['Address', 'Algorithms', 'Clinical', 'Code', 'Complex', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Distributed Databases', 'Effectiveness', 'Elderly woman', 'Epidemiologic Methods', 'Epidemiology', 'Etiology', 'Evaluation', 'Foundations', 'Head', 'Health Planning', 'Healthcare', 'Heterogeneity', 'Individual', 'Insurance', 'Internet', 'Knowledge', 'Lead', 'Libraries', 'Marketing', 'Medical Informatics', 'Medicare/Medicaid', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Placebos', 'Population', 'Pregnant Women', 'Process', 'Publications', 'Randomized', 'Research', 'Research Personnel', 'Safety', 'Scoring Method', 'Sentinel', 'Severity of illness', 'Solid', 'Solutions', 'Specific qualifier value', 'Speed', 'Structure', 'Subgroup', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'To specify', 'Training', 'base', 'biomedical informatics', 'comparative', 'comparative effectiveness', 'compare effectiveness', 'data mining', 'improved', 'indexing', 'lectures', 'novel', 'outcome forecast', 'patient privacy', 'programs', 'routine care', 'symposium', 'treatment effect']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2011,389385,0.011860340312399319
"Near Miss Narratives from the Fire Service: A Bayesian Analysis    DESCRIPTION (provided by applicant): This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source that has not yet been rigorously investigated. The proposal has 3 aims:  I. to use recently developed auto coding methods to characterize firefighter near miss narratives and classify these narratives into mechanisms of risk/injury. This analysis will apply the International Classification of External Cause of Injuries (ICECI) using Bayesian machine learning techniques to identify the various mechanisms captured in the near miss narratives and their relative prevalence.  II. To identify the correlation between each mechanism of risk/injury and each of the ""Contributing Factors"" listed on the NFFNMRS reporting form. The results will reveal any patterns and trends in the distribution of the contributing factors among the mechanisms, creating a deeper understanding of near miss circumstances, as well as a basis for improving the quality of future near miss data collection.  III. To use manual coding to identify actual injury incidents contained within a random sample of 1,000 near miss narratives and correlate these injuries with the ""Loss Potential"" categories on the NFFNMRS reporting form. The results will demonstrate how actual injuries are distributed within the reporting form's ""Loss Potential"" categories. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention. This study addresses a major gap in firefighter safety knowledge, i.e. the insufficient understanding of near miss events, and will have a high impact on efforts to improve the occupational health and safety of firefighters.      PUBLIC HEALTH RELEVANCE:  This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source, the NFFNMRS that has not yet been rigorously investigated. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention.            This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source, the NFFNMRS that has not yet been rigorously investigated. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention.         ",Near Miss Narratives from the Fire Service: A Bayesian Analysis,8206110,R03OH009984,[' '],NIOSH,DREXEL UNIVERSITY,R03,2011,74075,0.011773802883647802
"A Hybrid General Natural Language Processing Architecture    DESCRIPTION (provided by applicant): Electronic medical records and exchanges offer new opportunities for the analysis of population health data; however, new methods in natural language processing (NLP) must first be developed to structure and codify these records, since most medical data is in the form of free text which cannot be stored and manipulated by computers. Once this is accomplished, population health data can be analyzed which will lead to better treatment guidelines, targeted drug therapy, and more cost effective care. Logical Semantics, Inc. (LSI) proposes to develop new statistical NLP methods for analyzing large scale medical domains. These methods will leverage LSI's semantic annotation technology, which has created the largest semantically annotated clinical corpus in the world. LSI's goal is to semantically index large medical record repositories accurately against propositions arranged in knowledge ontologies and make these indices available for text mining applications. The phase one research is focused on three specific aims that will lead to breakthroughs in the science of NLP: (1) Develop new statistical NLP algorithms employing a large semantically annotated medical corpus, (2) Semi-automate knowledge ontology generation, and (3) Develop and combine rule based with statistical NLP algorithms to create a superior hybrid NLP system. The achievement of these aims will result in computer systems that can extract the meaning from free text medical records so researchers, policy makers, and clinicians can use health analytics to improve healthcare.      PUBLIC HEALTH RELEVANCE: Natural language processing (NLP) has been successful in extracting specific findings and diagnoses from free text medical records. However, for NLP to be useful in health analytics, methods must be devised to capture most of the findings in a medical record. Logical Semantics, Inc. (LSI) proposes to build new statistical algorithms that can scale against the numerous complex findings in medical reports. LSI will leverage its advanced semantic annotation technology which employs corpus linguistics and sentential logic to build these new algorithms. The goal is to abstract over 80% of a free text records into computer readable form so that researchers can develop new treatment guidelines, improve decision support, and deliver more cost effective care.           Project Narrative Natural language processing (NLP) has been successful in extracting specific findings and diagnoses from free text medical records. However, for NLP to be useful in health analytics, methods must be devised to capture most of the findings in a medical record. Logical Semantics, Inc. (LSI) proposes to build new statistical algorithms that can scale against the numerous complex findings in medical reports. LSI will leverage its advanced semantic annotation technology which employs corpus linguistics and sentential logic to build these new algorithms. The goal is to abstract over 80% of a free text records into computer readable form so that researchers can develop new treatment guidelines, improve decision support, and deliver more cost effective care.",A Hybrid General Natural Language Processing Architecture,7996937,R43LM010846,"['Achievement', 'Address', 'Algorithms', 'Architecture', 'Businesses', 'Caring', 'Clinical', 'Communities', 'Complex', 'Computer Systems', 'Computerized Medical Record', 'Computers', 'Data', 'Diagnosis', 'Discipline', 'Generations', 'Goals', 'Guidelines', 'Health', 'Healthcare', 'Hybrids', 'Knowledge', 'Lead', 'Legal patent', 'Linguistics', 'Logic', 'Measures', 'Medical', 'Medical Records', 'Methods', 'Metric', 'Mining', 'Natural Language Processing', 'Ontology', 'Pattern', 'Pharmacotherapy', 'Phase', 'Policy Maker', 'Process', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Work', 'abstracting', 'base', 'cost effective', 'improved', 'indexing', 'knowledge base', 'operation', 'phrases', 'population health', 'public health relevance', 'repository', 'stem', 'success', 'text searching', 'tool']",NLM,"LOGICAL SEMANTICS, INC.",R43,2010,148180,0.012962603066006325
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,7866149,R01LM010681,"['Abbreviations', 'Algorithms', 'Architecture', 'Caring', 'Cessation of life', 'Clinical', 'Communication', 'Computer Systems', 'Computer software', 'Computers', 'Confusion', 'Coronary Arteriosclerosis', 'Databases', 'Detection', 'Disease', 'Documentation', 'Effectiveness', 'Electronics', 'Equipment and supply inventories', 'Frequencies', 'Health Personnel', 'Healthcare', 'Individual', 'Informatics', 'Joints', 'Machine Learning', 'Manuals', 'Medical Records', 'Methods', 'Names', 'Natural Language Processing', 'Nitroglycerin', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Provider', 'Reader', 'Records', 'Serious Adverse Event', 'Solutions', 'System', 'Technology', 'Text', 'Time', 'Work', 'Writing', 'acronyms', 'base', 'health care delivery', 'health record', 'improved', 'innovation', 'insight', 'knowledge base', 'novel', 'phrases', 'satisfaction']",NLM,VANDERBILT UNIVERSITY,R01,2010,387500,0.02705034682262247
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7936991,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,BOSTON CHILDREN'S HOSPITAL,RC1,2010,491408,0.00893214272143386
"Natural Language Processing for Cancer Research Network Surveillance Studies    DESCRIPTION (provided by applicant): This application addresses Broad Challenge Area: (10) Information Technology for Processing Health Care Data and specific Challenge Topic: 10-CA-107 Expand Spectrum of Cancer Surveillance through Informatics Approaches. The proposed project launches a collaborative effort to advance adoption within the HMO Cancer Research Network (CRN) of ""industrial-strength"" natural language processing (NLP) systems useful for mining valuable, research-grade information from unstructured clinical text. Such text is available for processing, now in the electronic medical record (EMR) systems of affiliated CRN health plans. The proposed NLP methods   will create ongoing capacity to tap what has recently been described as ""a treasure trove of historical   unstructured data that provides essential information for the study of disease progression, treatment   effectiveness and long-term outcomes"" (5). The vision of advancing widespread NLP capacity across the CRN, as well as the approach we present here for implementing it, grew out of an in-depth strategic planning effort we completed in December 2008. That effort involved participants from six CRN sites guided by a blue-ribbon panel of NLP experts from three of the nation's leading centers of clinical NLP research: University of Pittsburgh Medical Center, Vanderbilt University, and Mayo Clinic. The vision is to deploy a powerful NLP system locally, manage it with newly hired and trained local NLP technical staff, and conduct NLP-based research projects initiated by local investigators, in consultation with higher-level external NLP experts. Our planning efforts suggest this collaborative model is feasible; we will test the model in the context of the proposed project. An important development in April 2009 yielded what we believe is a potentially transformative opportunity to accelerate adoption of NLP capacity in applied research settings: release of the open-source Clinical Text Analysis and Knowledge Extraction System (cTAKES) software. This software was the result of a collaborative effort between IBM and Mayo Clinic. Built on the same framework Mayo Clinic currently uses to process its repository of over 40 million clinical documents, cTAKES dramatically lowers the cost of adopting a comprehensive and flexible NLP system. Deployment and use of such systems was previously only feasible in institutions with large, academically-oriented biomedical informatics research programs.   Still, other deployment challenges and the need to acquire NLP training for local staff present residual   barriers to adopting comprehensive NLP systems such as cTAKES. In collaboration with five other CRN sites the proposed project mitigates these challenges in two ways: 1) it develops configurable open-source software modules needed to streamline and therefore reduce the cost of deploying cTAKES, and 2) it presents and tests a model for training local staff through hands-on NLP projects overseen by outside NLP expert consultants. The potential impact of this project is evident most clearly in the vast untapped opportunities for text mining represented in CRN-affiliated health plans, where EMR systems have been in place since at least 2005, and whose patients represent 4% of the U.S. population. Clinical text mining offers the potential to provide new or improved data elements for cancer surveillance and other types of research requiring information about patient functional status, medication side-effects, details of therapeutic approaches, and differential information about clinical findings. Another significant impact of this project is its plan to integrate into the cTAKES system   an open-source de-identification tool based on state of the art, best of breed NLP approaches developed by the MITRE Corporation. De-identification of clinical text will make it easier for researchers to get access to clinical text, and will also facilitate multi-site collaborations while protecting patient privacy. Finally, if successful, the NLP algorithm we propose as a proof-of-principle project at Group Health-which will classify sets of patient charts as either containing or not containing a diagnosis of recurrent breast cancer-could dramatically reduce the cost of research in this area; currently all recurrent breast cancer endpoints must be established through costly manual chart abstraction.   Novel aspects of the proposed project include its talented and transdisciplinary research team,   including national experts in NLP, and its resourceful strategy for building the technical resources and ""human capital"" needed to support an ongoing program of applied NLP research. Natural language processing is itself a highly innovative technology; when successfully established in multiple CRN in the future it will represent a watershed moment in the CRN's already impressive history of exploiting data systems to support innovative research. Newly hired staff positions total approximately 2.0 FTE in each project year, most of which we anticipate will be supported by ongoing new research programs after the proposed project concludes. Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.           Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.",Natural Language Processing for Cancer Research Network Surveillance Studies,7944035,RC1CA146917,"['Address', 'Adopted', 'Adoption', 'Adverse effects', 'Algorithms', 'Applied Research', 'Area', 'Arts', 'Bioinformatics', 'Breeding', 'Cancer Research Network', 'Charge', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Complex', 'Comprehensive Health Care', 'Computer software', 'Computerized Medical Record', 'Consultations', 'Data', 'Data Element', 'Data Quality', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Doctor of Philosophy', 'Environment', 'Epidemiology', 'Exercise', 'Future', 'Generic Drugs', 'Hand', 'Health', 'Health Planning', 'Health system plans', 'Healthcare', 'Human Resources', 'Individual', 'Informatics', 'Information Systems', 'Information Technology', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Licensing', 'Life', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Mining', 'Modeling', 'NCI Center for Cancer Research', 'Natural Language Processing', 'Operating System', 'Outcome', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Process', 'Public Health', 'Recording of previous events', 'Recurrence', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Resources', 'Risk', 'Site', 'Solutions', 'Strategic Planning', 'System', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Training', 'Treatment Effectiveness', 'Universities', 'Vision', 'Woman', 'base', 'biomedical informatics', 'breast cancer diagnosis', 'cost', 'design', 'experience', 'feeding', 'firewall', 'flexibility', 'functional status', 'human capital', 'improved', 'innovation', 'innovative technologies', 'malignant breast neoplasm', 'novel', 'open source', 'patient privacy', 'programs', 'repository', 'skills', 'software systems', 'surveillance study', 'text searching', 'tool']",NCI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,RC1,2010,494477,0.005703252833152773
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7779983,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,343397,0.014404018369623775
"Using medical records repositories to improve the alert system design    DESCRIPTION (provided by applicant):       Rapid and accurate alerting of concerning patient events and conditions remains an important problem in clinical practice. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into the monitoring and alerting systems. However, it is often time-consuming and costly to extract and codify such knowledge; hence such systems are typically built to cover only very specific conditions. In addition, it is difficult for an expert to foresee the performance of the deployed systems and their potential drawbacks, especially their false alarm rates. It is not uncommon that computer alerting systems are discarded or must undergo multiple costly modification cycles before they reach clinically acceptable levels of performance.    Electronic health record (EHR) repositories today provide an opportunity to test various theories and develop new computational solutions to various clinical problems. The objective of this project is to investigate methods for using the data in such repositories to assist in the development of alerting systems. The project goals include the building of an evidence-driven framework for the evaluation and optimization of alerting systems with the help of past data. The framework will be able to provide early feedback and future performance estimates of an alerting system before it is deployed, which is anticipated to decrease the expert effort required to design such a system and lead to a shorter alerting system design cycle. The evidence-driven framework will be tested and evaluated on multiple clinical conditions and compared to the performance of alerting rules currently deployed at the University of Pittsburgh Medical Center (UPMC). The project investigators consist of a multidisciplinary team with expertise in rule-based alerting in the hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, and knowledge based systems.           Project Narrative: There remain numerous opportunities to reduce medical errors by sending computer-based reminders and alerts to clinicians. This project investigates a novel combination of past patient data stored in electronic form and statistical machine-learning methods to help develop and refine computer-based alerts, which are expected to improve healthcare quality and reduce costs.",Using medical records repositories to improve the alert system design,7935413,R01LM010019,"['Amiodarone', 'Archives', 'Clinical', 'Clinical Data', 'Complex', 'Computers', 'Data', 'Databases', 'Detection', 'Development', 'Electronic Health Record', 'Electronics', 'Evaluation', 'Event', 'Expert Opinion', 'Feedback', 'Future', 'Goals', 'Gold', 'Heparin', 'Hospitals', 'Human', 'Information Systems', 'Knowledge', 'Knowledge acquisition', 'Label', 'Laboratories', 'Lead', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Medical Errors', 'Medical Informatics', 'Medical Records', 'Medical center', 'Medicine', 'Methods', 'Metric', 'Modeling', 'Modification', 'Monitor', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Pharmacy facility', 'Physicians', 'Research', 'Research Personnel', 'Solutions', 'Source', 'Statistical Methods', 'Stream', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Time', 'Toxic effect', 'Training', 'Uncertainty', 'Universities', 'base', 'biomedical informatics', 'clinical practice', 'cost', 'design', 'evaluation/testing', 'flexibility', 'health care quality', 'improved', 'knowledge base', 'multidisciplinary', 'novel', 'predictive modeling', 'repository', 'response', 'statistics', 'success', 'theories', 'treatment response']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,377274,0.029416682191504045
"An NLP Approach to Generating Patient Record Summaries :  The long-term goal of this proposal is to enhance the manner in which physicians access, process and marshal medical information by providing them with an automatically generated, comprehensive, and up-to date summary of the information appearing in a patient record. At the point of patient care, physicians must often rapidly process a potentially overwhelming quantity of information pertaining to a patient. Failure to do so effectively may lead to provision of suboptimal care. Some electronic health record systems provide an automatically produced “cover sheet” geared to help physicians with a broad overview of a given patient, but the information is derived from the structured data fields in the patient record, ignoring the valuable narrative text entered by clinicians over time. We are building upon our prior work in summarization and natural language processing and leveraging our expertise in cognitive research studying information needs and decision making of clinicians to build a patient record summarizer that gathers information narrative (unstructured) as well as structured parts in the record. We focus on producing a summary for patients with kidney disease, as they often have a complex medical history with numerous conditions, procedures and medications. Providing a holistic, up-to-date summary of their chart would prove valuable to physicians in general and nephrologists in particular. The following three aims will be carried out: (1) conduct a formative study to determine how physicians prioritize and mentally represent relevant information when reviewing a patient chart; (2) create a set of automated methods to select salient pieces of information in the patient record and organize them into a coherent summary; and (3) evaluate the efficacy, efficiency and physician-user satisfaction associated with the use of the summarizer. A primary strength of this proposal is that we are addressing the problem of information overload, a bottleneck in the use of electronic health records, and evaluate the impact of our solution on clinicians’ actions and patients’ health outcomes. Furthermore, we propose to use novel natural language processing, knowledge-based and data mining methods to extract and organize salient information. Finally, we contribute to informatics research by extending the electronic health record functionalities to go beyond a simple documentation-entry system towards a useful reference and decision-making tool for physicians  Project Narrative We propose to design an automatically generated, comprehensive, and up-to-date summary of the information appearing in a patient record. Such a summary would enhance the manner in which both patients and their physicians access, process and marshal medical information.",An NLP Approach to Generating Patient Record Summaries,7925659,R01LM010027,"['Address', 'Allergic', 'Caring', 'Clinic', 'Clinical', 'Cognitive', 'Complex', 'Data', 'Data Analyses', 'Decision Making', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation Studies', 'Face', 'Failure', 'Feasibility Studies', 'Goals', 'Hand', 'Health', 'Health Status', 'Informatics', 'Information Resources', 'Interview', 'Kidney Diseases', 'Knowledge', 'Laboratories', 'Lead', 'Link', 'Marshal', 'Medical', 'Medical History', 'MedlinePlus', 'Methods', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Procedures', 'Process', 'Records', 'Research', 'Resources', 'Solutions', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Visit', 'Work', 'data mining', 'design', 'health literacy', 'information gathering', 'knowledge base', 'literate', 'medical schools', 'meetings', 'novel', 'research study', 'satisfaction', 'stem', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,456856,0.017318347245420453
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7784533,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,341606,0.008601118362087327
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8056227,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,177422,0.008601118362087327
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",7985218,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Caring', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,1,0.039624202622556545
"Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2)    DESCRIPTION (provided by applicant): The eTfor2 project will develop and evaluate open-source programs and knowledge representations to better characterize patients for translational and clinical research studies. The project addresses National Library of Medicine (NLM) RFA initiatives for: (a) information & knowledge processing, including natural language processing and text summarization, (b) approaches for linking phenomic and genomic information, and (c) integration of information from heterogeneous sources. Translational studies correlate clinical patient descriptors (phenome) with results of genomic investigations, e.g., genome-wide association studies (GWAS). Standard methods for defining phenotypes require costly, labor-intensive cohort enrollments to identify patients with diseases and appropriate controls. Recently, translational and clinical researchers have used electronic medical record (EMR) data as an alternative to identifying patient characteristics. However, EMR case extraction requires substantial manual review and ""tuning"" for case selection, due to the inaccuracies inherent in ICD9 billing codes. While relevant and useful natural language processing (NLP) approaches to facilitate EMR text extraction have proliferated, the target patient descriptors these approaches employ typically remain non-standard and locally defined, and vary from disease to disease, project to project and institution to institution. At best, such NLP applications use standard terminology descriptors such as SNOMED-CT as EMR extraction targets. Yet, there is no generally utilized ""standard"" knowledge base that links such ""extractable"" descriptors to an academic-quality knowledge source detailing what findings have been reliably reported to occur in each disease. To facilitate translational and clinical research, the eTfor2 project will make available an open-source, evidence-based, electronic clinical knowledge base (KB) and related NLP tools enabling researchers at any site to extract a standard ""target"" set of EMR-based phenomic descriptors at both the finding and disease levels. It will further include diagnostic decision support logic to confirm the degree of support for patients' diagnoses in their EMR records. The eTfor2 project will decrease effort required to harvest EMR patient descriptors for clinical and translational studies, and enable new translational work that identifies genomic associations at both finding and disease levels. The eTfor2 resources should improve the quality and cross-institutional validity of EMR-based translational and clinical studies.           Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2) Project Narrative When completed successfully, the eTfor2 project will enable researchers at disparate institutions to extract from their respective EMR systems a shared ""target"" set of common phenomic descriptors, in a standard, reproducible manner. Doing so should improve the quality and cross-institutional validity of EMR-based translational and clinical studies.",Evidence-based Diagnostic Tools for Translational and Clinical Research (eTfor2),7950411,R01LM010828,"['18 year old', 'Abdomen', 'Abdominal Pain', 'Address', 'Adult', 'Algorithms', 'Automated Abstracting', 'Biopsy', 'Characteristics', 'Child', 'Clinical', 'Clinical Research', 'Code', 'Cohort Studies', 'Companions', 'Computer-Assisted Diagnosis', 'Computerized Medical Record', 'Core Facility', 'DNA', 'DNA Databases', 'Data', 'Data Analyses', 'Descriptor', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Electronics', 'Enrollment', 'Epigastrium', 'Evaluation Studies', 'Exhibits', 'Generic Drugs', 'Genes', 'Genomics', 'Goals', 'Gold', 'Harvest', 'Human', 'Image', 'Individual', 'Institution', 'Intellectual Property', 'Internal Medicine', 'Internist', 'Intra-abdominal', 'Investigation', 'Knowledge', 'Laboratories', 'Licensing', 'Link', 'Literature', 'Logic', 'Manuals', 'Maps', 'Methods', 'Metric', 'Names', 'Natural Language Processing', 'Negative Finding', 'Normal Range', 'Outcome', 'Pain', 'Patient Care', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Physical Examination', 'Process', 'Proliferating', 'Property Rights', 'Proteomics', 'Publishing', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'SNOMED Clinical Terms', 'Sampling', 'Side', 'Site', 'Source', 'Specific qualifier value', 'Splenomegaly', 'Supplementation', 'Symptoms', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Translational Research', 'United States National Library of Medicine', 'Universities', 'Visceromegaly', 'Vocabulary', 'Work', 'base', 'case control', 'clinical phenotype', 'cohort', 'evidence base', 'genome wide association study', 'improved', 'information organization', 'innovation', 'interest', 'knowledge base', 'meetings', 'member', 'open source', 'phenome', 'phenomics', 'programs', 'research study', 'success', 'theories', 'tool', 'translational study']",NLM,VANDERBILT UNIVERSITY,R01,2010,388125,0.02867487427678448
"An in-silico method for epidemiological studies using Electronic Medical Records DESCRIPTION: Observational epidemiological studies are effective methods for identifying factors affecting the health and illness of populations, as well as for determining optimal treatments for diseases, such as cancers. However, conventional epidemiological research usually involves personnel-intensive effort (such as manual chart and public records review) and can be very time consuming before conclusive results are obtained. Recently, a large amount of detailed longitudinal clinical data has been accumulated at hospitals' Electronic Medical Records (EMR) systems and it has become a valuable data source for epidemiological studies. However, there are two obstacles that prevent the wide usage of EMR data in epidemiological studies. First, most of the detailed clinical information in EMRs is embedded in narrative text and it is very costly to extract that information manually. Second, EMRs usually have data quality problems such as selection bias and missing data, which require adaptation of conventional statistical methods developed for randomized controlled trials.   In this study, we propose an in silico informatics-based approach for observational epidemiological studies using EMR data. We hypothesize that existing EMR data can be used for certain types of epidemiological studies in a very efficient manner with the help of informatics methods. The informatics-based approach will contain two major components. One is an NLP (Natural Language Processing) based information extraction system that can automatically extract detailed clinical information from EMR and another is a set of statistical and informatics methods that can be used to analyze EMR-derived data. If the feasibility of this approach is proven, it will change the standard paradigm of observational epidemiological research, because it has the capability to answer an epidemiological question in a very short time at a very low cost. The specific aim of this study is to develop an automated informatics approach to extract both fine-grained cancer findings and general clinical information from EMRs and use them to conduct cancer related epidemiological studies. We will perform both casecontrol and cohort studies related to prevention and treatment of breast and colon cancers using EMR data. The informatics approach will be validated on EMRs from two major hospitals to demonstrate its generalizability. Epidemiological findings from our study will be compared to reported findings for validation. Narrative: According to the American Cancer Society, about 7.6 million people died from various types of cancer in the world during 2007. It is very important to identify risk factors of  cancers and to determine optimal treatments of cancers, and epidemiological study is  one of the methods to achieve it. This proposed study will use natural language  processing technologies to automatically extract fine-grained cancer information from  existing patient electronic medical records and use it to conduct cancer related  epidemiological studies, thus accelerating knowledge accumulation of cancer research.",An in-silico method for epidemiological studies using Electronic Medical Records,7925776,R01CA141307,"['Affect', 'American Cancer Society', 'Breast', 'Cereals', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Colon Carcinoma', 'Computer Simulation', 'Computerized Medical Record', 'Data', 'Data Quality', 'Data Sources', 'Disease', 'Epidemiologic Studies', 'Epidemiology', 'Health', 'Hospitals', 'Human Resources', 'Informatics', 'Knowledge', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Natural Language Processing', 'Patients', 'Population', 'Prevention', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research', 'Risk Factors', 'Selection Bias', 'Statistical Methods', 'System', 'Technology', 'Text', 'Time', 'Validation', 'anticancer research', 'base', 'cancer therapy', 'cancer type', 'cost', 'prevent']",NCI,VANDERBILT UNIVERSITY,R01,2010,259993,0.024652015859172288
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,8129905,R43HG005046,"['Address', 'Benefits and Risks', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2010,35000,0.0009703234356238767
"Online Social Networking as an Alternative Information Source for Clinical Resear    DESCRIPTION (provided by applicant): Clinical trials and patient records have been the main information sources for clinical research. While well- designed clinical trials can produce high quality data, they are generally very expensive and time consuming. Prior studies have also shown that patients enrolled in clinical trials are not necessarily representative of the general patient population. Chart reviews, which rely on the patient records, avoid some of the drawbacks of the clinical trials approach. Although chart review studies are more labor intensive, new developments in structured data entry and natural language processing (NLP) are helping to automate the process. However, studies which use chart reviews are limited by the accuracy and completeness of the data in the records.       In the past decade, online social networks have grown exponentially. Some health-focused social network sites have attracted large numbers of users and begun accumulating large quantities of detailed clinical information. The PatientsLikeMe site, for instance, has about 3,200 amyotrophic lateral sclerosis (ALS) patients worldwide, and includes about 5% of the ALS population in the US. Information gathered by online social networks is primarily intended for patients to share with each other. Such information has also begun to attract the attention of medical researchers.[3, 4]       Because using information from online social networks for medical research is a fairly new phenomenon, the value and limitation of this type of information source have not been systematically examined. To do so, we propose to conduct a comparison study of patient-contributed information from PatientsLikeMe and records from a large medical record data repository - the Research Patient Data Registry (RPDR) of the Partners Healthcare Systems. The proposed study will focus on ALS, multiple sclerosis (MS), and Parkinson's disease (PD). The general goal is to explore how the medical record and online networking data differ, and if and how online networking data could complement the medical record data. The specific aims are:    1) Extract symptom and treatment information from the two different data sources.    2) Compare the prevalence of symptoms and treatments from the two information sources and analyze the difference.    3) Extract treatment response of prescription medications from PatientsLikeMe and analyze the confounding effect of the misunderstanding of medication indication.      PUBLIC HEALTH RELEVANCE: The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.           The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.",Online Social Networking as an Alternative Information Source for Clinical Resear,7941839,R21NS067463,"['Amyotrophic Lateral Sclerosis', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Communities', 'Comparative Study', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Enrollment', 'Frequencies', 'Goals', 'Healthcare Systems', 'Medical Records', 'Medical Research', 'Multiple Sclerosis', 'Natural Language Processing', 'Nature', 'Parkinson Disease', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Prevalence', 'Process', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Site', 'Source', 'Structure', 'Symptoms', 'System', 'Text', 'Time', 'Update', 'experience', 'information gathering', 'medical attention', 'patient population', 'public health relevance', 'social networking website', 'statistics', 'treatment response', 'web-based social networking']",NINDS,UNIVERSITY OF UTAH,R21,2010,233374,-0.010164793881612932
"Temporal Processing for Medical Discharge Summaries    DESCRIPTION (provided by applicant):       The goals of our project are as follows:     1. Create a corpus of temporally annotated data. Under the supervision of our consultants Dr. Frank Sacks, Dr. Vincent Carey, and two Registered Nurses, we will create a gold-standard annotation of events and temporal information within patient narratives from de- identified Electronic Health Record data using the CLEF and TimeML guidelines. We will use the framework of the Brandeis Annotation Tool, a system we have designed to facilitate the quick construction of accurately annotated corpora against a specified guideline. Extensions to the current event library and lexicon with medical event references will be made during the annotation process, under the guidance of the Registered Nurses.          2. Adapt the TARSQI Toolkit (TTK) to targeted temporal properties and relations in the EHR domain. We will use the TARSQI toolkit, a robust set of temporal processing algorithms we have designed for parsing natural language text, to automatically annotate the events and temporal information in EHR data. Combined with the Brandeis AcroMed Medical Abbreviation Server and those terms introduced in part 1, we will employ the Specialist Lexicon and other medical resources to extend the toolkit capabilities for recognizing and interpreting medical event information. Algorithms for identifying events, temporal expressions, and event anchorings and orderings will be trained against the gold standard created in Aim 1, and tested against held-out data.     3. Create a cross-document temporal database of medical events. Using the recognition algorithms introduced in Aim 2, we will create a searchable, temporally ordered database of medical events such as diseases, symptoms, surgeries/interventions, and test results. Events referred to multiple times in the data will be merged using a constraint- satisfaction analysis in order to create a more coherent narrative for a single patient over multiple records.           Project Narrative It is becoming increasingly common for medical researchers to use Electronic Health Records (EHRs) as a primary source of data for researching correlations between various medical issues and concepts. However, EHRs typically contain unstructured text, making them difficult to mine. This research will create a database of temporal orderings from events extracted from EHR patient narratives, using algorithms previously applied to news articles.",Temporal Processing for Medical Discharge Summaries,7941063,R21LM009633,"['Abbreviations', 'Adopted', 'Algorithms', 'Authorization documentation', 'Clinical', 'Clinical Research', 'Data', 'Data Sources', 'Databases', 'Diagnosis', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Event', 'Goals', 'Gold', 'Guidelines', 'Intervention', 'Language', 'Libraries', 'Licensing', 'Machine Learning', 'Medical', 'Medical History', 'Medical Libraries', 'Mining', 'Operative Surgical Procedures', 'Patients', 'Process', 'Property', 'Records', 'Registered nurse', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Science', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Supervision', 'Symptoms', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Vocabulary', 'acronyms', 'base', 'design', 'evidence base', 'experience', 'interest', 'lexical', 'natural language', 'news', 'open source', 'relational database', 'repository', 'satisfaction', 'tool']",NLM,BRANDEIS UNIVERSITY,R21,2010,175973,0.030614768825674078
"Voice Based, Workflow Enhancing, Primary Care Medical Data Input System    DESCRIPTION (provided by applicant): The adoption of electronic health records (EHR) in hospitals and physician offices has been widely promoted as a single solution to a wide variety of health care issues. Yet 84% of small and medium business (SMB) physician practices in the US have not adopted EHR systems. Interventional Dynamics Corporation (IDC) has conducted more than 200 primary care physician interviews, finding that the major disincentives to adoption are workflow delay and expense. The single greatest factor in the reduction of workflow speed is the data input process. IDC's proposed project has this specific aim: Utilize an innovative voice entry technique and open source code systems to develop a low-cost, automated solution to allow primary care physicians to complete a primary care note entirely during the patient examination process. The narrative speech input will be analyzed in a context-sensitive, domain-restricted manner to produce structured clinical data that can be readily integrated into standards-compliant electronic medical records. By using speech inputs that are converted directly to relevant EHR entries, physicians can increase the accuracy of their notes, eliminate third party transcription errors and avoid workflow delays. The project approach will include:    Further testing and final development of DocTalk, the IDC patent pending speech system that allows accurate natural language processing of structured medical information; Development of a proof-of-concept data system that converts physician voice input from voice to text to structured text to EHR data using domain enhanced open source code; The evaluation of the effectiveness of the proof-of-concept system against traditional EHR  input methods with the following goals: Achieve 50% or more reduction in charting time,  achieve 90% or more accuracy in output, and score greater than 4 of 5 on subjective  metrics including learnability, workflow fit, usability, and overall satisfaction. Successful completion of the proposed program will provide IDC with a viable technology platform that can immediately be useful to primary care physicians in generating structured documents for use with their current EHR platforms. Furthermore, the technology developed and refined within this program can be expanded in multiple ways.      PUBLIC HEALTH RELEVANCE:  The IDC technology is designed to circumvent the normal barriers to adoption in the SMB market and allow for quick increases in workflow and quality of patient care at a minimal price point. IDC will provide physicians who currently use pen and paper a more natural and faster way to input clinical data, eliminating time spent on hunt-and-peck keyboard entry or complicated EHR screen navigation. The system will generate structured clinical data that enables the exchange of health information, the portability of patient records, billing, data analytics (both local practice and public health), marketing, and other benefits, resulting in the reduction of overall healthcare costs.           Narrative The IDC technology is designed to circumvent the normal barriers to adoption in the SMB market and allow for quick increases in workflow and quality of patient care at a minimal price point. IDC will provide physicians who currently use pen and paper a more natural and faster way to input clinical data, eliminating time spent on hunt-and-peck keyboard entry or complicated EHR screen navigation. The system will generate structured clinical data that enables the exchange of health information, the portability of patient records, billing, data analytics (both local practice and public health), marketing, and other benefits, resulting in the reduction of overall healthcare costs.","Voice Based, Workflow Enhancing, Primary Care Medical Data Input System",7924457,R43LM010750,"['Adopted', 'Adoption', 'Architecture', 'Businesses', 'Clinical', 'Clinical Data', 'Code', 'Computer Assisted', 'Computerized Medical Record', 'Data', 'Data Storage and Retrieval', 'Development', 'Disincentive', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Genetic Transcription', 'Goals', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitals', 'Industry', 'Information Systems', 'International', 'Interview', 'Legal patent', 'Marketing', 'Medical', 'Medical Records', 'Methods', 'Metric', 'Natural Language Processing', 'Output', 'Paper', 'Patient Care', 'Patients', 'Pediatric Surgical Procedures', 'Physicians', 'Physicians&apos', ' Offices', 'Price', 'Primary Care Physician', 'Primary Health Care', 'Process', 'Public Health Practice', 'Records', 'Research', 'Research Project Grants', 'Services', 'Solutions', 'Source Code', 'Speech', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Universities', 'Voice', 'base', 'billing data', 'cost', 'design', 'flexibility', 'innovation', 'open source', 'portability', 'programs', 'public health relevance', 'satisfaction', 'speech recognition', 'usability']",NLM,"VMT, INC.",R43,2010,241875,0.03883304408531539
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7893787,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2010,1512082,0.019752062475471188
"Enhancing Clinical Effectiveness Research with Natural Language Processing of EMR    DESCRIPTION (provided by applicant): To successfully use large linked clinical databases for comparative effectiveness research (CER) requires addressing some key informatics challenges associated with distributed, heterogeneous clinical data. Electronic networks of researchers are part of the solution because they can bridge the physical and organizational divides created by distinct health systems' individual electronic medical records (EMRs). In addition, informatics research has demonstrated the feasibility of automatically coding clinical text, enhancing the capacity to integrate both unstructured and non-standardized clinical data from EMRs. With this study, we propose to develop CER infrastructure, make broadly available the proven MediClass technology for automated classification of EMRs containing both coded data and text clinical notes, and demonstrate the potential of this infrastructure for addressing CER questions within the asthma and tobacco-using patient populations of 6 diverse health systems. Asthma and smoking each impose huge and modifiable burdens on the healthcare system, and multiple morbidities related to asthma and smoking have been targeted by the IOM and AHRQ as priority areas in efforts to improve the healthcare system through comparative effectiveness research. We propose to develop, deploy, operate and evaluate the CER HUB, an Internet-based platform for conducting CER, and to demonstrate its utility in studying clinical interventions in asthma and smoking. Researchers who register to use the HUB, beginning with the research team from the 6 participating study sites, will be able to use a secure website to configure and download MediClass applications addressing CER questions within their respective healthcare organizations, to contribute these IRB-approved, processed datasets back to a centralized data coordinating center to be pooled with data similarly processed from other healthcare organizations, and to use the pooled database to answer diverse comparative effectiveness questions of large, real-world populations. A central function of the CER HUB will be facilitating (through online, interactive tools) development of a shared library of MediClass knowledge modules that afford uniform, standardized coding of EMR data. This shared library of knowledge modules could permit researchers to assess effectiveness in multiple areas of healthcare and gain access to data otherwise locked away in text clinical notes. A goal of the CER HUB is to accelerate creation of standardized knowledge used to normalize heterogeneous EMR data as representations of clinical events for CER. During the project period we will conduct 2 studies using this infrastructure to address the effectiveness of interventions for asthmatics and tobacco users across the 6 participating health systems. As an ongoing resource, the HUB will provide a collaborative development platform for enhancing comparative effectiveness research in potentially any health care domain.      CER researchers can build software applications that will process their EMRs, creating standardized datasets permitting CER using a secure website to configure and download MediClass applications addressing CER questions within their respective healthcare organizations, to contribute these IRB-approved, processed datasets back to a centralized data coordinating center to be pooled with data similarly processed from other healthcare organizations, and to use the pooled database to answer diverse comparative effectiveness questions of large, real-world populations      PUBLIC HEALTH RELEVANCE: Comparative effectiveness research (CER) requires that clinical data be in standard forms allowing multiple, large databases to be efficiently combined, and requires that all of the data be coded so that automated summarization of the data is possible. However, much of the clinical data necessary for CER is in the text clinical notes written by clinicians when caring for patients. We will build a centralized website where CER researchers can build software applications that will process their electronic medical records, including both the text and coded data, creating standardized datasets permitting comparative effectiveness research. We will demonstrate the utility of this infrastructure by conducting CER studies investigating the effectiveness of interventions in asthma and smoking, across the 6 participating health systems.           PROJECT NARRATIVE Comparative effectiveness research (CER) requires that clinical data be in standard forms allowing multiple, large databases to be efficiently combined, and requires that all of the data be coded so that automated summarization of the data is possible. However, much of the clinical data necessary for CER is in the text clinical notes written by clinicians when caring for patients. We will build a centralized website where CER researchers can build software applications that will process their electronic medical records, including both the text and coded data, creating standardized datasets permitting comparative effectiveness research. We will demonstrate the utility of this infrastructure by conducting CER studies investigating the effectiveness of interventions in asthma and smoking, across the 6 participating health systems.",Enhancing Clinical Effectiveness Research with Natural Language Processing of EMR,8032928,R01HS019828,[' '],AHRQ,KAISER FOUNDATION RESEARCH INSTITUTE,R01,2010,8696942,-0.0588090498291531
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",8231171,R01GM090187,[' '],NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,642650,0.039624202622556545
"Data Structuring and Visualization System for Neuro-oncology    DESCRIPTION (provided by applicant):       The medical record for a neuro-oncology patient is complex, consisting of typically a large number of both text and imaging data. It includes descriptions of prior observations, interpretations, and interventions which need to be integrated into decisions regarding current patient care. An appropriate review of a patient's medical record often requires that a physician review multiple clinical documents while mentally noting issues related to what the findings were, the chronology of events, spatial/temporal patterns of disease progression, the effects of interventions, and the possible causal lines of explanation of observed findings. Additionally, imaging data and imaging- derived conclusions are poorly integrated into patient care and management decisions. The physician also needs to filter out those pieces of information not related to the current clinical context of care. Given the time constraints, data complexity and data volume associated with chronic patient cases, an appropriate review of a patient's chart is in reality rarely performed. Additionally, the lack of tools for formalizing the representation of the accounts of current and prior cases impedes the development of clinical databases that can be ultimately used to learn patterns of disease.       This proposal addresses the development of a system for facilitating the review of clinical patient data intended to promote an orderly process of medical problem understanding and care. The specific aims of the proposal are summarized as follows: 1) Development of a backend tool to facilitate the structured representation of observations, events, and inferences stated within medical reports. 2) Development of an application interface for visualizing, navigating, and editing structured patient data. 3) Evaluation of the effectiveness of the application in the domain of neuro-oncology.       Relation to public health. If the goals of this proposal can be realized, neuro-oncologist should be able to more easily seek desired patient data and detect patterns of evidence as compared to the current mode of operation (HIS, RIS, PACS). The structuring tools should lead to improvements in the quality of clinical research databases.           Narrative Medical records for neuro-oncology patients are difficult to review due to volume and complexity of information. A novel system for partitioning data along the information axes of space, time, existence, and causality is proposed to improve navigation and assimilation of data within the medical record.",Data Structuring and Visualization System for Neuro-oncology,7877057,R01LM009961,"['Abnormal coordination', 'Accounting', 'Address', 'Anatomy', 'Appearance', 'Assimilations', 'Caregivers', 'Caring', 'Chronic', 'Chronology', 'Clinic', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Effectiveness', 'Etiology', 'Evaluation', 'Event', 'Goals', 'Image', 'Imagery', 'Intervention', 'Lead', 'Learning', 'Medical', 'Medical Records', 'Metric', 'Natural Language Processing', 'Oncologist', 'Patient Care', 'Patient Care Management', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Property', 'Public Health', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Training', 'abstracting', 'follow-up', 'improved', 'innovation', 'intervention effect', 'neuro-oncology', 'novel', 'open source', 'operation', 'physical state', 'satisfaction', 'tool']",NLM,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2010,771496,0.03538732443888387
"Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects    DESCRIPTION (provided by applicant):        	Epidemiologic analyses of health care data can provide critical evidence on the effectiveness and safety of therapeutics. This is particularly vital during the transition from the point of regulatory approval through the early marketing of new drugs, a time when physicians, regulators and payers are all struggling with incomplete data. Health plans pay for these drugs without knowing how their effectiveness and safety compares with established alternatives, as new compounds are tested against placebos rather than active agents, and tested only in selected patients. Non-randomized studies in large healthcare databases can provide fast and less costly evidence on drug effects. However, conventional adjustment methods that rely on a small number of investigator-specified confounders often fail and may produce biased results.     We propose and have preliminary evidence that employing modern medical informatics algorithms that structure and search databases to empirically identify thousands of new covariates. These will then enter established propensity score-based models and so make far more effective use of the information contained in health care databases and electronic medical records (EMRs), resulting in more valid causal interpretations of treatment effects. We will:    - Develop algorithms that make greater use of information contained in longitudinal claims and EMR databases by empirically identifying thousands of potential confounders. The performance of these approaches will be evaluated in 6 example studies encompassing recent drug safety and comparative effectiveness problems, and will be implemented in multiple large claims databases supplemented by such data as lab values and EMR information in subgroups.    -- Develop novel methods for confounding adjustment based on textual information found in EMRs.    -- Expand the newly developed mining algorithms into a framework that integrates distributed database networks with uneven information content, similar to the Sentinel Network recently initiated by FDA.                            This project is likely to produce groundbreaking results at the interface of medicine, biomedical informatics, and epidemiologic methods. After completion of this project a library of documented and validated algorithms will be available to significantly improve confounder control in a range of healthcare databases. The theoretical foundation and the ready-to-use algorithms will likely lead to a fundamental shift in how databases contribute to the fast and accurate assessment of newly-marketed medications.            Large healthcare databases are used to assess the safety and effectiveness of drugs. However, conventional adjustment methods that rely on a limited number of investigator-specific covariates often fail to produce unbiased results. We will develop algorithms that make greater use of information contained in longitudinal claims data and electronic medical records databases by empirically identifying thousands of potential confounders. This will result in improved causal inference on the comparative safety and effectiveness of newly marketed medications that is both less susceptible to investigator omissions and faster than conventional approaches.",Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects,7940855,R01LM010213,"['Address', 'Algorithms', 'Clinical', 'Code', 'Complex', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Distributed Databases', 'Effectiveness', 'Elderly', 'Epidemiologic Methods', 'Epidemiology', 'Etiology', 'Evaluation', 'Foundations', 'Head', 'Health Planning', 'Healthcare', 'Heterogeneity', 'Individual', 'Insurance', 'Internet', 'Knowledge', 'Lead', 'Libraries', 'Marketing', 'Medical Informatics', 'Medicare/Medicaid', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Placebos', 'Population', 'Pregnant Women', 'Process', 'Publications', 'Randomized', 'Research', 'Research Personnel', 'Safety', 'Scoring Method', 'Sentinel', 'Severity of illness', 'Solid', 'Solutions', 'Specific qualifier value', 'Speed', 'Staging', 'Structure', 'Subgroup', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'To specify', 'Training', 'base', 'biomedical informatics', 'comparative', 'comparative effectiveness', 'compare effectiveness', 'data mining', 'improved', 'indexing', 'lectures', 'novel', 'outcome forecast', 'patient privacy', 'programs', 'routine care', 'symposium', 'treatment effect']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2010,417999,0.011860340312399319
"Extracting Semantic Knowledge from Clinical Reports    DESCRIPTION (provided by applicant): Analyzing and processing free-text medical reports for data mining and clinical data interchange is one of the most challenging problems in medical informatics, yet it is crucial for continued research advances and improvements in clinical care. Natural language processing (NLP) is an important enabling technology, but has been held back because it is difficult to understand human language, since it requires extensive domain knowledge. In Phase I, we developed new statistical and machine learning methods that apply domain specific knowledge to the semantic analysis of free-text radiology reports. The methods enabled the creation of two new prototype applications - a SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms) coding service called SnomedCoder, and a text mining tool for analyzing a large corpus of medical reports, called DataMiner. In Phase II, we will accomplish the following specific aims: 1) Improve the semantic extraction methods developed in Phase I, 2) Expand the semantic knowledge base and classify at least two million new unique sentences from multiple medical institutions, 3) Provide a SNOMED CT auto coding service (alpha service) to participating Indiana Health Information Exchange hospitals, and 4) Build a commercial version of the DataMiner software, and test its functionality using researchers at the Regenstrief Institute.       These scientific innovations will revolutionize the ability of health care researchers to analyze vast repositories of clinical information currently locked up in electronic medical records, and correlate this data with new biomedical discoveries in proteonomics and genomics. The ability to codify text rapidly will extend the potential for clinical decision support beyond its narrow base of numeric and structured medical data, and enable SNOMED CT to become a useful coding standard. Phase III will offer coding and data mining services to healthcare payers (both private and government), pharmaceuticals, and academic researchers. A key advantage of our approach over other NLP systems is that we attempt to codify all the information in the report and not just a limited subset, and insist on expert validation which provides a high degree of confidence in the accuracy of the coded data.Project Narrative           n/a",Extracting Semantic Knowledge from Clinical Reports,7554153,R44RR024929,"['Address', 'Algorithms', 'Back', 'Bioinformatics', 'Body of uterus', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Code', 'Collection', 'Computer software', 'Computerized Medical Record', 'Data', 'Decision Making', 'Effectiveness', 'Genomics', 'Goals', 'Government', 'Health', 'Healthcare', 'Hospitals', 'Human', 'Indiana', 'Institutes', 'Institution', 'Journals', 'Knowledge', 'Language', 'Longitudinal Studies', 'Machine Learning', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Natural Language Processing', 'Paper', 'Pharmacologic Substance', 'Phase', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Speed', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Testing', 'Text', 'Trees', 'Trust', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'base', 'clinical care', 'computerized', 'data mining', 'health care quality', 'improved', 'indexing', 'innovation', 'knowledge base', 'novel strategies', 'patient safety', 'prototype', 'repository', 'research and development', 'success', 'text searching', 'tool']",NCRR,"LOGICAL SEMANTICS, INC.",R44,2009,422728,0.015580973162130146
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7842799,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,MAYO CLINIC ROCHESTER,RC1,2009,497477,0.00893214272143386
"Natural Language Processing for Cancer Research Network Surveillance Studies    DESCRIPTION (provided by applicant): This application addresses Broad Challenge Area: (10) Information Technology for Processing Health Care Data and specific Challenge Topic: 10-CA-107 Expand Spectrum of Cancer Surveillance through Informatics Approaches. The proposed project launches a collaborative effort to advance adoption within the HMO Cancer Research Network (CRN) of ""industrial-strength"" natural language processing (NLP) systems useful for mining valuable, research-grade information from unstructured clinical text. Such text is available for processing, now in the electronic medical record (EMR) systems of affiliated CRN health plans. The proposed NLP methods   will create ongoing capacity to tap what has recently been described as ""a treasure trove of historical   unstructured data that provides essential information for the study of disease progression, treatment   effectiveness and long-term outcomes"" (5). The vision of advancing widespread NLP capacity across the CRN, as well as the approach we present here for implementing it, grew out of an in-depth strategic planning effort we completed in December 2008. That effort involved participants from six CRN sites guided by a blue-ribbon panel of NLP experts from three of the nation's leading centers of clinical NLP research: University of Pittsburgh Medical Center, Vanderbilt University, and Mayo Clinic. The vision is to deploy a powerful NLP system locally, manage it with newly hired and trained local NLP technical staff, and conduct NLP-based research projects initiated by local investigators, in consultation with higher-level external NLP experts. Our planning efforts suggest this collaborative model is feasible; we will test the model in the context of the proposed project. An important development in April 2009 yielded what we believe is a potentially transformative opportunity to accelerate adoption of NLP capacity in applied research settings: release of the open-source Clinical Text Analysis and Knowledge Extraction System (cTAKES) software. This software was the result of a collaborative effort between IBM and Mayo Clinic. Built on the same framework Mayo Clinic currently uses to process its repository of over 40 million clinical documents, cTAKES dramatically lowers the cost of adopting a comprehensive and flexible NLP system. Deployment and use of such systems was previously only feasible in institutions with large, academically-oriented biomedical informatics research programs.   Still, other deployment challenges and the need to acquire NLP training for local staff present residual   barriers to adopting comprehensive NLP systems such as cTAKES. In collaboration with five other CRN sites the proposed project mitigates these challenges in two ways: 1) it develops configurable open-source software modules needed to streamline and therefore reduce the cost of deploying cTAKES, and 2) it presents and tests a model for training local staff through hands-on NLP projects overseen by outside NLP expert consultants. The potential impact of this project is evident most clearly in the vast untapped opportunities for text mining represented in CRN-affiliated health plans, where EMR systems have been in place since at least 2005, and whose patients represent 4% of the U.S. population. Clinical text mining offers the potential to provide new or improved data elements for cancer surveillance and other types of research requiring information about patient functional status, medication side-effects, details of therapeutic approaches, and differential information about clinical findings. Another significant impact of this project is its plan to integrate into the cTAKES system   an open-source de-identification tool based on state of the art, best of breed NLP approaches developed by the MITRE Corporation. De-identification of clinical text will make it easier for researchers to get access to clinical text, and will also facilitate multi-site collaborations while protecting patient privacy. Finally, if successful, the NLP algorithm we propose as a proof-of-principle project at Group Health-which will classify sets of patient charts as either containing or not containing a diagnosis of recurrent breast cancer-could dramatically reduce the cost of research in this area; currently all recurrent breast cancer endpoints must be established through costly manual chart abstraction.   Novel aspects of the proposed project include its talented and transdisciplinary research team,   including national experts in NLP, and its resourceful strategy for building the technical resources and ""human capital"" needed to support an ongoing program of applied NLP research. Natural language processing is itself a highly innovative technology; when successfully established in multiple CRN in the future it will represent a watershed moment in the CRN's already impressive history of exploiting data systems to support innovative research. Newly hired staff positions total approximately 2.0 FTE in each project year, most of which we anticipate will be supported by ongoing new research programs after the proposed project concludes. Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.           Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.",Natural Language Processing for Cancer Research Network Surveillance Studies,7839706,RC1CA146917,"['Address', 'Adopted', 'Adoption', 'Adverse effects', 'Algorithms', 'Applied Research', 'Area', 'Arts', 'Bioinformatics', 'Breeding', 'Cancer Research Network', 'Charge', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Complex', 'Comprehensive Health Care', 'Computer software', 'Computerized Medical Record', 'Consultations', 'Data', 'Data Element', 'Data Quality', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Doctor of Philosophy', 'Environment', 'Exercise', 'Future', 'Generic Drugs', 'Hand', 'Health', 'Health Planning', 'Health system plans', 'Healthcare', 'Human Resources', 'Individual', 'Informatics', 'Information Systems', 'Information Technology', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Licensing', 'Life', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Mining', 'Modeling', 'NCI Center for Cancer Research', 'Natural Language Processing', 'Operating System', 'Outcome', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Process', 'Public Health', 'Recording of previous events', 'Recurrence', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Resources', 'Risk', 'Site', 'Solutions', 'Strategic Planning', 'System', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Training', 'Treatment Effectiveness', 'Universities', 'Vision', 'Woman', 'base', 'biomedical informatics', 'breast cancer diagnosis', 'cost', 'design', 'experience', 'feeding', 'firewall', 'flexibility', 'functional status', 'human capital', 'improved', 'innovation', 'innovative technologies', 'malignant breast neoplasm', 'novel', 'open source', 'patient privacy', 'programs', 'repository', 'skills', 'software systems', 'surveillance study', 'text searching', 'tool']",NCI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,RC1,2009,497857,0.005703252833152773
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7870862,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,172278,0.014404018369623775
"Investigating the generalizability of natural language processing of EMR data    DESCRIPTION (provided by applicant):       The electronic medical record (EMR) offers impressive opportunities for increasing care quality, but challenges stand in the way of realizing this vision. For example, coded EMR data readily available for analysis typically are incomplete (due to the prevalence of free-text clinical notes in EMR implementations), and data from different EMRs are often incommensurate due to differences in standard vocabularies and system implementations. While informatics research has shown the feasibility of automatically coding specific aspects of clinical text using Natural Language Processing (NLP), challenges remain for translating these informatics developments into large-scale care quality assessments. To date, successful NLP solutions for automated quality assessment have tended to be applications that are specific to (a) the target problem or clinical focus, (b) the EMR data system, and (c) the person or team that implements the NLP solution. In this study, we propose to begin addressing the problem of implementation team specificity by developing, evaluating, and making freely available a generalizable NLP development tool suite. The tools will enable widespread adoption of NLP systems to extract and code data from free text clinical notes. The Knowledge Editing Toolkit will simplify development of problem-specific knowledge by helping the user define the rules, concepts, and terms that constitute a domain-specific knowledge module, thus allowing any informaticist to develop an NLP application. The NLP Application Validation Toolkit will allow rapid testing and evaluation of the application against a gold standard of independently-coded test records from any EMR. To evaluate the effects of the toolkits on NLP generalizability, we will have three clinical informaticists each build two NLP applications (for a total of six distinct applications). One of their applications will identify a constellation of common clinical signs or symptoms (e.g., ""persistent cough"") that are relatively discrete concepts using simple language terms for many different clinical purposes. Their second application will assess behavioral counseling (e.g., ""alcohol counseling""), which uses complex language constructs for dedicated clinical purposes. We will describe and evaluate the accuracy of the solutions against independently coded test sets of medical records. We will quantify and compare the difficulty of creating these solutions as measured by the time, number of iterations required to build the applications, and the number of concepts and rules employed, as well as analyze variability in content and accuracy of the solutions created. In addition, we will use qualitative techniques to assess the ease of using the development tools; the difficulty in learning the tools; and specific types of problems, limitations, and bugs encountered. Such an NLP development tool suite has the potential to allow simple, elegant, and reliably good NLP solutions regardless of the clinical problem domain or the person developing the solution.           n/a",Investigating the generalizability of natural language processing of EMR data,7691692,R21LM009728,"['Address', 'Adopted', 'Adoption', 'Affect', 'Alcohols', 'Architecture', 'Behavioral', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'Computer Systems Development', 'Computerized Medical Record', 'Coughing', 'Counseling', 'Data', 'Databases', 'Development', 'Event', 'Gold', 'Healthcare Systems', 'Human Resources', 'Informatics', 'Information Systems', 'Information Technology', 'Institute of Medicine (U.S.)', 'Knowledge', 'Language', 'Learning', 'Measures', 'Medical Records', 'Modification', 'Natural Language Processing', 'Performance', 'Persons', 'Positioning Attribute', 'Prevalence', 'Process', 'Proliferating', 'Quality of Care', 'Records', 'Research', 'Running', 'Solutions', 'Source', 'Specificity', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Translating', 'United States National Academy of Sciences', 'Ursidae Family', 'Validation', 'Vision', 'Vocabulary', 'care delivery', 'cost', 'design', 'evaluation/testing', 'experience', 'health care quality', 'stem', 'success', 'tool', 'tool development', 'web site']",NLM,KAISER FOUNDATION RESEARCH INSTITUTE,R21,2009,177750,0.03255374717962246
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7631876,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,344239,0.014404018369623775
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7937173,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,172210,0.014404018369623775
"Investigating the generalizability of natural language processing of EMR data    DESCRIPTION (provided by applicant):       The electronic medical record (EMR) offers impressive opportunities for increasing care quality, but challenges stand in the way of realizing this vision. For example, coded EMR data readily available for analysis typically are incomplete (due to the prevalence of free-text clinical notes in EMR implementations), and data from different EMRs are often incommensurate due to differences in standard vocabularies and system implementations. While informatics research has shown the feasibility of automatically coding specific aspects of clinical text using Natural Language Processing (NLP), challenges remain for translating these informatics developments into large-scale care quality assessments. To date, successful NLP solutions for automated quality assessment have tended to be applications that are specific to (a) the target problem or clinical focus, (b) the EMR data system, and (c) the person or team that implements the NLP solution. In this study, we propose to begin addressing the problem of implementation team specificity by developing, evaluating, and making freely available a generalizable NLP development tool suite. The tools will enable widespread adoption of NLP systems to extract and code data from free text clinical notes. The Knowledge Editing Toolkit will simplify development of problem-specific knowledge by helping the user define the rules, concepts, and terms that constitute a domain-specific knowledge module, thus allowing any informaticist to develop an NLP application. The NLP Application Validation Toolkit will allow rapid testing and evaluation of the application against a gold standard of independently-coded test records from any EMR. To evaluate the effects of the toolkits on NLP generalizability, we will have three clinical informaticists each build two NLP applications (for a total of six distinct applications). One of their applications will identify a constellation of common clinical signs or symptoms (e.g., ""persistent cough"") that are relatively discrete concepts using simple language terms for many different clinical purposes. Their second application will assess behavioral counseling (e.g., ""alcohol counseling""), which uses complex language constructs for dedicated clinical purposes. We will describe and evaluate the accuracy of the solutions against independently coded test sets of medical records. We will quantify and compare the difficulty of creating these solutions as measured by the time, number of iterations required to build the applications, and the number of concepts and rules employed, as well as analyze variability in content and accuracy of the solutions created. In addition, we will use qualitative techniques to assess the ease of using the development tools; the difficulty in learning the tools; and specific types of problems, limitations, and bugs encountered. Such an NLP development tool suite has the potential to allow simple, elegant, and reliably good NLP solutions regardless of the clinical problem domain or the person developing the solution.           n/a",Investigating the generalizability of natural language processing of EMR data,7850343,R21LM009728,"['Address', 'Adopted', 'Adoption', 'Affect', 'Alcohols', 'Architecture', 'Behavioral', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'Computer Systems Development', 'Computerized Medical Record', 'Coughing', 'Counseling', 'Data', 'Databases', 'Development', 'Event', 'Gold', 'Healthcare Systems', 'Human Resources', 'Informatics', 'Information Systems', 'Information Technology', 'Institute of Medicine (U.S.)', 'Knowledge', 'Language', 'Learning', 'Measures', 'Medical Records', 'Modification', 'Natural Language Processing', 'Performance', 'Persons', 'Positioning Attribute', 'Prevalence', 'Process', 'Proliferating', 'Quality of Care', 'Records', 'Research', 'Running', 'Solutions', 'Source', 'Specificity', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Translating', 'United States National Academy of Sciences', 'Ursidae Family', 'Validation', 'Vision', 'Vocabulary', 'care delivery', 'cost', 'design', 'evaluation/testing', 'experience', 'health care quality', 'stem', 'success', 'tool', 'tool development', 'web site']",NLM,KAISER FOUNDATION RESEARCH INSTITUTE,R21,2009,99971,0.03255374717962246
"Using medical records repositories to improve the alert system design    DESCRIPTION (provided by applicant):       Rapid and accurate alerting of concerning patient events and conditions remains an important problem in clinical practice. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into the monitoring and alerting systems. However, it is often time-consuming and costly to extract and codify such knowledge; hence such systems are typically built to cover only very specific conditions. In addition, it is difficult for an expert to foresee the performance of the deployed systems and their potential drawbacks, especially their false alarm rates. It is not uncommon that computer alerting systems are discarded or must undergo multiple costly modification cycles before they reach clinically acceptable levels of performance.    Electronic health record (EHR) repositories today provide an opportunity to test various theories and develop new computational solutions to various clinical problems. The objective of this project is to investigate methods for using the data in such repositories to assist in the development of alerting systems. The project goals include the building of an evidence-driven framework for the evaluation and optimization of alerting systems with the help of past data. The framework will be able to provide early feedback and future performance estimates of an alerting system before it is deployed, which is anticipated to decrease the expert effort required to design such a system and lead to a shorter alerting system design cycle. The evidence-driven framework will be tested and evaluated on multiple clinical conditions and compared to the performance of alerting rules currently deployed at the University of Pittsburgh Medical Center (UPMC). The project investigators consist of a multidisciplinary team with expertise in rule-based alerting in the hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, and knowledge based systems.           Project Narrative: There remain numerous opportunities to reduce medical errors by sending computer-based reminders and alerts to clinicians. This project investigates a novel combination of past patient data stored in electronic form and statistical machine-learning methods to help develop and refine computer-based alerts, which are expected to improve healthcare quality and reduce costs.",Using medical records repositories to improve the alert system design,7784403,R01LM010019,"['Amiodarone', 'Archives', 'Clinical', 'Clinical Data', 'Complex', 'Computers', 'Data', 'Databases', 'Detection', 'Development', 'Electronic Health Record', 'Electronics', 'Evaluation', 'Event', 'Expert Opinion', 'Feedback', 'Future', 'Goals', 'Gold', 'Heparin', 'Hospitals', 'Human', 'Information Systems', 'Knowledge', 'Knowledge acquisition', 'Label', 'Laboratories', 'Lead', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Medical Errors', 'Medical Informatics', 'Medical Records', 'Medical center', 'Medicine', 'Methods', 'Metric', 'Modeling', 'Modification', 'Monitor', 'Outcome', 'Patients', 'Pattern', 'Performance', 'Pharmacy facility', 'Physicians', 'Research', 'Research Personnel', 'Solutions', 'Source', 'Statistical Methods', 'Stream', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Time', 'Toxic effect', 'Training', 'Uncertainty', 'Universities', 'base', 'biomedical informatics', 'clinical practice', 'cost', 'design', 'evaluation/testing', 'flexibility', 'health care quality', 'improved', 'knowledge base', 'multidisciplinary', 'novel', 'predictive modeling', 'repository', 'response', 'statistics', 'success', 'theories', 'treatment response']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,370642,0.029416682191504045
"An NLP Approach to Generating Patient Record Summaries :  The long-term goal of this proposal is to enhance the manner in which physicians access, process and marshal medical information by providing them with an automatically generated, comprehensive, and up-to date summary of the information appearing in a patient record. At the point of patient care, physicians must often rapidly process a potentially overwhelming quantity of information pertaining to a patient. Failure to do so effectively may lead to provision of suboptimal care. Some electronic health record systems provide an automatically produced “cover sheet” geared to help physicians with a broad overview of a given patient, but the information is derived from the structured data fields in the patient record, ignoring the valuable narrative text entered by clinicians over time. We are building upon our prior work in summarization and natural language processing and leveraging our expertise in cognitive research studying information needs and decision making of clinicians to build a patient record summarizer that gathers information narrative (unstructured) as well as structured parts in the record. We focus on producing a summary for patients with kidney disease, as they often have a complex medical history with numerous conditions, procedures and medications. Providing a holistic, up-to-date summary of their chart would prove valuable to physicians in general and nephrologists in particular. The following three aims will be carried out: (1) conduct a formative study to determine how physicians prioritize and mentally represent relevant information when reviewing a patient chart; (2) create a set of automated methods to select salient pieces of information in the patient record and organize them into a coherent summary; and (3) evaluate the efficacy, efficiency and physician-user satisfaction associated with the use of the summarizer. A primary strength of this proposal is that we are addressing the problem of information overload, a bottleneck in the use of electronic health records, and evaluate the impact of our solution on clinicians’ actions and patients’ health outcomes. Furthermore, we propose to use novel natural language processing, knowledge-based and data mining methods to extract and organize salient information. Finally, we contribute to informatics research by extending the electronic health record functionalities to go beyond a simple documentation-entry system towards a useful reference and decision-making tool for physicians  Project Narrative We propose to design an automatically generated, comprehensive, and up-to-date summary of the information appearing in a patient record. Such a summary would enhance the manner in which both patients and their physicians access, process and marshal medical information.",An NLP Approach to Generating Patient Record Summaries,7635002,R01LM010027,"['Address', 'Allergic', 'Caring', 'Clinic', 'Clinical', 'Cognitive', 'Complex', 'Data', 'Data Analyses', 'Decision Making', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation Studies', 'Failure', 'Goals', 'Harvest', 'Health', 'Informatics', 'Information Resources', 'Interview', 'Kidney Diseases', 'Kidney Function Tests', 'Knowledge', 'Laboratories', 'Lead', 'Marshal', 'Medical', 'Medical History', 'Methods', 'Natural Language Processing', 'Outcome', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Physicians', 'Procedures', 'Process', 'Research', 'Solutions', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Text', 'Time', 'Visit', 'Work', 'data mining', 'design', 'information gathering', 'knowledge base', 'medical schools', 'meetings', 'novel', 'research study', 'satisfaction', 'stem', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,455605,0.017318347245420453
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7690941,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'clinical practice', 'design', 'foot', 'journal article', 'language processing', 'meetings', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2009,351549,-0.00723159261206169
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7908952,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'clinical practice', 'design', 'foot', 'journal article', 'language processing', 'meetings', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2009,170662,-0.00723159261206169
"An in-silico method for epidemiological studies using Electronic Medical Records    DESCRIPTION: Observational epidemiological studies are effective methods for identifying factors affecting the health and illness of populations, as well as for determining optimal treatments for diseases, such as cancers. However, conventional epidemiological research usually involves personnel-intensive effort (such as manual chart and public records review) and can be very time consuming before conclusive results are obtained. Recently, a large amount of detailed longitudinal clinical data has been accumulated at hospitals' Electronic Medical Records (EMR) systems and it has become a valuable data source for epidemiological studies. However, there are two obstacles that prevent the wide usage of EMR data in epidemiological studies. First, most of the detailed clinical information in EMRs is embedded in narrative text and it is very costly to extract that information manually. Second, EMRs usually have data quality problems such as selection bias and missing data, which require adaptation of conventional statistical methods developed for randomized controlled trials.   In this study, we propose an in silico informatics-based approach for observational epidemiological studies using EMR data. We hypothesize that existing EMR data can be used for certain types of epidemiological studies in a very efficient manner with the help of informatics methods. The informatics-based approach will contain two major components. One is an NLP (Natural Language Processing) based information extraction system that can automatically extract detailed clinical information from EMR and another is a set of statistical and informatics methods that can be used to analyze EMR-derived data. If the feasibility of this approach is proven, it will change the standard paradigm of observational epidemiological research, because it has the capability to answer an epidemiological question in a very short time at a very low cost. The specific aim of this study is to develop an automated informatics approach to extract both fine-grained cancer findings and general clinical information from EMRs and use them to conduct cancer related epidemiological studies. We will perform both casecontrol and cohort studies related to prevention and treatment of breast and colon cancers using EMR data. The informatics approach will be validated on EMRs from two major hospitals to demonstrate its generalizability. Epidemiological findings from our study will be compared to reported findings for validation.      Narrative: According to the American Cancer Society, about 7.6 million people died from various types of cancer in the world during 2007. It is very important to identify risk factors of   cancers and to determine optimal treatments of cancers, and epidemiological study is   one of the methods to achieve it. This proposed study will use natural language   processing technologies to automatically extract fine-grained cancer information from   existing patient electronic medical records and use it to conduct cancer related   epidemiological studies, thus accelerating knowledge accumulation of cancer research.      SPECIAL REVIEW NOTE: In order to conform to the scientific objectives outlined in the program announcement RFA-GM-09-008, EUREKA applications submitted to the NCI were initially evaluated by a group of reviewers representing diverse scientific interests. The priority score reflects the average of all the scores given by the full committee after a thorough discussion.            Project Narrative According to the American Cancer Society, about 7.6 million people died from various types of cancer in the world during 2007. It is very important to identify risk factors of cancers and to determine optimal treatments of cancers, and epidemiological study is one of the methods to achieve it. This proposed study will use natural language processing technologies to automatically extract fine-grained cancer information from existing patient electronic medical records and use it to conduct cancer related epidemiological studies, thus accelerating knowledge accumulation of cancer research.",An in-silico method for epidemiological studies using Electronic Medical Records,7726747,R01CA141307,"['Affect', 'American Cancer Society', 'Breast', 'Cereals', 'Clinical', 'Clinical Data', 'Cohort Studies', 'Colon Carcinoma', 'Computer Simulation', 'Computerized Medical Record', 'Data', 'Data Quality', 'Data Sources', 'Disease', 'Epidemiologic Studies', 'Epidemiology', 'Health', 'Hospitals', 'Human Resources', 'Informatics', 'Knowledge', 'Malignant Neoplasms', 'Manuals', 'Methods', 'NIH Program Announcements', 'Natural Language Processing', 'Patients', 'Population', 'Prevention', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research', 'Risk Factors', 'Selection Bias', 'Statistical Methods', 'System', 'Technology', 'Text', 'Time', 'Validation', 'anticancer research', 'base', 'cancer therapy', 'cancer type', 'cost', 'interest', 'prevent']",NCI,VANDERBILT UNIVERSITY,R01,2009,273337,-0.0019270328384685608
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7653874,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Databases', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,357875,0.008601118362087327
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,7748337,R43HG005046,"['Address', 'Benefits and Risks', 'Body of uterus', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2009,119499,0.0009703234356238767
"Online Social Networking as an Alternative Information Source for Clinical Resear    DESCRIPTION (provided by applicant): Clinical trials and patient records have been the main information sources for clinical research. While well- designed clinical trials can produce high quality data, they are generally very expensive and time consuming. Prior studies have also shown that patients enrolled in clinical trials are not necessarily representative of the general patient population. Chart reviews, which rely on the patient records, avoid some of the drawbacks of the clinical trials approach. Although chart review studies are more labor intensive, new developments in structured data entry and natural language processing (NLP) are helping to automate the process. However, studies which use chart reviews are limited by the accuracy and completeness of the data in the records.       In the past decade, online social networks have grown exponentially. Some health-focused social network sites have attracted large numbers of users and begun accumulating large quantities of detailed clinical information. The PatientsLikeMe site, for instance, has about 3,200 amyotrophic lateral sclerosis (ALS) patients worldwide, and includes about 5% of the ALS population in the US. Information gathered by online social networks is primarily intended for patients to share with each other. Such information has also begun to attract the attention of medical researchers.[3, 4]       Because using information from online social networks for medical research is a fairly new phenomenon, the value and limitation of this type of information source have not been systematically examined. To do so, we propose to conduct a comparison study of patient-contributed information from PatientsLikeMe and records from a large medical record data repository - the Research Patient Data Registry (RPDR) of the Partners Healthcare Systems. The proposed study will focus on ALS, multiple sclerosis (MS), and Parkinson's disease (PD). The general goal is to explore how the medical record and online networking data differ, and if and how online networking data could complement the medical record data. The specific aims are:    1) Extract symptom and treatment information from the two different data sources.    2) Compare the prevalence of symptoms and treatments from the two information sources and analyze the difference.    3) Extract treatment response of prescription medications from PatientsLikeMe and analyze the confounding effect of the misunderstanding of medication indication.      PUBLIC HEALTH RELEVANCE: The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.           The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.",Online Social Networking as an Alternative Information Source for Clinical Resear,7777633,R21NS067463,"['Amyotrophic Lateral Sclerosis', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Communities', 'Comparative Study', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Enrollment', 'Frequencies', 'Goals', 'Healthcare Systems', 'Medical Records', 'Medical Research', 'Multiple Sclerosis', 'Natural Language Processing', 'Nature', 'Parkinson Disease', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Prevalence', 'Process', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Site', 'Source', 'Structure', 'Symptoms', 'System', 'Text', 'Time', 'Update', 'experience', 'information gathering', 'medical attention', 'patient population', 'public health relevance', 'social networking website', 'statistics', 'treatment response', 'web-based social networking']",NINDS,UNIVERSITY OF UTAH,R21,2009,219896,-0.010164793881612932
"Temporal Processing for Medical Discharge Summaries    DESCRIPTION (provided by applicant):       The goals of our project are as follows:     1. Create a corpus of temporally annotated data. Under the supervision of our consultants Dr. Frank Sacks, Dr. Vincent Carey, and two Registered Nurses, we will create a gold-standard annotation of events and temporal information within patient narratives from de- identified Electronic Health Record data using the CLEF and TimeML guidelines. We will use the framework of the Brandeis Annotation Tool, a system we have designed to facilitate the quick construction of accurately annotated corpora against a specified guideline. Extensions to the current event library and lexicon with medical event references will be made during the annotation process, under the guidance of the Registered Nurses.          2. Adapt the TARSQI Toolkit (TTK) to targeted temporal properties and relations in the EHR domain. We will use the TARSQI toolkit, a robust set of temporal processing algorithms we have designed for parsing natural language text, to automatically annotate the events and temporal information in EHR data. Combined with the Brandeis AcroMed Medical Abbreviation Server and those terms introduced in part 1, we will employ the Specialist Lexicon and other medical resources to extend the toolkit capabilities for recognizing and interpreting medical event information. Algorithms for identifying events, temporal expressions, and event anchorings and orderings will be trained against the gold standard created in Aim 1, and tested against held-out data.     3. Create a cross-document temporal database of medical events. Using the recognition algorithms introduced in Aim 2, we will create a searchable, temporally ordered database of medical events such as diseases, symptoms, surgeries/interventions, and test results. Events referred to multiple times in the data will be merged using a constraint- satisfaction analysis in order to create a more coherent narrative for a single patient over multiple records.           Project Narrative It is becoming increasingly common for medical researchers to use Electronic Health Records (EHRs) as a primary source of data for researching correlations between various medical issues and concepts. However, EHRs typically contain unstructured text, making them difficult to mine. This research will create a database of temporal orderings from events extracted from EHR patient narratives, using algorithms previously applied to news articles.",Temporal Processing for Medical Discharge Summaries,7789943,R21LM009633,"['Abbreviations', 'Adopted', 'Algorithms', 'Authorization documentation', 'Body of uterus', 'Clinical', 'Clinical Research', 'Data', 'Data Sources', 'Databases', 'Diagnosis', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Event', 'Goals', 'Gold', 'Guidelines', 'Information Resources', 'Intervention', 'Language', 'Libraries', 'Licensing', 'Machine Learning', 'Medical', 'Medical History', 'Medical Libraries', 'Mining', 'Operative Surgical Procedures', 'Patients', 'Process', 'Property', 'Records', 'Registered nurse', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Science', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Supervision', 'Symptoms', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Vocabulary', 'acronyms', 'base', 'design', 'evidence base', 'experience', 'interest', 'lexical', 'natural language', 'news', 'open source', 'repository', 'satisfaction', 'tool']",NLM,BRANDEIS UNIVERSITY,R21,2009,177750,0.030614768825674078
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7671509,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,1455866,0.019752062475471188
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7922465,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,415228,0.019752062475471188
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7911405,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,229436,0.019752062475471188
"Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach    DESCRIPTION (provided by applicant):       The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The study design is prospective observational study. Scope is limited to cancer patients. There are three specific aims for this project. The first aim is to identify concepts that overlap between the electronic medical record's (EMR) clinical notes and the free text of clinical trial announcements. The PI will use the concepts to develop mapping frames that connect concepts in the text of trial announcements to those found in clinical notes in the medical record. When he has the mapping frames he will build the NLP module for the application. In the software development work he will utilize as many publicly available software components as possible. He will experiment with UIMA, GATE, MetaMap, Stanford Parser, NegEx algorithm and others. The PI will develop the tool around the National Library of Medicine's Unified Medical Language System knowledgebase. He will use Java for programming. The second aim is to create an algorithm that automatically generates questions to request information directly from the patient if the information is not available or accessible in the records. The third aim is to evaluate the in-vitro, laboratory performance of the application. For performance evaluation purposes the PI will recruit cancer care specialists to generate the gold standard lists of eligible clinical trials for study patients. He will publicly release the developed code at the end of the grant period. This K99/R00 project will serve the foundation for future R01 grant applications. The PI is fully committed to become faculty in the Clinical Research Informatics domain with a specialization in biomedical NLP. The support of the K99/R00 grant will enable him to acquire substantial formal training in Computational Linguistics while contributing to the body of knowledge of the Clinical Research Informatics field. The five-year grant support will ensure success in his endeavor. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, EMR based clinical trial recommendations directly to the patients. The results of this research will empower the patients and elevate their role in the decision making process.           Relevance The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, electronic medical record based clinical trial recommendations directly to the patients. The results of this research will empower patients and elevate their role in the decision making process.",Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach,7770648,K99LM010227,"['Adult', 'Age', 'Algorithms', 'Applications Grants', 'Arts', 'Biomedical Research', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Code', 'Commit', 'Complement', 'Computer software', 'Computerized Medical Record', 'Decision Making', 'Development', 'Elderly', 'Elements', 'Eligibility Determination', 'Enrollment', 'Ensure', 'Equation', 'Evaluation', 'Faculty', 'Foundations', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'In Vitro', 'Informatics', 'Java', 'Knowledge', 'Laboratories', 'Linguistics', 'Malignant Neoplasms', 'Maps', 'Medical Records', 'Medicine', 'Methods', 'Minority', 'Modification', 'Natural Language Processing', 'Newly Diagnosed', 'Observational Study', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protocols documentation', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Role', 'Screening for cancer', 'Screening procedure', 'Specialist', 'Surgeon', 'Text', 'Training', 'Unified Medical Language System', 'United States National Library of Medicine', 'Work', 'base', 'cancer care', 'empowered', 'ethnic minority population', 'information organization', 'novel', 'older patient', 'patient oriented', 'programs', 'prospective', 'research study', 'software development', 'success', 'tool']",NLM,UNIVERSITY OF WASHINGTON,K99,2009,84306,0.013991529113082499
"Early Warning Method and Technologies for Improving Cancer Care and Targeted Inte    DESCRIPTION (provided by applicant): Substantial evidence gathered over the last 50 years shows that adherence poses a crucial barrier to effective treatment and survival for cancer and other chronic diseases. At least one in five cancer patients do not adhere to treatment regimen, with much higher disease-specific rates. This non-adherence, or deviation from the recommended and expected clinical path, can dramatically increase costs of care, hospitalizations, adverse outcomes and the chance of preventable death. What causes non-adherence to treatment regimens is currently not rigorously understood. Current adherence research methods largely rely on survey instruments that have limited scale and scope, provide lagging information that inhibits timely intervention, and offer little actionable information to help patients to adhere to their care regimens. Further, the nature and timing of intervention to improve adherence have not been researched in depth. With continuous changes in cancer treatment, newer proactive approaches and methods for surveillance of patient adherence and targeted interventions are needed. In this project, we examine the feasibility and validity of a novel approach that uses a computational model to glean fine-grained attributes of cancer patients from standard electronic medical records. Our preliminary work has shown that electronic records to contain free-form text describing patient sentiment, vitals, medical condition, side effects, social history and family status written by physicians, nurses, medical assistants, and other staff during every visit encounter. With the steady adoption of electronic medical records by clinicians across the US (currently 29% and rising at 12% per year), clinical notes found in electronic records offer a tantalizing source of insight into patient adherence and behavior. Current adherence research has not tapped this rich source of data, even though many disciplines including biomedical informatics have employed natural language processing and text-mining techniques to glean patterns in semi- structured biomedical data. We aim to employ similar but novel, scalable computational models to glean a rich set of risk factors for patient non-adherence from 1 million patient encounter records, corresponding to 24,050 patients that span a 10 year time-horizon. Our objectives are to estimate the risk of a patient's ability to adhere to a prescribed regimen and enable targeted and timely interventions by using computational analysis of unstructured and structured fields in standard clinical documentation.  PUBLIC HEALTH RELEVANCE: We aim to show the feasibility of an early warning system that detects and estimates a cancer patient's risk of non-adherence to treatment regimens by analyzing unstructured text in standard medical records. This technology has tremendous relevance for improved quality of care, proactive management of chronic diseases and patient safety.                        Project Narrative We aim to show the feasibility of an early warning system that detects and estimates a cancer patient's risk of non-adherence to treatment regimens by analyzing unstructured text in standard medical records. This technology has tremendous relevance for improved quality of care, proactive management of chronic diseases and patient safety.  ",Early Warning Method and Technologies for Improving Cancer Care and Targeted Inte,7746912,R43CA141899,"['Adherence', 'Adoption', 'Adverse effects', 'Behavior', 'Behavioral', 'Cancer Patient', 'Caring', 'Cereals', 'Cessation of life', 'Chronic Disease', 'Clinical', 'Clinical Paths', 'Clinical Trials', 'Community Clinical Oncology Program', 'Computer Analysis', 'Computer Simulation', 'Computerized Medical Record', 'Data', 'Data Set', 'Data Sources', 'Discipline', 'Disease', 'Documentation', 'Electronics', 'Emotional', 'Employment', 'Family history of', 'Glean', 'Hospitalization', 'Individual', 'Intervention', 'Malignant Neoplasms', 'Medical', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Nurses', 'Nutritionist', 'Outcome', 'Patient Noncompliance', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Psychologist', 'Quality of Care', 'Records', 'Research', 'Research Methodology', 'Risk', 'Risk Assessment', 'Risk Estimate', 'Risk Factors', 'Scientific Advances and Accomplishments', 'Semantics', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Social Workers', 'Source', 'Structure', 'Surveillance Methods', 'Surveys', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Treatment Protocols', 'Visit', 'Weight', 'Work', 'Writing', 'base', 'biomedical informatics', 'cancer care', 'cancer therapy', 'clinical practice', 'compliance behavior', 'cost', 'effective therapy', 'follow-up', 'improved', 'innovation', 'insight', 'mathematical algorithm', 'novel', 'novel strategies', 'patient safety', 'psychologic', 'public health relevance', 'social', 'text searching']",NCI,"360FRESH, INC.",R43,2009,100000,0.02024230002222238
"NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports    DESCRIPTION (provided by applicant):       Many NLP applications have been successfully developed to extract information from text. Most of the   applications have focused on identifying individual clinical conditions in textual records, which is the first step in making the conditions available to computerized applications. However, identifying individual instances of clinical conditions is not sufficient for many medical informatics tasks - the context surrounding the condition is crucial for integrating the information within the text to determine the clinical state of a patient. We propose to perform in-depth studies on NLP issues requiring knowledge of the context of clinical conditions in clinical records. We will focus our research by using syndromic surveillance from emergency department (ED) reports as a case study.      For this proposal, we will test the following hypothesis: An NLP system that indexes clinical concepts and integrates contextual information modifying the concepts can identify acute clinical conditions from ED reports as well as physicians can.      We will identify clinical concepts necessary for surveillance of seven syndromes, including respiratory,   gastrointestinal, neurological, rash, hemorrhagic, constitutional, and botulinic. To evaluate the hypothesis, we will perform the following specific aims:      Aim 1. Perform in-depth, foundational studies on four NLP topics to gain a deeper understanding of the      pertinent NLP research capabilities required for identification of acute clinical conditions from ED reports, including negation, uncertainty, temporal discrimination, and finding validation;      Aim 2. Apply the knowledge learned from the foundational studies to develop and evaluate an automated application for ED reports that will determine the values for clinical variables relevant to identifying patients with any of seven syndromes.      The research is innovative, because it will generate an in-depth study of multiple NLP topics crucial to   understanding a patient's clinical state from textual records and will focus on contextual understanding and analysis. The research will be guided by linguistic principles, by the semantics and discourse structure of ED reports, and by the application area of biosurveillance. Because we will develop research methods and tools that are customized to a particular domain, we will constrain the research space, which will provide direction and enhance the chance for success. However, the methods and tools generated by this research should be extensible to other clinical report types and to other domain applications, because we will explicitly specify and study NLP concepts and relationships that are common to many application areas.             n/a",NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports,7908086,R01LM009427,"['Accident and Emergency department', 'Acute', 'Area', 'Case Study', 'Clinical', 'Clinical Data', 'Constitutional', 'Detection', 'Discrimination', 'Exanthema', 'Exhibits', 'Individual', 'Informatics', 'Knowledge', 'Learning', 'Linguistics', 'Medical Informatics', 'Medical Informatics Applications', 'Methods', 'Natural Language Processing', 'Neurologic', 'Ontology', 'Patients', 'Physicians', 'Records', 'Reporting', 'Research', 'Research Methodology', 'Semantics', 'Specific qualifier value', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Uncertainty', 'Validation', 'computerized', 'gastrointestinal', 'indexing', 'innovation', 'quality assurance', 'respiratory', 'success', 'syndromic surveillance', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,130902,0.033490188528213484
"NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports    DESCRIPTION (provided by applicant):       Many NLP applications have been successfully developed to extract information from text. Most of the   applications have focused on identifying individual clinical conditions in textual records, which is the first step in making the conditions available to computerized applications. However, identifying individual instances of clinical conditions is not sufficient for many medical informatics tasks - the context surrounding the condition is crucial for integrating the information within the text to determine the clinical state of a patient. We propose to perform in-depth studies on NLP issues requiring knowledge of the context of clinical conditions in clinical records. We will focus our research by using syndromic surveillance from emergency department (ED) reports as a case study.      For this proposal, we will test the following hypothesis: An NLP system that indexes clinical concepts and integrates contextual information modifying the concepts can identify acute clinical conditions from ED reports as well as physicians can.      We will identify clinical concepts necessary for surveillance of seven syndromes, including respiratory,   gastrointestinal, neurological, rash, hemorrhagic, constitutional, and botulinic. To evaluate the hypothesis, we will perform the following specific aims:      Aim 1. Perform in-depth, foundational studies on four NLP topics to gain a deeper understanding of the      pertinent NLP research capabilities required for identification of acute clinical conditions from ED reports, including negation, uncertainty, temporal discrimination, and finding validation;      Aim 2. Apply the knowledge learned from the foundational studies to develop and evaluate an automated application for ED reports that will determine the values for clinical variables relevant to identifying patients with any of seven syndromes.      The research is innovative, because it will generate an in-depth study of multiple NLP topics crucial to   understanding a patient's clinical state from textual records and will focus on contextual understanding and analysis. The research will be guided by linguistic principles, by the semantics and discourse structure of ED reports, and by the application area of biosurveillance. Because we will develop research methods and tools that are customized to a particular domain, we will constrain the research space, which will provide direction and enhance the chance for success. However, the methods and tools generated by this research should be extensible to other clinical report types and to other domain applications, because we will explicitly specify and study NLP concepts and relationships that are common to many application areas.             n/a",NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports,7660312,R01LM009427,"['Accident and Emergency department', 'Acute', 'Area', 'Case Study', 'Clinical', 'Clinical Data', 'Constitutional', 'Detection', 'Discrimination', 'Exanthema', 'Exhibits', 'Individual', 'Informatics', 'Knowledge', 'Learning', 'Linguistics', 'Medical Informatics', 'Medical Informatics Applications', 'Methods', 'Natural Language Processing', 'Neurologic', 'Ontology', 'Patients', 'Physicians', 'Records', 'Reporting', 'Research', 'Research Methodology', 'Semantics', 'Specific qualifier value', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Uncertainty', 'Validation', 'computerized', 'gastrointestinal', 'indexing', 'innovation', 'quality assurance', 'respiratory', 'success', 'syndromic surveillance', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,362514,0.033490188528213484
"Data Structuring and Visualization System for Neuro-oncology    DESCRIPTION (provided by applicant):       The medical record for a neuro-oncology patient is complex, consisting of typically a large number of both text and imaging data. It includes descriptions of prior observations, interpretations, and interventions which need to be integrated into decisions regarding current patient care. An appropriate review of a patient's medical record often requires that a physician review multiple clinical documents while mentally noting issues related to what the findings were, the chronology of events, spatial/temporal patterns of disease progression, the effects of interventions, and the possible causal lines of explanation of observed findings. Additionally, imaging data and imaging- derived conclusions are poorly integrated into patient care and management decisions. The physician also needs to filter out those pieces of information not related to the current clinical context of care. Given the time constraints, data complexity and data volume associated with chronic patient cases, an appropriate review of a patient's chart is in reality rarely performed. Additionally, the lack of tools for formalizing the representation of the accounts of current and prior cases impedes the development of clinical databases that can be ultimately used to learn patterns of disease.       This proposal addresses the development of a system for facilitating the review of clinical patient data intended to promote an orderly process of medical problem understanding and care. The specific aims of the proposal are summarized as follows: 1) Development of a backend tool to facilitate the structured representation of observations, events, and inferences stated within medical reports. 2) Development of an application interface for visualizing, navigating, and editing structured patient data. 3) Evaluation of the effectiveness of the application in the domain of neuro-oncology.       Relation to public health. If the goals of this proposal can be realized, neuro-oncologist should be able to more easily seek desired patient data and detect patterns of evidence as compared to the current mode of operation (HIS, RIS, PACS). The structuring tools should lead to improvements in the quality of clinical research databases.           Narrative Medical records for neuro-oncology patients are difficult to review due to volume and complexity of information. A novel system for partitioning data along the information axes of space, time, existence, and causality is proposed to improve navigation and assimilation of data within the medical record.",Data Structuring and Visualization System for Neuro-oncology,7567140,R01LM009961,"['Abnormal coordination', 'Accounting', 'Address', 'Anatomy', 'Appearance', 'Assimilations', 'Caregivers', 'Caring', 'Chronic', 'Chronology', 'Clinic', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Effectiveness', 'Etiology', 'Evaluation', 'Event', 'Goals', 'Image', 'Imagery', 'Intervention', 'Lead', 'Learning', 'Medical', 'Medical Records', 'Metric', 'Natural Language Processing', 'Oncologist', 'Operative Surgical Procedures', 'Patient Care', 'Patient Care Management', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Property', 'Public Health', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Training', 'abstracting', 'follow-up', 'improved', 'innovation', 'intervention effect', 'neuro-oncology', 'novel', 'open source', 'physical state', 'satisfaction', 'tool']",NLM,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2009,953185,0.03538732443888387
"POET: Consolidated, Comprehensive Clinical Text Preprocessing    DESCRIPTION (provided by applicant):       As electronic health records (EHRs) continue their expansion into clinical settings, there has been a corresponding increase in interest in mining the data they contain, both for research as well as for clinical decision support. Informaticists are increasingly studying ways to mine EHR textual content. This is an important trend, because there is a wealth of information contained in clinical text not represented anywhere else in the EHR. There is a low level text-as-data issue which presents a significant obstacle to the widespread use of available medical NLP systems: hand-typed clinical narratives in EHRs are usually ungrammatical; short or telegraphic in style; full of abbreviations, acronyms, and misspellings; formatted in a templated or pseudo-tabular form; and contain embedded non-text such as a list of laboratory values cut-and-pasted from elsewhere in the EHR. As we show in the Preliminary Studies Section, this makes high-level processing by popular tools like MedLEE and MetaMap effectively useless for all but a few ""clean"" document types like discharge summaries or consult reports (e.g., pathology or radiology reports). This in turn explains why there is so little published about what is certainly the preponderance of clinical texts, those that are not as well-behaved lexically and syntactically as a discharge summary.       In this application we distinguish clinical narratives (e.g., a progress note) from biomedical narratives (e.g., a PubMed abstract). We are interested in texts that arise in the clinical or research setting; texts that are composed by clinicians and researchers directly into a computer system. We propose to build and publish a tool called POET (Parsable Output Extracted from Text). POET will be designed to accept unstructured textual documents and return structured, linguistic equivalents that are, to the extent possible, parsable by higher-level NLP engines. POET will have an architecture is modular, extensible, and based on open-source platforms and sources (e.g., Java, Perl, UMLS, NegEx, the Stanford Parser, HL7 Clinical Document Architecture, caGRID, etc.). To implement POET, we will collect, program, and evaluate published as well as novel algorithms for: acronym/abbreviation resolution; spelling correction; template and pseudo-table re-writing; and removal of embedded non-text. To test POET we will use a large corpus of cross-discipline (e.g., medical, nursing, pharmacy, etc.) clinical note types, as well as the clinical research texts MedWatch reports and IRB adverse event reports. The development of POET will combine the best practices found in the literature and new research efforts as part of the project. To validate the fidelity of POET processing we plan a formal analysis of information loss and information gain pre- and post-process. To ensure broad access to the tools, POET will be released under an open-source license. Finally, we plan to assess the feasibility of offering POET as a Web service for remote processing.           Public Health Relevance This project attempts the construction of POET, a low-level preprocessing system for full text that can be used to open up large portions of the electronic health record (EHR) to high-level NLP systems. The potential public health implications are: 1) POET will allow the expansion of the use of well-proven clinical NLP systems  (currently limited to only a few document types found in the EHR) to the entire  clinical text record; with the entirety of the clinical record accessible to NLP,  serious and realistic attempts at real-time clinical text surveillance can be  mounted to improve patient safety and quality of care; 2) POET will be made available through open source distribution and other means  to encourage the practical deployment of innovative decision support systems  using large healthcare network EHRs across the country; 3) POET meets an important translational public health informatics need by solving  persistent low-level barriers to effective data mining of clinical marriages in the  EHR. The wider public health implications include promoting effective computerized decision support and data mining to improve both personal and public health outcomes.","POET: Consolidated, Comprehensive Clinical Text Preprocessing",7847940,R21LM009967,"['Abbreviations', 'Adverse event', 'Algorithms', 'Architecture', 'Body of uterus', 'Clinical', 'Clinical Pharmacists', 'Clinical Research', 'Computer Systems', 'Consult', 'Country', 'Data', 'Decision Support Systems', 'Development', 'Discipline', 'Discipline of Nursing', 'Electronic Health Record', 'Ensure', 'Excision', 'Hand', 'Healthcare', 'Internet', 'Java', 'Laboratories', 'Licensing', 'Linguistics', 'Literature', 'Marriage', 'Medical', 'Mind', 'Mining', 'Natural Language Processing', 'Nature', 'Nurses', 'Outcome', 'Output', 'Paste substance', 'Pathology', 'Pathology Report', 'Pharmacy facility', 'Physical assessment', 'Process', 'PubMed', 'Public Health', 'Public Health Informatics', 'Publishing', 'Quality of Care', 'Radiology Specialty', 'Report (document)', 'Reporting', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resolution', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Study Section', 'System', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'Vocabulary', 'Work', 'Writing', 'abstracting', 'acronyms', 'base', 'caGrid', 'computerized', 'data mining', 'design', 'discrete data', 'improved', 'innovation', 'interest', 'meetings', 'novel', 'open source', 'patient safety', 'programs', 'public health relevance', 'spelling', 'tool', 'trend']",NLM,UNIVERSITY OF UTAH,R21,2009,84657,0.05793725830900061
"POET: Consolidated, Comprehensive Clinical Text Preprocessing    DESCRIPTION (provided by applicant):       As electronic health records (EHRs) continue their expansion into clinical settings, there has been a corresponding increase in interest in mining the data they contain, both for research as well as for clinical decision support. Informaticists are increasingly studying ways to mine EHR textual content. This is an important trend, because there is a wealth of information contained in clinical text not represented anywhere else in the EHR. There is a low level text-as-data issue which presents a significant obstacle to the widespread use of available medical NLP systems: hand-typed clinical narratives in EHRs are usually ungrammatical; short or telegraphic in style; full of abbreviations, acronyms, and misspellings; formatted in a templated or pseudo-tabular form; and contain embedded non-text such as a list of laboratory values cut-and-pasted from elsewhere in the EHR. As we show in the Preliminary Studies Section, this makes high-level processing by popular tools like MedLEE and MetaMap effectively useless for all but a few ""clean"" document types like discharge summaries or consult reports (e.g., pathology or radiology reports). This in turn explains why there is so little published about what is certainly the preponderance of clinical texts, those that are not as well-behaved lexically and syntactically as a discharge summary.       In this application we distinguish clinical narratives (e.g., a progress note) from biomedical narratives (e.g., a PubMed abstract). We are interested in texts that arise in the clinical or research setting; texts that are composed by clinicians and researchers directly into a computer system. We propose to build and publish a tool called POET (Parsable Output Extracted from Text). POET will be designed to accept unstructured textual documents and return structured, linguistic equivalents that are, to the extent possible, parsable by higher-level NLP engines. POET will have an architecture is modular, extensible, and based on open-source platforms and sources (e.g., Java, Perl, UMLS, NegEx, the Stanford Parser, HL7 Clinical Document Architecture, caGRID, etc.). To implement POET, we will collect, program, and evaluate published as well as novel algorithms for: acronym/abbreviation resolution; spelling correction; template and pseudo-table re-writing; and removal of embedded non-text. To test POET we will use a large corpus of cross-discipline (e.g., medical, nursing, pharmacy, etc.) clinical note types, as well as the clinical research texts MedWatch reports and IRB adverse event reports. The development of POET will combine the best practices found in the literature and new research efforts as part of the project. To validate the fidelity of POET processing we plan a formal analysis of information loss and information gain pre- and post-process. To ensure broad access to the tools, POET will be released under an open-source license. Finally, we plan to assess the feasibility of offering POET as a Web service for remote processing.           Public Health Relevance This project attempts the construction of POET, a low-level preprocessing system for full text that can be used to open up large portions of the electronic health record (EHR) to high-level NLP systems. The potential public health implications are: 1) POET will allow the expansion of the use of well-proven clinical NLP systems  (currently limited to only a few document types found in the EHR) to the entire  clinical text record; with the entirety of the clinical record accessible to NLP,  serious and realistic attempts at real-time clinical text surveillance can be  mounted to improve patient safety and quality of care; 2) POET will be made available through open source distribution and other means  to encourage the practical deployment of innovative decision support systems  using large healthcare network EHRs across the country; 3) POET meets an important translational public health informatics need by solving  persistent low-level barriers to effective data mining of clinical marriages in the  EHR. The wider public health implications include promoting effective computerized decision support and data mining to improve both personal and public health outcomes.","POET: Consolidated, Comprehensive Clinical Text Preprocessing",7689273,R21LM009967,"['Abbreviations', 'Adverse event', 'Algorithms', 'Architecture', 'Body of uterus', 'Clinical', 'Clinical Pharmacists', 'Clinical Research', 'Computer Systems', 'Consult', 'Country', 'Data', 'Decision Support Systems', 'Development', 'Discipline', 'Discipline of Nursing', 'Electronic Health Record', 'Ensure', 'Excision', 'Hand', 'Healthcare', 'Internet', 'Java', 'Laboratories', 'Licensing', 'Linguistics', 'Literature', 'Marriage', 'Medical', 'Mind', 'Mining', 'Natural Language Processing', 'Nature', 'Nurses', 'Outcome', 'Output', 'Paste substance', 'Pathology', 'Pathology Report', 'Pharmacy facility', 'Physical assessment', 'Process', 'PubMed', 'Public Health', 'Public Health Informatics', 'Publishing', 'Quality of Care', 'Radiology Specialty', 'Report (document)', 'Reporting', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resolution', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Study Section', 'System', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'Vocabulary', 'Work', 'Writing', 'abstracting', 'acronyms', 'base', 'caGrid', 'computerized', 'data mining', 'design', 'discrete data', 'improved', 'innovation', 'interest', 'meetings', 'novel', 'open source', 'patient safety', 'programs', 'public health relevance', 'spelling', 'tool', 'trend']",NLM,UNIVERSITY OF UTAH,R21,2009,166590,0.05793725830900061
"Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects    DESCRIPTION (provided by applicant):        	Epidemiologic analyses of health care data can provide critical evidence on the effectiveness and safety of therapeutics. This is particularly vital during the transition from the point of regulatory approval through the early marketing of new drugs, a time when physicians, regulators and payers are all struggling with incomplete data. Health plans pay for these drugs without knowing how their effectiveness and safety compares with established alternatives, as new compounds are tested against placebos rather than active agents, and tested only in selected patients. Non-randomized studies in large healthcare databases can provide fast and less costly evidence on drug effects. However, conventional adjustment methods that rely on a small number of investigator-specified confounders often fail and may produce biased results.     We propose and have preliminary evidence that employing modern medical informatics algorithms that structure and search databases to empirically identify thousands of new covariates. These will then enter established propensity score-based models and so make far more effective use of the information contained in health care databases and electronic medical records (EMRs), resulting in more valid causal interpretations of treatment effects. We will:    - Develop algorithms that make greater use of information contained in longitudinal claims and EMR databases by empirically identifying thousands of potential confounders. The performance of these approaches will be evaluated in 6 example studies encompassing recent drug safety and comparative effectiveness problems, and will be implemented in multiple large claims databases supplemented by such data as lab values and EMR information in subgroups.    -- Develop novel methods for confounding adjustment based on textual information found in EMRs.    -- Expand the newly developed mining algorithms into a framework that integrates distributed database networks with uneven information content, similar to the Sentinel Network recently initiated by FDA.                            This project is likely to produce groundbreaking results at the interface of medicine, biomedical informatics, and epidemiologic methods. After completion of this project a library of documented and validated algorithms will be available to significantly improve confounder control in a range of healthcare databases. The theoretical foundation and the ready-to-use algorithms will likely lead to a fundamental shift in how databases contribute to the fast and accurate assessment of newly-marketed medications.            Large healthcare databases are used to assess the safety and effectiveness of drugs. However, conventional adjustment methods that rely on a limited number of investigator-specific covariates often fail to produce unbiased results. We will develop algorithms that make greater use of information contained in longitudinal claims data and electronic medical records databases by empirically identifying thousands of potential confounders. This will result in improved causal inference on the comparative safety and effectiveness of newly marketed medications that is both less susceptible to investigator omissions and faster than conventional approaches.",Analyzing Complex Healthcare Data to Determine Causality of Observed Drug Effects,7767483,R01LM010213,"['Address', 'Algorithms', 'Clinical', 'Code', 'Complex', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Distributed Databases', 'Effectiveness', 'Elderly', 'Epidemiologic Methods', 'Epidemiology', 'Etiology', 'Evaluation', 'Foundations', 'Head', 'Health Planning', 'Healthcare', 'Heterogeneity', 'Individual', 'Insurance', 'Internet', 'Knowledge', 'Lead', 'Libraries', 'Marketing', 'Medical Informatics', 'Medicare/Medicaid', 'Medicine', 'Methods', 'Mining', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Placebos', 'Population', 'Pregnant Women', 'Process', 'Publications', 'Randomized', 'Research', 'Research Personnel', 'Safety', 'Scoring Method', 'Sentinel', 'Severity of illness', 'Solid', 'Solutions', 'Specific qualifier value', 'Speed', 'Staging', 'Structure', 'Subgroup', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'To specify', 'Training', 'base', 'biomedical informatics', 'comparative', 'comparative effectiveness', 'compare effectiveness', 'data mining', 'improved', 'indexing', 'lectures', 'novel', 'outcome forecast', 'patient privacy', 'programs', 'routine care', 'symposium', 'treatment effect']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2009,314000,0.011860340312399319
"Extracting Semantic Knowledge from Clinical Reports    DESCRIPTION (provided by applicant): Analyzing and processing free-text medical reports for data mining and clinical data interchange is one of the most challenging problems in medical informatics, yet it is crucial for continued research advances and improvements in clinical care. Natural language processing (NLP) is an important enabling technology, but has been held back because it is difficult to understand human language, since it requires extensive domain knowledge. In Phase I, we developed new statistical and machine learning methods that apply domain specific knowledge to the semantic analysis of free-text radiology reports. The methods enabled the creation of two new prototype applications - a SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms) coding service called SnomedCoder, and a text mining tool for analyzing a large corpus of medical reports, called DataMiner. In Phase II, we will accomplish the following specific aims: 1) Improve the semantic extraction methods developed in Phase I, 2) Expand the semantic knowledge base and classify at least two million new unique sentences from multiple medical institutions, 3) Provide a SNOMED CT auto coding service (alpha service) to participating Indiana Health Information Exchange hospitals, and 4) Build a commercial version of the DataMiner software, and test its functionality using researchers at the Regenstrief Institute.       These scientific innovations will revolutionize the ability of health care researchers to analyze vast repositories of clinical information currently locked up in electronic medical records, and correlate this data with new biomedical discoveries in proteonomics and genomics. The ability to codify text rapidly will extend the potential for clinical decision support beyond its narrow base of numeric and structured medical data, and enable SNOMED CT to become a useful coding standard. Phase III will offer coding and data mining services to healthcare payers (both private and government), pharmaceuticals, and academic researchers. A key advantage of our approach over other NLP systems is that we attempt to codify all the information in the report and not just a limited subset, and insist on expert validation which provides a high degree of confidence in the accuracy of the coded data.Project Narrative           n/a",Extracting Semantic Knowledge from Clinical Reports,7394699,R44RR024929,"['Address', 'Algorithms', 'Back', 'Bioinformatics', 'Body of uterus', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Code', 'Collection', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Reporting', 'Decision Making', 'Effectiveness', 'Genomics', 'Goals', 'Government', 'Health', 'Healthcare', 'Hospitals', 'Human', 'Indiana', 'Institutes', 'Institution', 'Journals', 'Knowledge', 'Language', 'Longitudinal Studies', 'Machine Learning', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Natural Language Processing', 'Paper', 'Pharmacologic Substance', 'Phase', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Testing', 'Text', 'Thinking', 'Trees', 'Trust', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'base', 'computerized', 'data mining', 'health care quality', 'improved', 'indexing', 'innovation', 'knowledge base', 'novel strategies', 'patient safety', 'prototype', 'repository', 'research and development', 'success', 'text searching', 'tool']",NCRR,"LOGICAL SEMANTICS, INC.",R44,2008,429955,0.015580973162130146
"Investigating the generalizability of natural language processing of EMR data    DESCRIPTION (provided by applicant):       The electronic medical record (EMR) offers impressive opportunities for increasing care quality, but challenges stand in the way of realizing this vision. For example, coded EMR data readily available for analysis typically are incomplete (due to the prevalence of free-text clinical notes in EMR implementations), and data from different EMRs are often incommensurate due to differences in standard vocabularies and system implementations. While informatics research has shown the feasibility of automatically coding specific aspects of clinical text using Natural Language Processing (NLP), challenges remain for translating these informatics developments into large-scale care quality assessments. To date, successful NLP solutions for automated quality assessment have tended to be applications that are specific to (a) the target problem or clinical focus, (b) the EMR data system, and (c) the person or team that implements the NLP solution. In this study, we propose to begin addressing the problem of implementation team specificity by developing, evaluating, and making freely available a generalizable NLP development tool suite. The tools will enable widespread adoption of NLP systems to extract and code data from free text clinical notes. The Knowledge Editing Toolkit will simplify development of problem-specific knowledge by helping the user define the rules, concepts, and terms that constitute a domain-specific knowledge module, thus allowing any informaticist to develop an NLP application. The NLP Application Validation Toolkit will allow rapid testing and evaluation of the application against a gold standard of independently-coded test records from any EMR. To evaluate the effects of the toolkits on NLP generalizability, we will have three clinical informaticists each build two NLP applications (for a total of six distinct applications). One of their applications will identify a constellation of common clinical signs or symptoms (e.g., ""persistent cough"") that are relatively discrete concepts using simple language terms for many different clinical purposes. Their second application will assess behavioral counseling (e.g., ""alcohol counseling""), which uses complex language constructs for dedicated clinical purposes. We will describe and evaluate the accuracy of the solutions against independently coded test sets of medical records. We will quantify and compare the difficulty of creating these solutions as measured by the time, number of iterations required to build the applications, and the number of concepts and rules employed, as well as analyze variability in content and accuracy of the solutions created. In addition, we will use qualitative techniques to assess the ease of using the development tools; the difficulty in learning the tools; and specific types of problems, limitations, and bugs encountered. Such an NLP development tool suite has the potential to allow simple, elegant, and reliably good NLP solutions regardless of the clinical problem domain or the person developing the solution.           n/a",Investigating the generalizability of natural language processing of EMR data,7529967,R21LM009728,"['Address', 'Adopted', 'Adoption', 'Affect', 'Alcohols', 'Architecture', 'Behavioral', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'Computer Systems Development', 'Computerized Medical Record', 'Coughing', 'Counseling', 'Data', 'Databases', 'Development', 'Event', 'Gold', 'Healthcare Systems', 'Human Resources', 'Informatics', 'Information Systems', 'Information Technology', 'Institute of Medicine (U.S.)', 'Knowledge', 'Language', 'Learning', 'Measures', 'Medical Records', 'Modification', 'Natural Language Processing', 'Numbers', 'Performance', 'Persons', 'Positioning Attribute', 'Prevalence', 'Process', 'Proliferating', 'Purpose', 'Quality of Care', 'Records', 'Research', 'Running', 'Solutions', 'Source', 'Specificity', 'Standards of Weights and Measures', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Translating', 'United States National Academy of Sciences', 'Ursidae Family', 'Validation', 'Vision', 'Vocabulary', 'care delivery', 'concept', 'cost', 'design', 'evaluation/testing', 'experience', 'health care quality', 'stem', 'success', 'tool', 'tool development']",NLM,KAISER FOUNDATION RESEARCH INSTITUTE,R21,2008,213300,0.03255374717962246
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7502749,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Numbers', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'design', 'foot', 'journal article', 'language processing', 'mecarzole', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2008,352226,-0.00723159261206169
"Evidence based anomaly detection in clinical databases.    DESCRIPTION (provided by applicant):       Medical errors and their timely identification remain an important problem in clinical practice. Electronic medical record repositories and electronic data processing offer an opportunity to identify such errors in time to prevent them or at least attenuate their harm.  Typical computer-based error detection methods rely on the use of clinical knowledge, such as expert-derived rules, that is incorporated into the monitoring and alerting systems. Alerting that is based on knowledge is generally reliable; however, it is time-consuming and costly to extract and codify such knowledge, and as a consequence such systems are relatively narrow in their scope.  We propose to develop and evaluate a data-based approach for detecting clinical outliers (anomalies) that is complementary to knowledge-based approaches. This new approach is based on comparing clinical actions, such as medications given and labs ordered, taken for the current patient to those actions taken for similar patients in the recent past, as recorded in a clinical database. If a clinical action for the current patient is highly unusual, then a cautionary alert is raised along with an explanation for why the action appears to be unusual. Key advantages of the new technique are that it works with minimal prior knowledge, and it may detect anomalies for which no rules have yet been written. Thus, this data-driven approach to clinical anomaly detection is expected to complement knowledge-based alerting methods. We propose to implement a data-driven anomaly detection method, and then evaluate it in a laboratory setting using retrospective data for the cohort of surgical cardiac patients.  The project investigators comprise a multidisciplinary team with expertise in rule-based alerting in a hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.          n/a",Evidence based anomaly detection in clinical databases.,7385073,R21LM009102,"['Anticoagulants', 'Arts', 'Attenuated', 'Automatic Data Processing', 'Blood Platelets', 'Cardiac', 'Catheters', 'Cessation of life', 'Clinical', 'Complement', 'Computerized Medical Record', 'Computers', 'Condition', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Detection', 'Development', 'Disadvantaged', 'Dose', 'Drops', 'Evaluation', 'Event', 'Flushing', 'General Population', 'Healthcare', 'Heparin', 'Hospitals', 'Imagery', 'Information Resources Management', 'Invasive', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Lead', 'Left', 'Life', 'Low-Molecular-Weight Heparin', 'Machine Learning', 'Manuals', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Monitor', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Physicians', 'Platelet Count measurement', 'Platelet Transfusion', 'Practice Management', 'Procedures', 'Range', 'Records', 'Research', 'Research Personnel', 'Risk', 'Signal Transduction', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Validation', 'Warfarin', 'Work', 'Writing', 'base', 'biomedical informatics', 'cohort', 'computer based statistical methods', 'cost', 'design', 'improved', 'interest', 'knowledge base', 'medical specialties', 'multidisciplinary', 'novel', 'novel strategies', 'prevent', 'prophylactic', 'repository', 'response', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2008,193536,0.004525602974953364
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7502672,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'concept', 'data modeling', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2008,1432331,0.019752062475471188
"NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports    DESCRIPTION (provided by applicant):       Many NLP applications have been successfully developed to extract information from text. Most of the   applications have focused on identifying individual clinical conditions in textual records, which is the first step in making the conditions available to computerized applications. However, identifying individual instances of clinical conditions is not sufficient for many medical informatics tasks - the context surrounding the condition is crucial for integrating the information within the text to determine the clinical state of a patient. We propose to perform in-depth studies on NLP issues requiring knowledge of the context of clinical conditions in clinical records. We will focus our research by using syndromic surveillance from emergency department (ED) reports as a case study.      For this proposal, we will test the following hypothesis: An NLP system that indexes clinical concepts and integrates contextual information modifying the concepts can identify acute clinical conditions from ED reports as well as physicians can.      We will identify clinical concepts necessary for surveillance of seven syndromes, including respiratory,   gastrointestinal, neurological, rash, hemorrhagic, constitutional, and botulinic. To evaluate the hypothesis, we will perform the following specific aims:      Aim 1. Perform in-depth, foundational studies on four NLP topics to gain a deeper understanding of the      pertinent NLP research capabilities required for identification of acute clinical conditions from ED reports, including negation, uncertainty, temporal discrimination, and finding validation;      Aim 2. Apply the knowledge learned from the foundational studies to develop and evaluate an automated application for ED reports that will determine the values for clinical variables relevant to identifying patients with any of seven syndromes.      The research is innovative, because it will generate an in-depth study of multiple NLP topics crucial to   understanding a patient's clinical state from textual records and will focus on contextual understanding and analysis. The research will be guided by linguistic principles, by the semantics and discourse structure of ED reports, and by the application area of biosurveillance. Because we will develop research methods and tools that are customized to a particular domain, we will constrain the research space, which will provide direction and enhance the chance for success. However, the methods and tools generated by this research should be extensible to other clinical report types and to other domain applications, because we will explicitly specify and study NLP concepts and relationships that are common to many application areas.             n/a",NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports,7469551,R01LM009427,"['Accident and Emergency department', 'Acute', 'Area', 'Case Study', 'Clinical', 'Clinical Data', 'Condition', 'Constitutional', 'Depth', 'Detection', 'Discrimination', 'Exanthema', 'Exhibits', 'Individual', 'Informatics', 'Knowledge', 'Learning', 'Linguistics', 'Medical Informatics', 'Medical Informatics Applications', 'Medical Surveillance', 'Methods', 'Natural Language Processing', 'Neurologic', 'Ontology', 'Patients', 'Physicians', 'Records', 'Reporting', 'Research', 'Research Methodology', 'Semantics', 'Specific qualifier value', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Uncertainty', 'Validation', 'computerized', 'concept', 'gastrointestinal', 'indexing', 'innovation', 'quality assurance', 'respiratory', 'success', 'syndromic surveillance', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2008,392337,0.033490188528213484
"POET: Consolidated, Comprehensive Clinical Text Preprocessing    DESCRIPTION (provided by applicant):       As electronic health records (EHRs) continue their expansion into clinical settings, there has been a corresponding increase in interest in mining the data they contain, both for research as well as for clinical decision support. Informaticists are increasingly studying ways to mine EHR textual content. This is an important trend, because there is a wealth of information contained in clinical text not represented anywhere else in the EHR. There is a low level text-as-data issue which presents a significant obstacle to the widespread use of available medical NLP systems: hand-typed clinical narratives in EHRs are usually ungrammatical; short or telegraphic in style; full of abbreviations, acronyms, and misspellings; formatted in a templated or pseudo-tabular form; and contain embedded non-text such as a list of laboratory values cut-and-pasted from elsewhere in the EHR. As we show in the Preliminary Studies Section, this makes high-level processing by popular tools like MedLEE and MetaMap effectively useless for all but a few ""clean"" document types like discharge summaries or consult reports (e.g., pathology or radiology reports). This in turn explains why there is so little published about what is certainly the preponderance of clinical texts, those that are not as well-behaved lexically and syntactically as a discharge summary.       In this application we distinguish clinical narratives (e.g., a progress note) from biomedical narratives (e.g., a PubMed abstract). We are interested in texts that arise in the clinical or research setting; texts that are composed by clinicians and researchers directly into a computer system. We propose to build and publish a tool called POET (Parsable Output Extracted from Text). POET will be designed to accept unstructured textual documents and return structured, linguistic equivalents that are, to the extent possible, parsable by higher-level NLP engines. POET will have an architecture is modular, extensible, and based on open-source platforms and sources (e.g., Java, Perl, UMLS, NegEx, the Stanford Parser, HL7 Clinical Document Architecture, caGRID, etc.). To implement POET, we will collect, program, and evaluate published as well as novel algorithms for: acronym/abbreviation resolution; spelling correction; template and pseudo-table re-writing; and removal of embedded non-text. To test POET we will use a large corpus of cross-discipline (e.g., medical, nursing, pharmacy, etc.) clinical note types, as well as the clinical research texts MedWatch reports and IRB adverse event reports. The development of POET will combine the best practices found in the literature and new research efforts as part of the project. To validate the fidelity of POET processing we plan a formal analysis of information loss and information gain pre- and post-process. To ensure broad access to the tools, POET will be released under an open-source license. Finally, we plan to assess the feasibility of offering POET as a Web service for remote processing.           Public Health Relevance This project attempts the construction of POET, a low-level preprocessing system for full text that can be used to open up large portions of the electronic health record (EHR) to high-level NLP systems. The potential public health implications are: 1) POET will allow the expansion of the use of well-proven clinical NLP systems  (currently limited to only a few document types found in the EHR) to the entire  clinical text record; with the entirety of the clinical record accessible to NLP,  serious and realistic attempts at real-time clinical text surveillance can be  mounted to improve patient safety and quality of care; 2) POET will be made available through open source distribution and other means  to encourage the practical deployment of innovative decision support systems  using large healthcare network EHRs across the country; 3) POET meets an important translational public health informatics need by solving  persistent low-level barriers to effective data mining of clinical marriages in the  EHR. The wider public health implications include promoting effective computerized decision support and data mining to improve both personal and public health outcomes.","POET: Consolidated, Comprehensive Clinical Text Preprocessing",7570254,R21LM009967,"['Abbreviations', 'Adverse event', 'Algorithms', 'Architecture', 'Body of uterus', 'Clinical', 'Clinical Pharmacists', 'Clinical Research', 'Computer Systems', 'Consult', 'Data', 'Development', 'Discipline', 'Discipline of Nursing', 'Electronic Health Record', 'Ensure', 'Excision', 'Hand', 'Internet', 'Java', 'Laboratories', 'Licensing', 'Linguistics', 'Literature', 'Medical', 'Mind', 'Mining', 'Natural Language Processing', 'Nature', 'Nurses', 'Output', 'Paste substance', 'Pathology', 'Pathology Report', 'Pharmacy facility', 'Physical assessment', 'Process', 'PubMed', 'Publishing', 'Radiology Specialty', 'Report (document)', 'Reporting', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resolution', 'Services', 'Source', 'Specific qualifier value', 'Standards of Weights and Measures', 'Structure', 'Study Section', 'System', 'Testing', 'Text', 'Thinking', 'Unified Medical Language System', 'Vocabulary', 'Work', 'Writing', 'abstracting', 'base', 'data mining', 'design', 'discrete data', 'interest', 'novel', 'open source', 'programs', 'spelling', 'tool', 'trend']",NLM,UNIVERSITY OF UTAH,R21,2008,169313,0.05793725830900061
"Statistical NLP Analysis of Cross-discipline Clinical Text emerging trend in computational linguistics is melding natural language processing (NLP) and machine learning (ML) to help computers make sense of human-generated free text. The blending of these disciplines is relatively rare in biomedical inforrnatics. Past medical NLP/ML research work is biased heavily towards linguistic methods that attempt to reason about grammar and syntax aided by a domain-focal knowledge base (e.g., one for radiology or one for clinical pathology). The aim of the work proposed here takes a different tack: exploring the utility of a statistical approach to clinical NLP, one augmented by machine learning and concentrating on general progress notes from across multiple clinical domains. The specific clinical goal will be to identify adverse drug events described implicitly or explicitly in inpatient progress notes. Rather than relying on a narrow domain focus to provide enough context restriction to make text interpretation tractable, this approach will use statistical patterns in note author information (e.g., profession, note type, treating ward) and patient information (e.g., admit diagnosis, procedures performed, temporal note relationships) for context restriction. The research component of this proposal is divided into two categories: three small-scale projects designed to rapidly hone new skills developed under the training component, and a large-scale project that assesses the feasibility of cross-discipline clinical text analysis. n/a",Statistical NLP Analysis of Cross-discipline Clinical Text,6944955,F38LM008478,"['Categories', 'Clinical', 'Clinical Pathology', 'Computers', 'Coupled', 'Diagnosis', 'Discipline', 'Event', 'Fellowship', 'Goals', 'Human', 'Inpatients', 'Linguistics', 'Machine Learning', 'Medical', 'Methods', 'Natural Language Processing', 'Patients', 'Pattern', 'Personal Satisfaction', 'Pharmaceutical Preparations', 'Procedures', 'Radiology Specialty', 'Research', 'Statistical Study', 'Text', 'Training', 'Work', 'Writing', 'design', 'experience', 'knowledge base', 'skills', 'syntax', 'theories', 'tool', 'trend', 'ward']",NLM,UNIVERSITY OF UTAH,F38,2007,38768,0.017014649940939253
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7380099,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Numbers', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'design', 'foot', 'journal article', 'language processing', 'mecarzole', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2007,383550,-0.00723159261206169
"Evidence based anomaly detection in clinical databases.    DESCRIPTION (provided by applicant):       Medical errors and their timely identification remain an important problem in clinical practice. Electronic medical record repositories and electronic data processing offer an opportunity to identify such errors in time to prevent them or at least attenuate their harm.  Typical computer-based error detection methods rely on the use of clinical knowledge, such as expert-derived rules, that is incorporated into the monitoring and alerting systems. Alerting that is based on knowledge is generally reliable; however, it is time-consuming and costly to extract and codify such knowledge, and as a consequence such systems are relatively narrow in their scope.  We propose to develop and evaluate a data-based approach for detecting clinical outliers (anomalies) that is complementary to knowledge-based approaches. This new approach is based on comparing clinical actions, such as medications given and labs ordered, taken for the current patient to those actions taken for similar patients in the recent past, as recorded in a clinical database. If a clinical action for the current patient is highly unusual, then a cautionary alert is raised along with an explanation for why the action appears to be unusual. Key advantages of the new technique are that it works with minimal prior knowledge, and it may detect anomalies for which no rules have yet been written. Thus, this data-driven approach to clinical anomaly detection is expected to complement knowledge-based alerting methods. We propose to implement a data-driven anomaly detection method, and then evaluate it in a laboratory setting using retrospective data for the cohort of surgical cardiac patients.  The project investigators comprise a multidisciplinary team with expertise in rule-based alerting in a hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.          n/a",Evidence based anomaly detection in clinical databases.,7197167,R21LM009102,"['Anticoagulants', 'Arts', 'Attenuated', 'Automatic Data Processing', 'Blood Platelets', 'Cardiac', 'Catheters', 'Cessation of life', 'Clinical', 'Complement', 'Computerized Medical Record', 'Computers', 'Condition', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Detection', 'Development', 'Disadvantaged', 'Dose', 'Drops', 'Evaluation', 'Event', 'Flushing', 'General Population', 'Healthcare', 'Heparin', 'Hospitals', 'Imagery', 'Information Resources Management', 'Invasive', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Lead', 'Left', 'Life', 'Low-Molecular-Weight Heparin', 'Machine Learning', 'Manuals', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Monitor', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Physicians', 'Platelet Count measurement', 'Platelet Transfusion', 'Practice Management', 'Procedures', 'Range', 'Records', 'Research', 'Research Personnel', 'Risk', 'Signal Transduction', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Validation', 'Warfarin', 'Work', 'Writing', 'base', 'biomedical informatics', 'cohort', 'computer based statistical methods', 'cost', 'design', 'improved', 'interest', 'knowledge base', 'medical specialties', 'multidisciplinary', 'novel', 'novel strategies', 'prevent', 'prophylactic', 'repository', 'response', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2007,161562,0.004525602974953364
"Automated Detection of Medical Errors DESCRIPTION:    The long-term goal of this proposal is to use the electronic medical record, including narrative text, to understand and encode the process of care for individual patients in order to improve patient safety.   Achieving this goal has the potential to help detect adverse events, and to differentiate medical errors from appropriately tailored care. The specific aims for this proposal are as follows: 1) To understand and encode the process of care for individual patients using data in the electronic medical record, including narrative text.   2) To use a more detailed understanding of patients' processes of care to improve automated adverse event detection. 3) To match processes of care for individual patients against accepted care pathways in order to identify discrepancies. We will capitalize on three core technologies that are in active use by clinicians and researchers in our busy clinical setting: 1) a Web-based clinical information system and its associated clinical data repository (WebCIS), 2) a full medical language parser (MedLEE), and 3) a semi-structured, electronic physician documentation system built by the applicant specifically to support this project (eNote).   Methods will include evaluating the performance (sensitivity, specificity and positive predictive value) of our system, DETER+MINE (DETecting ERrors Mining Narrative Electronically), to model the care process and detect adverse events and pathway deviations. We will utilize explicit process criteria and manual, retrospective chart review as a gold standard.   This research is intended to provide proof of concept that combining natural language processing of clinical narrative with traditional sources of coded data is required for effective screening with automated defection systems. This approach has the potential to impact significantly on our ability to detect and investigate medical errors, adverse medical events, and pathway deviations by reducing reliance on costly and slow manual chart reviews. n/a",Automated Detection of Medical Errors,7282343,K22LM008805,"['Address', 'Adverse event', 'Applications Grants', 'Caring', 'Causations', 'Cause of Death', 'Clinical', 'Clinical Data', 'Clinical Decision Support Systems', 'Clinical Pathways', 'Code', 'Computerized Medical Record', 'Critical Care', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Documentation', 'Effectiveness', 'Electronics', 'Event', 'Goals', 'Gold', 'Guidelines', 'Hospitals', 'Human', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Language', 'Manuals', 'Medical', 'Medical Errors', 'Methods', 'Mining', 'Modeling', 'Natural Language Processing', 'Negligence', 'Online Systems', 'Pathway interactions', 'Patients', 'Performance', 'Physicians', 'Predictive Value', 'Prevention', 'Process', 'Range', 'Recommendation', 'Reliance', 'Reporting', 'Research', 'Research Personnel', 'Screening procedure', 'Sensitivity and Specificity', 'Source Code', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'United States', 'base', 'computerized', 'concept', 'experience', 'improved', 'patient safety', 'programs', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,K22,2007,135000,0.019998648921745785
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,7249382,R01EB000362,"['Address', 'Architecture', 'Caring', 'Chronic', 'Clinical', 'Clinical Medicine', 'Complex', 'Computer Architectures', 'Condition', 'Data', 'Data Collection', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Documentation', 'Early Diagnosis', 'Effectiveness', 'Engineering', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Extensible Markup Language', 'Geographic Locations', 'Healthcare', 'Image', 'Imagery', 'Individual', 'Information Management', 'Laboratories', 'Language', 'Link', 'Management Information Systems', 'Measures', 'Medical', 'Medical Records', 'Medicine', 'Methodology', 'Metric', 'Multimedia', 'Natural Language Processing', 'Numbers', 'Online Systems', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Problem Solving', 'Procedures', 'Process', 'Provider', 'Radiology Specialty', 'Records', 'Reporting', 'Research', 'Research Design', 'Series', 'Services', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'TimeLine', 'United States', 'Urology', 'Visit', 'base', 'concept', 'cost', 'data acquisition', 'data management', 'data modeling', 'improved', 'information gathering', 'innovation', 'innovative technologies', 'insight', 'peer', 'point of care', 'portability', 'research clinical testing', 'urologic', 'usability']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2007,517331,0.03456697243520172
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,7272822,R01EB002247,"['Address', 'Algorithms', 'Anatomy', 'Area', 'Atlases', 'Body of uterus', 'Cancer Patient', 'Caregivers', 'Chronic', 'Chronic Disease', 'Clinical', 'Communication', 'Communities', 'Condition', 'Consultations', 'Data', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Documentation', 'Eating', 'Environment', 'Evaluation', 'Functional Magnetic Resonance Imaging', 'Health Personnel', 'Healthcare', 'Image', 'Label', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medical Records', 'Methods', 'Metric', 'Modeling', 'Musculoskeletal', 'Musculoskeletal Pain', 'Natural Language Processing', 'Neurologic', 'Oncologist', 'Optics', 'Patients', 'Performance', 'Physicians', 'Primary Care Physician', 'Primary Health Care', 'Process', 'Quality of Care', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Slice', 'Specialist', 'Structure', 'Surgeon', 'System', 'Techniques', 'Technology', 'Teleconsultations', 'Telemedicine', 'Testing', 'Time', 'Upper arm', 'Work', 'base', 'chemotherapy', 'data mining', 'diagnostic accuracy', 'health care quality', 'image registration', 'improved', 'interest', 'knowledge base', 'medical specialties', 'novel', 'research clinical testing', 'size', 'telehealth']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2007,394864,0.020511917575710227
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7427367,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'concept', 'data modeling', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY MED CTR,U01,2007,1463449,0.019752062475471188
"Automated Detection of Medical Errors DESCRIPTION:    The long-term goal of this proposal is to use the electronic medical record, including narrative text, to understand and encode the process of care for individual patients in order to improve patient safety.   Achieving this goal has the potential to help detect adverse events, and to differentiate medical errors from appropriately tailored care. The specific aims for this proposal are as follows: 1) To understand and encode the process of care for individual patients using data in the electronic medical record, including narrative text.   2) To use a more detailed understanding of patients' processes of care to improve automated adverse event detection. 3) To match processes of care for individual patients against accepted care pathways in order to identify discrepancies. We will capitalize on three core technologies that are in active use by clinicians and researchers in our busy clinical setting: 1) a Web-based clinical information system and its associated clinical data repository (WebCIS), 2) a full medical language parser (MedLEE), and 3) a semi-structured, electronic physician documentation system built by the applicant specifically to support this project (eNote).   Methods will include evaluating the performance (sensitivity, specificity and positive predictive value) of our system, DETER+MINE (DETecting ERrors Mining Narrative Electronically), to model the care process and detect adverse events and pathway deviations. We will utilize explicit process criteria and manual, retrospective chart review as a gold standard.   This research is intended to provide proof of concept that combining natural language processing of clinical narrative with traditional sources of coded data is required for effective screening with automated defection systems. This approach has the potential to impact significantly on our ability to detect and investigate medical errors, adverse medical events, and pathway deviations by reducing reliance on costly and slow manual chart reviews. n/a",Automated Detection of Medical Errors,7124706,K22LM008805,"['automated medical record system', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'health care quality', 'health services research tag', 'human data', 'information retrieval', 'medical records', 'method development', 'patient care management', 'patient safety /medical error']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,K22,2006,135000,0.019998648921745785
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,7115855,R01EB002247,"['anatomy', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer assisted patient care', 'computer data analysis', 'computer graphics /printing', 'computer system design /evaluation', 'diagnosis quality /standard', 'health care quality', 'health care referral /consultation', 'human data', 'image enhancement', 'image processing', 'magnetic resonance imaging', 'musculoskeletal disorder', 'nervous system disorder', 'statistics /biometry', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2006,396341,0.020511917575710227
"Capturing and linking genomic and clinical information DESCRIPTION (provided by applicant):  The long-term aim of this project is to use natural language processing (NLP) to build a high throughput tool for facilitating cancer research by automatically extracting and organizing clinical and genetic information from the Electronic Medical Record (EMR) and from journal articles.  Our research involves advanced NLP techniques to:  1) enable the mining of phenotypic and genotypic data in the EMR; 2) automatically amass knowledge concerned with cancer and biomolecular relationships from journals; 3) develop a WEB-enabled visualization tool for researchers that will present diverse views of the knowledge; and 4) develop an Infrastructure that will link to the clinical data warehouse at New York Presbyterian Hospital (NYPH) and to GeneWays, a related project that allows researchers to visualize pathways.   More specifically, MedLEE (the NLP system we developed that extracts and encodes clinical and environmental information from the EMR) will be extended to extract genetic information contained in the EMR; subsequently, twelve years of patient reports will be processed and the extracted data added to the warehouse.  In addition, a new system, PhenoGenes, will be developed based on MedLEE and GeneWays (which contains another NLP system we developed that extracts and codifies biomolecular relations from journal articles).  PhenoGenes will capture biomolecular interactions directly associated with the treatment, diagnosis, and prognosis of cancer.  It will also generate an XML knowledge base that will integrate and organize the information that will be captured, and a Web-enabled tool that will allow users to browse and view the knowledge clustered according to different orientations (e.g. gene, disease, tissue, interaction, etc.).  The knowledge base will be linked to the GeneWays system, so that relevant pathways can be visualized.   MedLEE is utilized operationally at NYPH.  It also has been demonstrated that both NLP systems are highly effective.  This current project builds upon our experience and success with these systems.  The availability of related compatible clinical and biomolecular NLP systems, provide an exceptional opportunity to pave the way for capture, integration and organization of phenotypic and genotypic data and knowledge that will be used to radically improve patient care. n/a",Capturing and linking genomic and clinical information,7110256,R01LM007659,"['cancer information system', 'clinical research', 'data collection', 'health science research', 'human data', 'informatics', 'library', 'medical records', 'molecular biology information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,478733,0.03663104763458009
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,7083613,R01EB000362,"['automated medical record system', 'clinical research', 'computer assisted medical decision making', 'data collection methodology /evaluation', 'human data', 'informatics', 'information display', 'information system analysis', 'mathematical model', 'medical records', 'outcomes research', 'patient care management', 'performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2006,535569,0.03456697243520172
"Automated Detection of Medical Errors DESCRIPTION:    The long-term goal of this proposal is to use the electronic medical record, including narrative text, to understand and encode the process of care for individual patients in order to improve patient safety.   Achieving this goal has the potential to help detect adverse events, and to differentiate medical errors from appropriately tailored care. The specific aims for this proposal are as follows: 1) To understand and encode the process of care for individual patients using data in the electronic medical record, including narrative text.   2) To use a more detailed understanding of patients' processes of care to improve automated adverse event detection. 3) To match processes of care for individual patients against accepted care pathways in order to identify discrepancies. We will capitalize on three core technologies that are in active use by clinicians and researchers in our busy clinical setting: 1) a Web-based clinical information system and its associated clinical data repository (WebCIS), 2) a full medical language parser (MedLEE), and 3) a semi-structured, electronic physician documentation system built by the applicant specifically to support this project (eNote).   Methods will include evaluating the performance (sensitivity, specificity and positive predictive value) of our system, DETER+MINE (DETecting ERrors Mining Narrative Electronically), to model the care process and detect adverse events and pathway deviations. We will utilize explicit process criteria and manual, retrospective chart review as a gold standard.   This research is intended to provide proof of concept that combining natural language processing of clinical narrative with traditional sources of coded data is required for effective screening with automated defection systems. This approach has the potential to impact significantly on our ability to detect and investigate medical errors, adverse medical events, and pathway deviations by reducing reliance on costly and slow manual chart reviews. n/a",Automated Detection of Medical Errors,6958394,K22LM008805,"['automated medical record system', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'health care quality', 'health services research tag', 'human data', 'information retrieval', 'medical records', 'method development', 'patient care management', 'patient safety /medical error']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,K22,2005,134800,0.019998648921745785
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,6948251,R01EB002247,"['anatomy', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer assisted patient care', 'computer data analysis', 'computer graphics /printing', 'computer system design /evaluation', 'diagnosis quality /standard', 'health care quality', 'health care referral /consultation', 'human data', 'image enhancement', 'image processing', 'magnetic resonance imaging', 'musculoskeletal disorder', 'nervous system disorder', 'statistics /biometry', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2005,399327,0.020511917575710227
"Capturing and linking genomic and clinical information DESCRIPTION (provided by applicant):  The long-term aim of this project is to use natural language processing (NLP) to build a high throughput tool for facilitating cancer research by automatically extracting and organizing clinical and genetic information from the Electronic Medical Record (EMR) and from journal articles.  Our research involves advanced NLP techniques to:  1) enable the mining of phenotypic and genotypic data in the EMR; 2) automatically amass knowledge concerned with cancer and biomolecular relationships from journals; 3) develop a WEB-enabled visualization tool for researchers that will present diverse views of the knowledge; and 4) develop an Infrastructure that will link to the clinical data warehouse at New York Presbyterian Hospital (NYPH) and to GeneWays, a related project that allows researchers to visualize pathways.   More specifically, MedLEE (the NLP system we developed that extracts and encodes clinical and environmental information from the EMR) will be extended to extract genetic information contained in the EMR; subsequently, twelve years of patient reports will be processed and the extracted data added to the warehouse.  In addition, a new system, PhenoGenes, will be developed based on MedLEE and GeneWays (which contains another NLP system we developed that extracts and codifies biomolecular relations from journal articles).  PhenoGenes will capture biomolecular interactions directly associated with the treatment, diagnosis, and prognosis of cancer.  It will also generate an XML knowledge base that will integrate and organize the information that will be captured, and a Web-enabled tool that will allow users to browse and view the knowledge clustered according to different orientations (e.g. gene, disease, tissue, interaction, etc.).  The knowledge base will be linked to the GeneWays system, so that relevant pathways can be visualized.   MedLEE is utilized operationally at NYPH.  It also has been demonstrated that both NLP systems are highly effective.  This current project builds upon our experience and success with these systems.  The availability of related compatible clinical and biomolecular NLP systems, provide an exceptional opportunity to pave the way for capture, integration and organization of phenotypic and genotypic data and knowledge that will be used to radically improve patient care. n/a",Capturing and linking genomic and clinical information,6912634,R01LM007659,"['cancer information system', 'clinical research', 'data collection', 'health science research', 'human data', 'informatics', 'library', 'medical records', 'molecular biology information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,478937,0.03663104763458009
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,6917854,R01EB000362,"['automated medical record system', 'clinical research', 'computer assisted medical decision making', 'data collection methodology /evaluation', 'human data', 'informatics', 'information display', 'information system analysis', 'mathematical model', 'medical records', 'outcomes research', 'patient care management', 'performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2005,593264,0.03456697243520172
"Statistical NLP Analysis of Cross-discipline Clinical Text DESCRIPTION (provided by applicant):     An emerging trend in computational linguistics is melding natural language processing (NLP) and machine learning (ML) to help computers make sense of human-generated free text. The blending of these disciplines is relatively rare in biomedical informatics. Past medical NLP/ML research work is biased heavily towards linguistic methods that attempt to reason about grammar and syntax aided by a domain-focal knowledge base (e.g., one for radiology or one for clinical pathology). The aim of the work proposed here takes a different tack: exploring the utility of a statistical approach to clinical NLP, one augmented by machine learning and concentrating on general progress notes from across multiple clinical domains. The specific clinical goal will be to identify adverse drug events described implicitly or explicitly in inpatient progress notes. Rather than relying on a narrow domain focus to provide enough context restriction to make text interpretation tractable, this approach will use statistical patterns in note author information (e.g., profession, note type, treating ward) and patient information (e.g., admit diagnosis, procedures performed, temporal note relationships) for context restriction. The research component of this proposal is divided into two categories: three small-scale projects designed to rapidly hone new skills developed under the training component, and a large-scale project that assesses the feasibility of cross-discipline clinical text analysis. n/a",Statistical NLP Analysis of Cross-discipline Clinical Text,6836781,F38LM008478,"['bioinformatics', 'clinical research', 'computational biology', 'human data', 'library', 'mathematical model', 'public health', 'statistics /biometry']",NLM,UNIVERSITY OF UTAH,F38,2004,94545,0.018142829644007145
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6733529,R01LM007273,"['Internet', 'behavioral /social science research tag', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'confidentiality', 'data management', 'decision making', 'health care facility information system', 'health care policy', 'human data', 'human rights', 'information dissemination', 'information retrieval', 'mathematical model', 'medical records', 'model design /development', 'patient oriented research', 'statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2004,406979,0.01546381219307546
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,6802269,R01EB002247,"['anatomy', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer assisted patient care', 'computer data analysis', 'computer graphics /printing', 'computer system design /evaluation', 'diagnosis quality /standard', 'health care quality', 'health care referral /consultation', 'human data', 'image enhancement', 'image processing', 'magnetic resonance imaging', 'musculoskeletal disorder', 'nervous system disorder', 'statistics /biometry', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,387825,0.020511917575710227
"Using Narrative Data to Enrich the Online Medical Record  DESCRIPTION (provided by applicant):    Narrative information is vital to health care, because it enables physicians to synthesize the raw facts and provide a context and interpretation for them.  Electronic medical record systems contain a wealth of clinical data, but typically lack the clinical narrative found in paper records, e.g., the patient history and progress notes.  Numerous barriers prevent the timely acquisition of narrative data, and most computer systems are unable to use such information productively.  Current approaches offer a tradeoff, capture of rich clinical data that lacks structure (using transcription services or speech technology), versus entry of structured data that lacks flexibility and expressiveness (using template systems).  Natural language processing can integrate these approaches by allowing physicians full freedom of expression while producing structured documents that preserve the richness and enable further computer processing.   This proposal seeks to capture and structure narrative in the online medical record in order to improve entry time, completeness, information content and consistency of clinical documentation.  The specific aims of this proposal are:  1) Maintain the continuity of the medical record; a lengthy medical record requires significant time to review and digest.  Many facts from past narratives remain true in the present or persist with minor changes.  By automatically bringing these facts forward into the current narrative, the system can reduce the time to enter the document, and improve the completeness of documentation by maintaining continuity of what is known about a patient; 2) Integrate the medical record:  Electronic medical records contain a vast amount of data.  However, most of these data are raw facts.  By helping the physician to connect, interpret and summarize these facts, the system can improve the usefulness of the information in the record, and reduce the time to enter documents by performing some syntheses automatically; and 3) Harmonize the medical record; the multidisciplinary nature of health care creates the potential for the differing perspectives and interpretations in the medical record, and even contradictions.  By bringing possible discrepancies to the attention of the physician, the system can help resolve the inconsistencies.     n/a",Using Narrative Data to Enrich the Online Medical Record,6802698,R01LM007268,"['automated medical record system', 'clinical research', 'computer data analysis', 'data collection methodology /evaluation', 'human data', 'information systems', 'medical records', 'online computer', 'primary care physician', 'vocabulary']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,437194,0.022701137497279405
"Capturing and linking genomic and clinical information DESCRIPTION (provided by applicant):  The long-term aim of this project is to use natural language processing (NLP) to build a high throughput tool for facilitating cancer research by automatically extracting and organizing clinical and genetic information from the Electronic Medical Record (EMR) and from journal articles.  Our research involves advanced NLP techniques to:  1) enable the mining of phenotypic and genotypic data in the EMR; 2) automatically amass knowledge concerned with cancer and biomolecular relationships from journals; 3) develop a WEB-enabled visualization tool for researchers that will present diverse views of the knowledge; and 4) develop an Infrastructure that will link to the clinical data warehouse at New York Presbyterian Hospital (NYPH) and to GeneWays, a related project that allows researchers to visualize pathways.   More specifically, MedLEE (the NLP system we developed that extracts and encodes clinical and environmental information from the EMR) will be extended to extract genetic information contained in the EMR; subsequently, twelve years of patient reports will be processed and the extracted data added to the warehouse.  In addition, a new system, PhenoGenes, will be developed based on MedLEE and GeneWays (which contains another NLP system we developed that extracts and codifies biomolecular relations from journal articles).  PhenoGenes will capture biomolecular interactions directly associated with the treatment, diagnosis, and prognosis of cancer.  It will also generate an XML knowledge base that will integrate and organize the information that will be captured, and a Web-enabled tool that will allow users to browse and view the knowledge clustered according to different orientations (e.g. gene, disease, tissue, interaction, etc.).  The knowledge base will be linked to the GeneWays system, so that relevant pathways can be visualized.   MedLEE is utilized operationally at NYPH.  It also has been demonstrated that both NLP systems are highly effective.  This current project builds upon our experience and success with these systems.  The availability of related compatible clinical and biomolecular NLP systems, provide an exceptional opportunity to pave the way for capture, integration and organization of phenotypic and genotypic data and knowledge that will be used to radically improve patient care. n/a",Capturing and linking genomic and clinical information,6781785,R01LM007659,"['cancer information system', 'clinical research', 'data collection', 'health science research', 'human data', 'informatics', 'library', 'medical records', 'molecular biology information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,468590,0.03663104763458009
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,6749459,R01EB000362,"['automated medical record system', 'clinical research', 'computer assisted medical decision making', 'data collection methodology /evaluation', 'human data', 'informatics', 'information display', 'information system analysis', 'mathematical model', 'medical records', 'outcomes research', 'patient care management', 'performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,594553,0.03456697243520172
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6717704,P01EB000216,"['automated medical record system', 'health care facility information system', 'radiology', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2004,1723576,0.03397773609917071
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6637557,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2003,148818,0.01674692688720293
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6620783,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2003,380761,0.01546381219307546
"Using Narrative Data to Enrich the Online Medical Record  DESCRIPTION (provided by applicant):    Narrative information is vital to health care, because it enables physicians to synthesize the raw facts and provide a context and interpretation for them.  Electronic medical record systems contain a wealth of clinical data, but typically lack the clinical narrative found in paper records, e.g., the patient history and progress notes.  Numerous barriers prevent the timely acquisition of narrative data, and most computer systems are unable to use such information productively.  Current approaches offer a tradeoff, capture of rich clinical data that lacks structure (using transcription services or speech technology), versus entry of structured data that lacks flexibility and expressiveness (using template systems).  Natural language processing can integrate these approaches by allowing physicians full freedom of expression while producing structured documents that preserve the richness and enable further computer processing.   This proposal seeks to capture and structure narrative in the online medical record in order to improve entry time, completeness, information content and consistency of clinical documentation.  The specific aims of this proposal are:  1) Maintain the continuity of the medical record; a lengthy medical record requires significant time to review and digest.  Many facts from past narratives remain true in the present or persist with minor changes.  By automatically bringing these facts forward into the current narrative, the system can reduce the time to enter the document, and improve the completeness of documentation by maintaining continuity of what is known about a patient; 2) Integrate the medical record:  Electronic medical records contain a vast amount of data.  However, most of these data are raw facts.  By helping the physician to connect, interpret and summarize these facts, the system can improve the usefulness of the information in the record, and reduce the time to enter documents by performing some syntheses automatically; and 3) Harmonize the medical record; the multidisciplinary nature of health care creates the potential for the differing perspectives and interpretations in the medical record, and even contradictions.  By bringing possible discrepancies to the attention of the physician, the system can help resolve the inconsistencies.     n/a",Using Narrative Data to Enrich the Online Medical Record,6665504,R01LM007268,"['automated medical record system', ' clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' information systems', ' medical records', ' online computer', ' primary care physician', ' vocabulary']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,424735,0.022701137497279405
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,6725819,R01EB002247,"['anatomy', ' clinical research', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer assisted patient care', ' computer data analysis', ' computer graphics /printing', ' computer system design /evaluation', ' diagnosis quality /standard', ' health care quality', ' health care referral /consultation', ' human data', ' image enhancement', ' image processing', ' magnetic resonance imaging', ' musculoskeletal disorder', ' nervous system disorder', ' statistics /biometry', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,383268,0.020511917575710227
"Capturing and linking genomic and clinical information DESCRIPTION (provided by applicant):  The long-term aim of this project is to use natural language processing (NLP) to build a high throughput tool for facilitating cancer research by automatically extracting and organizing clinical and genetic information from the Electronic Medical Record (EMR) and from journal articles.  Our research involves advanced NLP techniques to:  1) enable the mining of phenotypic and genotypic data in the EMR; 2) automatically amass knowledge concerned with cancer and biomolecular relationships from journals; 3) develop a WEB-enabled visualization tool for researchers that will present diverse views of the knowledge; and 4) develop an Infrastructure that will link to the clinical data warehouse at New York Presbyterian Hospital (NYPH) and to GeneWays, a related project that allows researchers to visualize pathways.   More specifically, MedLEE (the NLP system we developed that extracts and encodes clinical and environmental information from the EMR) will be extended to extract genetic information contained in the EMR; subsequently, twelve years of patient reports will be processed and the extracted data added to the warehouse.  In addition, a new system, PhenoGenes, will be developed based on MedLEE and GeneWays (which contains another NLP system we developed that extracts and codifies biomolecular relations from journal articles).  PhenoGenes will capture biomolecular interactions directly associated with the treatment, diagnosis, and prognosis of cancer.  It will also generate an XML knowledge base that will integrate and organize the information that will be captured, and a Web-enabled tool that will allow users to browse and view the knowledge clustered according to different orientations (e.g. gene, disease, tissue, interaction, etc.).  The knowledge base will be linked to the GeneWays system, so that relevant pathways can be visualized.   MedLEE is utilized operationally at NYPH.  It also has been demonstrated that both NLP systems are highly effective.  This current project builds upon our experience and success with these systems.  The availability of related compatible clinical and biomolecular NLP systems, provide an exceptional opportunity to pave the way for capture, integration and organization of phenotypic and genotypic data and knowledge that will be used to radically improve patient care. n/a",Capturing and linking genomic and clinical information,6558664,R01LM007659,"['cancer information system', ' clinical research', ' data collection', ' health science research', ' human data', ' informatics', ' library', ' medical records', ' molecular biology information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,464049,0.03663104763458009
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,6678913,R01EB000362,"['automated medical record system', ' clinical research', ' computer assisted medical decision making', ' data collection methodology /evaluation', ' human data', ' informatics', ' information display', ' information system analysis', ' mathematical model', ' medical records', ' outcomes research', ' patient care management', ' performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,572597,0.03456697243520172
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6682850,P01EB000216,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2003,1645392,0.03397773609917071
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6530779,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2002,144484,0.01674692688720293
"INFORMATICS TOOLS & MEDICAL PERINATAL KNOWLEDGE BUILDING DESCRIPTION (Taken from application abstract):  This proposed study will         replicate and extend methodology used in earlier studies and will use            extensive clinical data repositories, informatics tools, and expert              practitioners for perinatal medical knowledge building.                                                                                                           Clinical Data Repository:  Duke University's Medical Center (DUMC) TMR (The      Medical Record) data repository will be used for this study, and contains        45,922 electronic medical records for both low and high-risk pregnant women      (and their infants) who have received prenatal care at DUMC, and its             affiliated regional clinics, between 1/1/86 and 12/3l/95.  Each patient's        electronic data is used for clinical patient care and contains a potential       4000 variables per record.  This volume of data requires new approaches for      data analysis and medical decision support, since human information              processing limitations become quickly overloaded by both an individual           patient s data and the aggregate information collected for the perinatal         patient population.                                                                                                                                               lnformatics Tools:  Informatics techniques for knowledge acquisition and         data mining will use machine learning programs, statistical analysis, and        domain expert input to articulate relationships between the data and             perinatal patent outcomes.  The goal is to provide decision support for          perinatal care providers to accurately identify patients at risk and assist      them with modifiable preterm birth ask factors.  An expert system will use       data-generated and verified knowledge bases to test its predictive validity      when new patient cases are induced to the expert system.  Earlier studies        found 53-90% predictive accuracies for an expert system prototype, as            compared to 17-38% accuracies, reported in the literature, using current         manual techniques.  Mapping the expert system's knowledge base terms to          medical library resources will be explored for additional decision support.                                                                                       Expert Practitioner:  The perinatal expert panel will consist of the             Principal Investigator, a Board Certified OB-Gyn Physician, and a certified      Perinatal RN.  Each of the panel members has more than 20 years of perinatal     experience.  Participating informatics experts are known, both nationally        and internationally for their expertise in the field of Medical Informatics.      n/a",INFORMATICS TOOLS & MEDICAL PERINATAL KNOWLEDGE BUILDING,6577458,R01LM006488,"['artificial intelligence', ' computer system design /evaluation', ' human data', ' information systems', ' prenatal care']",NLM,DUKE UNIVERSITY,R01,2002,382114,0.03312543712483149
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6421732,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2002,384388,0.01546381219307546
"Using Narrative Data to Enrich the Online Medical Record  DESCRIPTION (provided by applicant):    Narrative information is vital to health care, because it enables physicians to synthesize the raw facts and provide a context and interpretation for them.  Electronic medical record systems contain a wealth of clinical data, but typically lack the clinical narrative found in paper records, e.g., the patient history and progress notes.  Numerous barriers prevent the timely acquisition of narrative data, and most computer systems are unable to use such information productively.  Current approaches offer a tradeoff, capture of rich clinical data that lacks structure (using transcription services or speech technology), versus entry of structured data that lacks flexibility and expressiveness (using template systems).  Natural language processing can integrate these approaches by allowing physicians full freedom of expression while producing structured documents that preserve the richness and enable further computer processing.   This proposal seeks to capture and structure narrative in the online medical record in order to improve entry time, completeness, information content and consistency of clinical documentation.  The specific aims of this proposal are:  1) Maintain the continuity of the medical record; a lengthy medical record requires significant time to review and digest.  Many facts from past narratives remain true in the present or persist with minor changes.  By automatically bringing these facts forward into the current narrative, the system can reduce the time to enter the document, and improve the completeness of documentation by maintaining continuity of what is known about a patient; 2) Integrate the medical record:  Electronic medical records contain a vast amount of data.  However, most of these data are raw facts.  By helping the physician to connect, interpret and summarize these facts, the system can improve the usefulness of the information in the record, and reduce the time to enter documents by performing some syntheses automatically; and 3) Harmonize the medical record; the multidisciplinary nature of health care creates the potential for the differing perspectives and interpretations in the medical record, and even contradictions.  By bringing possible discrepancies to the attention of the physician, the system can help resolve the inconsistencies.     n/a",Using Narrative Data to Enrich the Online Medical Record,6535939,R01LM007268,"['automated medical record system', ' clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' information systems', ' medical records', ' online computer', ' primary care physician', ' vocabulary']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2002,395391,0.022701137497279405
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING   DESCRIPTION (adapted from the Abstract):                                              The long-term aim of this project is to use natural language processing (NLP)        to help realize the full potential of the Electronic Medical Record (EMR).           Our research involves advanced NLP techniques to: 1) extract and encode              information in textual reports; 2) map terms to an authoritative vocabulary;         3) obtain comprehensive domain coverage based on the processing of domain            corpora; and 4) facilitate vocabulary development by providing visualization         tools using the Extensible Markup Language (XML).  It has already been               demonstrated that MedLEE, the NLP system we developed, accurately extracts and       codifies information in the EMR.  This current project builds upon our               experience with MedLEE and uses it to accomplish the latter three goals              concerning vocabulary development and standardization.                                                                                                                    More specifically, MedLEE will be used to map source terms to UMLS concepts.         MedLEE will process and structure the source terms and candidate UMLS                concepts.  Suitable matches will be found based on structural similarity             between components of the source term and candidate concepts.  This should           enhance current methods because knowledge of the type of modifiers that match        should improve the quality of the matches.  We will also use MedLEE to process       a large corpus and generate structured output in XML format.  Statistics based       on the structured output will be computed, and then clinically relevant              composite terms will be detected based on frequencies of the structures              containing the more elementary terms.  Our method differs from other discovery       methods because we use NLP techniques that identify semantic modifiers and           complex relations even if the terms are distant from each other, whereas other       methods use statistical co-occurrence data based on adjacency.  The individual       XML structures and statistics will be combined and mapped into a single XML          tree.  It will be possible to visualize the tree and frequencies using an XML        tree viewer, to navigate the tree, to manipulate the tree, and to reorganize         the tree according to different axes (i.e., procedure, body location,                finding).                                                                                                                                                                 The use of a sophisticated NLP system, such as MedLEE, is ideal as a                 foundation for our proposed work in vocabulary development and                       standardization; medical terminology is an integral part of medical language         and a state of the art NLP system is especially equipped to handle the               inherent complexities of language. n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,6490773,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,2002,288252,0.01594578196864561
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6512665,P01EB000216,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2002,2120168,0.03397773609917071
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6258188,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2001,140274,0.01674692688720293
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING   DESCRIPTION (adapted from the Abstract):                                              The long-term aim of this project is to use natural language processing (NLP)        to help realize the full potential of the Electronic Medical Record (EMR).           Our research involves advanced NLP techniques to: 1) extract and encode              information in textual reports; 2) map terms to an authoritative vocabulary;         3) obtain comprehensive domain coverage based on the processing of domain            corpora; and 4) facilitate vocabulary development by providing visualization         tools using the Extensible Markup Language (XML).  It has already been               demonstrated that MedLEE, the NLP system we developed, accurately extracts and       codifies information in the EMR.  This current project builds upon our               experience with MedLEE and uses it to accomplish the latter three goals              concerning vocabulary development and standardization.                                                                                                                    More specifically, MedLEE will be used to map source terms to UMLS concepts.         MedLEE will process and structure the source terms and candidate UMLS                concepts.  Suitable matches will be found based on structural similarity             between components of the source term and candidate concepts.  This should           enhance current methods because knowledge of the type of modifiers that match        should improve the quality of the matches.  We will also use MedLEE to process       a large corpus and generate structured output in XML format.  Statistics based       on the structured output will be computed, and then clinically relevant              composite terms will be detected based on frequencies of the structures              containing the more elementary terms.  Our method differs from other discovery       methods because we use NLP techniques that identify semantic modifiers and           complex relations even if the terms are distant from each other, whereas other       methods use statistical co-occurrence data based on adjacency.  The individual       XML structures and statistics will be combined and mapped into a single XML          tree.  It will be possible to visualize the tree and frequencies using an XML        tree viewer, to navigate the tree, to manipulate the tree, and to reorganize         the tree according to different axes (i.e., procedure, body location,                finding).                                                                                                                                                                 The use of a sophisticated NLP system, such as MedLEE, is ideal as a                 foundation for our proposed work in vocabulary development and                       standardization; medical terminology is an integral part of medical language         and a state of the art NLP system is especially equipped to handle the               inherent complexities of language. n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,6095940,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,2001,303860,0.01594578196864561
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6375859,P01CA051198,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2001,2071652,0.03397773609917071
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6094628,P01CA051198,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2000,1926078,0.03397773609917071
"A framework to enhance radiology structured report by invoking NLP and DL:  Models and Applications PROJECT SUMMARY/ABSTRACT  In radiology practices, timely and accurate formulation of reports is closely linked to patient satisfaction, physician productivity, and reimbursement. While the American College of Radiology and the Radiological Soci- ety of North America have recommended implementation of structured reporting to facilitate clear and consistent communication between radiologists and referring clinicians, cumbersome nature of current structured reporting systems made them unpopular amongst their users. Recently, the emerging techniques of deep learning have been widely and successfully applied in many different natural language processing tasks (NLP). However, when adopted in a certain speciﬁc domain, such as radiology, these techniques should be combined with extensive domain knowledge to improve efﬁciency and accuracy. There is, therefore, a critical need to take advantage of clinical NLP and deep learning to fundamentally change the radiology reporting. The long-term goal in this appli- cation is to improve the form, content, and quality of radiology reports and to facilitate rapid generation of radiol- ogy reports with consistent organization and standardized texts. The overall objective is to use radiology-speciﬁc ontology, NLP and computer vision techniques, and deep learning to construct a radiology-speciﬁc knowledge graph, which will then be used to build a reporting system that can assist radiologists to quickly generate struc- tured and standardized text reports. The rationale for this project is that through integration of new clinical NLP technologies, radiology-speciﬁc knowledge graphs, and development of new reporting system, we can build au- tomatous systems with a higher-level understanding of the radiological world. The speciﬁc aims of this project are to: (1) recognize and normalize named entities in radiology reports; (2) construct a radiology-speciﬁc knowledge graph from free-text and images; and (3) build a reporting system that can dynamically adjust templates based on radiologists' prior entries. The research proposed in this application is innovative, in the applicant's opinion, because it combines deep learning, NLP techniques, and domain knowledge in a single framework to construct comprehensive and accurate knowledge graphs that will enhance the workﬂow of the current reporting systems. The proposed research is signiﬁcant because a novel reporting system can expedite radiologists' workﬂow and acquire well-annotated datasets that facilitate machine learning and data science. To develop such a method, the candidate, Dr. Yifan Peng, requires additional training and mentoring in clinical NLP and radiology. During the K99 phase, Dr. Peng will conduct this research as a research fellow at the National Center for Biotechnology Information. He will be mentored by Dr. Zhiyong Lu, a leading text mining and deep learning researcher, and co- mentored by Dr. Ronald M. Summers, a leading radiologist and clinical informatics researcher. This application for the NIH Pathway to Independence Award (K99/R00) describes a career development plan that will allow Dr. Peng to achieve the career goals of becoming an independent investigator and leader in the study of clinical NLP. PROJECT NARRATIVE The proposed research is relevant to public health because it entails a new strategy to construct a radiology- speciﬁc knowledge graph to facilitate the development of a new reporting system that enables rapid generation of structured radiology reports. The proposed knowledge graph and reporting system will contribute to advancement in understanding of the radiological world, and promise to enhance clinical communication and patient-centric care. Thus, the proposed research is relevant to the part of the NLM's mission that pertains to applying deep knowledge of clinical terminology and natural language processing to improve clinical data science and health services.",A framework to enhance radiology structured report by invoking NLP and DL:  Models and Applications,10197509,R00LM013001,"['Address', 'Adopted', 'American College of Radiology', 'Award', 'Biotechnology', 'Caring', 'Client satisfaction', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Communication', 'Complex', 'Computer Vision Systems', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Formulation', 'Generations', 'Goals', 'Health Services', 'Hospitals', 'Hybrids', 'Image', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Medical', 'Mentors', 'Methods', 'Mission', 'Modeling', 'Mus', 'Names', 'Natural Language Processing', 'Nature', 'Nomenclature', 'North America', 'Ontology', 'Outcome', 'Pathway interactions', 'Patients', 'Phase', 'Physicians', 'Picture Archiving and Communication System', 'Process', 'Productivity', 'Public Health', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resort', 'Societies', 'Standardization', 'Structure', 'System', 'Systems Development', 'Techniques', 'Technology', 'Terminology', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Voice', 'Writing', 'base', 'career', 'career development', 'convolutional neural network', 'deep learning', 'deep neural network', 'impression', 'improved', 'innovation', 'knowledge graph', 'lexical', 'long short term memory', 'neural network', 'neural network architecture', 'novel', 'radiologist', 'repository', 'response', 'syntax', 'text searching']",NLM,WEILL MEDICAL COLL OF CORNELL UNIV,R00,2020,236549,0.004786599303437595
"Automated domain adaptation for clinical natural language processing Project Summary Automatic extraction of useful information from clinical texts enables new clinical research tasks and new technologies at the point of care. The natural language processing (NLP) systems that perform this extraction rely on supervised machine learning. The learning process uses manually labeled datasets that are limited in size and scope, and as a result, applying NLP systems to unseen datasets often results in severely degraded performance. Obtaining larger and broader datasets is unlikely due to the expense of the manual labeling process and the difficulty of sharing text data between multiple different institutions. Therefore, this project develops unsupervised domain adaptation algorithms to adapt NLP systems to new data. Domain adaptation describes the process of adapting a machine learning system to new data sources. The proposed methods are unsupervised in that they do not require manual labels for the new data. This project has three aims. The first aim makes use of multiple existing datasets for the same task to study the differences in domains, and uses this information to develop new domain adaptation algorithms. Evaluation uses standard machine learning metrics, and analysis of performance is tightly bounded by strong baselines from below and realistic upper bounds, both based on theoretical research on machine learning generalization. The second aim develops open source software tools to simplify the process of incorporating domain adaptation into clinical text processing workflows. This software will have input interfaces to connect to methods developed in Aim 1 and output interfaces to connect with Apache cTAKES, a widely used open- source NLP tool. Aim 3 tests these methods in an end-to-end use case, adverse drug event (ADE) extraction on a dataset of pediatric pulmonary hypertension notes. ADE extraction relies on multiple NLP systems, so this use case is able to show how broad improvements to NLP methods can improve downstream methods. This aim also creates new manual labels for the dataset for an end-to-end evaluation that directly measures how improvements to the NLP systems lead to improvement in ADE extraction. Project Narrative Software systems that use machine learning to understand clinical text often suffer severe performance loss when they are applied to new data that looks different than the data that they originally learned from. In this project, we develop and implement methods that allow these systems to automatically adapt to the characteristics of a new data source. We evaluate these methods on the clinical research task of adverse drug event detection, which relies on many different variables found in the text of electronic health records.",Automated domain adaptation for clinical natural language processing,9986899,R01LM012918,"['Adult', 'Adverse drug event', 'Algorithms', 'Apache', 'Area', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Colon Carcinoma', 'Communities', 'Computer software', 'Computers', 'Conceptions', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Dimensions', 'Ecosystem', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation', 'Human', 'Information Retrieval', 'Institution', 'Knowledge', 'Label', 'Language', 'Lead', 'Learning', 'Linguistics', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Network-based', 'Output', 'Pathology', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Pulmonary Hypertension', 'Radiology Specialty', 'Research', 'Software Tools', 'Source', 'Statistical Models', 'System', 'Testing', 'Text', 'TimeLine', 'Training', 'Update', 'Vision', 'Work', 'adaptation algorithm', 'base', 'case finding', 'improved', 'machine learning method', 'malignant breast neoplasm', 'method development', 'natural language', 'neural network', 'new technology', 'news', 'novel', 'open source', 'point of care', 'side effect', 'social media', 'software systems', 'statistics', 'structured data', 'supervised learning', 'tool', 'tumor', 'unsupervised learning']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2020,383874,0.012589197505892616
"Natural Language Processing and Automated Speech Recognition to Identify Older Adults with Cognitive Impairment Project Summary The purpose of this proposal is to develop two strategies, natural language processing (NLP) and automated speech analysis (ASA), to enable automated identification of patients with cognitive impairment (CI), from mild cognitive impairment (MCI) to Alzheimer’s Disease Related Dementias (ADRD) in clinical settings. The number of older adults in the United States with MCI and ADRD is increasing and yet the ability of clinicians and researchers to identify them at scale has advanced little over recent decades and screening with clinical assessments is done inconsistently. Alternative strategies using available data, like analysis of diagnostic codes in the clinical record or insurance claims, have very low sensitivity. NLP and ASA used with machine learning are technologies that could greatly increase ability to detect MCI and ADRD in clinical contexts. NLP automatically converts text in the electronic health record (EHR) into structured concepts suitable for analysis. Thus, clinicians’ documentation of signs and symptoms or orders of tests and services that reflect or address cognitive limitations can be efficiently captured, possibly long before the clinician uses an ADRD-related diagnostic code. ASA directly measures cognition by recognizing different features of cognition captured in speech. Extracting features through both NLP and ASA could thus provide a unique measure of cognition and its impact on the individual and their caregivers. Early detection of MCI and ADRD can help researchers identify appropriate patients for research and help clinicians and health systems target patients for preventive care and care coordination. For these reasons, more efficient, highly scalable strategies are needed to identify people with MCI and ADRD. The Specific Aims of this proposal are to (1) Develop and validate a ML algorithm using features extracted from the EHR with NLP to identify patients with CI, (2) Develop and validate a ML algorithm using features extracted from ASA of audio recordings of patient-provider encounters during routine primary care visits to identify patients with CI, (3) Develop and validate a ML algorithm using both NLP and ASA extracted features to create an integrated CI diagnostic algorithm. We will develop machine learning algorithms using NLP and ASA extracted features trained against neurocognitive assessment data on 800 primary care patients in New York City and validate them in an independent sample of 200 patients in Chicago. In secondary analyses we will train ML algorithms to identify MCI and its subtypes. This project will be the most rigorous development of NLP, ASA, and ML algorithms for CI yet performed, the first to test ASA in primary care settings, and the first to test NLP and ASA feature extraction strategies in combination. The multi-disciplinary team of clinicians, health services researchers, and neurocognitive and data scientists will apply machine learning to develop these highly scalable, automated technologies for identification of MCI and ADRD. 1 Project Narrative The ability of clinicians, health systems and researchers to identify patients with mild cognitive impairment (MCI) and Alzheimer’s Disease Related Dementias (ADRD) is limited. This project will apply machine learning to natural language processing (NLP) of electronic health record data and automated speech analysis (ASA) of patient-doctor conversations during primary care visits to identify patients with MCI and ADRD using automated and scalable procedures. The analytic algorithms will be developed with neurocognitive assessment data on 800 primary care patients in New York City and validated in an independent sample of 200 patients in Chicago. 1",Natural Language Processing and Automated Speech Recognition to Identify Older Adults with Cognitive Impairment,9998610,R01AG066471,"['Acoustics', 'Acute', 'Address', 'Algorithms', 'Alzheimer&apos', 's disease related dementia', 'Caregivers', 'Chicago', 'Clinical', 'Clinical assessments', 'Code', 'Cognition', 'Cognitive', 'Data', 'Data Analyses', 'Data Element', 'Data Scientist', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Documentation', 'Early Diagnosis', 'Elderly', 'Electronic Health Record', 'Health Services', 'Health system', 'Impaired cognition', 'Individual', 'Insurance Carriers', 'Machine Learning', 'Measures', 'Mental disorders', 'Methods', 'Natural Language Processing', 'Neurocognitive', 'New York City', 'Parkinson Disease', 'Patient Care', 'Patients', 'Persons', 'Physicians', 'Population', 'Positioning Attribute', 'Preventive care', 'Primary Health Care', 'Procedures', 'Provider', 'Psychiatric Diagnosis', 'Reference Standards', 'Research', 'Research Personnel', 'Resource Allocation', 'Risk Factors', 'Sampling', 'Semantics', 'Sensitivity and Specificity', 'Services', 'Signs and Symptoms', 'Speech', 'Structure', 'Study Subject', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'United States', 'Validation', 'Visit', 'adverse event risk', 'aging population', 'automated speech recognition', 'base', 'care coordination', 'clinical encounter', 'cognitive function', 'cognitive testing', 'deep learning', 'demographics', 'electronic data', 'electronic structure', 'falls', 'feature extraction', 'financial incentive', 'health care settings', 'improved', 'insurance claims', 'learning classifier', 'machine learning algorithm', 'mental state', 'mild cognitive impairment', 'multidisciplinary', 'prevent', 'primary care setting', 'recruit', 'risk mitigation', 'screening', 'secondary analysis', 'structured data', 'success', 'testing services', 'tool', 'treatment choice', 'unstructured data']",NIA,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2020,855710,0.006662504654897572
"Fine-grained spatial information extraction for radiology reports ABSTRACT Automated biomedical image classification has seen enormous improvements in performance over recent years, particularly in radiology. However, the machine learning (ML) methods that have achieved this remarkable performance often require enormous amounts of labeled data for training. An increasingly accepted means of acquiring this data is through the use of natural language processing (NLP) on the free-text reports associated with an image For example, take the following brain MRI report snippet:  There is evidence of left parietal encephalomalacia consistent with known history of prior stroke. Small  focal area of hemosiderin deposition along the lateral margins of the left lateral ventricle. Here, the associated MRI could be labeled for both Encephalomalacia and Hemosiderin. NLP methods to automatically label images in this way have been used to create several large image classification datasets However, as this example demonstrates, radiology reports often contain far more granular information than prior NLP methods attempted to extract. Both findings in the above example mention their anatomical location, which linguistically is referred to as a spatial grounding, as the location anchors the finding in a spatial reference. Further, the encephalomalacia finding is connected to the related diagnosis of stroke, while the hemosiderin finding provides a morphological description (small focal area). This granular information is important for image classification, as advanced deep learning methods are capable of utilizing highly granular structured data. This is logical, as for instance a lung tumor has a slightly different presentation than a liver tumor. If an ML algorithm can leverage both the coarse information (the general presentation of a tumor) while also recognizing the subtle granular differences, it can find an optimal balance between specificity and generalizability. From an imaging perspective, this can also be seen as a middle ground between image-level labels (which are cheap but require significant data for training—a typical dataset has thousands of images or more) and segmentation (which is expensive to obtain, but provides better training data—a typical dataset has 40 to 200 images), as the fine-grained spatial labels correspond to natural anatomical segments. Our fundamental hypothesis in this project is that if granular information can be extracted from radiology reports with NLP, this will improve downstream radiological image classification when training on a sufficiently large dataset. For radiology, the primary form of granularity is spatial (location, shape, orientation, etc.), so this will be the focus of our efforts. We further hypothesize that these NLP techniques will be generalizable to most types of radiology reports. For the purpose of this R21-scale project, however, we will focus on three distinct types of reports with different challenges: chest X-rays (one of the most-studied and largest-scale image classification types), extremity X-rays (which offer different findings than chest X-rays), and brain MRIs (which present a different image modality and the additional complexity of three dimensions). NARRATIVE This project is interested in developing natural language processing (NLP) methods for better understanding the spatial relationships described in the free text data within radiology reports found in electronic health record (EHR) systems. We will (i) develop an ontology, (ii) manually create a dataset for training NLP methods, (iii) develop automatic NLP methods compatible the ontology and corpus, and (iv) evaluate automatic image classification methods that use the output of the NLP system as image labels.",Fine-grained spatial information extraction for radiology reports,9957898,R21EB029575,"['3-Dimensional', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Brain', 'Classification', 'Data', 'Data Set', 'Deposition', 'Devices', 'Diagnosis', 'Electronic Health Record', 'Encephalomalacia', 'Equilibrium', 'Goals', 'Grain', 'Hemosiderin', 'Human', 'Image', 'Information Retrieval', 'Label', 'Lateral', 'Left', 'Limb structure', 'Linguistics', 'Liver neoplasms', 'Location', 'Lung Neoplasms', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Methods', 'Morphology', 'Natural Language Processing', 'Ontology', 'Output', 'Parietal', 'Performance', 'Radiology Specialty', 'Recording of previous events', 'Reporting', 'Research', 'Roentgen Rays', 'Shapes', 'Specificity', 'Stroke', 'System', 'Techniques', 'Text', 'Thoracic Radiography', 'Training', 'Trust', 'base', 'bioimaging', 'deep learning', 'design', 'imaging modality', 'improved', 'innovation', 'interest', 'large datasets', 'lateral ventricle', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'radiological imaging', 'scale up', 'spatial relationship', 'structured data', 'tool', 'tumor']",NIBIB,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R21,2020,271250,0.004959790570491184
"Learning Universal Patient Representations from Clinical Text with Hierarchical Recurrent Neural Networks Project Summary In this project we develop new methods for extracting important information from electronic health records based on recurrent neural networks. These methods represent the hierarchical and sequential nature of human language, leverage large scale datasets to make learning sophisticated representations possible, and make use of novel sources of supervision that are available at this scale. The model architecture we propose is a hierarchical recurrent neural network (RNN). This architecture explicitly represents temporality at multiple different time scales, with stacked RNN layers representing words, sentences, paragraphs, and documents. At the word level, the model is trained to predict important pieces of clinical information, such as negation and temporality, using existing labeled data sets. Training for clinical information extraction at the lowest level ensures that the higher-level models have a foundation of medically relevant inputs. We are still left with the challenge of training higher-level networks, because these models require massive amounts of labeled training data to learn. We solve this problem by taking advantage of the temporal aspect of information in an EHR, and having each higher-level recurrent layer train getting supervision from the future. For example, the document RNN is trained to predict billing codes and NLP concept codes that were found in the subsequent document. This source of supervision is scalable, and our preliminary data shows that it is effective at learning how to generate generalizable patient representations. The patient representations that our model learns are shareable across multiple tasks, potentially streamlining EHR-based research by eliminating what was previously a manual step – designing text-based variables to represent patients. We demonstrate a new workflow for text-based EHR research, showing how the same representations can be used for two completely distinct phenotyping tasks. These phenotyping studies make use of high-quality datasets of patients with pulmonary hypertension and autism spectrum disorder at Boston Children’s Hospital. PH is relatively rare, so finding every patient with a phenotyping algorithm is important for clinical research. ASD has several sub-phenotypes, and finding large numbers of patients from each sub- phenotype can help to better understand the mechanisms of ASD. Along with demonstrating the applicability of our representations on these specific clinical research use cases, we incorporate our patient representations into the i2b2 clinical research software, making them available to all clinical investigators using this platform at Boston Children’s Hospital. Project Narrative This project develops methods for extracting universal patient representations from unstructured text in electronic health records. These methods leverage huge amounts of clinical data, recurrent neural network architectures, and novel training techniques to incorporate information at multiple time scales. These methods are evaluated using public datasets to promote reproducibility, and applied to clinical research tasks that extend the knowledge of patients with pulmonary hypertension and autism spectrum disorder at Boston Children’s Hospital.",Learning Universal Patient Representations from Clinical Text with Hierarchical Recurrent Neural Networks,9868326,R01LM012973,"['Architecture', 'Boston', 'Brain', 'Childhood', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Research', 'Code', 'Computer software', 'Data', 'Data Set', 'Electronic Health Record', 'Ensure', 'Event', 'Face', 'Felis catus', 'Foundations', 'Future', 'Healthcare Systems', 'Human', 'Human Characteristics', 'Human Resources', 'Information Retrieval', 'Intensive Care Units', 'Israel', 'Knowledge', 'Label', 'Language', 'Learning', 'Left', 'Linguistics', 'Location', 'Logistic Regressions', 'Machine Learning', 'Manuals', 'Medical', 'Medical center', 'Methods', 'Modeling', 'Natural Language Processing', 'Neural Network Simulation', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phenotype', 'Problem Solving', 'Process', 'Pulmonary Hypertension', 'Rare Diseases', 'Records', 'Recurrence', 'Reproducibility', 'Research', 'Research Personnel', 'Source', 'Statistical Methods', 'Supervision', 'System', 'Text', 'Time', 'Training', 'Training Technics', 'Uncertainty', 'autism spectrum disorder', 'base', 'clinically relevant', 'cohort', 'comorbidity', 'data resource', 'deep neural network', 'design', 'disease phenotype', 'large scale data', 'learning strategy', 'machine translation', 'neural network', 'neural network architecture', 'novel', 'phenotyping algorithm', 'recurrent neural network', 'relating to nervous system']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2020,367184,0.0008928203646112171
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,10005506,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Pooling', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Infrastructure', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Research', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data infrastructure', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phenotyping algorithm', 'phrases', 'portability', 'preservation', 'privacy preservation', 'recruit', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2020,1500847,0.020017800382485663
"Optimizing the Utility of Large Electronic Health Records Data in Data-Driven Health Research Optimizing the Utility of Electronic Medical Records Data in  Data-driven Health Research ABSTRACT Medical centers continue to archive patient follow-up data in Electronic Medical Records (EMR), which have tremendous value in discovering new knowledge and insights. The large volume of EMR data can play an important role in improving the accuracy and generalizability of predictive models in healthcare, especially when misdiagnosis is known to be the third leading cause of death in the United States. Despite these merits, EMR data are invariably corrupted by factors like missing values, outliers, and unrealistic measurements, which prevent researchers from fully utilizing such abundant data in many important studies. Many studies simply discard a large number of samples to get rid of missingness and eventually bias their data-driven analytical models. Existing techniques for missing data imputation use simplified linear models and are mostly suitable for imputing cross-sectional data missingness that ignore longitudinal missingness in patient follow-up data. This proposal aims to investigate novel artificial intelligence (AI) based models to improve the quality and utility of EMR data in preparation for data-driven retrospective studies. Toward this preparation, the goal of the project is 1) to investigate more accurate and robust data imputation models compared to existing ones and 2) adapt state-of-the-art deep learning techniques in preparing optimal representation of large EMR data. The proposed research will 1) maximize the quality and utility of EMR data to support a multitude of retrospective studies, 2) enable visualization of complex patient data, 3) identify more important and predictive clinical parameters, 4) yield a compact and optimal representation of large EMR datasets. We hypothesize that optimally processed EMR data with state-of-the-art AI models can most accurately model patient risk when compared to existing statistical and clinical risk models. This project will combine the complementary expertise of the collaborators, Dr. Manar Samad, PhD (Computer Science), Dr. Owen Johnson, DPH (Biostatistics and Public Health), and Dr. Edilberto Raynes, MD, PhD (Medicine) along with the participating undergraduate students at Tennessee State University (TSU). The proposal entails several research and development components that will allow undergraduate students to gain valuable research and analytical skills in data science, programming, and health informatics. The project activities will expose health science students to AI-based computing solutions to broaden their scope of future health research and career. This project will help TSU prepare a strong workforce of minority students who will gain competitive skill sets in data science and health informatics that are currently high in demand almost everywhere. Overall, the project will develop a data-capable workforce to strengthen an interdisciplinary research capacity and collaboration between the Departments of Computer and Health science at TSU. Project Narrative Electronic Medical Record (EMR) data are invariably corrupted by data missingness and data redundancy that limit their application in many valuable data-driven research studies aiming to achieve the goals of precision medicine. This project aims to develop a number of innovative computational frameworks that will optimally prepare and utilize EMR data to facilitate research studies with relevance to patient risk modeling, discovery of new health markers, and patient-specific prognosis and therapeutic strategy. The project will leverage recent advances in machine learning and data science by involving an interdisciplinary team of researchers consisting of three faculty members, two graduate students, and four undergraduate students, which will establish a data-centric research collaboration between the Departments of Computer and Health Science.",Optimizing the Utility of Large Electronic Health Records Data in Data-Driven Health Research,10111205,R15LM013569,"['Affect', 'Algorithms', 'Anatomy', 'Archives', 'Artificial Intelligence', 'Biometry', 'Cause of Death', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Collaborations', 'Complex', 'Computer Models', 'Computerized Medical Record', 'Consumption', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Diagnostic', 'Dimensions', 'Disease', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Faculty', 'Future', 'Goals', 'Health', 'Health Sciences', 'Healthcare', 'High Performance Computing', 'Hybrids', 'Image', 'Interdisciplinary Study', 'International Classification of Diseases', 'Knowledge', 'Laboratories', 'Linear Models', 'Literature', 'Machine Learning', 'Measurement', 'Medical center', 'Medicine', 'Modeling', 'Network-based', 'Outcome', 'Outcomes Research', 'Patient risk', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physiological', 'Play', 'Preparation', 'Process', 'Public Health', 'Public Health Informatics', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Retrospective Studies', 'Risk stratification', 'Role', 'Sample Size', 'Sampling', 'Statistical Models', 'Structure', 'Students', 'Techniques', 'Tennessee', 'Testing', 'Therapeutic', 'Time', 'United States', 'Universities', 'Validation', 'Variant', 'Visit', 'Visualization', 'base', 'career', 'clinical decision-making', 'clinical predictors', 'clinical risk', 'complex data ', 'computational platform', 'computer framework', 'computer science', 'computing resources', 'data quality', 'data structure', 'data visualization', 'deep learning', 'deep neural network', 'digital', 'follow-up', 'graduate student', 'health science research', 'high dimensionality', 'improved', 'individual patient', 'innovation', 'insight', 'member', 'minority student', 'neural network', 'non-linear transformation', 'novel', 'outcome forecast', 'precision medicine', 'predictive modeling', 'prevent', 'prognostic', 'recurrent neural network', 'research and development', 'research study', 'skills', 'structured data', 'survival prediction', 'undergraduate student']",NLM,TENNESSEE STATE UNIVERSITY,R15,2020,421700,0.01408395189875158
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9884791,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical data science', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'patient health information', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2020,92070,0.013148180497727814
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions PROJECT SUMMARY Most U.S. adults (68%) take dietary supplements and there is increasing evidence of drug-supplement interactions (DSIs); In recent years, there has been increasing evidence supporting the role of DSs in ADRD in preventing cognitive impairment but there is limited evidence and the sample sizes have been small. Real- world data (RWD) especially the EHR contain detailed treatment and response information from patients and could be used to detect the usage and effect of DSs, DSIs, which is more translational to clinical outcomes (e.g., MCI to ADRD conversion). To the best of our knowledge, there is no investigation on DSs usage and safety among patients in MCI and ADRD using EHR data. Our current parent award is focusing on the development of a translational informatics framework to enable the discovery of drug-supplement interactions (DSIs) by linking scientific evidence from the biomedical literature. To response to NOT-AG-20-008, this administrative supplement application will complement our parent award in multiple aspects: (1) developing novel and advanced data analytic methods for mining RWD in EHR, (2) identifying DSs usage information among patients with ADRD, and (3) detecting safety and effect of DS among patients with ADRD from existing EHR data. In our preliminary work, we have investigated the methods to identify DSs terms on EHR and developed natural language processing (NLP) methods to identify use status of DSs. We will further our efforts to collect a EHR dataset with DSs usage and AE-DSs signals from AD patients and develop innovative informatics methods to extract such information. Our specific aims are: (1) identifying DSs usage among patients with MCI and ADRD from existing EHR data; and (2) detecting the DSs safety signals and exploring the effect of DSs use on the conversion from MCI to ADRD from existing EHR data. The successful completion of this project will stimulate our further investigation on the role of DS use in patients with ADRD in a larger scale involving EHR data from other healthcare institutions. PROJECT NARRATIVE In this administrative supplement application, we will evaluate the usage and safety of dietary supplements (DSs) usage in patients with Mild Cognitive Impairment (MCI) and Alzheimer’s disease and related dementias (ADRD), respectively using electronic health records (EHR) data. We will also explore the feasibility of using EHR to detect DS effect on the conversion from MCI to ADRD. This research will address a critical and unmet need to conduct large-scale clinical research in DSs and improve evidence bases for healthcare practice. The successful accomplishment of this supplement project will deliver a novel informatics methods and generate DSI signals among patients with ADRD. This project will stimulate our further investigation on the role of DS use in patients with ADRD in a larger scale involving EHR data from other healthcare institutions.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,10119590,R01AT009457,"['Address', 'Administrative Supplement', 'Adult', 'Alzheimer&apos', 's disease patient', 'Alzheimer&apos', 's disease related dementia', 'Artificial Intelligence', 'Award', 'Big Data', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Complement', 'Computerized Medical Record', 'Consumption', 'Data', 'Data Analytics', 'Data Set', 'Data Sources', 'Dementia', 'Detection', 'Development', 'Elderly', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Florida', 'Future', 'Genomics', 'Healthcare', 'Healthcare Systems', 'Human', 'Impaired cognition', 'Individual', 'Informatics', 'Institution', 'Intake', 'Investigation', 'Letters', 'Link', 'Literature', 'Methods', 'Mining', 'Natural Language Processing', 'Outcome', 'Parents', 'Patients', 'Pharmaceutical Preparations', 'Records', 'Research', 'Role', 'Safety', 'Sample Size', 'Signal Transduction', 'Surveys', 'Terminology', 'Training', 'Universities', 'Vascular Dementia', 'Work', 'analytical method', 'base', 'cohort', 'deep learning', 'dietary supplements', 'evidence base', 'genomic data', 'improved', 'innovation', 'interest', 'learning strategy', 'mild cognitive impairment', 'novel', 'pharmacovigilance', 'prevent', 'response', 'treatment response']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2020,339298,0.05375042649686075
"Big Data Methods for Comprehensive Similarity based Risk Prediction Project Summary Electronic health records (EHR) provide rich source of data about representative populations and are yet to be fully utilized to enhance clinical decision-making. Conventional approaches in clinical decision-making start with the identification of relevant biomarkers based on subject-matter knowledge, followed by detailed but limited analysis using these biomarkers exclusively. As the current scientific literature indicates, many human disorders share a complex etiological basis and exhibit correlated disease progression. Therefore, it is desirable to use comprehensive patient data for patient similarity. This proposal focuses on deriving a comprehensive and integrated score of patient similarity from complete patient characteristics currently available, including but not limited to 1) demographic similarity; 2) genetic similarity; 3) clinical phenotype similarity; 4) treatment similarity; and 5) exposome similarity (here exposome defined as all available attributes of the living environment an individual is exposed to), when some of the aspects may overlap and interact. We will optimize information fusion and task-dependent feature selection for assessing patient similarity for clinical risk prediction. Since currently there does not exist a pipeline that is able to extract executable complete patient determinant data, to achieve the research goal described above, we propose first deliver an open- source data preparation pipeline that is based on a widely used clinical data standard, the OMOP (Observational Medical Outcomes Partnership) Common Data Model (CMD) version 5.2. Moreover, to mitigate common missingness and sparsity challenges in clinical data, we describe the first attempt to represent patients' sparse clinical information with missingness, including diagnosis information, medication data, treatment intervention, with a fixed-length feature vector (i.e. the Patient2Vec). This project has four specific aims. Aim 1 is to develop a clinical data processing pipeline for harmonizing patient information from multiple sources into a standards-based uniformed data representation and to evaluate its efficiency, interoperability, and accuracy. Aim 2 is to leverage a powerful machine learning technique, Document2Vec, from the natural language processing literature, to create an open-source Patient2Vec framework for the derivation of informative numerical representations of patients. Aim 3 is to develop a unified machine learning clinical- outcome-prediction framework for Optimized Patient Similarity Fusion (OptPSF) that integrates traditional medical covariates with the derived numerical patient representations from Patient2Vec (Aim 2) for improved clinical risk prediction. Aim 4 is to evaluate our similarity framework for predicting 1) the risk of end-stage kidney disease (ESKD) in general EHR patient population and 2) the risk of death among patients with chronic kidney disease (CKD). The project focus on developing a novel data science pipeline which includes a clinical data processing pipeline to format comprehensive patient health determinants from a variety of sources of clinical, genomic, socioenvironmental data, and a clinical-outcome-prediction framework that optimally fuses relevant patient health determinants to define patient similarity for improved clinical risk predictions.",Big Data Methods for Comprehensive Similarity based Risk Prediction,9870948,R01LM013061,"['Address', 'Automation', 'Big Data', 'Big Data Methods', 'Biological Markers', 'Biological Process', 'Biometry', 'Case Study', 'Characteristics', 'Chronic', 'Chronic Disease', 'Chronic Kidney Failure', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complex', 'Data', 'Data Reporting', 'Data Science', 'Derivation procedure', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronic Health Record', 'End stage renal failure', 'Environment', 'Etiology', 'Exhibits', 'Exposure to', 'Genetic', 'Genomics', 'Goals', 'Health', 'Health Professional', 'Healthcare', 'Heterogeneity', 'Human', 'Individual', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Knowledge', 'Length', 'Life', 'Literature', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Preparation', 'Reporting', 'Reproducibility', 'Research', 'Risk', 'Social Environment', 'Source', 'Surveys', 'Techniques', 'base', 'biomedical informatics', 'clinical decision support', 'clinical decision-making', 'clinical phenotype', 'clinical risk', 'data analysis pipeline', 'data modeling', 'data standards', 'design', 'disease diagnosis', 'feature selection', 'health data', 'improved', 'interoperability', 'mortality risk', 'novel', 'open data', 'open source', 'outcome prediction', 'patient health information', 'patient population', 'precision medicine', 'predict clinical outcome', 'socioeconomics', 'support tools', 'vector']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,420434,-0.0005807661037539767
"National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives Project Summary and Abstract Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Natural language processing (NLP) technologies can extract this information and convert it in to a structured format that is more readily accessible by computerized systems. However, the development of NLP systems is contingent on access to relevant data and EHRs are notoriously difficult to obtain because of privacy reasons. Despite the recent efforts to de-identify and release narrative EHRs for research, these data are still very rare. As a result, clinical NLP, as a field has lagged behind. To address this problem, since 2006, we organized thirteen shared tasks, accompanied with workshops and journal publications. Twelve of these shared tasks have focused on the development of clinical NLP systems and the remaining one on the usability of these systems. We have covered both depth and breadth in terms of shared tasks, preparing tasks that study cutting-edge NLP problems on a variety of EHR data from multiple institutions. Our shared tasks are the longest running series of clinical NLP shared tasks, with ever growing EHR data sets, tasks, and participation. Our most popular three data sets have been cited 495 (2010 data), 284 (2006 de-id data), and 274 (2009 data) times, respectively, representing hundreds of articles that have come out of these three data sets alone. Our goal in this proposal is to continue the efforts we started in 2006 under i2b2 shared task challenges (i2b2, NIH NLM U54LM008748, PI: Kohane and R13 LM011411, PI: Uzuner) to de-identify EHRs, annotate them with gold- standard annotations for clinical NLP tasks, and release them to the research community for the development and head-to-head comparison of clinical NLP systems, for the advancement of the state of the art. Continuing our efforts under National NLP Clinical Challenges (n2c2) based at the Health Data Science program of the newly established Department of Biomedical Informatics at Harvard Medical School, we aim to form partnerships with the community to grow the shared task efforts in several ways: (1) grow the available de-identified EHR data sets through partnerships that can contribute to the volume and variety of the data, and (2) grow the available gold-standard annotations in terms of depth and breadth of NLP tasks. Given these aims and partnerships, we plan to hold a series of shared tasks. We will complement these shared tasks with workshops that meet in conjunction with the Fall Symposium of the American Medical Informatics Association and with journal special issues so that advancement of the state of the art can be sped up and future generations can build on the past. Project Narrative We propose to organize a series of shared tasks, workshops, and journal publications for fostering the continuous development of clinical Natural Language Processing (NLP) technologies that can extract information from narratives of Electronic Health Records (EHRs). Our aim is to grow the annotated gold standard EHR data sets that are available to the research community through partnerships and to bring together clinical NLP researchers with informatics researchers for building collaborations. We will engage the community in shared tasks and disseminate the knowledge generated by these shared tasks through workshops and journal special issues for the advancement of the state of the art.",National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives,9930152,R13LM013127,"['Access to Information', 'Address', 'American', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Community Developments', 'Complement', 'Data', 'Data Science', 'Data Set', 'Development', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Future', 'Future Generations', 'Goals', 'Gold', 'Grant', 'Growth', 'Hand', 'Head', 'Healthcare', 'Improve Access', 'Individual', 'Informatics', 'Institution', 'Israel', 'Journals', 'Knowledge', 'Measures', 'Medical Informatics', 'Medical center', 'Methodology', 'Natural Language Processing', 'Outcome', 'Paper', 'Peer Review', 'Performance', 'Privacy', 'Publications', 'Publishing', 'Records', 'Research', 'Research Personnel', 'Rest', 'Running', 'Series', 'Source', 'Structure', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'clinical development', 'computerized', 'falls', 'head-to-head comparison', 'health data', 'indexing', 'medical schools', 'meetings', 'practical application', 'programs', 'symposium', 'usability', 'working group']",NLM,GEORGE MASON UNIVERSITY,R13,2020,20000,0.015645435850698625
"Machine Learning Clinical Order Recommendations for Specialty Consultation Care Summary: Machine Learning Clinical Order Recommendations for Specialty Consultation Care  A future vision of clinical decision support must transcend constraints in scalability, maintainability, and adaptability. The shortage of 100,000 physicians by 2030 reflects unmet (and unlimited) demand for the scarcest healthcare resource, clinical expertise. Over 25 million in the US alone have deficient access to medical specialty care, with delays contributing to 20% higher mortality. There is no quality without access.  Our goal is to develop a radically different paradigm for outpatient specialty consultations by inductively learning clinical workups embedded in clinical data. We focus on predicting the concrete clinical orders for medications and diagnostic tests that result from specialty consultations. This can power a tier of fully automated guides that will enable clinicians to initiate care that would otherwise await in-person specialty visits, opening access for more patients.  The major scientific barriers are advances in data science and decision support methods for collating clinical knowledge, with continuous improvement through clinical experience, crowdsourcing, and machine learning. Our innovative approach is inspired by collaborative filtering algorithms that power “Customers like you also bought this...” recommender systems with the scalability to answer unlimited queries, maintainability through statistical learning, and adaptability to respond to evolving clinical practices.  Our team uniquely combines expertise in clinical medicine, electronic medical records, clinical decision support, statistics and machine learning to enhance medical specialty consultations through aims that seek to: (1) Develop methods to generate clinical decision support by predicting the clinical orders that will result from Endocrinology and Hematology specialty consultations; (2) Evaluate and iteratively design clinical collaborative filtering prototypes based on clinical user input on usability and acceptability; and (3) Determine which consult clinical order patterns are associated with better results through reinforcement learning and causal inference frameworks. Completion of these aims will yield a sustained, powerful impact on clinical information retrieval and knowledge discovery for synthesizing clinical practices from real-world data. By addressing grand challenges in clinical decision support, adoption of these methods will fulfill a vision that empowers clinicians to practice to the top of their license, making healthcare more scalable in reach, responsiveness, and reproducibility Project Narrative There can be no quality without access, and over 25 million in the US alone have deficient access to the scarcest healthcare resource: Human medical expertise. Building on methods analogous to commercial product recommender systems, the proposed research will automatically learn practice patterns from electronic medical records. Distributing predictable practices of medical specialty consultations can then enable healthcare systems to achieve broader patient access to timely and consistent care.",Machine Learning Clinical Order Recommendations for Specialty Consultation Care,10265158,R56LM013365,"['Active Learning', 'Acute', 'Address', 'Adopted', 'Adoption', 'Algorithms', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complex', 'Computerized Medical Record', 'Consult', 'Consultations', 'Data', 'Data Reporting', 'Data Science', 'Diagnosis', 'Diagnostic tests', 'Electronic Mail', 'Endocrinology', 'Environment', 'Evaluation', 'Event', 'Feedback', 'Foundations', 'Future', 'Goals', 'Graph', 'Healthcare', 'Healthcare Systems', 'Hematology', 'Human', 'Human Resources', 'Hyperthyroidism', 'Individual', 'Industry', 'Information Retrieval', 'Inpatients', 'Knowledge', 'Knowledge Discovery', 'Learning', 'Licensing', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'Modern Medicine', 'Monitor', 'Outcome', 'Outpatients', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Persons', 'Pharmaceutical Preparations', 'Physicians', 'Primary Health Care', 'Psychological reinforcement', 'Recommendation', 'Records', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Running', 'Specialist', 'Suggestion', 'System', 'Test Result', 'Testing', 'Thrombocytopenia', 'Time', 'Training', 'Transcend', 'Vision', 'Visit', 'Work', 'base', 'clinical care', 'clinical decision support', 'clinical practice', 'clinical predictors', 'convolutional neural network', 'crowdsourcing', 'data streams', 'design', 'experience', 'individual patient', 'innovation', 'iterative design', 'medical specialties', 'mortality', 'novel', 'open source', 'outcome forecast', 'personalized decision', 'personalized predictions', 'predictive modeling', 'prototype', 'statistical learning', 'statistics', 'tool', 'usability']",NLM,STANFORD UNIVERSITY,R56,2020,394250,0.024206378835039583
"Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records PROJECT SUMMARY  Today, more patients can access their health records online than ever before. However, clinical acronyms hinder patients' comprehension of their records and decrease the benefits of transparency. An automated system for expanding clinical acronyms should have major clinical significance and far-reaching consequences for improving patient-provider communication, shared decision-making, and health outcomes. Existing systems have limited power to expand clinical acronyms, primarily due to the lack of comprehensiveness (or generali- zability) of existing acronym sense inventories. Because developing comprehensive sense inventories is difficult, existing knowledge engineering methods have primarily focused on developing institution-specific sense inventories. Institution-specific sense inventories may not be generalizable to other geographical regions and medical specialties. Furthermore, developing an institution-specific sense inventory at every US healthcare organization is not feasible, especially without automated methods which currently do not exist.  I developed advanced knowledge engineering methods to overcome these limitations through the use of fully automated techniques to generalize existing sense inventories from different geographical regions and medical specialties. My methods leverage the extensive resources already devoted to developing institution- specific sense inventories in the U.S., and may help generalize existing sense inventories to institutions without the resources to develop them. Although promising, challenges remain with the optimization and evaluation of these methods. The objective of the proposed project is to use knowledge engineering to improve patients' comprehension of their health records, focusing specifically on clinical acronyms. In Aim 1, I will develop new knowledge engineering methods to facilitate the automated integration of sense inventories, using literature- based quality heuristics and a Siamese neural network to establish synonymy. I will evaluate these methods using multiple metrics to assess redundancy, quality, and coverage in two test corpora with over 17 million clinical notes. In Aim 2, I will evaluate whether the knowledge engineering methods improve comprehension of doctors' notes in 60 hospitalized patients with advanced heart failure. With success, I will create novel, automated knowledge engineering methods that can be directly applied to improve patient care. This research is in support of my mentored doctoral training at Columbia University Department of Biomedical Informatics (DBMI) under Drs. David Vawdrey, George Hripcsak, Carol Friedman, Suzanne Bakken, and Chunhua Weng, and will include coursework on deep learning, oral presentations at major annual conferences, and career development planning, among other activities. DBMI is frequently recognized as one of the oldest and best programs of its kind in the world, and provides an exception training environment for my development into an independent and productive academic investigator. PROJECT NARRATIVE Clinical acronyms make it difficult for patients to understand their medical records, decreasing the benefits of transparency. This project applies advanced knowledge engineering methods and machine learning to generate comprehensive acronym sense inventories used to aid consumers' comprehension of their health records. The project is in support of the applicant's mentored doctoral dissertation research.",Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records,9895430,F31LM013054,"['Abbreviations', 'Award', 'Clinical', 'Clinical Medicine', 'Comprehension', 'Controlled Vocabulary', 'Development', 'Development Plans', 'Engineering', 'Environment', 'Equipment and supply inventories', 'Evaluation', 'Future', 'Geographic Locations', 'Goals', 'Grant', 'Health', 'Healthcare', 'Heart failure', 'Hospitals', 'Informatics', 'Information Resources', 'Institution', 'Knowledge', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Mentors', 'Mentorship', 'Methods', 'Natural Language Processing', 'Oral', 'Outcome', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Positioning Attribute', 'Publishing', 'Questionnaires', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Safety', 'Source', 'Support System', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Unified Medical Language System', 'Universities', 'acronyms', 'base', 'biomedical informatics', 'career development', 'clinically significant', 'deep learning', 'doctoral student', 'federal policy', 'health care service organization', 'health record', 'heuristics', 'improved', 'information organization', 'medical specialties', 'method development', 'multidisciplinary', 'neural network', 'novel', 'patient portal', 'patient-clinician communication', 'programs', 'shared decision making', 'success', 'symposium', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,F31,2020,47355,0.009671118362497032
"Data-driven shared decision-making to reduce symptom burden in atrial fibrillation PROJECT SUMMARY Atrial fibrillation (AF) is the most common cardiac arrhythmia with symptoms that directly impair health-related quality of life (HRQoL). While catheter ablation is routinely performed to reduce AF symptoms and improve HRQoL, we lack evidence about which symptoms are likely to improve and for which patients. Ablations themselves may cause complications that lead to lower HRQoL. Shared decision-making (SDM) is a widely encouraged practice to navigate such complex choices by aligning treatment benefits and risks with the patient's stated values. However, no SDM interventions have focused explicitly on AF symptoms due to a lack of rigorous evidence about post-ablation symptom patterns and the decision aids necessary to communicate those findings. In this K99/R00 application, we propose to use data from electronic health records (EHRs) to characterize post-ablation symptom patterns, and display them in decision-aid visualizations to support personalized SDM about the best treatment modalities for an individual's patient's AF symptoms. In the K99 phase, we will use natural language processing (NLP) and machine learning (ML) to extract and analyze symptom data from narrative notes in EHRs. We will also employ a rigorous, user-centered design protocol created during my postdoctoral work to develop decision-aid visualizations. In the R00 phase, we will conduct a feasibility study in which the interactive decision-aid visualizations are introduced during consultations about ablation in clinical electrophysiology practices. Our specific aims are: (1) identify common symptom patterns in patients with paroxysmal AF post-catheter ablation (n>32,014); (2) develop and evaluate decision-aid visualizations of common AF symptom patterns (n=50); and (3) evaluate the feasibility of implementing the decision-aid visualizations in clinical practice (n=75). The training objectives of this project include mastering competencies in NLP, ML, human-computer interaction, symptom science, and implementation science. The long-term training goal is to assist Dr. Reading Turchioe to become a faculty member with an independent program of research. She seeks to lead an interdisciplinary team of scientists and clinicians committed to improving symptom management and HRQoL for individuals living with AF and other chronic cardiovascular conditions, with an eye towards health equity. To ensure success for the planned research and training activities, a multidisciplinary team of mentors with complementary expertise, established, well-funded programs of research, and a record of mentoring high-quality trainees will advise her. Moreover, this research will be conducted in a world-class academic medical center with exceptional resources for building and implementing technology and data science methods using EHR data. The proposed research is both significant and innovative: NLP and ML methods to extract EHR data for decision-aid visualizations are a novel approach to SDM in the understudied area of AF symptoms. Together, these techniques promise to enhance HRQoL for other AF treatment modalities (e.g. medications, lifestyle changes) and other chronic cardiovascular conditions. ! ! PROJECT NARRATIVE Our research has public health significance because atrial fibrillation is the most common cardiac arrhythmia and expected to double in prevalence in the next decade as the population ages. “Real-world evidence” from electronic health record data displayed in personalized decision-aid visualizations can enable patients and providers to choose together the optimal treatments for symptom reduction. Our proposal addresses NINR's key areas of “Symptom Science: Promoting Personalized Health Strategies” and “Promoting Innovation: Technology to Improve Health” by leveraging innovative technologies to deliver personalized interventions that reduce symptom burden and improve quality of life for individuals with chronic conditions. !",Data-driven shared decision-making to reduce symptom burden in atrial fibrillation,9953270,K99NR019124,"['Ablation', 'Academic Medical Centers', 'Address', 'Affect', 'Age', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Award', 'Benefits and Risks', 'Cardiac ablation', 'Cardiovascular system', 'Caring', 'Characteristics', 'Chronic', 'Clinical', 'Competence', 'Complex', 'Consultations', 'Data', 'Data Display', 'Data Reporting', 'Data Science', 'Decision Aid', 'Devices', 'Dyspnea', 'Electronic Health Record', 'Electrophysiology (science)', 'Enabling Factors', 'Ensure', 'Eye', 'Faculty', 'Fatigue', 'Feasibility Studies', 'Funding', 'Goals', 'Gold', 'Grant', 'Health', 'Health Personnel', 'Healthcare', 'Impaired health', 'Impairment', 'Individual', 'International', 'Intervention', 'Interview', 'Investigation', 'Lead', 'Learning', 'Life Style', 'Machine Learning', 'Measures', 'Mentors', 'Methods', 'Mobile Health Application', 'Modality', 'Monitor', 'Natural Language Processing', 'Nursing Informatics', 'Outcome', 'Palpitations', 'Patient Outcomes Assessments', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Positioning Attribute', 'Predisposing Factor', 'Prevalence', 'Procedures', 'Process', 'Protocols documentation', 'Provider', 'Public Health', 'Quality of life', 'Reading', 'Reinforcing Factor', 'Reporting', 'Research', 'Research Activity', 'Resources', 'Risk', 'Sampling', 'Scientist', 'Site', 'Symptoms', 'Techniques', 'Technology', 'Training', 'Training Activity', 'Validation', 'Visualization', 'Work', 'associated symptom', 'biomedical informatics', 'career', 'clinical practice', 'common symptom', 'comorbidity', 'computer human interaction', 'data visualization', 'electronic data', 'experience', 'health equity', 'health related quality of life', 'implementation science', 'improved', 'individual patient', 'innovation', 'innovative technologies', 'insight', 'instrument', 'mHealth', 'machine learning method', 'member', 'minimally invasive', 'multidisciplinary', 'novel strategies', 'optimal treatments', 'personalized decision', 'personalized intervention', 'pre-doctoral', 'programs', 'reduce symptoms', 'shared decision making', 'statistics', 'success', 'symptom management', 'symptom science', 'symptomatic improvement', 'treatment risk', 'user centered design']",NINR,WEILL MEDICAL COLL OF CORNELL UNIV,K99,2020,87632,0.02142978912707739
"Statistical Methods for Incorporating Machine Learning Tools in Inference and Large-Scale Surveillance using Electronic Medical Records Data SUMMARY The modernization and standardization of clinical care information systems is creating large networks of linked electronic health records (EHR) that capture key treatments and select patient outcomes for millions of patients throughout the country. The observational data emerging from these systems provide an unparalleled opportunity to learn about the effectiveness of existing and novel treatments, and to monitor potential safety issues that may arise when interventions are used in broad patient populations. However, observational clinical data have exposures that are driven by many factors and therefore aggressive adjustment is needed to remove as much confounding bias as possible in order to make attribution regarding select exposures. The field of machine learning provides a powerful collection of data-driven approaches for performing flexible, thorough confounding adjustment, but performing reliable statistical inference is particularly challenging when these techniques are used as part of the analytic strategy. We propose to advance reproducible research methods by developing and illustrating novel targeted learning tools that leverage the flexibility of machine learning methods to detect and characterize health effect signals using large-scale EHR data. Specifically, we will first develop techniques for making efficient, statistically valid and robust inference for treatment effects using state-of-the-art machine learning tools. We will also develop online learning techniques to make such inference in the context of streaming EHR data. Methodological advances will enable us to formulate a formal, rigorous and practical framework for conducting continuous, effective and reliable surveillance for safety endpoints. Finally, we will develop statistical approaches for incorporating prior information -- including demographic, epidemiologic or pharmacodynamic knowledge, for example -- to improve health effect estimation and inference when the health outcome of interest is rare and the statistical problem is thus difficult, as often occurs in safety surveillance. The ultimate goal of the proposed research is to enable biomedical researchers and public health regulators to carefully monitor and protect the health of the public by allowing them to more effectively and more reliably detect critical health effect signals that may be contained in population-scale EHR data. PROJECT NARRATIVE The modernization and standardization of clinical care information systems is creating large networks of linked electronic medical records that capture key treatments and select patient outcomes for millions of U.S. subjects. The population scale of contemporary health care data is opening new opportunities for quickly learning from observational data, and is now supporting on-going national surveillance that will monitor the risks and benefits of both existing and novel treatment paths. The objective of this proposal is to provide an inferential framework that leverages the flexibility of machine learning methods to detect health effect signals, including in the important setting of high-dimensional confounders and/or rare events, and to develop a real-time sequential updating methodology for safety signal detection.",Statistical Methods for Incorporating Machine Learning Tools in Inference and Large-Scale Surveillance using Electronic Medical Records Data,9979940,R01HL137808,"['Algorithms', 'Benefits and Risks', 'Characteristics', 'Clinical Data', 'Complex', 'Computer software', 'Computerized Medical Record', 'Confidence Intervals', 'Country', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Dimensions', 'E-learning', 'Early Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Estimation Techniques', 'Event', 'Frequencies', 'Goals', 'Health', 'Healthcare', 'Heterogeneity', 'Information Systems', 'Infrastructure', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modernization', 'Monitor', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Pharmacodynamics', 'Population', 'Population Surveillance', 'Procedures', 'Public Health', 'Records', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Safety', 'Sentinel', 'Signal Transduction', 'Standardization', 'Statistical Methods', 'Stream', 'Structural Models', 'Subgroup', 'Surveillance Program', 'System', 'Techniques', 'Testing', 'Time', 'Treatment outcome', 'Update', 'base', 'clinical care', 'comparative treatment', 'data streams', 'flexibility', 'high dimensionality', 'improved', 'interest', 'machine learning method', 'national surveillance', 'novel', 'open source', 'patient population', 'patient subsets', 'risk minimization', 'software development', 'surveillance data', 'tool', 'treatment effect', 'user-friendly']",NHLBI,UNIVERSITY OF WASHINGTON,R01,2020,485987,0.03754270073101666
"Multi-objective representation learning methods for interpetable predictions of patient outcomesusing electronic health records Project Summary/Abstract This project proposes new methods for representing data in electronic health records (EHR) to improve pre- dictive modeling and interpretation of patient outcomes. EHR data offer a promising opportunity for advancing the understanding of how clinical decisions and patient conditions interact over time to inﬂuence patient health. However, EHR data are difﬁcult to use for predictive modeling due to the various data types they contain (con- tinuous, categorical, text, etc.), their longitudinal nature, the high amount of non-random missingness for certain measurements, and other concerns. Furthermore, patient outcomes often have heterogenous causes and re- quire information to be synthesized from several clinical lab measures and patient visits. The core challenge at hand is overcoming the mismatch between data representations in the EHR and the assumptions underly- ing commonly used statistical and machine learning (ML) methods. To this end, this project proposes novel wrapper-based methods for learning informative features from EHR data. Both methods propose specialized operators to handle sequential data, time delays, and variable interactions, and have the capacity to discover underlying clinical rules/decisions that affect patient outcomes. Importantly, both methods also produce archives of possible models that represent the best trade-offs between complexity and accuracy, which assists in model interpretation. These method advances are made possible by encoding a rich set of data operations as nodes in a directed acyclic graph, and optimizing the graph structures using multi-objective optimization. The central hypothesis of this research is that multi-objective optimization can learn effective data representations from the EHR to produce accurate, explanatory models of patient outcomes. Preliminary work has shown that these methods can effectively learn low-order data representations that improve the predictive ability of several state- of-the-art ML methods. This technique demonstrates good scaling properties with high-dimensional biomedical data. Aim 1 (K99) is to develop a multi-objective feature engineering method that pairs with existing ML methods to iteratively improve their performance by constructing new features from the raw data and using feedback from the trained model to guide feature construction. In Aim 2 (K99), this method is applied to form predictive models of the risk of heart disease and heart failure using longitudinal EHR data. The resultant models will be inter- preted with the help of mentors in order to translate predictions into clinical recommendations. For Aim 3 (R00), a second method is proposed that uses a similar framework to optimize existing neural network approaches in order to simplify their structure as much as possible while maintaining accuracy. The goal of Aim 4 (R00) is to identify hospital patients who are at risk of readmission and propose point-of-care strategies to mitigate that risk. This goal is facilitated through the application of the proposed methods to patient data collected from the Hospital of the University of Pennsylvania, the Geisinger Health System, and publicly available EHR databases. Project Narrative  Understanding how clinical decisions interact with a patient's health and environmental over time to inﬂuence patient outcomes is central to the goals of enhancing health, reducing illness and improving quality of life. The proposed research provides important methodological advances for extracting these insights from widely available patient health records.",Multi-objective representation learning methods for interpetable predictions of patient outcomesusing electronic health records,9936444,K99LM012926,"['Address', 'Affect', 'Archives', 'Area', 'Automobile Driving', 'Cardiovascular Diseases', 'Categories', 'Clinical', 'Communities', 'Complex', 'Couples', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Development', 'Disease', 'Electronic Health Record', 'Engineering', 'Feedback', 'Goals', 'Graph', 'Hand', 'Health', 'Health Sciences', 'Health system', 'Heart Diseases', 'Heart failure', 'Hospitals', 'Inpatients', 'Knowledge', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Nature', 'Outcome', 'Pathology', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Pennsylvania', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Process', 'Property', 'Protocols documentation', 'Quality of life', 'Recommendation', 'Replacement Arthroplasty', 'Research', 'Research Personnel', 'Risk', 'Structure', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'University Hospitals', 'Visit', 'Work', 'base', 'care costs', 'cluster computing', 'data archive', 'deep learning', 'deep neural network', 'design', 'disease diagnosis', 'disorder subtype', 'heart disease risk', 'high dimensionality', 'hospital readmission', 'improved', 'insight', 'learning strategy', 'machine learning method', 'network architecture', 'neural network', 'novel', 'open source', 'operation', 'patient health information', 'point of care', 'predictive modeling', 'readmission rates', 'readmission risk', 'statistical and machine learning', 'tool']",NLM,UNIVERSITY OF PENNSYLVANIA,K99,2020,89401,0.04414177805538221
"Developing scalable algorithms to incorporate unstructured electronic health records for causal inference based on real-world data Project Summary/Abstract The routine operation of the US Healthcare system produces an abundance of electronically-stored data that captures the care of patients as it is provided in settings outside of controlled research environments. The potential for utilizing these data to inform future treatment choices and improve patient care and outcomes of all patients in the very system that generates the data is widely acknowledged. Given these key properties of the routine-care data and the abundance of electronic healthcare databases covering millions of patients, it is critical to strengthen the rigor of analyses of such data. Our group has previously developed an analytic approach to reduce bias when analyzing routine-care databases, which has proven effective in more than 50 empirical research studies across a range of topics and data sources. However, this approach currently cannot incorporate free-text information that is recorded in electronic health records, such as clinical notes and reports. This limitation has left a large amount of rich patient information underutilized for clinical research. We thus aim to adapt and refine a set of established computerized natural language processing algorithms that can identify and extract useful information from the clinical notes and reports in electronic health records and incorporate them into our validated analytical approach for balancing background risks of different comparison groups, a key step to ensure fair evaluation when comparing different therapeutic options. To test this newly integrated and augmented approach, we will implement and adapt it in simulation studies where we can evaluate and improve the performance of these new analytic methods in a controlled but realistic fashion. In addition, we will assess the performance of our new approach in 8 practical studies comparing medical or surgical treatments that are highly relevant to patients. To ensure highest level of data completeness and quality, we have linked multiple healthcare utilization (claims) databases, spanning from 2007 to 2016, with 3 electronic health records systems, including one each in Massachusetts, North Carolina, and Texas. This data will allow testing of our newly integrated approach in a variety of care delivery systems and data environments, which will be very informative for the application of our products in the real-world settings. Narrative The project will yield a highly flexible and effective analytical method for reducing confounding bias in studies that utilize routine-care data to compare effects of medical or surgical treatments. This method will enable researchers to leverage a large amount of patient information recorded in the clinical notes and reports that are contained within electronic health records to adjust for differences in background risks of different comparison groups. Our proposal can improve the quality of evidence based on electronic healthcare data generated in the routine-care settings to better inform patient care and optimal prescribing.",Developing scalable algorithms to incorporate unstructured electronic health records for causal inference based on real-world data,9970594,R01LM013204,"['Address', 'Algorithms', 'Clinical', 'Clinical Research', 'Code', 'Cohort Studies', 'Comparative Effectiveness Research', 'Complex', 'Confounding Factors (Epidemiology)', 'Consumption', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Disease', 'Electronic Health Record', 'Elements', 'Empirical Research', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Future', 'Gold', 'Healthcare', 'Healthcare Systems', 'Influentials', 'Knowledge', 'Knowledge acquisition', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Massachusetts', 'Medical', 'Medicare/Medicaid', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'North Carolina', 'Operative Surgical Procedures', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Probability', 'Property', 'Proxy', 'Randomized Controlled Trials', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Risk', 'Risk Factors', 'Semantics', 'Severities', 'Specific qualifier value', 'Stratification', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Texas', 'Text', 'Therapeutic', 'Time', 'Training', 'Treatment outcome', 'Validation', 'Weight', 'Work', 'analytical method', 'base', 'care delivery', 'care outcomes', 'comparative effectiveness', 'comparison group', 'computerized', 'cost', 'disorder risk', 'evidence base', 'flexibility', 'health care service utilization', 'high dimensionality', 'improved', 'innovation', 'machine learning method', 'novel strategies', 'operation', 'outcome forecast', 'preservation', 'randomized trial', 'research study', 'routine care', 'safety study', 'simulation', 'sound', 'structured data', 'tool', 'treatment choice', 'unstructured data']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2020,469927,0.033550666286087535
"Discovering and Applying Knowledge in Clinical Databases PROJECT SUMMARY / ABSTRACT The long-term goal of our ongoing project, “Discovering and applying knowledge in clinical databases,” is to learn from data in the electronic health record (EHR) and to apply that knowledge to understand and improve health. The EHR, because of its broad capture of human health, greatly amplifies our ability to carry out observational research, opening the possibility of covering emerging problems, diverse populations, rare diseases, and chronic diseases in long-term longitudinal studies. Unfortunately, the strength of EHR data—its breadth and flexible nature—imposes additional challenges. We have found that the biggest challenge comes from the inaccuracy, incompleteness, complexity, and resulting bias inherent in the recording of the health care process. We previously showed that health care process bias exists to the extent, for example, that simple use of the data can create signals implying the opposite of what we know to be true. One of the most important factors is sparse, irregular sampling; we found that sampling bias can be reduced by reparameterizing time and that prediction techniques that can accommodate EHR-specific data and resist their biases like data assimilation can be used on EHR data to produce good estimates of glucose and HA1c. The previous cycle of this project produced 75 publications. We propose to develop methods to accommodate health care process bias, using both knowledge engineering and experience with health care process bias as well as advanced statistical techniques that employ dynamical models and latent variables. We hypothesize that heuristics and models combined with knowledge can improve our ability to generate inferences and learn phenotypes despite health care process bias. Our aims are as follows: (1) Taking a knowledge engineering approach, study the effect of preprocessing and analytic choices on reducing health care process bias, and using machine learning techniques, learn more about health care process bias. (2) Taking a more empirical approach, use dynamic latent factor modeling and variation inference to accommodate health care process bias, learning how a patient's health state and health processes affect censoring, exploiting information from many variables at once. (3) Use data assimilation and mechanistic models to learn otherwise unmeasurable physiologic phenotypes despite irregular, sparse sampling typical of electronic health records. (4) Use the developed models and generated phenotypes to answer clinical questions, and disseminate the results. PROJECT NARRATIVE This project studies the biases that health care processes bring to electronic health record data, and it develops methods to overcome those biases to improve reuse of the data for purposes such as clinical research and quality improvement.",Discovering and Applying Knowledge in Clinical Databases,9873996,R01LM006910,"['Active Learning', 'Adopted', 'Adverse drug effect', 'Affect', 'Algorithms', 'Area', 'Assimilations', 'Award', 'Chronic Disease', 'Clinical', 'Clinical Research', 'Complex', 'Data', 'Data Reporting', 'Electronic Health Record', 'Engineering', 'Functional disorder', 'Generations', 'Glucose', 'Goals', 'Health', 'Healthcare', 'Human', 'Insulin', 'Knowledge', 'Learning', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Metabolism', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Nonlinear Dynamics', 'Observational Study', 'Patients', 'Performance', 'Phenotype', 'Physiological', 'Physiology', 'Population Heterogeneity', 'Process', 'Publications', 'Rare Diseases', 'Research', 'Sampling', 'Sampling Biases', 'Signal Transduction', 'Structure', 'Techniques', 'Testing', 'Time', 'Training', 'Variant', 'Work', 'abstracting', 'clinical database', 'data reuse', 'deep learning', 'experience', 'flexibility', 'heuristics', 'improved', 'novel', 'precision medicine', 'predictive modeling']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,582678,0.03496250850292797
"Investigating the documentation of E-cigarette use in the VA EHR PROJECT SUMMARY Electronic cigarettes were developed in China in the early 2000s and first introduced to the US market in 2007. Once established in the US, the product experienced explosive growth, with the number of electronic cigarette users doubling every year between 2008 and 2012. In 2012, it was estimated that 75% of US adults had heard of electronic cigarettes, and 8% had tried them. While electronic cigarettes have been studied over the last sev- eral years, no scientific consensus has emerged regarding either the safety of electronic cigarettes, or their po- tential as a smoking cessation aid. With this proposal, we will investigate how electronic cigarette use is documented in the Veterans Association Electronic Health Record, focusing specifically on the relationship between electronic cigarette use and com- bustible tobacco use, with the goal of understanding both how electronic cigarette use is documented in the context of the United States’ only nationwide health system, and how electronic cigarette related information can be reliably extracted from narrative clinical text using fully automated Natural Language Processing meth- ods. PROJECT NARRATIVE The proposed research focuses on the use of Natural Language Processing methods to automatically extract mentions of electronic cigarette use from the Veterans Association Electronic Health Record. The research will provide insight into important, currently unresolved questions regarding how clinicians record electronic cigarette use in the context of a nationwide health system, and whether patients report the use of electronic cigarettes as a smoking cessation aid or use the devices in conjunction with combustible tobacco.",Investigating the documentation of E-cigarette use in the VA EHR,9852435,R03DA047577,"['Address', 'Adult', 'Algorithms', 'American', 'Area', 'China', 'Cities', 'Clinical', 'Consensus', 'Dangerousness', 'Data', 'Data Set', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Electronic Nicotine Delivery Systems', 'Electronic cigarette', 'Environment', 'Epidemiology', 'Evaluation', 'Geography', 'Goals', 'Government', 'Growth', 'Health', 'Health system', 'Healthcare Systems', 'Hearing', 'Individual', 'Methods', 'Natural Language Processing', 'Patients', 'Pattern', 'Professional Organizations', 'Public Health', 'Public Health Applications Research', 'Reporting', 'Research', 'Risk', 'Safety', 'Scheme', 'Smoking', 'Sodium Chloride', 'Source', 'Technology', 'Text', 'Tobacco', 'Tobacco use', 'United States', 'Universities', 'Utah', 'Variant', 'Veterans', 'Work', 'authority', 'electronic cigarette use', 'electronic cigarette user', 'electronic hookah', 'evidence base', 'experience', 'information model', 'innovation', 'insight', 'smoking cessation', 'structured data', 'success', 'systems research', 'tobacco control', 'tool', 'vape pens']",NIDA,UNIVERSITY OF UTAH,R03,2020,76250,-0.0005753108772693665
"From enrichment to insights Project Summary Most medical decisions are made without the support of rigorous evidence in large part due to the cost and complexity of performing randomized trials for most clinical situations. In practice, clinicians must use their judgement, informed by their own and the collective experience of their colleagues. The advent of the electronic health record (EHR) enables the modern practitioner to algorithmically check the records of thousands or millions of patients to rapidly find similar cases and compare outcomes. In addition to filling the inferential gap in actionable evidence, these kinds of analyses avoid issues of ethics, practicality, and generalizability that plague randomized clinical trials (RCTs). Unfortunately, identifying patients with the appropriate phenotypes, properly leveraging available data to adjust results, and matching similar patients to reduce confounding remain critical challenges in every study that uses EHR data. Overcoming these challenges to improve the accuracy of observational studies conducted with EHR data is of paramount importance. Studies using EHR data begin by defining a set of patients with specific phenotypes, analogous to amassing a cohort for a clinical trial. This process of electronic phenotyping, is typically done via a set of rules defined by experts. Machine learning approaches are increasingly used to complement consensus definitions created by experts and we propose several advances to validate and improve this practice. We will explore and quantify the effects of feature engineering choices to transform the diagnoses, procedures, medications, laboratory tests and clinical notes in the EHR into a computable feature matrix. Finally, building on recent advances, we plan to characterize the performance of existing methods and develop EHR-specific strategies for patient matching. Our work is significant because we will take on three challenging problems--electronic phenotyping, feature engineering, and patient matching--that stand in the way of generating insights via EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes. Narrative The advent of the electronic health record (EHR) enables the search of thousands or millions of patients to rapidly find similar cases and compare outcomes. We will develop methods for feature engineering, electronic phenotyping and patient matching from real-world EHR data. If we are successful, we will significantly advance our ability to generate insights from the large amounts of health data that are routinely generated as a byproduct of clinical processes.",From enrichment to insights,10000216,R01LM011369,"['Address', 'Algorithms', 'Area', 'Clinical', 'Clinical Trials', 'Code', 'Complement', 'Consensus', 'Data', 'Data Element', 'Data Set', 'Diagnosis', 'Effectiveness', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evaluation', 'Frequencies', 'Future', 'Goals', 'Health system', 'Healthcare Systems', 'Institution', 'Judgment', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Modernization', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Plague', 'Procedures', 'Process', 'Randomized Clinical Trials', 'Records', 'Resources', 'Scheme', 'Source', 'Statistical Data Interpretation', 'Test Result', 'Testing', 'Time', 'Training', 'Work', 'base', 'cohort', 'cost', 'electronic data', 'experience', 'health data', 'improved', 'innovation', 'insight', 'large datasets', 'machine learning algorithm', 'novel', 'portability', 'randomized trial', 'simulation', 'treatment effect', 'vector']",NLM,STANFORD UNIVERSITY,R01,2020,643304,0.06356173447648324
"Biases introduced by filtering electronic health records for patients with ""complete data"" PROJECT SUMMARY Nationwide adoption of electronic health records (EHRs) has led to the increasing availability of large clinical datasets. With statistical modeling and machine learning, these datasets have been be used in a wide range of applications, including diagnosis, decision support, cost reduction, and personalized medicine. However, because the same patient could be treated at multiple health care institutions, data from only a single EHR might not contain the complete medical history for that patient, with critical events potentially missing. A common approach to addressing this problem is to apply data checks that filter the EHR for patients whose data appear to be more “complete”. Examples of filters include requiring at least one visit per year or ensuring that age, sex, and race are all recorded. However, in a previous study using EHR data from seven institutions, we showed that these filters can greatly reduce the sample size and introduce unexpected biases by selecting sicker patients who seek care more often and changing the demographics of the resulting cohorts. This project extends this prior research by implementing an expanded set of data completeness filters and testing their accuracy and potential biases using a combination of national claims data and EHR data from dozens of hospitals and healthcare centers across the country. This will enable us to understand how data completeness varies in different EHRs and quantify the tradeoffs of different approaches to correcting for gaps in patients' records. First, we will develop and measure the accuracy of data completeness filters using national claims data. This provides a “gold standard” of longitudinal data where patients' complete medical histories are known during the periods in which they were enrolled in the insurance plan. After partitioning the data by provider groups to model gaps in EHR data, we will test how well data completeness filters, individually and in combined machine learning models, select patients with fewer gaps. We will then test whether the filters introduce biases by selecting sicker patients (more diagnoses, more visits, etc.) or changing their demographic characteristics (age, sex, and zip code). Then, we will test the filters on EHR data, first at a single large medical center, and then across a national network of 57 institutions, representing different geographic regions, patient populations, number of years of data, and types of health care facilities. We will evaluate the filters by measuring whether they improve the performance of a machine learning model for predicting hospital admissions. Our ultimate goals are to (a) help researchers balance the need for complete data with the biases this might introduce to their models and (b) help them predict how well models trained on one EHR dataset might work on other EHRs with different data completeness profiles. PROJECT NARRATIVE Nationwide adoption of electronic health records (EHRs) has led to the increasing availability of large clinical datasets. However, because the same patient could be treated at multiple health care institutions, data from only a single EHR might not contain the complete medical history for that patient, with critical events potentially missing. This study identifies biases that are introduced by selecting patients with fewer gaps in their record.","Biases introduced by filtering electronic health records for patients with ""complete data""",10121437,R01LM013345,"['Address', 'Adopted', 'Adoption', 'Age', 'Characteristics', 'Clinical', 'Clinical Trials', 'Clinical Trials Network', 'Clinical and Translational Science Awards', 'Code', 'Computer software', 'Country', 'Data', 'Data Set', 'Databases', 'Diagnosis', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Equilibrium', 'Event', 'Funding', 'Geographic Locations', 'Goals', 'Gold', 'Health', 'Health care facility', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Institution', 'Insurance Carriers', 'Israel', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Medical History', 'Medical center', 'Modeling', 'Ontology', 'Patients', 'Performance', 'Probability', 'Procedures', 'Provider', 'Race', 'Recording of previous events', 'Records', 'Research', 'Research Personnel', 'Sample Size', 'Site', 'Statistical Models', 'System', 'Testing', 'Training', 'United States National Institutes of Health', 'Visit', 'Weight', 'Work', 'care seeking', 'clinical database', 'cohort', 'cost', 'demographics', 'improved', 'insurance plan', 'open source', 'patient health information', 'patient population', 'personalized medicine', 'predictive modeling', 'sex']",NLM,HARVARD MEDICAL SCHOOL,R01,2020,373990,0.0533464420494672
"Collaborative Research: Statistical algorithms for anomaly detection and patterns recognition in patient care and safety event reports Medical errors have been shown to be the third leading cause of death in the United States. The Institute of Medicine and several state legislatures have recommended the use of patient safety event reporting systems (PSRS) to better understand and improve safety hazards. Numerous healthcare providers have adopted these systems, which provide a framework for healthcare provlder staff to report patient safety events. Public databases like MAUDE and VAERS have also been created to collect and trend safety events across healthcare systems. A patient safety event (PSE) report generally consists of both structured and unstructured data elements. Structured data are pre-defined, fixed fields that solicit specific information about the event. The unstructured data fields generally include a free text field where the reporter can enter a text description of the event. The text descriptions are often a rich data source in that the reporter ls not constrained to limited categories or selection options and is able to freely descrlbe the details of the event. The goal of this project is to develop novel statistical methods to analyze unstructured text like patient safety event reports arising in healthcare, which can lead to significant improvements to patient safety and enable timely intervention strategies. We address three problems: (a) Building realistic and meaningful baseline models for near misses, and detecting systematic deterioration of adverse outcomes relative to such baselines; (b) Understanding critical factors that lead to near misses & quantifying severity of outcomes; and (c) ldentifylng document groups of interest. We will use novel statistical approaches that combine Natural Language Processing with Statistical Process Monitoring, Statistical Networks Analysis, and Spatio-temporal Modeling to build a generalizable toolbox that can address these issues in healthcare. An important advantage of our research team is the involvement of healthcare domain experts and access to frontline staff, and we will leverage this strength to develop our algorithms. A key feature of our work is the generalizability of our methods, which will be applicable to biomedical documents arising across a remarkable variety of areas, such as patient safety and equipment malfunction reports, electronic health records, adverse drug or vaccine reports, etc. We will also release open source software via R packages & GitHub, which will enable healthcare staff and researchers to execute our methods on their datasets. Estimates of preventable adverse events in healthcare are staggering, despite the frequently cited Institute of Medicine (IOM) report that first brought attention to the problem over ten years ago. Identifying temporal trends and patterns in the data is particularly important to improving patient safety and patient care. Using our algorithms to effectively analyze documents from reporting systems has the potential to dramatically improve the safety and quality of care by exposing possible weaknesses in the care process.",Collaborative Research: Statistical algorithms for anomaly detection and patterns recognition in patient care and safety event reports,10211805,R01LM013309,"['Address', 'Adopted', 'Adverse event', 'Algorithms', 'Area', 'Attention', 'Caring', 'Categories', 'Cause of Death', 'Computer software', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Deterioration', 'Electronic Health Record', 'Equipment Malfunction', 'Event', 'Goals', 'Health Personnel', 'Healthcare', 'Healthcare Systems', 'Institute of Medicine (U.S.)', 'Interest Group', 'Intervention', 'Lead', 'Medical Errors', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Nurses', 'Outcome', 'Pathway Analysis', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Process', 'Quality of Care', 'Report (document)', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Statistical Algorithm', 'Statistical Computing', 'Statistical Methods', 'System', 'Text', 'Time', 'Time trend', 'United States', 'Vaccines', 'Work', 'adverse outcome', 'hazard', 'improved', 'novel', 'open source', 'patient safety', 'spatiotemporal', 'structured data', 'trend', 'unstructured data']",NLM,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2020,278731,0.03948500345771068
"Health Information Technology for Surveillance of Health Care-Associated Infections PROJECT SUMMARY/ABSTRACT Health care-associated infections (HAIs) affect one in every 20 hospitalized patients and account for $10 billion dollars in potentially preventable health care expenditures annually. Current efforts at detection of HAIs are limited to manual chart review which hinders the generalizability and scalability of HAI detection. My goal in seeking a Mentored Clinical Scientist Career Development Award is to acquire the necessary training, practical experience, and knowledge to develop a health services research career as a principal investigator focusing on leveraging novel health information technology (HIT) tools to improve the measurement of surgical health care quality, safety, and effectiveness. To continue my progress towards this goal, the objective of this project is to address the challenges of HAI detection by developing a robust and portable automated HAI surveillance toolkit. This toolkit will combine structured electronic health record (EHR) data with rich information locked in clinical notes using machine learning and natural language processing (NLP) to identify HAIs after surgical procedures. Our overall hypothesis is that combining structured variables from the EHR supplemented with NLP will improve our ability to identify HAIs after surgical procedures. To test the central hypothesis and accomplish the objectives for this application, I will pursue the following three specific aims: 1) Determine the EHR data elements indicative of postoperative HAIs and evaluate the performance of a novel HAI surveillance algorithm; 2) Identify the presence of postoperative SSIs from clinical notes using an automated portable NLP-based algorithm; 3) Apply user-centered design to create a high fidelity prototype of a surgical quality dashboard incorporating our HAI case detection methodology. This contribution is a significant first step in a continuum of research that utilizes the large amounts of data in the EHR combined with novel HIT methods to improve the measurement of surgical health-care quality, safety, and effectiveness. This approach is significant because the tools developed in this proposal have potential to serve as a prototype for identification and monitoring hospitals adverse events and could be replicated on a national scale. The proposed research is innovative in its approach using a combination of structured and unstructured data in the EHR along with novel machine learning and NLP tools to create a generalizable surveillance toolkit for the detection of HAIs. This proposal is responsive to the AHRQ Special Emphasis Notice (NOT-HS-13-011) specifically addressing the use of HIT to improve quality measurement. I have assembled a mentoring team who all internationally recognized experts with long and successful track records of funding and trainee mentorship. This project will provide the means to place me on a trajectory towards a health services research career focused on improving the measurement of surgical health-care quality, safety, and effectiveness using novel HIT tools. In summary, my previous training and experience, innovative research plan, high-quality training plan, first-rate mentorship team, and supportive research environment give me the highest likelihood of success to research independence with the proposed K08 award. PROJECT NARRATIVE/RELEVANCE TO PUBLIC HEALTH The proposed research and career development plan are relevant to public health because health care- associated infections (HAIs) affect about one in every 20 hospitalized patients and account for $30 billion dollars in potentially preventable health care expenditures annually. The objective of this project is to develop robust and portable automated surveillance toolkit that combines structured EHR data with rich information locked in clinical notes using natural language processing to identify HAIs after surgical procedures. The proposed research has the potential to serve as a prototype for identification and monitoring of numerous hospitals adverse events and could be replicated on a national scale.",Health Information Technology for Surveillance of Health Care-Associated Infections,9928343,K08HS025776,[' '],AHRQ,UNIVERSITY OF UTAH,K08,2020,160164,0.003230982708041627
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9854882,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Database Management Systems', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Information Retrieval', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Serious Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'risk prediction model', 'structured data', 'unstructured data', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2020,766226,0.014580961630196563
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format with tremendous potential but an equally large concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potential for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary use of clinical data and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for higher accuracy and much faster de- identification than manual approaches. Clinacuity, Inc. proposes to advance a text de-identification system from a prototype to an accurate, adaptable, and robust system, integrated into the research infrastructure at our implementation and testing site (Medical University of South Carolina, Charleston, SC), and ready for commercialization efforts. To accomplish this undertaking, we will focus on the following specific aims and related objectives, while continuing to prepare the commercialization of the integrated system, with detailed market analysis, commercial roadmap development, and modern media communication: 1) Enhance the text de-identification system performance, scalability, and quality to produce an enterprise-grade solution ready for deployment; 2) Enable use of structured data for enhanced text de-identification (when structured PII is available) and for complete patient records de-identification (i.e., records combining structured and unstructured data). This aim also includes implementing “one-way” pseudo-identifier cryptographic hashing to enable securely linking already de-identified patient records; 3) Integrate the text de-identification system with a research data capture and management system. This includes implementation of the de-identification system as a secure web service, with standards-based access and integration. This de-identification system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will ease research data sharing (as expected for larger NIH-funded research projects) and help healthcare organizations protect patient data confidentiality. Significant time-savings will also be offered, with a process at least 200-1000 times faster than manual de-identification. The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potential, but also equally growing concern for patient confidentiality breaches. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data and protect patient data confidentiality. This project will advance a text de-identification system from a prototype to an accurate, adaptable and robust system allowing for complete patient records de-identification, integrated in the research infrastructure at our implementation and testing site and ready for commercialization efforts. It will improve access to richer, more detailed, and more accurate clinical data for clinical researchers, ease research data sharing and help healthcare organizations protect patient data confidentiality. !",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,9908962,R42GM116479,"['Adoption', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communications Media', 'Confidentiality of Patient Information', 'Data', 'Development', 'Electronic Health Record', 'Enrollment', 'Environment', 'Fast Healthcare Interoperability Resources', 'Funding', 'Growth', 'Health Insurance Portability and Accountability Act', 'Improve Access', 'Link', 'Manuals', 'Medical', 'Modernization', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Performance', 'Personally Identifiable Information', 'Phase', 'Privacy', 'Process', 'Records', 'Reference Standards', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Risk', 'Savings', 'Secure', 'Site', 'South Carolina', 'Speed', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Visualization', 'base', 'commercial application', 'commercialization', 'cost', 'cryptography', 'data reuse', 'data sharing', 'health care quality', 'health care service organization', 'health care settings', 'health management', 'improved', 'large scale data', 'prototype', 'software development', 'standard measure', 'structured data', 'systems research', 'unstructured data', 'web services']",NIGMS,"CLINACUITY,INC.",R42,2020,759330,0.035925392047162115
"Enhance Arthroplasty Research through Electronic Health Records and Nlp-Enabled Informatics ABSTRACT Total joint arthroplasty (TJA) is the most common and fastest growing surgical procedure in the nation. Despite the high procedure volume, the evidence base for TJA procedures and associated interventions are limited. This is mainly due to lack of high quality data sources and the logistical difficulties associated with manually extracting TJA information from the unstructured text of the Electronic Health Records (EHR). Meanwhile, the rapid adoption of EHR and the advances in health information technology offer the potential to transform unstructured EHR notes into structured, codified format that can then be analyzed and shared with local and national arthroplasty registries and other agencies. We therefore propose to leverage unique data resources and natural language processing (NLP) technologies to build an informatics infrastructure for automated EHR data extraction and analysis. We will (1) develop a high performance, externally validated and user centric NLP- enabled algorithm for extraction of complex TJA-specific data elements from the structured and unstructured text of the EHR, (2) validate the algorithm externally in multiple EHR platforms and hospital settings, and (3) conduct a demonstration project focused on prediction of prosthetic joint infections using data elements collected by the NLP-enabled algorithm. Our overarching goal is to develop valid, open source and portable NLP-enabled data collection and risk prediction tools and disseminate them widely to hospitals participating in regional and national TJA registries. This research is significant as it leverages strong data resources and expertise to tackle the pressing need for high quality data and accurate prediction models in TJA. Automated data collection and processing capabilities will lead to an upsurge in secondary use of EHR to advance scientific knowledge on TJA risk factors, healthcare quality and patient outcomes. Accurate prediction of high risk patients for prosthetic joint infections will guide prevention and treatment decisions resulting in significant health benefits to TJA patients. The research is innovative because TJA-specific bioinformatics technology will shift TJA research from current under-powered, single-center studies to large, multi-center registry-based observational studies and clinical trials. Our deliverables have the potential to exert a sustained downstream effect on future TJA research, practice and policy. PUBLIC HEALTH RELEVANCE Lack of high quality data is a critical barrier to progress in total joint arthroplasty (TJA) research. We will utilize health information technology to automate extraction of rich TJA information from the electronic health records, and develop a robust risk prediction score for prosthetic joint infections, a devastating and yet preventable complication of TJA. Widespread adoption of these tools will enhance data collection capabilities and enable affordable large scale studies for practice improvements through secondary use of real-world data. Accurate prediction of post- operative infection risk among TJA candidates will guide individualized preventive strategies for modifiable risk factors, thereby reducing the burden of prosthetic joint infections in TJA patients.",Enhance Arthroplasty Research through Electronic Health Records and Nlp-Enabled Informatics,9880374,R01AR073147,"['Adoption', 'Algorithms', 'American', 'Bioinformatics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Complex', 'Complication', 'Data', 'Data Collection', 'Data Element', 'Data Set', 'Data Sources', 'Decision Making', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Epidemic', 'Evidence based practice', 'Future', 'Goals', 'Gold', 'Guide prevention', 'Health Benefit', 'Hospitals', 'Individual', 'Informatics', 'Institution', 'Intervention', 'Joint Prosthesis', 'Knowledge', 'Logistics', 'Manuals', 'Marketing', 'Medicare', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Policies', 'Postoperative Period', 'Prevention', 'Prevention strategy', 'Procedures', 'Provider', 'Publishing', 'Registries', 'Replacement Arthroplasty', 'Research', 'Risk', 'Risk Factors', 'Safety', 'Scientific Advances and Accomplishments', 'Source', 'Structure', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States', 'age group', 'base', 'computerized data processing', 'cost', 'data access', 'data resource', 'electronic data', 'electronic structure', 'epidemiology study', 'evidence base', 'health care quality', 'health information technology', 'high risk', 'improved', 'individual patient', 'infection risk', 'informatics infrastructure', 'informatics tool', 'innovation', 'joint infection', 'modifiable risk', 'novel', 'open source', 'outcome prediction', 'patient population', 'portability', 'pragmatic trial', 'predictive modeling', 'prototype', 'public health relevance', 'risk prediction model', 'structured data', 'surgery outcome', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,R01,2020,592632,0.05081544354267375
"Advancing Quality and Outcomes Measurement in Rheumatology PROJECT SUMMARY Healthcare has changed rapidly in the last decade with the widespread use of electronic health records (EHRs) and the creation of national EHR-based data networks that aim to improve the quality of care. The American College of Rheumatology's RISE registry is a federally Qualified Clinical Data Registry that collects EHR data from the practices of almost 1000 rheumatologists nationally, analyzes these data centrally, and continuously feeds back performance on quality measures to practices via a web-based dashboard. In this K24 proposal, the applicant proposes to utilize novel methods in clinical informatics to increase the accuracy of quality measurement, while also developing and testing new EHR-based quality measures relevant to rheumatic diseases. The proposed research will leverage her strong research portfolio, including grants from the National Institute of Arthritis and Musculoskeletal and Skin Diseases and the Agency for Healthcare Research and Quality, her successful track record of achieving national endorsement for EHR-based quality measures, existing data from over 1.4 million patients in the RISE database, and the outstanding institutional environment at the University of California, San Francisco. It will also support her ongoing career development in clinical informatics methods relevant to EHR-based clinical research. For this five year K24 award proposal, she plans to increase the time spent mentoring junior investigators in the field or quality and outcomes measurement in rheumatology, with the goal of helping trainees successfully launch academic research careers in patient- oriented research. Aligned with a comprehensive mentoring plan, the proposal outlines two specific aims, including using natural language processing to increase the accuracy of EHR-based quality measurement in RISE, and developing and validating new, prototype electronic clinical quality measures to monitor and address high impact gaps in care for patients with rheumatic disease. The work will prioritize outcome measures and use eMeasurement standards, including the Quality Data Model and Health Quality Measures Format to develop, specify and test measures. Measures developed through this research and mentoring program will be candidates for nationwide dissemination across rheumatology practices to improve care for individuals with rheumatic disease. PROJECT NARRATIVE This mid-career investigator award will support a program in patient-oriented research in rheumatic diseases at the University of California, San Francisco. The award will allow the applicant to expand her research on the development and validation of health care quality measures and support her mentoring of early investigators. The proposed research aims to create quality measures that can be deployed across rheumatology practices to improve the quality of care, while training new researchers to perform innovative patient-oriented research in the area of electronic health record-based quality and outcomes measurement.",Advancing Quality and Outcomes Measurement in Rheumatology,9873905,K24AR074534,"['Address', 'Algorithms', 'American', 'Area', 'Award', 'Back', 'Benchmarking', 'California', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Collection', 'Computer software', 'Consensus', 'Country', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Development', 'Dictionary', 'Disease', 'Electronic Health Record', 'Environment', 'Feeds', 'Foundations', 'Funding', 'Goals', 'Gout', 'Grant', 'Growth', 'Health', 'Healthcare', 'Healthcare Systems', 'High Prevalence', 'Individual', 'Informatics', 'Infrastructure', 'Institute of Medicine (U.S.)', 'Leadership', 'Learning', 'Measurement', 'Measures', 'Mentors', 'Mentorship', 'Methods', 'Mid-Career Clinical Scientist Award (K24)', 'Monitor', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Osteoporosis', 'Outcome', 'Outcome Measure', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Preventive care', 'Process', 'Public Health', 'Quality of Care', 'Registries', 'Research', 'Research Personnel', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Role', 'Safety', 'San Francisco', 'Scientist', 'Specific qualifier value', 'Testing', 'Text', 'Time', 'Training', 'United States', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Universities', 'Validation', 'Vision', 'Work', 'base', 'career', 'career development', 'college', 'comorbidity', 'dashboard', 'data mining', 'data modeling', 'data registry', 'data standards', 'data warehouse', 'design', 'digital', 'evidence base', 'health care quality', 'improved', 'informatics infrastructure', 'innovation', 'interest', 'learning progression', 'novel', 'patient oriented', 'patient oriented research', 'patient registry', 'patient safety', 'programs', 'prototype', 'research study', 'rheumatologist', 'structured data', 'success', 'tool']",NIAMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K24,2020,189666,0.030355616316148117
"Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics DESCRIPTION (provided by applicant): The goal of cancer pharmacoepidemiology is to identify adverse and/or long-term effects of chemotherapeutic agents and determine the impact of drugs on cancer risk, prevention, and response to treatments. Pharmacoepidemiology studies exert strong influence on defining optimal treatments and accelerating translational research. Therefore, it is imperative for these to be done efficiently and leveraging real-world patient data such as electronic health records (EHR). Massive clinical data from EHRs are being tapped into for research in disease-gene associations, comparative effectiveness and clinical outcomes. There is however paucity in pharmacoepidemiological studies using comprehensive EHR data due to the inherent challenges that exist for data abstraction, handling and analysis. The hurdles include heterogeneity of reports, embedding of detailed clinical information in narrative text, differing EHR platforms across different sites and missing data to name a few. In this study, we propose to integrate and extend preexisting tools to build an informatics infrastructure for EHR data extraction, interpretation, management and analysis to advance cancer pharmacoepidemiology research. We will leverage existing tools of natural language processing (NLP), standardized ontologies and clinical data management systems to extract and manipulate EHR data for cancer pharmacoepidemiological research. To achieve our goal we propose four specific aims. In aim 1, we intend to develop a high-performance, user- centric information extraction framework with advanced features such as active learning (to reduce annotation cost), domain adaptation (to transfer data across multiple sites) and user-friendly interfaces (for non-technical end users). In aim 2, we plan to improve data harmonization across differing platforms, develop components for seamless data export as well as expand methodologies to address impediments inherent to EHR-based data (such as the missing data problem). In aim 3, we will conduct demonstration projects of cancer pharmacoepidemiology including pharmacovigilance and pharmacogenomics of chemotherapeutic agents to evaluate, refine and validate the broad uses of our tools. Finally in aim 4, we propose to disseminate the methods and tools developed in this project to the cancer research and pharmacoepidemiology communities. PUBLIC HEALTH RELEVANCE: In this project, we propose to integrate and extend previously developed tools to build an informatics infrastructure for electronic health records (EHR) data extraction, interpretation, management, and analysis, to advance cancer pharmacoepidemiology research. Such methods can efficiently integrate and standardize cancer pharmacoepidemiology specific information from EHRs across different sites, thus advancing research in this field.",Advancing Cancer Pharmacoepidemiology Research Through EHRs and Informatics,9994228,U24CA194215,"['Active Learning', 'Address', 'Adopted', 'Advanced Malignant Neoplasm', 'Algorithms', 'American Association of Cancer Research', 'American Society of Clinical Oncology', 'Benefits and Risks', 'CCL4 gene', 'Cancer Intervention', 'Cancer Patient', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical and Translational Science Awards', 'Communities', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Management Resources', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Drug Exposure', 'Drug usage', 'Electronic Health Record', 'Ethics', 'Face', 'Funding', 'Genes', 'Genomics', 'Goals', 'Health Policy', 'Heterogeneity', 'Hybrids', 'Individual', 'Informatics', 'Information Retrieval', 'Letters', 'Long-Term Effects', 'Malignant Neoplasms', 'Medex', 'Methodology', 'Methods', 'Names', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacogenomics', 'Pharmacotherapy', 'Phenotype', 'Play', 'Population', 'Prevention', 'Preventive', 'Public Health Informatics', 'Randomized Controlled Trials', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Selection Bias', 'Site', 'Smoking Status', 'Standardization', 'System', 'Text', 'Therapeutic', 'Time', 'Toxic effect', 'Translational Research', 'Treatment outcome', 'United States National Institutes of Health', 'Vision', 'anticancer research', 'base', 'cancer risk', 'cancer type', 'chemotherapeutic agent', 'clinical implementation', 'clinical practice', 'cohort', 'comparative effectiveness', 'cost', 'data exchange', 'data harmonization', 'data management', 'follow-up', 'improved', 'informatics infrastructure', 'informatics tool', 'interest', 'learning algorithm', 'longitudinal dataset', 'novel', 'open source', 'optimal treatments', 'pharmacovigilance', 'public health relevance', 'rapid growth', 'success', 'tool', 'treatment response', 'user-friendly']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U24,2020,618291,0.013963725226351946
"Interdisciplinary Infrastructure for Aging Research: Rochester Epidemiology Project PROJECT SUMMARY The National Institutes on Aging (NIA) has recommended strengthening research infrastructures to address future aging research questions (2016 Data Infrastructure Review Committee Report and PAR-16-367). In particular, they recommend: 1) integrating biological data into larger population-based studies; 2) increasing use of electronic health record (EHR) data and linking to medical care claims data; and 3) developing new approaches to collecting data to answer important scientific questions about mechanisms of aging. The Rochester Epidemiology Project (REP; NIA R01 AG034676) is a unique infrastructure for studies of aging, because the REP collects longitudinal EHR data on all health conditions that come to medical attention for a large, Midwestern population. Therefore, the REP allows investigators to study all age-related diseases and outcomes. However, the REP has three significant gaps. First, the REP does not include biospecimens. Second, the REP is missing health care delivered outside of the health care institutions that partner with the REP, and it does not include information on filled prescriptions. Third, a significant proportion of EHR data is difficult to access due to two factors: 1) the full text of the EHRs includes extensive clinical notes about aging outcomes and geriatric syndromes, but these notes are not routinely coded for billing, and can only be accessed through laborious manual review; and 2) the REP health care partners use three different EHR systems, making it difficult to apply electronic data extraction tools across all partners. To address these three gaps, we will develop an interdisciplinary collaboration across experts in aging research, epidemiologic methods, biobanking, and medical informatics to create a new, comprehensive research infrastructure (“Bio-REP”) to support aging research. In the R21 phase, we will develop a comprehensive research infrastructure that combines the REP data with Mayo Clinic Biobank biospecimens, medical claims data from the Centers for Medicare and Medicaid Services (CMS; Aim 1), and geriatric syndrome data that are included in the unstructured EHR clinical notes using Natural Language Processing techniques (NLP; Aim 2). In the R33 phase, we will deploy NLP algorithms developed in Aim 2 in the clinical notes from two additional EHR systems (Aim 3), and we will conduct two demonstration projects. First, we will measure associations between novel aging-related biomarkers and aging-related outcomes (Aim 4). Second, we will determine whether two common medications that are hypothesized to impact aging (metformin and angiotensin receptor blockers) modify associations between aging biomarkers and aging outcomes (Aim 5). The new, robust Bio-REP infrastructure will support a wide range of efficient, cost-effective observational studies to characterize associations between aging-related biomarkers and specific diseases, geriatric syndromes, and drug utilization. Such studies are urgently needed to design effective clinical trials to improve the health span of the aging population. PROJECT NARRATIVE We will establish an interdisciplinary collaboration across experts in aging research, epidemiologic methods, biobanking, and medical informatics to develop and test a new, comprehensive research infrastructure (“Bio- REP”) to support aging research. The Bio-REP will support efficient, cost-effective studies to characterize associations between aging-related biomarkers and specific diseases, geriatric syndromes, and drug utilization. Such studies are urgently needed to design effective clinical trials to improve the health span of the aging population.",Interdisciplinary Infrastructure for Aging Research: Rochester Epidemiology Project,10208373,R33AG058738,"['Address', 'Age-Years', 'Aging', 'Agreement', 'Algorithms', 'Angiotensin Receptor', 'Area', 'Benchmarking', 'Biological', 'Biological Markers', 'Caring', 'Ceramides', 'Clinic', 'Clinical', 'Clinical Trials', 'Code', 'Data', 'Delirium', 'Diagnosis', 'Diagnostic Services', 'Disease', 'Disease Outcome', 'Drug Utilization', 'Elderly', 'Electronic Health Record', 'Enrollment', 'Epidemiologic Methods', 'Epidemiology', 'Future', 'Grant', 'Health', 'Healthcare', 'Infrastructure', 'Institution', 'Link', 'Manuals', 'Measures', 'Medical', 'Medical Informatics', 'Medical Records', 'Metformin', 'National Institute on Aging', 'Natural Language Processing', 'Observational Study', 'Outcome', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Polypharmacy', 'Population', 'Population Study', 'Proteins', 'Recommendation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Review Committee', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'United States Centers for Medicare and Medicaid Services', 'age related', 'aging population', 'biobank', 'cost effective', 'data infrastructure', 'design', 'electronic data', 'falls', 'healthspan', 'improved', 'interdisciplinary collaboration', 'medical attention', 'novel', 'novel marker', 'novel strategies', 'senescence', 'tool']",NIA,MAYO CLINIC ROCHESTER,R33,2020,792718,0.0073072534533122225
"Collaborative Research: Statistical Algorithms for Anomaly Detection and Patterns Recognition in Patient Care and Safety Event Reports Project Summary Medical errors have been shown to be the third leading cause of death in the United States. The Institute of Medicine and several state legislatures have recommended the use of patient safety event reporting systems (PSRS) to better understand and improve safety hazards. A patient safety event (PSE) report generally consists of both structured and unstructured data elements. Structured data are pre-defined, fixed fields that solicit specific information about the event. The unstructured data fields generally include a free text field where the reporter can enter a text description of the event. The text descriptions are often a rich data source in that the reporter is not constrained to limited categories or selection options and is able to freely describe the details of the event. The goal of this project is to develop novel statistical methods to analyze unstructured text like patient safety event reports arising in healthcare, which can lead to significant improvements to patient safety and enable timely intervention strategies. We address three problems: (a) Building realistic and meaningful baseline models for near misses, and detecting systematic deterioration of adverse outcomes relative to such baselines; (b) Understanding critical factors that lead to near misses & quantifying severity of outcomes; and (c) Identifying document groups of interest. We will use novel statistical approaches that combine Natural Language Processing with Statistical Process Monitoring, Statistical Networks Analysis, and Spatio-temporal Modeling to build a generalizable toolbox that can address these issues in healthcare. We will also release open source software via R packages & GitHub, which will enable healthcare staff and researchers to execute our methods on their datasets. The COVID-19 pandemic has resulted in increased patient volumes and increased patient acuity, leading to an excessive burden on many healthcare facilities across the United States. This greatly increases the risk of patient safety consequences arising from malfunctioning medical equipment or adverse reaction to medication. To ensure patient safety and the highest quality of healthcare during this crisis, we need a rapid response system to model and analyze COVID-specific safety issues at scale, and quickly disseminate the results to healthcare facilities, so that these risks can be mitigated at the point of care. In this supplement, we propose to do this by (a) mining public databases and EHRs to identify devices/medication being used for treating COVID and (b) applying our methods (based on NLP, SPC, and SPM) to understand risks associated with these items. This information will be disseminated nationally to all healthcare facilities so that it can be integrated into the EHR at the point of care to alert clinicians. Project Narrative Estimates of preventable adverse events in healthcare are staggering, and the risk is particularly high for COVID patients due to the rapidly increasing burden on healthcare facilities. Using our algorithms to identify temporal trends and analyze free text narratives from reporting systems can ensure the safety and quality of care for COVID patients by exposing and mitigating possible weaknesses in the care process.",Collaborative Research: Statistical Algorithms for Anomaly Detection and Patterns Recognition in Patient Care and Safety Event Reports,10254593,R01LM013309,"['Address', 'Adverse event', 'Adverse reactions', 'Algorithms', 'Architecture', 'Behavior', 'COVID-19 pandemic', 'Caring', 'Categories', 'Cause of Death', 'Computer software', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Detection', 'Deterioration', 'Devices', 'Disease Outbreaks', 'English Language', 'Ensure', 'Equipment', 'Equipment Malfunction', 'Event', 'Goals', 'Health', 'Health care facility', 'Healthcare', 'Institute of Medicine (U.S.)', 'Interest Group', 'Intervention', 'Investigation', 'Lead', 'Length', 'Measures', 'Medical', 'Medical Errors', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Monitor', 'Natural Language Processing', 'Operative Surgical Procedures', 'Outcome', 'Pathway Analysis', 'Pathway interactions', 'Patient Care', 'Patients', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Prevalence', 'Process', 'Property', 'Quality of Care', 'Records', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Severities', 'Side', 'Site', 'Statistical Algorithm', 'Statistical Computing', 'Statistical Methods', 'Statistical Models', 'Supervision', 'System', 'Techniques', 'Text', 'Time', 'Time trend', 'United States', 'Variant', 'Vocabulary', 'adverse outcome', 'base', 'checkup examination', 'cluster computing', 'coronavirus disease', 'dosage', 'hazard', 'health care quality', 'health care service', 'health care service organization', 'improved', 'insight', 'interest', 'mathematical model', 'novel', 'open source', 'patient safety', 'point of care', 'response', 'service delivery', 'spatiotemporal', 'structured data', 'trend', 'unstructured data']",NLM,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2020,74963,0.037229943695068955
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9838247,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Development', 'Diagnostic', 'Disease Surveillance', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Source', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'optimal treatments', 'outcome forecast', 'patient health information', 'population based', 'prevent', 'sociodemographics', 'socioeconomics', 'surveillance data', 'treatment guidelines', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2020,768763,0.014684016853767775
"Transforming Patient Safety Event Data into Actionable Insights through Advanced Analytics Abstract The objective of the proposed research is to develop an innovative algorithms and a software tool to reduce the burden of safety event report classification and analysis so that report data can be transformed to actionable insights. Making safety event data more actionable will support the proactive identification of safety hazards before patients are harmed. We will achieve our research objective through (1) the development of natural language processing algorithms to classify safety event reports into actionable medication error categories; (2) the development of prototype software that will automatically categorize and visualize safety event reports to support trend identification; and (3) the pilot testing of prototype software with hospital and patient safety organization safety analysts. This project utilizes the extensive expertise of the research team in human factors and safety science, including computer science, specifically regarding information retrieval and data classification. Our research team includes patient safety organizations and collaboration with the computer science department at Georgetown University. The proposal is directly aligned with AHRQ’s priority area of making health care safer. Contributions from this research will include an expansion of our understanding of natural language processing and its application to categorizing clinical text, advances in visual analytics, and the development of a software tool to support patient safety analysts. The outputs of this research will serve both healthcare organizations and patient safety organizations allowing them to more efficiently and effectively analyze safety report data. Project Narrative This project is relevant to public health because it applies human factors and computer science to develop software to improve the analysis of patient safety event report data to reduce safety hazards and prevent patient harm. Patient safety event report data will be analyzed using natural language processing algorithms to more efficiently classify events into error categories. Based on these algorithms, prototype software will be developed, tested, and disseminated with the goal of automatic categorization and visualization of safety event reports to identify important safety hazards.",Transforming Patient Safety Event Data into Actionable Insights through Advanced Analytics,9962801,R01HS026481,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2020,398080,0.0012720349613157165
"National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms PROJECT SUMMARY With the rapidly growing adoption of patient electronic health record systems (EHRs) due to Meaningful Use, and linkage of EHRs to research biorepositories, evaluating the suitability of EHR data for clinical and translational research is becoming ever more important, with ramifications for genomic and observational research, clinical trials, and comparative effectiveness studies. A key component for identifying patient cohorts in the EHR is to define inclusion and exclusion criteria that algorithmically select sets of patients based on stored clinical data. This process is commonly referred to, as “EHR-driven phenotyping” is time-consuming and tedious due to the lack of a widely accepted and standards-based formal information model for defining phenotyping algorithms. To address this overall challenge, the proposed project will design, build and promote an open-access community infrastructure for standards-based development and sharing of phenotyping algorithms, as well as provide tools and resources for investigators, researchers and their informatics support staff to implement and execute the algorithms on native EHR data. PROJECT NARRATIVE The identification of patient cohorts for clinical and genomic research is a costly and time-consuming process. This bottleneck adversely affects public health by delaying research findings, and in some cases by making research costs prohibitively high. To address this issue, leveraging electronic health records (EHRs) for identifying patient cohorts has become an increasingly attractive option. This proposal will investigate and implement standards based approaches for computable phenotype identification from multiple EHRs.",National Infrastructure for Standardized and Portable EHR Phenotyping Algorithms,10021669,R01GM105688,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Architecture', 'Benchmarking', 'Benign Prostatic Hypertrophy', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computerized Medical Record', 'Computing Methodologies', 'Consensus', 'Consumption', 'Data', 'Data Element', 'Data Reporting', 'Development', 'Educational workshop', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Engineering', 'Event', 'Exclusion Criteria', 'Fast Healthcare Interoperability Resources', 'Flowcharts', 'Genomics', 'Gold', 'Grant', 'Health', 'Health system', 'Healthcare Systems', 'Human', 'Informatics', 'Infrastructure', 'Intuition', 'Knowledge', 'Logic', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Needs Assessment', 'Observational Study', 'Outcome', 'Patients', 'Performance', 'Phase', 'Phenotype', 'Precision Medicine Initiative', 'Process', 'Public Health', 'Public Health Informatics', 'Rare Diseases', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk Factors', 'Running', 'Scientist', 'Services', 'Standardization', 'Structure', 'System', 'Techniques', 'Text', 'Time', 'Translational Research', 'United States National Institutes of Health', 'University Hospitals', 'authority', 'base', 'biobank', 'clinical phenotype', 'cohort', 'comparative effectiveness study', 'computable phenotypes', 'cost', 'data modeling', 'data warehouse', 'database query', 'deep learning', 'design', 'endophenotype', 'experience', 'inclusion criteria', 'informatics training', 'information model', 'knowledge base', 'meetings', 'phenotyping algorithm', 'portability', 'precision medicine', 'repository', 'structured data', 'syntax', 'tool', 'usability']",NIGMS,WEILL MEDICAL COLL OF CORNELL UNIV,R01,2020,706851,0.045371923383010944
"Efficient Statistical Learning Methods for Personalized Medicine Using Large Scale Biomedical Data Project Summary/Abstract  Current medical treatment guidelines largely rely on data from randomized controlled trials that study  average effects, which may be inadequate for making individualized decisions for real-world patients. Large-scale electronic health records (EHRs) data provide unprecedented opportunities to optimize personalized treatment strategies and generate evidence relevant to real-world patients. However, there are inherent challenges in the use of EHRs, including non-experimental nature of data collection processes, heterogeneous data types with complex dependencies, irregular measurement patterns, multiple dynamic treatment sequences, and the need to balance risk and benefit of treatments. Using two high-quality EHR databases, Columbia University Medical Center's clinical data warehouse and the Indiana Network for Patient Care database, and focusing on type 2 diabetes (T2D), this proposal will develop novel and scalable statistical learning approaches that overcome these challenges to discover optimal personalized treatment strategies for T2D from real-world patients. Specifically, under Aim 1, we will develop a unified framework to learn latent temporal processes for feature extraction and dynamic patient records representation. Our approach will accommodate large-scale variables of mixed types (continuous, binary, counts) measured at irregular intervals. They extract lower-dimensional components to reflect patients' dynamic health status, account for informative healthcare documentation processes, and characterize similarities between patients. Under Aim 2, we will develop fast and efficient multi-category machine learning methods, in order to evaluate treatment propensities and adaptively learn optimal dynamic treatment regimens (DTRs) among the extensive number of treatment options observed in the EHRs. The methods will provide  sequential decisions that determine the best treatment sequence for a T2D patient given his/her EHRs. Under Aim 3, we will develop statistical learning methods to assist multi-faceted treatment decision-making, which balances risks versus benefits when evaluating a DTR. Our approach will ensure maximizing benefit to the greatest extent while controlling all risk outcomes under the safety margins. For all aims, we will develop efficient stochastic resampling algorithms to scale up the optimization for massive data sizes. We will identify optimal DTRs for T2D using the extracted information from patients' comorbidity conditions, medications, and laboratory tests, as well as records-collection processes. Our methodologies will be applied and cross-validated between the two EHR databases. The treatment strategies learned from the representative EHR databases with a diverse patient  population will be beneficial for individual patient care, assisting clinicians to adaptively choose the optimal treatment for a patient. Finally, we will disseminate our methods and results through freely available software and outreach to the informatics and clinical experts at our Centers for Translational Science and elsewhere. Project Narrative  This proposal aims to develop novel and scalable statistical learning methods to analyze electronic health records (EHRs) and use two real-world, high-quality EHR databases for personalized medicine research. The methods will handle the non-experimental nature of data collection processes, along with heterogeneous data types, dynamic treatment sequences, and the trade-off between benefit and risk outcomes. The results will complement the current knowledge base for individual patient care using evidence generated from patients in real-world clinical practices.",Efficient Statistical Learning Methods for Personalized Medicine Using Large Scale Biomedical Data,9891071,R01GM124104,"['Academic Medical Centers', 'Address', 'Adverse event', 'Algorithms', 'Benefits and Risks', 'Categories', 'Center for Translational Science Activities', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Databases', 'Decision Making', 'Dependence', 'Dimensions', 'Documentation', 'Electronic Health Record', 'Ensure', 'Equilibrium', 'Exclusion Criteria', 'Formulation', 'Gaussian model', 'Goals', 'Health', 'Health Status', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Indiana', 'Informatics', 'International', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Measurement', 'Measures', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Process', 'Quality Control', 'Randomized Controlled Trials', 'Records', 'Research', 'Risk', 'Safety', 'Sampling', 'Structure', 'Testing', 'Time', 'Treatment Protocols', 'adaptive learning', 'adverse event risk', 'algorithmic methodologies', 'analytical tool', 'base', 'big biomedical data', 'clinical data warehouse', 'clinical decision-making', 'clinical encounter', 'clinical practice', 'comorbidity', 'complex data ', 'data modeling', 'data space', 'design', 'evidence base', 'feature extraction', 'heterogenous data', 'individual patient', 'individualized medicine', 'knowledge base', 'learning strategy', 'machine learning method', 'novel', 'optimal treatments', 'outreach', 'patient population', 'personalized decision', 'personalized medicine', 'population based', 'scale up', 'statistical learning', 'temporal measurement', 'theories', 'treatment effect', 'treatment guidelines', 'treatment response', 'treatment strategy']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,328853,0.020845512188004574
"Improving missing data analysis in distributed research networks ABSTRACT Electronic health record (EHR) databases collect data that reflect routine clinical care. These databases are increasingly used in comparative effectiveness research, patient-centered outcomes research, quality improvement assessment, and public health surveillance to generate actionable evidence that improves patient care. It is often necessary to analyze multiple databases that cover large and diverse populations to improve the statistical power of the study or generalizability of the findings. A common approach to analyzing multiple databases is the use of a distributed research network (DRN) architecture, in which data remains under the physical control of data partners. Although EHRs are generally thought to contain rich clinical information, the information is not uniformly collected. Certain information is available only for some patients, and only at some time points for a given patient. There are generally two types of missing information in EHRs. The first is the conventionally understood and obvious missing data in which some data fields (e.g., body mass index) are not complete for various reasons, e.g., the clinician does not collect the information or the patient chooses not to provide the information. The second is less obvious because the data field is not empty but the recorded value may be incorrect due to missing data. For example, EHRs generally do not have complete data for care that occurs in a different delivery system. A medical condition (e.g., asthma) may be coded as “no” but the true value would have been “yes” if more complete data had been available, e.g., from claims data as the other delivery system would submit a claim to the patient’s health plan for the care provided. In other words, one may incorrectly treat “absence of evidence” as “evidence of absence”. EHRs hold great promise but we must address several outstanding methodological challenges inherent in the databases, specifically missing data. Addressing missing data is more challenging in DRNs due to different missing data mechanisms across databases. The specific aims of the study are: (1) Apply and assess missing data methods developed in single-database settings to handle obvious and well-recognized missing data in DRNs; (2) Apply and assess machine learning and predictive modeling techniques to address less obvious and under-recognized missing data for select variables in DRNs; and (3) Apply and assess a comprehensive analytic approach that combines conventional missing data methods and machine learning techniques to address missing data in DRNs. The analytic methods developed in this project, including the extension of existing missing data methods to DRNs, the innovative use of machine learning techniques to address missing data, and their integration with privacy- protecting analytic methods, will have direct impact on the design and analysis of future comparative effectiveness and safety studies, and patient-centered outcomes research conducted in DRNs. PROJECT NARRATIVE The proposed project will refine existing methods and develop new methods to address missing data issues in electronic health record databases.",Improving missing data analysis in distributed research networks,10007887,R01HS026214,[' '],AHRQ,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2020,398881,0.031938493203818
"Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign Project Summary/Abstract Written The objective of the proposed research is to reduce the patient safety hazards associated with electronic medication administration records (eMARs) by, (1) understanding current usability and safety gaps, and (2) creating design and development documents, wireframes, and prototypes to serve as the foundation for future eMARs that will eliminate these gaps. In particular, we focus on communication and information flow challenges between nurses, pharmacists, and physicians during medication administration and use of the eMAR. The proposed research is in direct response to special emphasis notice (NOT-HS-16-009). We will develop a broad understanding of usability and safety hazards associated with eMARs by analyzing a large dataset of 1.7 million patient safety event reports and detailed medication error related narratives. We will then conduct heuristic analyses of current eMARs, and interviews and observations of physicians, nurses, and pharmacists. These data will serve to inform the development of eMAR design documents, wireframes, and prototypes as the foundation for future development. This project utilizes the extensive expertise of the research team in human factors and safety science, health information technology (health IT), informatics, and data science. Our research team includes physicians, nurses, pharmacists, and human factors engineers, and experts in natural language processing. In addition, our partnerships include a patient safety organization and a health IT vendor. The proposal addresses fundamental aspects of the call for proposals by providing new insights on the safety of health IT and improves current practices by developing use cases and new prototypes for immediate use by healthIT vendors. Contributions from this research will include a fundamental understanding of the role of health IT during medication administration with a focus on communication and information flow, design, development and testing documents for vendors and providers, and eMAR wireframes and prototypes to improve development. Our research will also provide organizations like the Office of the National Coordinator with medication related test scenarios to assess current health IT systems. Project Narrative This project is relevant to public health because it applies the sciences of human factors and informatics to improve the usability and safety of the electronic medication administration records (eMARs), which will ultimately improve patient care. Patient safety event report data will be analyzed to identify usability and safety hazards in eMARs, and interviews and observations will be conducted to identify clinician needs. Based on this knowledge eMAR design documents, wireframes, and prototypes will be developed, tested, and disseminated with the goal of improving communication and information flow to reduce medication related errors.",Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign,9912124,R01HS025136,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2020,398492,0.0028799540734190804
"Electronic Health Record-based Surveillance of Diabetes by Type in Young Adults in Pennsylvania 1 Currently there are 4.6 million young adults (18 to 44 years of age) with diabetes in the US and the incidence  2 and prevalence are increasing in this age group. However, due to limitations of traditional surveillance  3 strategies, it remains unknown whether these increases are in type 1 or type 2 diabetes. Electronic health  4 record (EHR)-based surveillance is a relatively simple, sustainable, and timely alternative to more traditional  5 methods. Recognizing the attributes of EHR-based surveillance, the Centers for Disease Control and  6 Prevention (CDC) has funded efforts to develop, evaluate, and deploy EHR-based surveillance of diabetes,  7 including the SEARCH for Diabetes in Youth (SEARCH) study and the Diabetes in Young Adults (DiYA) Study.  8 However, the geographic coverage of these studies has been limited. The geographical gaps in these studies  9 are problematic, as there are known geographic disparities in diabetes prevalence and incidence. Moreover, 10 little is known about how the methods applied in these studies will perform in other regions of the country, in 11 rural communities, and in other health systems. The proposed study will use more than two decades of EHR 12 data and administrative claims data to develop and implement EHR-based surveillance of type 1 and type 2 13 diabetes among young adults in a large region of Pennsylvania, the state with the 5th highest prevalence of 14 diabetes in this age group. This information is essential to informing public health strategies, assessing disease 15 burden, and prioritizing type-specific health services. We will use EHR data from Geisinger, a health system 16 serving a large and diverse region of Pennsylvania, to expand the geography of existing surveillance of 17 diabetes subtypes in young adults to the Middle Atlantic, an area without prior EHR-based diabetes 18 surveillance estimates. This region includes a combination of rural and urban communities, enabling us to 19 evaluate differences in the performance of EHR-based algorithms for case ascertainment by community type. 20 In the first phase of the study, we will evaluate the validity; simplicity; and consistency of EHR-based 21 algorithms for identifying diabetes subtypes. This work will build on previously developed algorithms from the 22 SEARCH and DiYA studies. We will use manual review of clinician notes as the gold standard to determine the 23 positive predictive value, sensitivity, and specificity of these algorithms. We propose to use an innovative, 24 efficient, and rigorous validation approach that incorporates natural language processing of clinician notes. We 25 will use a secondary data source, administrative claims data, to assess data completeness and our ability to 26 distinguish between incident and prevalent cases. In the second phase, we will use the best performing 27 algorithms to report on the annual incidence and prevalence of diabetes, by type, in young adults, between 28 2014 and 2024 in 38 Pennsylvania counties. All analyses will be stratified by age, sex, race/ethnicity, insurance 29 status, and community type (rural/urban). Finally, we will coordinate with CDC and other sites to conduct joint 30 analyses of aggregated data, greatly expanding the population under study. Narrative Currently there are 4.6 million young adults (18 to 44 years of age) with diabetes in the US and the incidence and prevalence estimates in this age group have been increasing. However, due to limitations of traditional surveillance strategies, it remains unknown whether these increases are in type 1 or type 2 diabetes. The proposed research will coordinate, develop, implement, and validate electronic-record based surveillance of diabetes in young adults, by subtype, to inform type-specific public health responses, assess disease burden, and identify priorities for type-specific health services.",Electronic Health Record-based Surveillance of Diabetes by Type in Young Adults in Pennsylvania,10085078,U18DP006509,[' '],NCCDPHP,GEISINGER CLINIC,U18,2020,249631,-0.0018594033123353958
"Clinical and Translational Science Award PROJECT SUMMARY: COMBATCOVID The coronavirus (COVID-19) pandemic has affected every corner of the globe and has redefined healthcare throughout the United States. COVID-19 cases in the New York City tri-state area have reached an extraordinarily high number and have quickly become the epicenter region of the crisis in the United States. In New York State alone, there are over 372,000 confirmed cases as of June 1, 2020. NYU Langone Health (NYULH) has been particularly hard hit, with more than 8,100 COVID-19 hospitalizations to date. In response, the entire clinical research community is marshalling resources in an attempt to improve our understanding of how the virus spreads, how it infects various tissues in the body, which patients are more susceptible to infection and fatal outcomes, which therapeutics improve symptoms and survival, whether the immune response confers long-lasting protection against reinfection, and many other crucially important questions. The complexity of the development of this disease and unpredictability of progression into severity, as well as the variety of phenotypic outcomes observed during and post COVID-19, pose major challenges in understanding, predicting, preventing, managing and treating this disease and its sequelae. Answers to these challenges can only be achieved through the comprehensive analysis of a significantly high number of COVID cases. Given how recent and unknown this disease is, and its inherent epidemic nature, there is a limited number of cases at individual medical institutions. The limitation of number of cases per institution becomes even more relevant when isolating subpopulations with specific health conditions and across the lifespan. This proposed study will aim to overcome the above-mentioned challenges by supporting the formation of a consortium comprising multiple medical institutions in the U.S.: COMBATCOVID (Consortium for Multisite Biomedical Analytics and Trials on COVID-19). COMBATCOVID will bring together electronic health records (EHR) data from multiple participating institutions into a shared centralized database. As part of the COMBATCOVID effort, biorepository data of COVID-19 patients collected by some of the participating institutions will also be shared and linked to the respective EHR data. The COMBATCOVID consortium will be responsible for transferring EHR data pertaining to participating institutions interested in contributing EHR data to the N3C database. PROJECT NARRATIVE: COMBATCOVID The coronavirus (COVID-19) pandemic has affected every corner of the globe and has redefined healthcare throughout the United States. To adequately understand the virus much more data is needed, necessitating sharing of data across multiple institutions. The COMBATCOVID (Consortium for Multisite Biomedical Analytics and Trials on COVID-19) initiative will support regional and national efforts by forming a consortium comprising multiple medical institutions in the U.S. to create a shared centralized database of COVID-related EHR data.",Clinical and Translational Science Award,10183901,UL1TR001445,"['2019-nCoV', 'Affect', 'Area', 'COVID-19', 'COVID-19 pandemic', 'Categories', 'Cessation of life', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical and Translational Science Awards', 'Code', 'Communities', 'Coronavirus', 'Critical Care', 'Critical Illness', 'Data', 'Data Science', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Epidemic', 'Event', 'Fatal Outcome', 'Health', 'Health system', 'Healthcare', 'Hospitalization', 'Immune response', 'Individual', 'Infection', 'Inflammatory', 'Informatics', 'Institution', 'Ischemic Stroke', 'Knowledge', 'Laboratories', 'Link', 'Longevity', 'Marshal', 'Medical', 'Natural Language Processing', 'Nature', 'New York', 'New York City', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Pharmacology', 'Phenotype', 'Pneumonia', 'Procedures', 'Records', 'Research Personnel', 'Resources', 'Respiratory Failure', 'Risk', 'Severities', 'Site', 'Subgroup', 'Syndrome', 'Testing', 'Text', 'Therapeutic', 'Thrombosis', 'Time', 'Tissues', 'Underrepresented Groups', 'United States', 'Upper respiratory tract', 'Venous', 'Viral Pneumonia', 'Virus', 'biobank', 'central database', 'cohort', 'coronavirus disease', 'data sharing', 'demographics', 'design', 'ethnic minority population', 'health data', 'improved', 'interest', 'medical specialties', 'meetings', 'member', 'pandemic disease', 'prevent', 'racial minority', 'response', 'structured data', 'symptomatic improvement', 'unstructured data', 'venous thromboembolism']",NCATS,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,UL1,2020,768511,0.029744794122177738
"Improving Quality of Electronic Health Record Ophthalmic Data for Big Data Analytics PROJECT SUMMARY The widespread adoption of EHRs has enabled the collection of massive amounts of digital ophthalmic data which have great potential for secondary use in research, quality improvement, and clinical decision support. While the amount of digital ophthalmic data recorded in the EHR is substantial and could be analyzed using the latest techniques for big data, questions about the quality of the data are a barrier to its reuse. Now that the American Academy of Ophthalmology has aggregated digital ophthalmic data from the EHR into the IRIS Registry, data quality is even more imperative for reaching the potential of the registry. To date, there has not be a comprehensive evaluation of the data quality of digital ophthalmic data, nor have there been any solutions for improving its quality. These are important gaps that will limit the utility of EHR data as a tool for knowledge discovery in ophthalmology. The goal of this grant is to assess the quality of digital ophthalmic exam data in order to improve its ability to be reused for research. Our hypothesis is that studying the variability of data quality in large datasets will provide insights into improving its quality. The first aim employs an established framework for data quality analysis to assess the intrinsic quality of a single institution’s EHR data as well as its fitness for use--the ability to be applied to a particular research scenario. In this proposal, we are evaluating the data’s ability to identify patient cohorts for clinical trials and to accurately calculate outcome based clinical quality measures. The variability in data’s quality and fitness among providers, subspecialties, diagnoses, and visit types will be analyzed. The second aim validates the analysis of the first aim by repeating it for all of the ophthalmic data in the IRIS Registry. For this analysis, the differences in quality and fitness between institutions and EHR vendors will also be assessed, along with the barriers to data quality and reuse. For both aims, ophthalmology experts will review the results to make recommendations for improving data quality and utility for digital ophthalmic data. In the future, these recommendations will provide a direction for correcting these quality issues and for ultimately advancing knowledge discovery in ophthalmic care. PROJECT NARRATIVE Electronic health records (EHRs) have not yet reached their potential for transforming healthcare, particularly for reusing clinical data for research. The American Academy of Ophthalmology has aggregated ophthalmic data from the EHR into the IRIS Registry, and data quality is even more imperative to achieve to reach the potential of this registry. Using methodological data quality analysis, we will analyze the quality of a single institution’s ophthalmic data and again for multiple institutions’ ophthalmic data in the IRIS Registry, documenting barriers for data quality and reuse that will lead to improving knowledge discovery from this data.",Improving Quality of Electronic Health Record Ophthalmic Data for Big Data Analytics,9953684,R21EY031443,"['Academy', 'Adoption', 'Age related macular degeneration', 'American', 'Big Data', 'Big Data Methods', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Cohort Studies', 'Collection', 'Complex', 'Data', 'Data Discovery', 'Diabetic Retinopathy', 'Diagnosis', 'Disease Progression', 'Electronic Health Record', 'Evaluation', 'Exfoliation Syndrome', 'Eye', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Hand', 'Health Sciences', 'Healthcare', 'Human', 'Image', 'Individual', 'Informatics', 'Institutes', 'Institution', 'Intelligence', 'Iris', 'Knowledge Discovery', 'Manuals', 'Measurement', 'Measures', 'Medical Informatics', 'Medicine', 'Methodology', 'Methods', 'Ophthalmology', 'Oregon', 'Outcome', 'Patients', 'Process', 'Provider', 'Recommendation', 'Registries', 'Research', 'Research Personnel', 'Retinopathy of Prematurity', 'Secondary to', 'Source', 'Structure', 'System', 'Techniques', 'Terminology', 'Treatment outcome', 'Universities', 'Vendor', 'Vision', 'Visit', 'base', 'clinical decision support', 'clinical encounter', 'cohort', 'data framework', 'data harmonization', 'data quality', 'data registry', 'data reuse', 'data submission', 'deep learning', 'digital', 'electronic data', 'experience', 'fitness', 'improved', 'insight', 'large datasets', 'large scale data', 'research study', 'tool', 'treatment comparison']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R21,2020,231000,0.030172405877758986
"Semi-supervised Algorithms for Risk Assessment with Noisy EHR Data PROJECT SUMMARY Large electronic health record research (EHR) data integrated with -omics data from linked biorepositories have expanded opportunities for precision medicine research. These integrated datasets open opportunities for developing accurate EHR-based personalized cancer risk and progression prediction models, which can be easily incorporated into clinical practice and ultimately realize the promise of precision oncology. However, efficiently and effectively using EHR for cancer research remains challenging due to practical and methodological obstacles. For example, obtaining precise event time information such as time of cancer recurrence is a major bottleneck in using EHR for precision medicine research due to the requirement of laborious medical record review and the lack of documentation. Simple estimates of the event time based on billing or procedure codes may poorly approximate the true event time. Naive use of such estimated event times can lead to highly biased estimates due to the approximation error. Such biases impose challenges to performing pragmatic trials when the study endpoint is time to events and captured using EHR. The overall goal of this proposal is to fill these methodological gaps in risk assessment for cancer research using EHR data, which will advance our ability to achieve the promise of precision oncology. Statistical algorithms and software will be developed to (i) automatically assign event time information using longitudinally recorded EHR information; and (ii) to perform accurate risk assessment using noisy proxies of event times. The proposed tools for risk assessment using imperfect EHR data without requiring extensive manual chart review could greatly improve the utility of EHR for oncology research. PROJECT NARRATIVE This proposal addresses major methodological gaps in effectively utilizing EHR data for risk assessment due to the noisy nature of EHR data. We propose novel statistical algorithms and software to (i) efficiently annotate event time by combining multiple longitudinally recorded code information and (ii) to provide precise risk estimates using noisy proxies of event times. By drawing upon the rich albeit imperfect data from bio-repository linked EHR, our algorithms will advance our ability to use EHR data for precision oncology research.",Semi-supervised Algorithms for Risk Assessment with Noisy EHR Data,9955220,R21CA242940,"['Address', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Biological', 'Boston', 'Cancer Patient', 'Clinical', 'Clinical Trials', 'Code', 'Cohort Studies', 'Data', 'Data Set', 'Diagnosis', 'Documentation', 'Electronic Health Record', 'Event', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Infrastructure', 'Label', 'Laboratories', 'Lead', 'Link', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measurement', 'Medical Records', 'Methodology', 'Methods', 'Nature', 'Oncology', 'Patients', 'Phenotype', 'Procedures', 'Progression-Free Survivals', 'Proxy', 'Registries', 'Research', 'Risk', 'Risk Assessment', 'Risk Estimate', 'Software Tools', 'Source', 'Statistical Algorithm', 'Supervision', 'Testing', 'Time', 'adjudicate', 'anticancer research', 'base', 'biobank', 'cancer recurrence', 'cancer risk', 'clinical practice', 'cohort', 'electronic data', 'evidence base', 'genomic data', 'improved', 'large datasets', 'learning algorithm', 'multimodality', 'novel', 'patient population', 'patient subsets', 'pragmatic trial', 'precision medicine', 'precision oncology', 'predictive modeling', 'repository', 'supervised learning', 'tool', 'tumor progression', 'user friendly software', 'web site']",NCI,HARVARD SCHOOL OF PUBLIC HEALTH,R21,2020,177510,0.030555736051897923
"EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia PROJECT SUMMARY/ABSTRACT Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by elevated plasma low-density lipoprotein cholesterol (LDL-C) and a dramatically increased lifetime risk for premature atherosclerotic cardiovascular disease (ASCVD). Available data suggest substantial under treatment of individuals with FH, and it is estimated that <5% of prevalent FH cases in the US are diagnosed and treated. The proposed research will develop electronic health record (EHR)-based strategies to reduce mortality and morbidity from FH. We will develop and validate a phenotyping algorithm for rapid and efficient identification of FH cases thereby enabling EHR-based surveillance of FH. We will deploy the phenotyping algorithm in the population-based setting of Olmsted County, Minnesota, to estimate prevalence and provide hitherto unavailable data on awareness, detection and control of FH. We will develop CDS to help care providers manage FH patients and an FH-specific decision aid to facilitate shared decision making related to lipid-lowering therapy and screening of family members. To accomplish these goals, we will leverage the following resources: a) the electronic phenotyping expertise available in the electronic Medical Records and Genomics (eMERGE) network; b) the Rochester Epidemiology Project (REP), that links medical records of Olmsted County MN residents thereby capturing nearly all health care delivered to residents of the community; and c) expertise in developing and deploying CDS in the EHR and in creating decision aids for disclosing cardiovascular risk and the benefits of lipid-lowering drugs. Our specific aims are: Aim 1. Develop and validate an electronic phenotyping algorithm to rapidly identify FH cases from the EHR. Aim 2. Conduct an e- epidemiology study to obtain hitherto unknown data regarding prevalence, awareness, detection, control of FH in a population-based setting in the US. Aim 3. a) Develop EHR-based tools to help care providers manage FH and facilitate shared decision making and cascade screening and b) assess outcomes after implementation of CDS and decision aid. The proposed research will enable rapid identification of FH in EHRs, provide hitherto unavailable data on the burden of FH in the community, facilitate EHR-based strategies for early detection, increase awareness of FH among care providers, provide guidance for management of FH at point of care and help both patients and providers make informed decisions about drug therapy and screening of family members. These are critical steps for early detection and treatment of FH to reduce the burden of premature ASCVD due to this condition. PROJECT NARRATIVE Familial hypercholesterolemia (FH) is a relatively common genetic disorder characterized by high cholesterol levels and increased risk of heart attack or sudden cardiac death. The proposed research will develop electronic health record (EHR)-based strategies to prevent adverse outcomes such as heart attack in FH patients. These include methods to rapidly identify FH patients, estimate prevalence of FH and develop clinical decision support to help care providers manage FH patients. The proposed work will have a significant impact on clinical management of FH patients.",EHR-Based Strategies to Improve Outcomes in Familial Hypercholesterolemia,9938607,R01HL135879,"['Academy', 'Address', 'Algorithms', 'Atherosclerosis', 'Awareness', 'Benefits and Risks', 'Cholesterol', 'Clinic', 'Clinical', 'Clinical Management', 'Communities', 'County', 'Data', 'Data Set', 'Decision Aid', 'Decision Making', 'Detection', 'Diagnosis', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Early treatment', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Epidemiology', 'Europe', 'Familial Hypercholesterolemia', 'Family', 'Family history of', 'Family member', 'Genetic Diseases', 'Genomics', 'Goals', 'Healthcare', 'Healthcare Systems', 'Hereditary Disease', 'Individual', 'Infrastructure', 'Institutes', 'LDL Cholesterol Lipoproteins', 'Label', 'Laboratories', 'Learning', 'Link', 'Lipids', 'Lipoprotein (a)', 'Low-Density Lipoproteins', 'Manuals', 'Medical Records', 'Medicine', 'Methods', 'Minnesota', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phenotype', 'Plasma', 'Population Study', 'Predictive Value', 'Prevalence', 'Prevalence Study', 'Primary Prevention', 'Provider', 'Public Health', 'Recommendation', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Resources', 'Risk', 'Structure', 'Test Result', 'Time', 'Work', 'World Health Organization', 'Xanthomas', 'adverse outcome', 'base', 'cardiovascular risk factor', 'care providers', 'case-based', 'clinical decision support', 'clinical implementation', 'epidemiology study', 'evaluation/testing', 'family burden', 'genetic testing', 'implementation science', 'improved', 'improved outcome', 'inhibitor/antagonist', 'innovation', 'lifetime risk', 'mortality', 'novel therapeutics', 'phenotyping algorithm', 'point of care', 'population based', 'precision medicine', 'premature', 'prevent', 'screening', 'screening program', 'shared decision making', 'sudden cardiac death', 'tool']",NHLBI,MAYO CLINIC ROCHESTER,R01,2020,638566,0.03351939063687697
