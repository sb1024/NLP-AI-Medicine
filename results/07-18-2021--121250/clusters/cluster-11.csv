text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,9739919,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Base Management', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistics', 'subchondral bone', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,588354,-0.008098744503399565
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9713512,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2019,647706,-0.017519750073882862
"AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space Project Summary  An extensible analysis platform will be developed to accurately perform the automated genotyping of PCR/capillary electrophoresis (CE) traces for multiple disease-associated short tandem repeater (STR) assays. This study will evaluate the feasibility of developing generalizable and adaptive molecular analysis models, and will ultimately establish a new paradigm for deep learning analytical tools in the molecular diagnostic space.  Advanced machine learning strategies will be applied to interpret genotypes of inherited disorders caused by genetically unstable STR DNA sequences. STRs have traditionally been difficult to investigate due to their length (on the order of kilobases) and low sequence complexity, which elude detection by traditional and next- generation sequencing technologies. However, advances in PCR/CE technology have enabled the amplification and fragment sizing of STR DNA fragments, advancing clinical research and diagnostic test development for several neurodegenerative disorders, such as fragile X syndrome and amyotrophic lateral sclerosis. Despite these advances, the analysis of PCR/CE data from assays targeting STRs remains a manual, burdensome, and subjective process. There is a clear need to create a system that can scale with the development of new assays, and the proposed approach utilizes modern breakthroughs in artificial intelligence to fulfill that need.  This method will leverage recent advances in representation learning to establish a generalized and adaptive framework for automated PCR/CE annotation that can scale to new assays and improve automatically with the inclusion of new data. The project will benefit from Asuragen’s experience in optimizing repeat-primed chemistries to develop and commercialize multiple high performance assays including the AmplideX PCR/CE FMR1 kit. Importantly, the proposed modeling strategy will borrow-strength across multiple established PCR/CE assays and generalize to future PCR/CE assays for novel STR disease associated biomarkers. This system will be paramount to enabling a continuous learning platform wherein computationally-assisted annotation of PCR/CE assays can be continuously improved and integrated in to clinical research tools and diagnostics. Project Narrative  We are developing AmplideX DeepNet, an artificial intelligence-based analysis system that can accurately perform computationally-assisted analysis of molecular diagnostic assays. The proposed system will build upon recent breakthroughs in artificial intelligence to allow it to easily adapt to new assays and to continue to improve. The system will be applied to assays for several disorders, including fragile X syndrome, amyotrophic lateral sclerosis (ALS), myotonic dystrophy, and Huntington’s disease, and will provide a number of benefits over current analysis methods by reducing turn-around time for assay results and assuring reproducible reporting between operators and labs.","AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space",9678895,R43GM128498,"['Alleles', 'American', 'Amyotrophic Lateral Sclerosis', 'Artificial Intelligence', 'Automated Annotation', 'Biological Assay', 'Biological Markers', 'C9ORF72', 'Capillary Electrophoresis', 'Chemistry', 'Clinical', 'Clinical Research', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'FMR1', 'FMR1 repeat', 'Fragile X Syndrome', 'Future', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Guidelines', 'Hand', 'Hereditary Disease', 'Heritability', 'Huntington Disease', 'Interruption', 'Learning', 'Length', 'Machine Learning', 'Manuals', 'Medical Genetics', 'Methods', 'Modeling', 'Modernization', 'Molecular Analysis', 'Myotonic Dystrophy', 'Neurodegenerative Disorders', 'Nucleotides', 'Pathogenicity', 'Performance', 'Phase', 'Process', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Running', 'Sampling', 'Short Tandem Repeat', 'System', 'Systems Analysis', 'Technology', 'Testing', 'Time', 'Training', 'analysis pipeline', 'analytical tool', 'automated analysis', 'base', 'clinical diagnostics', 'cohort', 'computer framework', 'deep learning', 'design', 'diagnostic assay', 'experience', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'heuristics', 'human-in-the-loop', 'improved', 'instrumentation', 'learning progression', 'learning strategy', 'medical schools', 'molecular diagnostics', 'nervous system disorder', 'next generation sequencing', 'novel', 'research and development', 'success', 'tool']",NIGMS,"ASURAGEN, INC.",R43,2019,269217,-0.007216194304257942
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9747977,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,115051,-0.029085621524836524
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9771473,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Hazard Models', 'Health system', 'Individual', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning algorithm', 'learning strategy', 'multimodality', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'prognostic', 'repository', 'response', 'risk prediction model', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2019,251983,-0.031474850643843415
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9645433,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2019,92070,-0.02804690157493218
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9743225,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2019,528283,0.01644675272617869
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9756832,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Trauma', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'deep learning', 'design', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neurophysiology', 'novel', 'predictive modeling', 'prevent', 'random forest', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,250346,-0.014451860133832166
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9636581,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2019,489919,0.007600649464473578
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9803774,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2019,631809,0.002977694011856563
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9724345,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'random forest', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2019,292500,-0.02564741919380322
"SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder  Intellectual Merit: This project will for the first time provide the fundamental tools to integrate unique multimodal data toward screening, diagnosis, and intervention in eating disorders, with an initial focus on children with ARFID and related developmental and health disorders. This work is critical for enriching the understanding of healthy development and for broadening the foundations of behavioral data science. ARFID ·motivates the development of new computer vision and data analysis tools critical for the analysis of multidimensional behavioral data. The main aims are: 1. Develop and user individualized and integrated continuous facial affect coding from videos to discern affective motivations for food avoidance, critical due to the unique sensory aspects of eating disorders, and resulting from active stimulation via friendly and carefully designed images/videos and real food presentation; 2. Use data analysis and machine learning to derive sensory profiles based on patterns of food consumption and preference from existing unique datasets of selective eaters; and 3. Translate the tools developed in Aims 1 and 2 into the clinic and home to assess the capacity of these tools to define a threshold of clinically significant food avoidance, to detect change in acceptability of food with repeated presentations, and to examine and modify the accuracy of our food suggestion algorithms. Broader Impacts: The impact of this application comprises two broad domains. First is the derivation of processes, tools, and strategies to analyze very disparate data across multiple levels of analysis and to codify those strategies to inform similar future work, in particular incorporating automatic behavioral coding. Second is the exploitation of these tools to address questions about the emergence of healthy/unhealthy food selectivity across the lifespan, including recommendation delivery via apps and at-home recordings. The health impact of even partial success in this project is very broad and significant. Undergraduate students will be involved in this project via the 6-weeks summer research program at the Information Initiative at Duke, a center dedicated to the fundamentals of data science and its applications; via the co-Pl's research lab devoted to eating disorders; and via the Pl's project dedicated to training undergraduate students to address eating disorders of their friends via an anonymous app. Outreach and dissemination will follow the broad use of the developed app, both in the clinic and the general population, including the Pl's connections with low-income and under-represented bi-lingual preK. RELEVANCE (See instructions): Eating disorders are potentially life-threatening mental illnesses affecting the general population; -90% of individuals never receive treatment, in part due to lack of awareness and access. Individuals with eating disorders experience a diminished quality of life, high mental and physical illness comorbidities, and an existence marked by profound loneliness and isolation. Combining expertise in eating disorders with computer vision and machine learning, we bring for the first time data science to this health challenge. PROJECT/PERFORMANCE S1TE(S) (If addItIonal space Is needed use Project/Performance Stte Format Page) n/a",SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder ,9927093,R01MH122370,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anxiety', 'Assessment tool', 'Attention', 'Awareness', 'Behavior Therapy', 'Behavioral', 'Caregivers', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Code', 'Comorbidity', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Depressed mood', 'Derivation procedure', 'Development', 'Diagnosis', 'Diet', 'Disease', 'Distress', 'Eating', 'Eating Disorders', 'Emotional', 'Emotions', 'Evolution', 'Exposure to', 'Face', 'Family', 'Food', 'Food Patterns', 'Food Preferences', 'Foundations', 'Friends', 'Fright', 'Future', 'General Population', 'Goals', 'Health', 'Health Personnel', 'Home environment', 'Image', 'Impairment', 'Individual', 'Industry', 'Instruction', 'Intervention', 'Life', 'Link', 'Literature', 'Loneliness', 'Longevity', 'Low income', 'Machine Learning', 'Maps', 'Mathematics', 'Measures', 'Mental disorders', 'Monitor', 'Motion', 'Motivation', 'Parents', 'Performance', 'Phenotype', 'Primary Health Care', 'Process', 'Psyche structure', 'Quality of life', 'Reaction', 'Recommendation', 'Research', 'Scientist', 'Sensory', 'Severities', 'Smell Perception', 'Standardization', 'Structure', 'Suggestion', 'System', 'Taste aversion', 'Time', 'Training', 'Translating', 'Uncertainty', 'Work', 'analytical tool', 'base', 'behavior change', 'clinically significant', 'computerized tools', 'design', 'experience', 'food avoidance', 'food consumption', 'gaze', 'improved', 'indexing', 'mathematical algorithm', 'multimodal data', 'novel', 'outreach', 'precision medicine', 'preference', 'programs', 'relating to nervous system', 'response', 'screening', 'success', 'summer research', 'tool', 'undergraduate student', 'wasting', 'willingness']",NIMH,DUKE UNIVERSITY,R01,2019,253545,-0.04501427277195322
"Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records PROJECT SUMMARY  Today, more patients can access their health records online than ever before. However, clinical acronyms hinder patients' comprehension of their records and decrease the benefits of transparency. An automated system for expanding clinical acronyms should have major clinical significance and far-reaching consequences for improving patient-provider communication, shared decision-making, and health outcomes. Existing systems have limited power to expand clinical acronyms, primarily due to the lack of comprehensiveness (or generali- zability) of existing acronym sense inventories. Because developing comprehensive sense inventories is difficult, existing knowledge engineering methods have primarily focused on developing institution-specific sense inventories. Institution-specific sense inventories may not be generalizable to other geographical regions and medical specialties. Furthermore, developing an institution-specific sense inventory at every US healthcare organization is not feasible, especially without automated methods which currently do not exist.  I developed advanced knowledge engineering methods to overcome these limitations through the use of fully automated techniques to generalize existing sense inventories from different geographical regions and medical specialties. My methods leverage the extensive resources already devoted to developing institution- specific sense inventories in the U.S., and may help generalize existing sense inventories to institutions without the resources to develop them. Although promising, challenges remain with the optimization and evaluation of these methods. The objective of the proposed project is to use knowledge engineering to improve patients' comprehension of their health records, focusing specifically on clinical acronyms. In Aim 1, I will develop new knowledge engineering methods to facilitate the automated integration of sense inventories, using literature- based quality heuristics and a Siamese neural network to establish synonymy. I will evaluate these methods using multiple metrics to assess redundancy, quality, and coverage in two test corpora with over 17 million clinical notes. In Aim 2, I will evaluate whether the knowledge engineering methods improve comprehension of doctors' notes in 60 hospitalized patients with advanced heart failure. With success, I will create novel, automated knowledge engineering methods that can be directly applied to improve patient care. This research is in support of my mentored doctoral training at Columbia University Department of Biomedical Informatics (DBMI) under Drs. David Vawdrey, George Hripcsak, Carol Friedman, Suzanne Bakken, and Chunhua Weng, and will include coursework on deep learning, oral presentations at major annual conferences, and career development planning, among other activities. DBMI is frequently recognized as one of the oldest and best programs of its kind in the world, and provides an exception training environment for my development into an independent and productive academic investigator. PROJECT NARRATIVE Clinical acronyms make it difficult for patients to understand their medical records, decreasing the benefits of transparency. This project applies advanced knowledge engineering methods and machine learning to generate comprehensive acronym sense inventories used to aid consumers' comprehension of their health records. The project is in support of the applicant's mentored doctoral dissertation research.",Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records,9681711,F31LM013054,"['Abbreviations', 'Award', 'Clinical', 'Clinical Medicine', 'Comprehension', 'Controlled Vocabulary', 'Development', 'Development Plans', 'Engineering', 'Environment', 'Equipment and supply inventories', 'Evaluation', 'Future', 'Geographic Locations', 'Goals', 'Grant', 'Health', 'Healthcare', 'Heart failure', 'Hospitals', 'Informatics', 'Information Resources', 'Institution', 'Knowledge', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Mentors', 'Mentorship', 'Methods', 'Natural Language Processing', 'Oral', 'Outcome', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Positioning Attribute', 'Publishing', 'Questionnaires', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Safety', 'Source', 'Support System', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Unified Medical Language System', 'Universities', 'acronyms', 'base', 'biomedical informatics', 'career development', 'clinically significant', 'deep learning', 'doctoral student', 'federal policy', 'health care service organization', 'health record', 'heuristics', 'improved', 'information organization', 'medical specialties', 'method development', 'multidisciplinary', 'neural network', 'novel', 'patient portal', 'patient-clinician communication', 'programs', 'shared decision making', 'success', 'symposium', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,F31,2019,50016,-0.01475216602598943
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,9885663,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Traction', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'success', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2019,206139,0.0013878550358259747
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,-0.014799574920723653
"Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations Q-Chem is a state-of-the-art commercial computational quantum chemistry program that has aided about 60,000 users in their modeling of molecular processes in a wide range of disciplines, including biology, chemistry, and materials science. In this proposal, we seek to significantly reduce the computational time (now around 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions. Specifically, we propose to use a multiple time step (MTS) simulation method, where a low-level (and less accurate) quantum chemistry method is used to propagate the system (i.e. move all atoms) at each time step (usually 0.5 or 1 fs), and then a high-level (i.e. more accurate and expensive) quantum chemistry method is used to correct the force on the atoms at longer time intervals. In this way, the simulation can be performed at the high-level energy surface in a fraction of time, compared with simulations performed only using the high-level quantum chemical method. In the Phase I proposal, our goal is to allow the high-level force update only once every 40—50 fs by identifying appropriate lower-level theories (Aim 1) and incorporating machine-learning techniques (Aim 2). This will accelerate accurate free energy simulations by 20—25 fold, reducing the overall computer time to around 25,000 CPU hours. Thus, our new MTS simulation method will make it feasible to routinely perform computational studies on enzymatic reaction mechanism. The addition of these new tools will also further strengthen Q-Chem's position as a global leader in the molecular modeling software market, making our program the most efficient and reliable computational quantum chemistry package for simulating large, complex chemical/biological systems. In this project, we seek to significantly reduce the computational time (ca. 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions to ca. 25,000 CPU Hours. Building upon sophisticated quantum mechanics, this can lead to reliable and quick predictions of enzyme activities.",Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations,9778517,R43GM133270,"['Acceleration', 'Accounting', 'Adopted', 'Back', 'Biochemical', 'Biochemical Reaction', 'Biology', 'Biomedical Research', 'Chemicals', 'Chemistry', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Development', 'Discipline', 'Enzymes', 'Foundations', 'Free Energy', 'Goals', 'Hour', 'Hybrids', 'Lead', 'Machine Learning', 'Maps', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Pathway interactions', 'Performance', 'Phase', 'Positioning Attribute', 'Potential Energy', 'Process', 'Protein Conformation', 'Proteins', 'Quantum Mechanics', 'Reaction', 'Recipe', 'Research', 'Research Personnel', 'Sampling', 'Scheme', 'Solvents', 'Surface', 'System', 'Techniques', 'Time', 'Update', 'biological systems', 'computer studies', 'cost', 'density', 'enzyme activity', 'enzyme model', 'improved', 'innovation', 'learning strategy', 'materials science', 'molecular mechanics', 'molecular modeling', 'programs', 'quantum', 'quantum chemistry', 'quantum computing', 'simulation', 'theories', 'time interval', 'tool']",NIGMS,"Q-CHEM, INC.",R43,2019,132011,-0.013782019206779865
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,-0.003705210241865795
"Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization Project Summary  This application is being submitted in response to NOT-RM-19-009 as a supplement to the parent award U54NS093793.  The Common Fund supports a number of resources that can significantly enhance gene and variant prioritization for study in the Model Organisms Screening Center of the Undiagnosed Diseases Network and beyond. To facilitate the use of these resources, we propose to create a tool that can be easily accessed by clinical geneticists and model organism scientists alike.  MARRVEL (Model organism Aggregated Resources for Rare Variant ExpLoration) was created two years ago because important data that is necessary for rare variant analysis for personalized medicine is spread throughout the internet in tens of different locations. To improve efficiency and streamline access to these data sources, we created a web-tool that allows users to query tens of data sources at once, including GTEx, and links to IMPC, the display portal for KOMP2.  In this proposal, our goal is to develop version 2 of MARRVEL to promote the use of Common Fund resources in the rare disease research community for manual and automated data analysis. This goal will be accomplished by developing MARRVEL 2.0 by integrating KOMP2 (IMPC) and PHAROS data and using the aggregated dataset to develop a machine-assisted gene and variant prioritization for diagnosis and animal model generation.  Our goals align with those of the NIH Common Fund to increase the utility of resources for broader use in the biomedical community. Project Narrative  We aim to promote the use of Common Fund resources and facilitate the diagnosis of rare diseases and the subsequent generation of animal models for the Undiagnosed Diseases Network and beyond. This goal will be accomplished by developing the web resource, MARRVEL 2.0.",Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization,9984757,U54NS093793,"['Affect', 'Animal Model', 'Artificial Intelligence', 'Award', 'Clinical', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Data Sources', 'Development', 'Diagnosis', 'Discipline', 'Disease', 'Disease model', 'Drosophila genus', 'Drug Targeting', 'Expert Systems', 'Family', 'Funding', 'Generations', 'Genes', 'Genetic Diseases', 'Genotype-Tissue Expression Project', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genetics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Manuals', 'Medical', 'Medical Genetics', 'Modeling', 'Mus', 'Parents', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Proteins', 'Rare Diseases', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Suggestion', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Time', 'Training', 'United States National Institutes of Health', 'Variant', 'Visit', 'Yeasts', 'Zebrafish', 'base', 'data wrangling', 'design', 'experimental study', 'feeding', 'fly', 'genetic disorder diagnosis', 'genetic variant', 'human data', 'improved', 'interest', 'learning community', 'machine learning algorithm', 'model organisms databases', 'online resource', 'personalized medicine', 'phenotypic data', 'rare genetic disorder', 'rare variant', 'response', 'screening', 'supervised learning', 'tool', 'web-based tool']",NINDS,BAYLOR COLLEGE OF MEDICINE,U54,2019,320000,0.0005100255336680278
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9671422,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Ingestion', 'Kinetics', 'Laboratories', 'Locomotion', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'automated image analysis', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'machine learning algorithm', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2019,475637,-0.010477663330421055
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9642618,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Effectiveness', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Imagery', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Universities', 'Virginia', 'absorption', 'artificial neural network', 'base', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2019,375602,-0.0077085986250331785
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9731665,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'informatics\xa0tool', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2019,599962,-0.005166595680152352
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9656981,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2019,431185,-0.029803465997532384
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,9859232,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'One-Step dentin bonding system', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2019,345016,-0.007421085906148131
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9857305,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'machine learning algorithm', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2019,249000,-0.001815144945396715
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9772886,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics\xa0tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2019,663419,-0.011268172344396055
"Quantifying causality for neuroscience Abstract: Causality is central to neuroscience. For example, we might ask about the causal effect of a neuron on another neuron, or its influence on perception, action, or cognition. Moreover, any medical approaches aim at producing a causal effect – effecting improvements for patients. Randomized controlled trials (RCTs) are the gold standard to establish causality, but they are not always practical. For example, while we can electrically or optogenetically activate entire areas, large-scale targeted stimulation of individual neurons is hard. Other ways of establishing causality are problematic: if we observe a correlation it is hard to know its cause. The problem is confounding: there are variables that we do not record that affect the variables we do. This also renders model comparisons problematic – a causally wrong model with few parameters may well fit the observed data better than a causally correct one with many parameters. We thus need data analysis tools that allow authoritatively asking causal questions without the need for random perturbation experiments.  Just like neuroscience now, the field of econometrics once focused on correlations. But since the 1980s, empirical economics has undergone a so-called credibility revolution, requiring the development of rigorous methods to establish causality. Several successful methods have emerged to become the workhorses of empirical economics. The idea underlying these methods is that if one can observe variables that approximate random perturbations, then one can still discover causal relations. This is what economists call a quasi-experiment. We here propose to carry over such quasi-experimental techniques to neuroscience. For example in neuroscience, if there is a random variable that affects only one neuron, then any activity in other neurons correlated with that variable must be causally affected by the neuron. Another famous quasi- experimental method is regression discontinuity design (RDD). This approach effectively uses the noise introduced at the threshold to identify causal relations. Importantly, such techniques have, thanks to decades of research in econometrics, very well understood statistical properties. These approaches promise to considerably enrich the approaches towards causality we have in neuroscience. We have a strong interdisciplinary team, spanning economics, experimental, and computational neuroscience, collaborating on adapting these quasi-experimental techniques to problems in neuroscience through a combination of machine learning and domain-specific engineering. This promises to be a major advance relative to current techniques that generally approach causality in neuroscience through model comparison. Project Narrative: The goal of this project is to develop a set of computational techniques that allow neuroscientists to quantify how neurons causally influence one another. To do so, it utilizes approaches popular in econometrics called quasiexperiments. Such approaches to quantify causality is important as medical perturbations of brains, e.g. treatments of epilepsy or depression are aimed at effecting or causing a change in the brain.",Quantifying causality for neuroscience,9775861,R01EB028162,"['Affect', 'Algorithms', 'Area', 'Brain', 'Code', 'Cognition', 'Communities', 'Computational Technique', 'Confounding Factors (Epidemiology)', 'Data', 'Data Analyses', 'Development', 'Economics', 'Engineering', 'Epilepsy', 'Etiology', 'Glean', 'Goals', 'Gold', 'Individual', 'Injections', 'Intervention', 'Learning', 'Machine Learning', 'Medical', 'Mental Depression', 'Methods', 'Modeling', 'Modernization', 'Neurons', 'Neurosciences', 'Noise', 'Organism', 'Output', 'Patients', 'Perception', 'Performance', 'Physiological', 'Property', 'Quasi-experiment', 'Randomized Controlled Trials', 'Refractory', 'Research', 'Synapses', 'Techniques', 'base', 'computational neuroscience', 'design', 'econometrics', 'experimental study', 'improved', 'optogenetics', 'phrases', 'relating to nervous system', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2019,803000,-0.024899529924372086
"Big Omics Data Engine 2 Supercomputer Computational and data science has transformed biomedical scientific discovery: its approaches are embedded into a wide range of workflows for diseases such as schizophrenia, depression, Alzheimer's, epilepsy, influenza, autism, drug addiction, pediatric cardiac care, Inflammatory Bowel Disease, prostate cancer and multiple myleloma. Sixty-one basic and translational researchers at Mount Sinai representing over $100 million in NIH funding, along with their collaborators from 75 external institutions, have utilized the Big Omics Data Engine (BODE) supercomputer to elucidate significant scientific findings in over 167 publications, including high impact journals such as Nature and Science, with 2,427 citations in three years. These researchers have also shared the data generated on BODE throughout their consortia and into national data sharing repositories. BODE is nearing the end of its vendor maintainable life, and researchers need increased computational throughput and storage space. To empower researchers to not only continue their inquiries, but to also tackle more complex scientific questions with decreased time to solution, we propose the Big Omics Data Engine 2 Supercomputer (BODE2). BODE2 will contain a total of 3,200 Intel Cascade Lake cores with 15 terabytes of memory and 14 petabytes of raw storage, and will leverage an existing 250 terabytes of SSDs. An instrument of this size is not available elsewhere affordably. With the proposed instrument, researchers will be able to take advantage of three major benefits: (1) the ability to receive results faster for overall greater scientific throughput; (2) the ability to increase the fidelity of their simulations and analyses; and (3) the ability to migrate research applications seamlessly to the software environment for greater scientific productivity. As with data produced on BODE, BODE2 data products will also be shared with the broader scientific community. BODE2 will provide the critical infrastructure needed by the wide range of researchers and clinicians for the genetics and population analysis, gene expression, machine learning and structural and chemical biology approaches used to make advances in these diseases. A specialized Big Omics Data Engine 2 Supercomputer instrument will provide necessary computational and data science infrastructure for 61 research projects with 75 collaborating institutions in diverse areas such as Alzheimer's, autism, schizophrenia, drug addiction, influenza, pediatric cardiac care, depression, epilepsy, prostate cancer and multiple myeloma. Data generated from this instrument will be shared in national databases.",Big Omics Data Engine 2 Supercomputer,9708160,S10OD026880,"['Alzheimer&apos', 's Disease', 'Biology', 'Cardiac', 'Caring', 'Chemicals', 'Childhood', 'Communities', 'Complex', 'Computational Science', 'Computer software', 'Data', 'Data Science', 'Disease', 'Drug Addiction', 'Environment', 'Epilepsy', 'Funding', 'Gene Expression', 'Inflammatory Bowel Diseases', 'Influenza', 'Infrastructure', 'Institution', 'Journals', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Memory', 'Mental Depression', 'Nature', 'Population Analysis', 'Productivity', 'Publications', 'Research', 'Research Personnel', 'Schizophrenia', 'Science', 'Structure', 'Time', 'United States National Institutes of Health', 'Vendor', 'autism spectrum disorder', 'data sharing', 'genetic analysis', 'instrument', 'petabyte', 'repository', 'simulation', 'supercomputer', 'terabyte', 'translational scientist']",OD,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,S10,2019,1998264,-0.0458733862273191
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9827788,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,33190,-0.040986075511068226
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9693030,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,36510,-0.040986075511068226
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9625823,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,343157,-0.040986075511068226
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9855180,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2019,702655,-0.0063645418313463984
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9911854,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2019,15000,-0.018150819274557455
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9786702,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2019,431816,-0.016500509255939714
"Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack Project Summary: Tufts University physics professor Allan Cormack pioneered the field of tomography. His seminal work, from 1963 and 1964, provided both the mathematical foundations of computerized tomography (CT), and tangible proof-of-concept by engineering a rudimentary CT scanner. Taken together, this effort represented the first practical method to ""see into"" an object without physically breaking it open. Along with the engineer Godfrey Hounsfield, he won the 1979 Nobel Prize in Physiology or Medicine for these contributions. Since then, tomography has broadened to include a wide range of modalities and problems. This field is unique for the rich interplay among applications in medicine, security, earth sciences, industry, physics, and the mathematics required to solve these problems. This international conference at Tufts, “Modern Challenges in Imaging: In the Footsteps of Allan Cormack” will honor the achievements of Cormack and reflect this diversity in the field by gathering top international researchers in mathematics, engineering, science, and medicine to communicate the most current research and challenges in the field. This will include work on mathematical models of emerging modalities, tomographic machine learning, dynamic methods, and spectral imaging with applications include medicine and security. The best research from the conference will be disseminated in a special issue of the journal Inverse Problems. Talks will be posted on the conference website. The organizers will recruit a diverse set of experienced participants and trainees, and the conference will be advertised in a range of publications reflecting the scientific and demographic diversity of the field. This conference is unique in that it combines high-level mathematical participants with experts in medical and industrial CT. It is structured to encourage participants from different fields to talk with each other, broaden their horizons, and make connections between problems and methodologies in the various fields. Several of the plenary talks will provide introductions to the areas. Trainees will be integrated into the conference through an informal welcome lunch and a poster session to introduce them to researchers in the field. This supports goals 1, 4, and 5, of the NIBIB: Researchers will present innovative biomedical technologies, engineering solutions, and mathematical methods to better image the body and objects more generally. The synergy between research areas will support the translation of technologies from the academic sphere to medical utility. The training opportunities for graduate students and beginners support the training of the next generation of diverse scientists. Project Narrative This conference will bring together medical, scientific, engineering, and applied mathematical researchers to present their newest research for a range of tomographic problems. Graduate students and beginners will be encouraged to participate and learn by being offered introductory talks, a student poster session, a welcome event, and an informal atmosphere. The conference will be structured so researchers will learn about important challenges in practical tomography as well as new techniques and methods, thereby creating synergies and research connections among the areas.",Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack,9837131,R13EB028700,"['Achievement', 'Advertising', 'Algorithms', 'Area', 'Biomedical Technology', 'Communication', 'Development', 'Earth science', 'Engineering', 'Environment', 'Event', 'Fertilization', 'Foundations', 'Goals', 'Image', 'Individual', 'Industrialization', 'Industry', 'International', 'Journals', 'Lead', 'Learning', 'Lightning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modality', 'Modernization', 'National Institute of Biomedical Imaging and Bioengineering', 'Nobel Prize', 'Outcome', 'Participant', 'Physics', 'Physiology', 'Population', 'Problem Solving', 'Publications', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Security', 'Seminal', 'Societies', 'Structure', 'Students', 'Techniques', 'Technology', 'Time', 'Training Support', 'Translations', 'Underrepresented Groups', 'Universities', 'Work', 'X-Ray Computed Tomography', 'cohort', 'demographics', 'design', 'experience', 'graduate student', 'higher level mathematics', 'informal atmosphere', 'innovation', 'mathematical methods', 'mathematical model', 'meetings', 'member', 'next generation', 'posters', 'professor', 'recruit', 'spectrograph', 'symposium', 'synergism', 'tomography', 'training opportunity', 'web site']",NIBIB,TUFTS UNIVERSITY MEDFORD,R13,2019,10000,-0.038818399522871895
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9747967,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2019,514129,-0.02038992505473483
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9789914,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2019,1500000,-0.01713388486205485
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9786847,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2019,747591,-0.024215923127083334
"Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences PROJECT SUMMARY/ABSTRACT In the era of newly emerging computational tools for data science, biostatisticians need to play a fundamental role in health sciences research. There is an urgent need to encourage US Citizens and Permanent Residents to pursue graduate training in biostatistics. The design, conduct, and analysis of clinical trials and observational studies; the setting of regulatory policy; and the conception of laboratory experiments have been shaped by the fundamental contributions of biostatisticians for decades. Advances in genomics, medical imaging technologies, and computational biology; the increasing emphasis on precision and evidence-based medicine; and the widespread adoption of electronic health records; demand the skills of biostatisticians trained to collaborate effectively in a multidisciplinary environment and to develop statistical and machine learning methods to address the challenges presented by this data-rich revolutionary era of health sciences research. The proposed summer program which includes world-renowned clinical scientists and biostatisticians from two local universities, will provide an immense opportunity for student participants to learn basic yet modern statistical methods that are critical to uncovering new insights from such big and complex biomedical data and also illustrate the potential pitfalls of confounding and bias that may arise when analyzing biomedical data. A unique feature of the proposed training program is thus to expose the participants to not only basic statistical methods but also to the topics of computer science and bioinformatics which will be invaluable in creating the multidisciplinary teams required to tackle the complex research questions that often requires multipronged approaches. The proposed six-week training program will be structured around the NIH's Translation Science Spectrum and will introduce participants to opportunities in biostatistics through the lens of the science advanced by the contributions of biostatisticians. Following an initial set of weeks on basic training of biostatistical methods, the program will culminate in a data hack-a-thon style competition in which participants will employ the statistical and scientific knowledge gained during the program to produce the most innovative, statistically-sound, scientifically-relevant and effectively-communicated response to a set of research questions. The proposed research education program will enroll up to 20 such participants from across the nation and, through lectures, field trips, and opportunities to analyze data from real health sciences, inspire them to pursue graduate training. The program will draw upon considerable past collaborations and complementary resources of two local world-renowned universities to provide participants with an unparalleled view of the field, including award-winning instructors, internationally known methodological and clinical researchers, and a local area rich in opportunities to showcase careers in biostatistics. Special efforts will be made to enroll participants from underrepresented groups. Participants will be followed after completion, and the numbers attending graduate school in statistics and pursuing biostatistics careers will be documented. PROJECT NARRATIVE Biostatisticians are indispensible contributors to health sciences research. The demand for professionals with advanced training in biostatistics is high and will continue to increase, especially with the expanding challenges posed by big biomedical data. This six week summer research education program, a joint effort of North Carolina State University and Duke University, will enroll up to 20 US citizen/permanent resident participants from across the nation in the summers of 2020-2022 and expose them to the opportunities presented by careers in biostatistics and encourage them to seek graduate training in the field.",Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences,9734597,R25HL147228,"['Address', 'Adoption', 'Area', 'Attention', 'Award', 'Bioinformatics', 'Biomedical Research', 'Biometry', 'Biostatistical Methods', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Conceptions', 'Data', 'Data Science', 'Development', 'Discipline', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Exposure to', 'Faculty', 'Future', 'Genomics', 'Goals', 'Health Sciences', 'Health system', 'Imaging technology', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Medical Imaging', 'Medical center', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Names', 'National Heart, Lung, and Blood Institute', 'North Carolina', 'Observational Study', 'Participant', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Program Effectiveness', 'Request for Applications', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Role', 'Schools', 'Science', 'Scientist', 'Statistical Methods', 'Strategic Planning', 'Structure', 'Students', 'Talents', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'analytical method', 'big biomedical data', 'career', 'career development', 'cohort', 'computer science', 'computerized tools', 'data resource', 'design', 'education research', 'experience', 'field trip', 'graduate student', 'health science research', 'innovation', 'insight', 'instructor', 'interest', 'investigator training', 'laboratory experiment', 'learning strategy', 'lectures', 'lens', 'multidisciplinary', 'next generation', 'programs', 'public health research', 'recruit', 'response', 'skills', 'sound', 'statistics', 'summer institute', 'summer program', 'summer research', 'tool', 'undergraduate student']",NHLBI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,249789,-0.03387743123846133
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9789907,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Infrastructure', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2019,366616,-0.003539252339474563
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,9851583,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'learning strategy', 'mHealth', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2019,741688,-0.02021415721040003
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9676043,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,308000,-0.012811118118361092
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. n/a",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9882672,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,149810,-0.015446362990411687
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. • ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. • UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types • U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database • St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9853317,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Infrastructure', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'multidimensional data', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2019,855741,-0.02788295357088705
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",9882865,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2019,340827,-0.012600909651988406
"Acquisition of a next-generation computing cluster We request funds to purchase our next-generation computing cluster to support computationally intensive NIH-funded research at Washington University in St. Louis. This system will become the foundation of the Center for High Performance Computing (CHPC) to support our active, diverse user community. It has been designed to meet our current and future computing needs. It adds additional capabilities to support emerging fields such as “Deep Learning”. The CHPC currently supports over 775 users from 300 different groups across 33 departments. 58 papers have cited the CHPC. The Center has a proven funding model and is economically sustainable. The Center has partnered with other University organizations to offer training workshops, not only on the use of the cluster, but also on introductory programming for users with no prior programming experience. If this proposal is funded, we will be able to continue to support this ever-growing diverse community of researchers. The proposed system would replace critical components including the management node, the login nodes, the storage, and upgrade the Infiniband networking. We would add substantial upgrades to our computing power with state-of-the-art processors, increased memory capacity for growing jobs, General Purpose Graphical Processing units (GPGPUs), and new capabilities for “Deep Learning”. Nearly all fields of NIH-funded research are faced with increasingly large data sets that require additional computing power to analyze. We propose building a next-generation computing cluster to support this research. Our Center has a proven track record in supporting a large, diverse group of users in all aspects of their computationally demanding research.",Acquisition of a next-generation computing cluster,9707936,S10OD025200,"['Communities', 'Educational workshop', 'Foundations', 'Funding', 'Future', 'High Performance Computing', 'Memory', 'Modeling', 'Occupations', 'Paper', 'Research', 'Research Personnel', 'System', 'Training', 'United States National Institutes of Health', 'Universities', 'Washington', 'cluster computing', 'deep learning', 'design', 'experience', 'next generation']",OD,WASHINGTON UNIVERSITY,S10,2019,597200,0.00666403007319628
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9737676,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Quality', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,804907,-0.007795703589082292
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,9882822,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2019,390806,-0.00429932375351552
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health.",New Jersey Alliance for Clinical Translational Science: NJ ACTS,9831333,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Community Health Centers', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2019,4776211,0.00023009824524779715
"The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers Project Summary  One of the most glaring yet easily addressable gaps in our current scientific workflow and publication system is improving the way that methods are reported, in particular, the lack of key methodological details necessary for interpreting and reproducing a study. Most authors continue to cite the name of the reagent, like an antibody using the vendor, and the city where the vendor is located, but omit the catalog and lot number making antibodies very difficult to track down, thereby reducing reproducibility of the paper. The Resource Identification, RRID, Initiative has successfully implemented a solution to this lack of identification, by asking authors to include a persistent unique identifier (RRID) for each antibody along with a standard syntax that includes the lot information. This syntax is now required in about 100 journals and accepted in at least 400, was accepted into the EQUATOR network of standards and is under consideration by the JATS committee, the NISO standard for journal article metadata. For antibodies, RRIDs are assigned by the AntibodyRegistry.org, which accepts full catalogs from antibody companies and individual antibody records from authors, who are unable to locate the record for the antibody that they used in a study or one which they created in their lab. This process should be made as easy as possible for authors, and through text analysis we have devised a set of tools that should help authors create better records with less work.  The AntibodyRegistry.org was created as part of an academic project and it has successfully incorporated millions of antibody records, thousands of which have now been cited in scientific papers using the RRID syntax. The use of RRID is growing, and in order to support the longer term sustainability of the AntibodyRegistry.org, a core community authority for RRIDs, this resource needs to be enhanced to align with the available commercial and non-commercial funding sources. We propose the addition of features, valuable to antibody companies and journals, to improve market intelligence and reporting around antibodies. We also propose to auto-generate antibody entries for authors and curators when submitting/curating an antibody to decrease the time it takes to complete the task, thereby reducing the barrier to entry and cost. Project Narrative The AntibodyRegistry.org is a catalog of antibodies used in research and serves as the antibody identifier source for the Research Resource Identifier (RRID) initiative. The use of these identifiers improves rigor and transparency in compliance with both journal and NIH standards. As the RRID initiative grows, the AntibodyRegistry.org is becoming an increasingly well-used and valuable resource; requiring increased attention from curation staff. Improvements to the AntibodyRegistry.org are needed to increase commercial value and thereby its sustainability, decrease curation cost, and provide a higher level of service to the scientific community.",The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers,9680073,R41GM131551,"['Adopted', 'Antibodies', 'Asia', 'Attention', 'Award', 'Businesses', 'California', 'Catalogs', 'Cities', 'Cloud Computing', 'Communities', 'Data', 'Data Analytics', 'Databases', 'Electronic Mail', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Funding', 'Funding Agency', 'Generations', 'Glare', 'Goals', 'Individual', 'Intelligence', 'International', 'Journals', 'Machine Learning', 'Marketing', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Names', 'Neurosciences', 'Paper', 'Performance', 'Phase', 'Process', 'Provider', 'Publications', 'Reagent', 'Records', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Services', 'Small Business Technology Transfer Research', 'Source', 'System', 'Text', 'Time', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Work', 'authority', 'base', 'cost', 'improved', 'information framework', 'journal article', 'syntax', 'text searching', 'tool']",NIGMS,"SCICRUNCH, INC.",R41,2019,149373,-0.003094131294699958
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,10012976,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2019,277141,-0.01015259411262775
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9767141,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2019,886748,-0.01015259411262775
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9744013,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Algorithms', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelium', 'Esophageal', 'Esophageal Adenocarcinoma', 'Esophagitis', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Intelligence', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stem cells', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2019,171720,-0.017758058645418746
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,9864664,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2019,269849,-0.006763481356079873
"Mental, measurement, and model complexity in neuroscience PROJECT SUMMARY Neuroscience is producing increasingly complex data sets, including measures and manipulations of sub- cellular, cellular, and multi-cellular mechanisms operating over multiple timescales and in the context of different behaviors and task conditions. These data sets pose several fundamental challenges. First, for a given data set, what are the relevant spatial, temporal, and computational scales in which the underlying information-processing dynamics are best understood? Second, what are the best ways to design and select models to account for these dynamics, given the inevitably limited, noisy, and uneven spatial and temporal sampling used to collect the data? Third, what can increasingly complex data sets, collected under increasingly complex conditions, tells us about how the brain itself processes complex information? The goal of this project is to develop and disseminate new, theoretically grounded methods to help researchers to overcome these challenges. Our primary hypothesis is that resolving, modeling, and interpreting relevant information- processing dynamics from complex data sets depends critically on approaches that are built upon understanding the notion of complexity itself. A key insight driving this proposal is that definitions of complexity that come from different fields, and often with different interpretations, in fact have a common mathematical foundation. This common foundation implies that different approaches, from direct analyses of empirical data to model fitting, can extract statistical features related to computational complexity that can be compared directly to each other and interpreted in the context of ideal-observer benchmarks. Starting with this idea, we will pursue three specific aims: 1) establish a common theoretical foundation for analyzing both data and model complexity; 2) develop practical, complexity-based tools for data analysis and model selection; and 3) establish the usefulness of complexity-based metrics for understanding how the brain processes complex information. Together, these Aims provide new theoretical and practical tools for understanding how the brain integrates information across large temporal and spatial scales, using formal, universal definitions of complexity to facilitate the analysis and interpretation of complex neural and behavioral data sets. PROJECT NARRATIVE The proposed work will establish new, theoretically grounded computational tools to help neuroscience researchers design and analyze studies of brain function. These tools, which will be made widely available to the neuroscience research community, will help support a broad range of studies of the brain, enhance scientific discovery, and promote rigor and reproducibility.","Mental, measurement, and model complexity in neuroscience",9789280,R01EB026945,"['Address', 'Algorithms', 'Automobile Driving', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Benchmarking', 'Brain', 'Characteristics', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Decision Making', 'Dimensions', 'Foundations', 'Goals', 'Guidelines', 'Human', 'Individual', 'Information Theory', 'Length', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Pattern', 'Physics', 'Process', 'Psyche structure', 'Reproducibility', 'Research Personnel', 'Rodent', 'Sampling', 'Series', 'Stream', 'Structure', 'System', 'Techniques', 'Time', 'Work', 'base', 'computer science', 'computerized tools', 'data modeling', 'design', 'information processing', 'insight', 'nonhuman primate', 'relating to nervous system', 'statistics', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2019,21171,-0.02151146091545368
"Administrative Supplement to the OAIC Pepper Center Coordinating Center We wish to advantage of 2 new key opportunities that could significantly enhance achievement of the overall goals of the OIAC Coordinating Center (OAIC CC) and 2 key, unexpected administrative needs. Project 1) Develop, test and implement an innovative set of tools to perform Integrative Data Analysis (IDA) for combining and analyzing independent data sets across the OAIC network An over-arching goal of the OAIC CC is to build collaborations between OAICs that unlock synergy. Each of the OAICs has many small/medium-sized completed studies relevant to the OAIC theme, and that have measured key domains of physical function. Combining these studies could provide large, powerful databases for answering critical questions not possible with individual studies. However, this is currently not possible because different measurement instruments are often used across centers and across studies. This project overcomes this critical limitation by taking advantage of 2 newly available technologies and an ongoing study. IDA is a set of strategies in which two or more independent data sets which contain measures addressing similar domains but using different measurement instruments are combined into one and then statistically analyzed. The proposed project is timely because it leverages an ongoing clinical study to validate new procedures for harmonizing measures of physical and cognitive function across 20 Pepper center studies. The resources created by the project will significantly enhance collaboration across the OAIC program network, benefiting researchers at all OAICs, and can be disseminated to other NIA center programs. Project 2) Develop a robust, interactive database of OAIC Program accomplishments that will automatically be updated via an efficient, streamlined, electronic annual reporting process.  It is widely believed that the NIA-funded Pepper Center program has been highly productive. However, there is no means of assessing the overall effectiveness of the Pepper Center, or of ‘cataloging’ its impressive accomplishments. This project will take advantage of new open-source technology to efficiently develop a robust, comprehensive, searchable, interactive database of past accomplishments. It will also develop a streamlined electronic Annual Directory Report template, and link it to the new OAIC database so that it is automatically updated each year. Achieving the goals of this project will reduce administrative burden for sites, facilitate NIA review of performance of centers, and create an annually updated database of OAIC accomplishments, projects, publications, and outcomes, and facilitate collaborations between centers and investigators across NIA programs. This application also requests support for 2 key, unexpected administrative needs that have arisen: 1) Increase in funding amount for the annual OAIC CC Multi-center pilot project. 2) Support for additional Pepper Centers that will soon be added to the OAIC network. Relevance Statement for OAIC Coordinating Center Administrative Supplement The Coordinating Center of the OAIC coordinates the activities of all the individual centers in the NIA- funded, OAIC network; its over-arching goal is to build collaborations between the individual OAICs and thereby unlock synergy and enable projects that could not be undertaken by any single OAIC center. This administrative supplement application proposes 2 developmental projects that will significantly enhance the capabilities of the OAIC to achieve these goals and which takes advantage of newly available methods and technology. This also includes additional support for the possible increase in the number of Pepper Centers and an increase in the pilot award budget.",Administrative Supplement to the OAIC Pepper Center Coordinating Center,9961004,U24AG059624,"['Achievement', 'Address', 'Administrative Supplement', 'Aging', 'Annual Reports', 'Award', 'Budgets', 'Capsicum', 'Cataloging', 'Catalogs', 'Clinical', 'Clinical Research', 'Cognition', 'Collaborations', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Directories', 'Effectiveness', 'Elderly', 'Equipment and supply inventories', 'Evaluation', 'Funding', 'Goals', 'Health', 'Individual', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Online Systems', 'Outcome', 'Participant', 'Performance', 'Physical Function', 'Pilot Projects', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Update', 'Walking', 'analytical method', 'analytical tool', 'base', 'cognitive function', 'cost effective', 'data modeling', 'forest', 'innovation', 'instrument', 'interest', 'lifestyle intervention', 'new technology', 'novel', 'open source', 'programs', 'recruit', 'response', 'synergism', 'theories', 'tool']",NIA,WAKE FOREST UNIVERSITY HEALTH SCIENCES,U24,2019,149775,-0.006428084899262014
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9751297,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Consumption', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2019,77750,-0.06158344709267789
"IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP The purpose of this contract is to provide bioinformatic support to researchers in the Divisions of National Toxicology Program (DNTP) and Intramural Research (DIR) at the National Institute of Environmental Health Sciences (NIEHS). NIEHS researchers conduct studies that produce large amounts of data, varying in size and complexity. Fields of scientific study are diverse and include toxicology, genomics, transcriptomics, high throughput screening (HTS) data and data extraction from diverse text resources. The variety and complexity of NIEHS scientific studies dictates the need for innovative analytical techniques and the development of new software tools. Bioinformatic data analyses are required to support accurate and precise interpretation of study results. Specific bioinformatics needs include data analysis, data mining, creating bioinformatics pipelines for gene expression and pathway analysis and computational support for the vast amount of data collected through studies conducted at NIEHS and NIEHS contract laboratories. n/a",IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP,9915697,73201700001C,"['Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'ChIP-seq', 'Chemical Exposure', 'Chemicals', 'Contractor', 'Contracts', 'DNA Methylation', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Epigenetic Process', 'Evaluation', 'Exons', 'Gene Expression', 'Genes', 'Genomics', 'Informatics', 'Intramural Research', 'Knowledge', 'Laboratories', 'Literature', 'Measures', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Output', 'Pathway Analysis', 'Peer Review', 'Privatization', 'Programming Languages', 'Proteomics', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scientific Evaluation', 'Scientist', 'Series', 'Software Tools', 'Specific qualifier value', 'Technology', 'Text', 'Toxicogenomics', 'Toxicology', 'analysis pipeline', 'bioinformatics tool', 'bisulfite sequencing', 'cheminformatics', 'computational intelligence', 'data integration', 'data mining', 'differential expression', 'high throughput screening', 'innovation', 'meetings', 'metabolomics', 'method development', 'next generation sequencing', 'physical property', 'programs', 'screening', 'technique development', 'transcriptomics', 'whole genome']",NIEHS,"SCIOME, LLC",N01,2019,2464037,-0.011588272521085059
"A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers PROJECT SUMMARY/ABSTRACT  This application, “A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers,” is in response to PAR-16-238, Dissemination and Implementation Research in Health (R01). Acute respiratory distress syndrome (ARDS) has high prevalence (10% of intensive care unit admissions) and mortality up to 46%. Low tidal volume ventilation (LTVV) is the most effective therapy for ARDS, lowering mortality by 20-25%, and is part of standard practice. However, use of LTVV is as low as 19% of ARDS patients. There is a poor understanding of the barriers to LTVV adoption: current approaches are deficient because they incorporate biases, lack consistency and comprehensiveness, ignore the influence of interpersonal network- or team- based factors, and do not address setting-specific variation. Our research team has previously identified some patient- and clinician-specific facilitators of and barriers to LTVV adoption. We have used two state-of-the-art data driven methods—data science and network analysis—to preliminarily quantify the impact of a diverse array of potential factors affecting LTVV adoption, including network- and team-based factors. The proposed research is guided by the Consolidated Framework for Implementation Research (CFIR) and Rogers' Diffusion of Innovations theory. The overall goals of the proposed research are to understand the differences in facilitators and barriers to LTVV adoption between academic and community settings through a definitive, systematic study in a large, diverse consortium of medical centers, and to advance implementation science by providing a model for how data science and network analysis can be applied to understand the adoption of a complex intervention. The overarching hypothesis is that there are different patient-, clinician-, network-, and team-based facilitators and barriers to LTVV adoption in academic and community settings. We will determine whether different patient- and clinician- (Aim 1 cohort study, clinician survey, and data science analysis), clinician interpersonal network- (Aim 2 network analysis), and team structure and dynamics-based (Aim 3 team construction and modeling) facilitators of and barriers to LTVV adoption exist between academic and community hospital settings. Successful completion of the proposed research will provide a comprehensive understanding of the differences in the facilitators of and barriers to LTVV adoption between academic and community settings, and will advance implementation science by serving as a model of how data science and network analysis can be applied to complex implementation problems. Implementation strategies that account for all these factors may be more likely to lead to significant practice change. PROJECT NARRATIVE  Acute respiratory distress syndrome (ARDS) has high prevalence and mortality among critically ill patients; low tidal volume ventilation is the most effective therapy for ARDS but is used infrequently. Successful completion of the proposed research will identify differences in the facilitators of and barriers to adoption of low tidal volume ventilation between academic and community hospital settings in a large and diverse consortium of medical centers. The proposed research will generate a model of how data science and network analysis can be used to understand the implementation of a complex evidence-based practice.",A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers,9781491,R01HL140362,"['Acute', 'Admission activity', 'Adoption', 'Adult Respiratory Distress Syndrome', 'Affect', 'Attitude', 'Caring', 'Characteristics', 'Cohort Studies', 'Community Hospitals', 'Complex', 'Critical Illness', 'Data', 'Data Science', 'Diffusion of Innovation', 'Environment', 'Environmental air flow', 'Evidence based practice', 'Goals', 'Health', 'Healthcare Systems', 'Height', 'High Prevalence', 'Hypoxemia', 'Individual', 'Inflammatory', 'Intensive Care Units', 'International', 'Intervention', 'Investigation', 'Knowledge', 'Lead', 'Machine Learning', 'Measurement', 'Medical center', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'Nurses', 'Pathway Analysis', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Physicians', 'Professional Organizations', 'Pulmonary Edema', 'Research', 'Severities', 'Speed', 'Structure', 'Surveys', 'Syndrome', 'System', 'Testing', 'Tidal Volume', 'Variant', 'base', 'community setting', 'computer science', 'dissemination research', 'effective therapy', 'experimental study', 'health care settings', 'implementation research', 'implementation science', 'implementation strategy', 'learning strategy', 'lung injury', 'mortality', 'multidisciplinary', 'novel', 'research data dissemination', 'respiratory', 'response', 'theories']",NHLBI,NORTHSHORE UNIVERSITY HEALTHSYSTEM,R01,2019,747700,-0.014892793805822162
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9716628,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2019,763474,-0.015035066211128307
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),9788202,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Infrastructure', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Privacy', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data management', 'data portal', 'data sharing', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'preservation', 'social', 'success', 'theories', 'therapy design', 'tool']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2019,114747,-0.0010061296907460879
"AUGS/DUKE UrogynCREST program PROJECT SUMMARY Health Services Research (HSR) and predictive analytics are rapidly growing fields and will have enormous implications for women’s health research in pelvic floor disorders (PFDs). The AUGS/DUKE Urogynecology Clinical Research Educational Scientist Training (UrogynCREST) program will prepare participants to recognize the critical role that data play in delivering high quality health care. It brings together expertise in health service and women’s health research, medical informatics and prediction modeling. This program will target Urogynecology Faculty at the Assistant Professor level who seek successful careers in health services research (HSR) and analytics. Participants will obtain skills through a combination of didactic and interactive coursework; hands-on manipulation of data through extraction, cleaning, and analysis; and project-based one on one mentoring. The UrogynCREST program will be an interactive, hands-on educational program with centralized activities organized and delivered by distance through a popular on-line learning platform called Sakai, with educational software designed to support teaching, research and collaboration. A diverse faculty with expertise in data sciences teaches courses and the advanced methodology required to perform HSR. Yearly in-person meetings at the annual American Urogynecologic Society meeting enhance networking and the development of partnerships between participants from various institutions, as well as, interactions with the mentors and other HSR in the field. The program’s strategy allows national leaders with particular skills in the field to provide their knowledge to the participants and help mentor them through development of a relevant research question and identification of an appropriate and existing database(s) to address the question. With the guidance of a dedicated statistician and analyst programmer, participants will learn and perform the necessary computer programming needed to extract, clean and analyze these data. Participants whose projects involve the development of prediction models in the form of scores, nomograms or other tools will learn how to build and validate such tools in the existing project. Each participant’s project will culminate in the completion of a submitted manuscript to a peer- reviewed journal or study proposal and publicly available tools when relevant. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and resources for invigorating data discovery and tools for investigations in HSR specifically addressing (PFDs). PROJECT NARRATIVE: The AUGS/DUKE UrogynCREST program will prepare participants to recognize the critical role that data play in delivering high quality health care for pelvic floor disorders. It will add structure to the health data science education for Assistant Professor Level Faculty in Urogynecology by bringing together expertise in health service and women’s health research, medical informatics, and prediction modeling. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and mentorship for invigorating data discovery and tools for investigations in health service research specifically addressing pelvic floor disorders.",AUGS/DUKE UrogynCREST program,9703267,R25HD094667,"['Address', 'Age', 'American', 'Area', 'Caring', 'Clinical Research', 'Collaborations', 'Communities', 'Connective Tissue', 'Data', 'Data Discovery', 'Data Science', 'Databases', 'Development', 'E-learning', 'Educational process of instructing', 'Faculty', 'Fecal Incontinence', 'Fostering', 'Future', 'Goals', 'Health Services', 'Health Services Research', 'Healthcare', 'Infrastructure', 'Institution', 'Instruction', 'Investigation', 'Journals', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Manuscripts', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modernization', 'Muscle', 'Nomograms', 'Participant', 'Peer Review', 'Pelvic Floor Disorders', 'Pelvis', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Public Health', 'Research', 'Resources', 'Role', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Shapes', 'Societies', 'Software Design', 'Structure', 'Techniques', 'Testing', 'Training Programs', 'Urinary Incontinence', 'Woman', 'Women&apos', 's Health', 'base', 'career', 'clinical decision-making', 'clinical development', 'computer program', 'computer science', 'design', 'health care quality', 'health data', 'improved', 'injured', 'innovation', 'meetings', 'pelvic organ prolapse', 'predictive modeling', 'professor', 'programs', 'recruit', 'science education', 'skills', 'social', 'tool']",NICHD,DUKE UNIVERSITY,R25,2019,161462,-0.005936854529088514
"COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data",9938885,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'Intelligence', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'commune', 'computational platform', 'computer framework', 'computing resources', 'connectome', 'cost', 'data anonymization', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'preservation', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2019,585151,-0.009437853198885121
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,9676860,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2019,7168390,-0.02214869461433943
"Mental Health Research Network III Practice-based research has the potential to dramatically improve the speed, efficiency, relevance, and impact of mental health clinical and services research. Realizing those gains will require a practice-based research network capable of anticipating and adapting to changes in mental health care delivery, including: • Research fully embedded in real-world practice • Alignment of research goals with priorities of patient and health system stakeholders • Large-scale data infrastructure available for rapid analysis • A culture of trust and transparency to facilitate collaborative learning and improvement We propose to expand the existing Mental Health Research Network (MHRN) to include 14 research centers embedded in health systems serving a combined population of over 25 million patients in 16 states. MHRN infrastructure will be enhanced to support a next-generation practice-based network, including: • Increased engagement of patients, health system leaders, and other stakeholders in network governance • An expanded public, open-source library of software tools and other technical resources • More formal processes for conducting feasibility pilot projects and rapid response to stakeholder queries • Expanded outreach to external stakeholders and research partners This Overall application requests support for an Administrative Core, a Methods core, and four research projects. The Administrative Core will support all network activities, including ongoing affiliated research projects, new research projects proposed in this application, and new projects developed during the coming funding cycle. An Organizational Unit will include executive leadership, governance, strategic planning, and responsibility for financial management and regulatory compliance. An Outreach and External Collaboration Unit will be responsible for stakeholder engagement and developing collaborations with new research partners. An Emerging Issues Unit will support timely responses to emerging scientific and public health questions. The Methods Core will support optimal use of existing methods across all network research and develop new methods to address high-priority public health questions. An Informatics Unit will maintain and improve network data infrastructure and disseminate network-developed informatics resources. A Scientific Analysis Unit will focus on improving the methodologic rigor of network projects and development of innovative analytic methods driven by stakeholder needs. These infrastructure resources will support two Signature research projects (a pragmatic trial of eHealth interventions for perinatal depression and a real-time evaluation of new medications to address suicide risk) and two Pilot research projects (an investigation of stakeholder perspectives on implementation of suicide risk prediction models and a pilot trial of outreach to reduce disparities in depression treatment initiation). The Mental Health Research Network will conduct practice-based mental health research in large healthcare systems serving over 25 million patients in 16 states. Infrastructure will include an Administrative Core responsible for governance, communications, and fostering research collaborations as well as a Methods Core responsible for advancing methods in mental health informatics and analytic methods. Four research projects will leverage this infrastructure to address questions of high priority to patient, health system, and policy stakeholders.",Mental Health Research Network III,9881691,U19MH121738,"['Address', 'Antipsychotic Agents', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Complex', 'Data', 'Development', 'Effectiveness', 'Evaluation', 'Feeling suicidal', 'Fostering', 'Funding', 'Future', 'Glutamate Receptor', 'Goals', 'Health Services Research', 'Health system', 'Healthcare Systems', 'Hybrids', 'Informatics', 'Infrastructure', 'Integrated Health Care Systems', 'Investigation', 'Leadership', 'Learning', 'Libraries', 'Machine Learning', 'Mental Depression', 'Mental Health', 'Methodology', 'Methods', 'National Institute of Mental Health', 'Network Infrastructure', 'Network-based', 'Observational Study', 'Observational epidemiology', 'Occupational activity of managing finances', 'Organization administrative structures', 'Patients', 'Pharmaceutical Preparations', 'Pilot Projects', 'Policies', 'Population', 'Practice based research', 'Primary Health Care', 'Process', 'Public Health', 'Public Health Informatics', 'Qualitative Research', 'Reaction Time', 'Recording of previous events', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resource Informatics', 'Resources', 'Site', 'Software Tools', 'Speed', 'Strategic Planning', 'Suicide prevention', 'Testing', 'Time', 'Trust', 'Youth', 'analytical method', 'base', 'design', 'disparity reduction', 'eHealth', 'first episode psychosis', 'health care delivery', 'implementation science', 'improved', 'innovation', 'member', 'next generation', 'open source', 'outreach', 'patient engagement', 'patient population', 'patient portal', 'perinatal intervention', 'peripartum depression', 'pilot trial', 'practice-based research network', 'pragmatic trial', 'programs', 'public health priorities', 'racial and ethnic disparities', 'response', 'risk prediction model', 'suicidal behavior', 'suicidal risk', 'treatment-resistant depression']",NIMH,KAISER FOUNDATION RESEARCH INSTITUTE,U19,2019,2220745,-0.03037469604711896
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9690782,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Dysregulation', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'bioinformatics tool', 'circadian', 'circadian regulation', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'informatics\xa0tool', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2019,328155,-0.018974457614949855
"PiNDA - Fully integrated software platform for Preimplantation Genetic Testing - Aneuploidy (PGT-A) PROJECT SUMMARY  Since its inception 40 years ago, in vitro fertilization (IVF) has resulted in the birth of more than 1 million babies in the United States, and has revolutionized the field of reproductive medicine. Unfortunately, the success rate of IVF is still exceedingly low, especially for women >40 years old, with only 15.5% of implanted embryos resulting in pregnancy. This is partly due to the cytological method used for pre-implantation screening, which cannot detect the most common genetic defect during IVF, aneuploidy (i.e. chromosomal copy-number variation). Aneuploidy is linked to higher rates of miscarriage, and occurs more often in women >40 years of age; thus, aneuploidy has been a frequent target for genetic screening to improve IVF outcomes.  Pre-implantation genetic testing for aneuploidy (PGT-A) refers to a variety of techniques aimed at detecting changes in chromosomal copy number, with the goal of identifying high-quality euploid embryos for implantation. Recent advances in next-generation sequencing (NGS) technologies have made it possible to screen embryos at higher levels of precision, and across a wider range of genetic defects, including mosaicism, triploidy and single nucleotide polymorphisms (SNPs). Despite these remarkable advances, there are still significant challenges with PGT-A sequencing. Indeed, the most commonly implemented software for PGT-A (i.e. BlueFuse® ) are bundled with specific sequencing platforms (i.e. VeriSeq®), and are only designed to test for aneuploidy. Furthermore, existing pipelines are not user-friendly or customizable, which is a serious obstacle prohibiting the use of NGS by clinicians / embryologists. A more accessible bioinformatics platform is desperately needed that will bridge the gap between PGT-A sequencing and IVF outcomes.  Basepair™ is an innovator in efficient, user-friendly, web-based NGS analysis systems, with fully automated ChIP-, RNA-, ATAC-, and DNA-Seq bioinformatics pipelines available online. Here, Basepair will deliver PiNDA™, the first fully integrated software solution for comprehensive PGT-A analysis. In Aim 1, we will develop modules to test for specific chromosomal abnormalities, including mosaicism and triploidy, and validate each model with training data derived from somatic cell lines with known chromosomal aberrations. In Aim 2, we will integrate our modules into the PiNDA software system, creating a user-friendly, web-based interface that will perform full data analysis (raw data to full summary report) in <15 minutes, with no manual input required. Final data will be accessible via Basepair’s online portal, facilitating rapid data transfer from embryologists to physicians, and supporting the integration of NGS tests in IVF. Our innovative bioinformatics platform will accelerate NGS analysis for IVF, improving rates of pregnancy and advancing research in the success of IVF procedures. PROJECT NARRATIVE  In vitro fertilization (IVF) methods have begun to leverage next-generation sequencing technologies for pre-implantation genetic testing of aneuploidy (PGT-A), expanding the array of chromosomal abnormalities that can be accurately detected. However, the vast majority of software can only distinguish one type of genetic defect (i.e. aneuploidy), are difficult to use, and are tied to distinct sequencing platforms, limiting the clinical utility of resulting analyses. Basepair™ Inc. is a pioneer in user-friendly, web-based bioinformatics pipelines, providing comprehensive services for a wide range of sequencing projects. Here, Basepair will develop an inclusive suite of software for PGT-A, compatible with sequencing data from multiple platforms. This product will be of high value to the field and will help bridge the gap between advances in DNA sequencing and IVF technology.",PiNDA - Fully integrated software platform for Preimplantation Genetic Testing - Aneuploidy (PGT-A),9846492,R43HD100280,"['ATAC-seq', 'Age-Years', 'Algorithms', 'Aneuploid Cells', 'Aneuploidy', 'Bioinformatics', 'Biopsy', 'Birth', 'Cell Line', 'Cell division', 'Centers for Disease Control and Prevention (U.S.)', 'ChIP-seq', 'Chromosome abnormality', 'Clinical', 'Complex', 'Computer software', 'Copy Number Polymorphism', 'Culture Media', 'Cytology', 'DNA sequencing', 'Data', 'Data Analyses', 'Embryo', 'Feedback', 'Fertility Agents', 'Fertilization in Vitro', 'Genetic Screening', 'Goals', 'Harvest', 'Implant', 'Letters', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Morphology', 'Mosaicism', 'Mutation', 'Online Systems', 'Outcome', 'Phase', 'Physicians', 'Polymorphism Analysis', 'Pregnancy', 'Pregnancy Rate', 'Preimplantation Diagnosis', 'Procedures', 'Reporting', 'Reproductive Medicine', 'Research', 'Role', 'Sampling', 'Services', 'Single Nucleotide Polymorphism', 'Somatic Cell', 'Specificity', 'Spontaneous abortion', 'Summary Reports', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Training', 'Triploidy', 'United States', 'Uterus', 'Woman', 'analysis pipeline', 'aneuploidy analysis', 'cell free DNA', 'design', 'early embryonic stage', 'egg', 'implantation', 'improved', 'innovation', 'natural Blastocyst Implantation', 'next generation sequencing', 'phase 1 study', 'preimplantation', 'screening', 'sequencing platform', 'software development', 'software systems', 'sperm cell', 'success', 'transcriptome sequencing', 'user-friendly', 'web based interface']",NICHD,"BASEPAIR, INC.",R43,2019,298717,-0.023872753995143957
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,-0.017724871112605546
"UTMB Clinical and Translational Science Award UTMB's CTSA-linked KL2 Scholars Program addresses the significant need for developing future clinical and translational (C&T) investigators, with emphases on team-based research, leadership development, and mentorship training. In our new CTSA hub, we will build on our current CTSA's successes over the past 5½ years, with unique innovative integration of exceptional training approaches, including: 1) mentored career development within CTSA-supported multidisciplinary translational teams (MTTs); 2) an emphasis on individual development plans (IDPs) and pre-established C&T research competencies tailored to the needs of the individual scholar, but also emphasizing team-based competencies; 3) development of leadership competencies through participation in a new Leadership Development Academy; 4) an Academy of Research Mentors (ARM), based within our Institute for Translational Sciences (ITS), to foster mentoring and mentor training; 5) a linked Translational Research Scholars Program (TRSP), which provides an expanded peer group and a wider impact across the institution, with twice-monthly career development seminars and a focus on scholars' research; 6) development of entrepreneurial skills and leadership through participation in a UT System-funded Healthcare Entrepreneurship Program; and 7) collaboration and dissemination of program experience to other CTSA hubs in the Texas Regional CTSA Consortium, and the national CTSA Consortium. An important feature of our KL2 Scholars Program is its linked position within the continuum of career development, from graduate student training in our CTSA TL1 Training Core to the production of independently funded C&T faculty members. Our program is enhanced with by financial support from our Provost's office for scholar expenses, ITS education administrators, the ARM, and the Office of Faculty Affairs, which is focused on faculty development at UTMB. UTMB also ranks nationally in promoting diversity, which will enhance our recruitment of underrepresented minority scholars by the participation of faculty from our Hispanic Center of Excellence, and our Medical School Enrichment Program. Our KL2 Program is well-integrated with other institutional education activities, including our Human Pathophysiology and Translational Medicine graduate program, and courses targeted toward scientific writing, biostatistics, and study design. Early-career faculty as Phase 1 TRSP scholars can compete to become CTSA-supported KL2 Scholars, and with acquisition of their first mentored K-level grants (e.g., K08), advance to a Phase 2 TRSP Scholar, then focusing on the transition from mentored to independent grant funding. Upon acquisition of independent (e.g. R01) funding, scholars advance to become Phase 3 TRSP Scholars and Associate Members in the ARM, to facilitate their development of mentoring skills, and eventually to full ARM membership, with demonstrated success in C&T science mentoring. Our KL2 Scholars Program thus integrates novel team-based training, leadership, and mentoring approaches to foster the career development of future leaders in C&T research. n/a",UTMB Clinical and Translational Science Award,9689119,KL2TR001441,"['Academy', 'Address', 'Administrator', 'Biological', 'Biological Markers', 'Biometry', 'Biopsy', 'Body Surface Area', 'Burn injury', 'Characteristics', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Competence', 'Data', 'Dependence', 'Development', 'Development Plans', 'Down-Regulation', 'Education', 'Enrollment', 'Entrepreneurship', 'Evolution', 'Expression Profiling', 'Faculty', 'Failure', 'Financial Support', 'Fostering', 'Freezing', 'Functional disorder', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Grant', 'Healthcare', 'Hepatitis C virus', 'Hispanics', 'Human', 'Individual', 'Infection', 'Inflammation', 'Institutes', 'Institution', 'Interferons', 'Intervention', 'Leadership', 'Link', 'Liver Fibrosis', 'Longitudinal Studies', 'Machine Learning', 'Measures', 'Medicare', 'Medicare claim', 'Medicine', 'Mentors', 'Mentorship', 'MicroRNAs', 'Modeling', 'NCI Scholars Program', 'Participant', 'Pathway interactions', 'Patients', 'Peer Group', 'Phase', 'Physicians', 'Positioning Attribute', 'Process', 'Production', 'Protocols documentation', 'Regimen', 'Research', 'Research Design', 'Rice', 'System', 'TNFSF15 gene', 'Teacher Professional Development', 'Techniques', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underrepresented Minority', 'Variant', 'Writing', 'base', 'career development', 'cohort', 'college', 'demographics', 'early-career faculty', 'experience', 'graduate student', 'innovation', 'leadership development', 'liver biopsy', 'medical schools', 'member', 'microbial', 'multidisciplinary', 'novel', 'outcome forecast', 'peripheral blood', 'predict clinical outcome', 'program dissemination', 'programs', 'recruit', 'response', 'science education', 'skills', 'structural genomics', 'student training', 'success', 'translational medicine', 'translational scientist']",NCATS,UNIVERSITY OF TEXAS MED BR GALVESTON,KL2,2019,423432,-0.02221991486094918
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive, PhysioBank, was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. PhysioToolkit, its software collection, supports exploration and quantitative analyses of PhysioBank and similar data with a wide range of well-documented, rigorously tested open-source software that can be run on any platform. PhysioNet's team of researchers leverages results of other funded projects to drive the creation and enrichment of: i) Data collections that provide increasingly comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC III (Medical Information Mart for Intensive Care) Database of critical care patients; ii) Analytic methods that lead to more timely and accurate diagnoses of major public health problems (such as life-threatening cardiac arrhythmias, infant apneas, fall risk in older individuals and those with neurologic disease, and seizures), and iii) Elucidation of dynamical changes associated with a variety of pathophysiologic processes and aging (such as cardiopulmonary interactions during sleep disordered breathing syndromes); User interfaces, reference materials and services that add value and improve accessibility to PhysioNet's data and software (such as PhysioNetWorks, a virtual laboratory for data sharing). Impact: Cited in The White House Fact Sheet on Big Data Across the Federal Government (March 29, 2012), PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are inaccessible otherwise. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world- wide, growing community of researchers, clinicians, educators, students, and medical instrument and software developers, retrieve about 380 GB of data per day. By providing free access to its unique and wide-ranging data and software collections, PhysioNet is invaluable to studies that currently result in an impressive average of nearly 250 new scholarly articles per month by academic, clinical, and industry-affiliated researchers worldwide. Over the next year we aim to sustain and enhance PhysioNet's impact with new technology and data; and complete the 2019 PhysioNet/Computing in Cardiology Challenge on sepsis. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,9993811,R01GM104987,"['Aging', 'Algorithms', 'Apnea', 'Area', 'Arrhythmia', 'Big Data', 'Biomedical Research', 'Boston', 'Bypass', 'Cardiology', 'Cardiopulmonary', 'Categories', 'Clinical', 'Clinical Data', 'Cloud Service', 'Collection', 'Communities', 'Community Outreach', 'Complex', 'Computer software', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Dedications', 'Development', 'Diagnostic radiologic examination', 'Entropy', 'FAIR principles', 'Federal Government', 'Functional disorder', 'Funding', 'Grant', 'Imagery', 'Individual', 'Industry', 'Infant', 'Infrastructure', 'Intensive Care', 'Israel', 'Journals', 'Laboratories', 'Lead', 'Licensing', 'Life', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical center', 'Methods', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase Transition', 'Physiological', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Roentgen Rays', 'Role', 'Running', 'Seizures', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Students', 'Switzerland', 'Syndrome', 'Testing', 'Thoracic Radiography', 'Time', 'United States National Institutes of Health', 'University Hospitals', 'Visit', 'accurate diagnosis', 'analytical method', 'clinical application', 'computerized data processing', 'computing resources', 'data archive', 'data sharing', 'experience', 'fall risk', 'heart rate variability', 'improved', 'innovation', 'instrument', 'instrumentation', 'interest', 'member', 'nervous system disorder', 'new technology', 'open source', 'preservation', 'repository', 'signal processing', 'software repository', 'symposium', 'time interval', 'virtual laboratory']",NIGMS,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2019,409563,-0.008483806779263277
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9765194,R44CA206782,"['Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'data warehouse', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'off-patent', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2019,238768,-0.01092896693777857
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9762963,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Infrastructure', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2019,10000,-0.0034292281389684052
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9902000,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health care facility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Infrastructure', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'implementation strategy', 'innovation', 'inpatient service', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2019,730148,-0.01643137914222039
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9607599,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2019,559237,-0.01451183588858766
"Learning Dynamics of Biological Processes from Time Course Omics Datasets Complex biological processes, including organ development, immune response and disease progression, are inherently dynamic. Learning their regulatory architecture requires understanding how components of a large system dynamically interact with each other and give rise to emergent behavior. Recent experimental advances have made ii possible to investigate these biological systems in a data-driven fashion al high temporal resolution, allowing identification of new genes and their regulatory interactions. Longitudinal omics data sets are becoming increasingly common in clinical practice as well. Information on these collections of interacting genes can be integrated to gain systems-level insights into the roles of biological pathways and processes, including progression of diseases. Consequently, developing interpretable methods for learning functional relationships among genes, proteins or metabolites from high-dimensional time series data has become a timely research problem. The nature of these time-course data sets presents exciting opportunities and interesting challenges from a statistical perspective. Typical time-course omics data sets are challenging because of their high-dimensionality and non-linear relationships among system components. To tackle these challenges, one needs sophisticated dimension-reduction techniques that are biologically meaningful, computationally efficient and allow uncertainty quantification. Methods that incorporate prior biological information (e.g., pathway membership, protein-protein interactions) into the data analysis are good candidates for analyzing such high-dimensional systems using small samples. Here, we will develop three core methods to address the above challenges - (Aim 1): an empirical Bayes framework for clustering high-dimensional omics time-course data using prior biological knowledge; (Aim 2): a quantile-based Granger causality framework for learning interactions among genes or metabolites from their lead-lag relationships; and (Aim 3): a decision tree ensemble framework for searching cascades of interactions among genes from their temporal expression profiles. Our interdisciplinary team of statisticians and scientists will analyze time-course omics data from three research projects: (i) innate immune response systems in Drosophila, (ii) developmental process in mouse models, and (ii) longitudinal metabolite profiling of TB patients. These insights will be used to build and validate our methodology, which will be implemented in a publicly available software. This proposal is innovative in its incorporation of prior biological knowledge in the framework of novel dimension reduction techniques for interrogating high-dimensional time-course omics data. This research is significant in that it will impact basic sciences by elucidating data-driven, testable hypotheses on the regulatory architecture of biological processes, and clinical practice by monitoring disease progression and prognosis. n/a",Learning Dynamics of Biological Processes from Time Course Omics Datasets,9903643,R01GM135926,"['Biological Process', 'Data Set', 'Instruction', 'Learning', 'Time']",NIGMS,CORNELL UNIVERSITY,R01,2019,351443,-0.02528784947977275
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine No abstract available PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,10063300,U01LM012675,[' '],NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2019,375751,-0.028480304386496264
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),9850367,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip region structure', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2019,644134,0.007106394158509185
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9747930,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,258870,-0.003918137454347447
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9992241,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,76935,-0.003918137454347447
"2020 Nanoscale Science and Engineering for Agriculture and Food Systems Gordon Research Conference and Gordon Research Seminar Project Summary The objectives of this GRC are: 1) To bring together experts and stakeholders to discuss nanotechnology advances, directions, and needs in food and agriculture; 2) To identify cutting-edge research and emerging opportunities to address global challenges of food security, environmental sustainability, food safety, and agricultural productivity. A major goal of this GRC is to promote exceptional early career and female investigators, as well as those of traditionally underrepresented populations, to ensure the continued growth in diversity of scholars in our vibrant community. A diverse range of invited speakers and discussion leaders will present a comprehensive vision of critical and emerging nanotechnology research advances across the field of agricultural science. The program illustrates a targeted excellent diversity of leadership (6), speakers (23), and discussion leaders (11) across the spectrum of: i) gender diversity (19 females, 21 males); ii) USA (24) /International participation (16), representing Austria, Brazil, Canada, Colombia, Denmark, Germany, India, Italy, Korea, Singapore, Spain, Switzerland, and Taiwan; and iii) industry and government (3), which includes the FDA, FAO, a founder of a blockchain startup company, and senior research scientist at a non-governmental organization. The following topics will be included in the program: Food and nutrition, bioinspired and targeted technologies, sensing and tracking, microbiomes, food contact materials and human health and disease. Discussions on the safe deployment of nanotechnology in these areas, public perception and policy, and influence on industry and entrepreneurship are integral to this GRC. The Center for Food Safety and Applied Nutrition (CFSAN), the FDA center responsible for protecting the health of US consumers, will find the following sessions of great interest, Convergence of Nanotechnology with Food & Agriculture; Advances in Nanomaterials; Environmental Nanotechnology; Nano-enabled Approaches to Improving Human & Animal Health; Translation of Nano-Based Science for Application in Food & Agriculture; Big Data, Machine Learning, AI & Modeling; and Nanotechnology's Impact on Food Safety. The significant contributions of this diverse population will provide the strong intellectual infrastructure required to develop safe and sustainable nano-enabled applications in food and agriculture in direct support of the FDA CFSAN goal of ensuring the safety, security, sanitation, and proper labeling of the U.S. food supply. Project Narrative Nanomaterials and nanotechnology are rapidly entering almost every industry around the world. The co-chairs of this GRC envision that the convergence between nanotechnology, biotechnology and information science within plant science, animal science, crop and food science/technology and environmental science will lead to revolutionary advances in improving public health. The successful achievement of this lofty vision requires the profound intellectual commitment of scientists and engineers in a highly integrated engagement across numerous disciplines. Some success examples with a focus on nanotechnology in food include: nano- biosensors for identification of pathogens, toxins and bacteria in foods; identification systems for tracking animal and plant materials from origination to consumption; development of nanotechnology-based foods with lower calories and with less fat, salt and sugar, while retaining flavor and texture; integrated systems for sensing, monitoring and active response intervention for plant and animal production; “smart field systems” to detect, locate, report and direct application of water; precision and controlled release of fertilizers and pesticides; development of plants that exhibit drought resistance and tolerance to salt and excess moisture; and nanoscale films for food packaging and contact materials that extend shelf life, retain quality, and reduce cooling requirements. Overall, nanoscale science and engineering has an important role in creating a safer and more productive agriculture and food system. Commercial advances and technological impacts have been limited somewhat due to the relative “newness” of nanotechnology in agriculture and food systems. Given that research at the nanoscale in agriculture and food systems is only into its second decade, safety of nano-enabled applications in agriculture are still up for debate. The Center for Food Safety and Applied Nutrition (CFSAN), the FDA center responsible for protecting the health of US consumers, will find the following sessions of great interest, Convergence of Nanotechnology with Food & Agriculture; Advances in Nanomaterials; Environmental Nanotechnology; Nano-enabled Approaches to Improving Human & Animal Health; Translation of Nano-Based Science for Application in Food & Agriculture; Big Data, Machine Learning, AI & Modeling; and Nanotechnology's Impact on Food Safety. In the spirit of the GRC mission, the 2020 Nano Ag & Food GRC and GRS will again bring together a diverse array of scientists and engineers, both senior and junior, postdocs, and graduate students from academia, as well as scientists and engineers from business, government, and Non-governmental organizations (NGOs); with an emphasis to attract researchers from around the world to engage in open scientific exchange. The significant contributions of this diverse population will provide the strong intellectual infrastructure required to develop safe and sustainable nano-enabled applications in food and agriculture in direct support of the FDA CFSAN goal of ensuring the safety, security, sanitation, and proper labeling of the U.S. food supply.",2020 Nanoscale Science and Engineering for Agriculture and Food Systems Gordon Research Conference and Gordon Research Seminar,9912335,R13FD006688,[' '],FDA,GORDON RESEARCH CONFERENCES,R13,2019,25000,-0.04562915671539323
"CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience The field of network neuroscience has developed powerful analysis tools for studying brain networks and holds promise for deepening our understanding of the role played by brain networks in health, disease, development, and cognition. Despite widespread interest, barriers exist that prevent these tools from having broader impact. These include (1) unstandardized practices for sharing and documenting software, (2) long delays from when a method is first introduced to when it becomes publicly available, and (3) gaps in theoretic knowledge and understanding leading to incorrect, delays due to mistakes, and errors in reported results. These barriers ultimately slow the rate of neuroscientific discovery and stall progress in applied domains. To overcome these challenges, we will use open science methods and cloud-computing, to increase the availability of network neuroscience tools. We will use the platform ""brainlife.io"" for sharing these tools, which will be packaged into self-contained, standardized, reproducible Apps, shared with and modified by a community of users, and integrated into existing brainlife.io analysis pipelines. Apps will also be accompanied by links to primary sources, in-depth tutorials, and documentation, and worked-through examples, highlighting their correct usage and offering solutions for mitigating possible pitfalls. In standardizing and packaging network neuroscience tools as Apps, this proposed research will engage a new generation of neuroscientists, providing them powerful new and leading to new discoveries. Second, the proposed research will contribute growing suite of modeling analysis that can be modified to suit specialized purposes. Finally, the Brainlife.io platform will serve as part of the infrastructure supporting neuroscience research. Altogether, these advances will lead to new opportunities in network neuroscience research and further stimulate its growth while increasing synergies with other domains in neuroscience. Structural and functional networks support cognitive processes. Miswiring networks lead to maladaptive behavior and neuropsychicatric disorders. Network neuroscience is a young field that provides a quantitative framework for modeling brain networks. This project will make network neuroscientific tools available to new users via open science and cloud-computing. New applications of these tools this will lead deeper insight into the role of networks in health as well as in clinical disorders.",CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience,9916138,R01EB029272,"['Address', 'Aging', 'Biophysics', 'Brain', 'Cloud Computing', 'Cognition', 'Communities', 'Complex Analysis', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ecosystem', 'Education', 'Elements', 'France', 'Funding', 'Generations', 'Graph', 'Growth', 'Instruction', 'Knowledge', 'Language', 'Lead', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Neurosciences', 'Pathway Analysis', 'Play', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Science', 'Sociology', 'Standardization', 'Structure', 'Study models', 'System', 'Techniques', 'Time', 'Training', 'analysis pipeline', 'brain computer interface', 'cloud based', 'cyber infrastructure', 'data sharing', 'experience', 'innovation', 'insight', 'learning strategy', 'network architecture', 'network models', 'neuroimaging', 'open data', 'prevent', 'relating to nervous system', 'statistics', 'tool']",NIBIB,INDIANA UNIVERSITY BLOOMINGTON,R01,2019,215134,-0.021840064511976093
"The Enzymatic Reader Project Summary At this point in time, it is generally understood and agreed upon that single-molecule sequencing (SMS) is the future of genomics, transcriptomics, epigenomics, and epitranscriptomics due to its significant advantages over other technologies and methods. However, in order for these advantages to be fully realized, and for SMS to become the “gold standard” sequencing approach, significant issues and hurdles must be solved and overcome. During this program, Electronic BioSciences, Inc. (EBS) aims to demonstrate a completely new and enabling SMS method that will possess the ability to directly and correctly identify individual nucleotides, including chemically modified nucleotides. During this project, we will both demonstrate the ability of this entirely new sequencing approach to sequence DNA with high accuracy (directly comparing the obtained accuracy, throughput, error mechanisms and associated rates to other SMS approaches) and correctly identify (and sequence) 5-methylcytosine (5mC) and its derivatives, at the single molecule level. At the conclusion of this Phase I project, we will have successfully demonstrated an entirely new and dramatically improved SMS approach, and reduced the associated risks involved with its full future commercial developments. There is a current need within the field of next generation sequencing (NGS) or so called third generation sequencing (TGS) for new, enabling instrumentation that is capable of high-accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats. The entirely new SMS methodology that will be developed during this project will overcome known hurdles and limitations of currently available NGS, TGS, and SMS technologies, resulting in technology that is cost-efficient, highly accurate, easy to setup and utilize, capable of de novo sequencing and modified base calling, and yields highly simplistic data for easy analysis and post possessing. Through significant advancements made during this program, this resulting technology will revolutionize the use of the genome and epigenome, radically change standard R&D and clinical practices, and greatly advance clinical diagnostics, prognostics, and therapeutic decision making. Project Narrative The novel single-molecule sequencing (SMS) technology developed during this project will enable high- accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats via a cost-efficient and easy-to-use methodology. The impact of these advances in SMS will eventually enable wide-scale, routine clinical care and diagnostics toward advanced precision medicine, not just R&D. The performance and accessibility of such technology will transform the understanding and application of genomics and epigenomics, the associated clinical practices, that ability to provide precision clinical diagnostics, prognostics, and therapeutic decision making for improved public healthcare and wellbeing.",The Enzymatic Reader,9677956,R43HG010427,"['Biological', 'Biological Sciences', 'Caliber', 'Chemicals', 'Chemistry', 'Church', 'Complex', 'DNA Primers', 'DNA Sequence', 'DNA polymerase A', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disadvantaged', 'Drops', 'Electrodes', 'Enzymes', 'Evaluation', 'Future', 'Genome', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Ions', 'Label', 'Length', 'Lipid Bilayers', 'Logistics', 'Methodology', 'Methods', 'Motor', 'Movement', 'Noise', 'Nucleotides', 'Performance', 'Personal Satisfaction', 'Phase', 'Polymerase', 'Polymers', 'Preparation', 'Process', 'Proteins', 'RNA', 'Reader', 'Reading Frames', 'Reproducibility', 'Risk', 'Sampling', 'Side', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Software Tools', 'Speed', 'Stretching', 'System', 'Technology', 'Therapeutic', 'Third Generation Sequencing', 'Time', 'base', 'clinical care', 'clinical diagnostics', 'clinical practice', 'cost', 'cost efficient', 'electric field', 'epigenome', 'epigenomics', 'epitranscriptomics', 'improved', 'instrumentation', 'machine learning algorithm', 'nanopore', 'next generation sequencing', 'novel', 'precision medicine', 'prevent', 'prognostic', 'programs', 'research and development', 'single molecule', 'solid state', 'transcriptomics']",NHGRI,"ELECTRONIC BIOSCIENCES, INC.",R43,2019,247611,-0.03610770105345194
"Toward a High Dimensional Computational Description of Variation in Human Decision-Making Across Psychiatric and Non-Psychiatric Populations TOWARD A HIGH DIMENSIONAL COMPUTATIONAL DESCRIPTION OF VARIATION IN HUMAN DECISION-MAKING ACROSS PSYCHIATRIC AND NON-PSYCHIATRIC POPULATIONS PI: Dr. John P. O'Doherty Institution: California Institute of Technology PROJECT SUMMARY  Computational psychiatry (CP) promises to gain deeper explanatory insight into psychiatric disorders through the application of formal computational models to task-related behavioral data and brain measures. However, research in CP to date has mostly involved a narrow unidimensional focus, utilizing either a relatively limited set of computational constructs such as simple model-free (MF) reinforcement learning (RL), and/or restricted to studying a specific task, a specific disease, or even a particular model parameter. For CP to reach its potential, we need to broaden the field's scope. To achieve this, it is necessary to develop an integrated theory and formal framework supported by a task battery that will enable the quantification of individual differences across a range of computational mechanisms pertinent to the diagnosis and treatment of clinical disorders. The objective of the current proposal is to implement the initial groundwork needed to build and test a computational framework and task battery that can facilitate a multi-dimensional computational description of individual variability in parameters relevant for characterizing psychiatric dysfunction.  We have constructed a computational assessment battery (CAB) consisting of four distinct yet inter- related tasks that move beyond simple RL to probe various aspects of learning, cognition, and decision-making. First, we assess learning about losses as well as rewards. Secondly, we measure model-based (MB) alongside simple MF learning and decision making, and the arbitration allocating control to either strategy. Thirdly, we examine strategies for solving the exploration/exploitation dilemma, in which individuals have to decide whether to exploit an option known to yield reward or explore an option whose outcomes are unknown. Finally, we assess social-learning, in which an individual can either infer the goals of another individual or simply imitate that agent's behavior.  We propose to build an integrated computational model that can capture relevant computations being implemented in each of the tasks in our battery, alongside a hierarchical model-fitting and parameter estimation framework to enable us to retrieve reliable parameter estimates for each computational variable of interest. We will leverage common computational mechanisms engaged across our task battery to improve estimability and generalizability. We will then acquire behavioral data in a large on-line (n=1000), and modest (n=100) in-lab sample on the CAB to establish the internal validity and test/re-test reliability of our computational model and parameter estimates. Finally, we will explore the relationship between model-estimated parameter estimates from behavior on our task battery and variability in self-reported traits and states relevant for psychiatric disease in our healthy samples, as well as in a pilot sample (n=60) of patients recruited from the UCLA psychiatry outpatient clinics. These efforts will form the basis of a Computational Psychiatry Toolbox for behavioral testing, model-fitting and parameter estimation that will eventually be released as a resource to the research community. PROJECT NARRATIVE Our goal is to develop a comprehensive approach to characterize an individual's ability to think, learn and make decisions in terms of a series of specific brain processes or `computations'. We then aim to determine whether variation in these computations can in turn be related to a variety of symptoms that can occur in psychiatric disorders. To achieve this, we will develop new mathematical and experimental tools to assess variation in behavior across individuals, and refine, test and validate these tools in a preliminary sample of healthy participants and psychiatric patients.",Toward a High Dimensional Computational Description of Variation in Human Decision-Making Across Psychiatric and Non-Psychiatric Populations,9827920,R21MH120805,"['Ambulatory Care Facilities', 'Anxiety', 'Arbitration', 'Behavior', 'Behavioral', 'Biological', 'Brain', 'California', 'Cell Nucleus', 'Classification', 'Clinical', 'Clinical Treatment', 'Cognition', 'Communities', 'Complex', 'Computational algorithm', 'Computer Simulation', 'Data', 'Decision Making', 'Development', 'Diagnosis', 'Dimensions', 'Discipline', 'Disease', 'Ensure', 'Functional disorder', 'General Population', 'Goals', 'Human', 'Impulsivity', 'Individual', 'Individual Differences', 'Institutes', 'Institution', 'Learning', 'Mathematics', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Moods', 'National Institute of Mental Health', 'Outcome', 'Participant', 'Patient Recruitments', 'Patient Self-Report', 'Patients', 'Performance', 'Plant Roots', 'Population', 'Procedures', 'Process', 'Psychiatric therapeutic procedure', 'Psychiatry', 'Psychological reinforcement', 'Research', 'Resources', 'Rewards', 'Sampling', 'Series', 'Supervision', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Treatment Protocols', 'Undifferentiated', 'Validation', 'Validity and Reliability', 'Variant', 'base', 'behavior test', 'cohort', 'computer framework', 'high dimensionality', 'improved', 'individual variation', 'insight', 'interest', 'large scale simulation', 'recruit', 'social learning', 'targeted treatment', 'theories', 'tool', 'trait', 'unsupervised learning']",NIMH,CALIFORNIA INSTITUTE OF TECHNOLOGY,R21,2019,260395,-0.021610720199301853
"INvestigations In Gout, Hyperuricemia, and comorbidiTies (INSIGHT) Center of Research Translation (CORT) Project Summary Gout impacts around 4% of the U.S. adult population and is the most common form of inflammatory arthritis in men. The incidence of gout is increasing worldwide. Gout significantly burdens the healthcare system, and is associated with both decreased work productivity and quality of life. With the ever-increasing impact of gout and its associated comorbidities on the population, investigation of novel translational mechanisms mediating gout flares as well as approaches to improving gout and hyperuricemia outcomes comprise an unmet and urgent medical need. When funded, the INvestigationS In Gout, Hyperuricemia, and comorbidiTies (INSIGHT) Center of Research Translation (CORT) included 4 active research projects and an Administrative Core focused on the theme, “Gout, Hyperuricemia, and Associated Comorbidities”. Projects include studies to: determine if adenosine monophosphate-activated protein kinase (AMPK) activity metabolomics have promise as gout biomarkers independent of serum urate (P1), examine the influence of key gene-environment interactions within an internet study of gout flares (P2), unravel the functional genomics of urate transporter genes identified in previously reported genome wide association studies (P3), and investigate the mechanism of urate lowering therapy on renal function within VA STOP-GOUT (P4). Projects range from basic research translation of underlying genetics and inflammatory pathobiology of gout, to understanding mechanisms of CKD progression in gout, and translation of genetic interaction with environmental factors and medications, to precision medicine. The proposed revision project (P5) aims to utilize a novel emergency department based intervention to test methods to improve the care gout patients in the Deep South, with a secondary goal of concurrently enhancing participation of minorities in research. The revised INSIGHT CORT aims to: (1) Conduct five outstanding, innovative, and synergistic research projects drawing on the unique strengths of multidisciplinary research teams at our four major centers: University of Alabama at Birmingham, Harvard University, University of California San Diego, and now Vanderbilt University Medical Center; (2) Foster the development of pilot and feasibility projects and the development and application of new translational methods to research in gout and hyperuricemia and their associated major comorbidities, particularly CKD and metabolic syndrome; and (3) Promote training of translational investigators in current methods of research applicable to gout and hyperuricemia through enrichment activities overseen by our Administrative Core. The proposed revision project directly ties to the CORT through patient enrollment into the Gout CORT Registry and Biorepository, which contributes clinical data and biological samples to projects. The INSIGHT CORT is a multi-disciplinary translational research program at UAB and partner institutions. We have assembled an outstanding team and are uniquely prepared and strongly committed to scientific rigor, innovation, and development of knowledge and translational techniques. Project Narrative The prevalence of gout has been steadily increasing over several decades and is correlated with the rising burden of obesity, chronic cardiac and renal disease; all conditions overrepresented in the Southeastern U.S. – particularly in African Americans. Through a novel emergency department led intervention we aim to improve the care patients with gout receive, both during acute exacerbations and long-term. A secondary goal of the project is to concurrently enhance participation of minorities in biomedical research in the Deep South.","INvestigations In Gout, Hyperuricemia, and comorbidiTies (INSIGHT) Center of Research Translation (CORT)",9902085,P50AR060772,"['Absenteeism', 'Academic Medical Centers', 'Accident and Emergency department', 'Acute', 'Address', 'Adenosine Monophosphate', 'Adult', 'Affect', 'African American', 'Alabama', 'Basic Science', 'Biological', 'Biological Markers', 'Biomedical Research', 'Biometry', 'California', 'Caring', 'Chronic', 'Chronic Disease', 'Chronic Kidney Failure', 'Clinical Data', 'Clinical Informatics', 'Clinical Trials', 'Communities', 'Comorbidity', 'Computerized Medical Record', 'Continuity of Patient Care', 'Deep South', 'Development', 'Diabetes Mellitus', 'Disease Progression', 'Doctor of Philosophy', 'Early identification', 'Educational Intervention', 'Emergency Department-based Intervention', 'Emergency Medicine', 'Enrollment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Face', 'Flare', 'Fostering', 'Frequencies', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Translation', 'Goals', 'Gout', 'Health', 'Healthcare', 'Healthcare Systems', 'Heart Diseases', 'Hyperuricemia', 'Incidence', 'Inflammatory', 'Inflammatory Arthritis', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Laboratories', 'Link', 'Mediating', 'Medical', 'Metabolic syndrome', 'Methodology', 'Methods', 'Minority', 'Minority Participation', 'Mission', 'Modernization', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Natural Language Processing', 'Nephrology', 'Obesity', 'Outcome', 'Patient Care', 'Patient Education', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Premature Mortality', 'Prevalence', 'Productivity', 'Protein Kinase', 'Quality of life', 'Recommendation', 'Registries', 'Renal function', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatology', 'Sampling', 'Serum', 'Specimen', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Translational Research', 'Universities', 'Urate', 'Work', 'aging population', 'base', 'biobank', 'clinically relevant', 'functional genomics', 'gene environment interaction', 'genome wide association study', 'health care service utilization', 'health disparity', 'improved', 'innovation', 'men', 'metabolomics', 'minority health', 'multidisciplinary', 'novel', 'personalized medicine', 'precision medicine', 'productivity loss', 'recruit', 'tool', 'translational research program', 'translational scientist', 'urate transporter']",NIAMS,UNIVERSITY OF ALABAMA AT BIRMINGHAM,P50,2019,961116,-0.017885194089826008
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9789913,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Simulation', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2019,750000,-0.007560771926925439
"Research and development of an open, extensible, web-based information extraction workbench for systematic review Project Summary  1 More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence-based  2 medicine, with each review requiring, on average, between six months to one year of effort to complete. One of the most  3 time consuming and repetitive aspects of this endeavor involves extraction of detailed information from a large number  4 of scientific documents. The specific data items extracted differ among disciplines, but within a given scientific domain,  5 certain data points are extracted repeatedly for each review conducted. Research on use of natural language processing  6 (NLP) for extracting individual data elements has shown that it has the potential to greatly reduce the laborious, time  7 intensive, and repetitive nature of this step. However, there is currently no integrated, automatic data extraction platform  8 that meets the needs of the systematic review community. We propose a web-based data extraction software platform  9 specifically designed for usage in the domain of systematic review. By combining multiple state-of-the-art data extraction 10 methods utilizing NLP, text mining and machine learning, into a single, unified user interface, we will thereby empower 11 the end-user with a powerful and novel tool for automating an otherwise arduous task. 12 The research we propose encompasses three specific aims: (1) develop new data extraction models using deep learning 13 and a new technique called “data programming”; (2) develop a web-based platform to semi-automate the process; (3) 14 design protocols and standards for packaging extraction models as software components and integrating work done by 15 other research groups and vendors. In the first aim, we will contribute novel data extraction modules designed and 16 trained specifically to extract data elements of interest to those conducting systematic reviews in the domain of 17 environmental health. For this research, we will employ state-of-the-art machine learning, NLP and text mining 18 methodologies to train and evaluate several novel extraction components. In our second aim, we will develop a web- 19 based workbench which will allow users to upload scientific documents for automated data extraction. Our system will 20 also be designed to allow for integration of data extraction approaches (components) from other research groups, thus 21 enabling end users to choose from a wide variety of advanced data extraction methodologies within one unified and 22 intuitive software environment. In our third aim, we will develop new protocols to standardize the inputs and outputs 23 for data extraction components. The resulting interface, which will enable seamless integration of third party extraction 24 components into the workbench, will also facilitate the incorporation of feedback from users such that extraction 25 components can be continuously improved based on real-time data. 26 Our overarching goal is to translate emerging semi-automated extraction technologies out of the lab and into practical 27 software and to bring to market both the software itself as well as several premium data extraction components. The 28 results of the research conducted for Aims 1-3 represent the first step in this direction and will provide the foundation for 29 future developments. These result will take us one step closer to the dream of creating “living systematic reviews,” which 30 are maintained using automated or semi-automated methods and updated regularly as new evidence becomes available. Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a flexible, extensible software system that automates the crucial and resource-intensive process of extracting key data elements from scientific documents, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.","Research and development of an open, extensible, web-based information extraction workbench for systematic review",9623091,R43ES029901,"['Communities', 'Computer software', 'Consensus', 'Data', 'Data Element', 'Development', 'Discipline', 'Dreams', 'Environment', 'Environmental Health', 'Evidence Based Medicine', 'Feedback', 'Foundations', 'Future', 'Goals', 'Health', 'Individual', 'Internet', 'Intuition', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Online Systems', 'Output', 'Process', 'Protocols documentation', 'Research', 'Resources', 'Source', 'Standardization', 'Supervision', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Translating', 'Update', 'Vendor', 'Work', 'artificial neural network', 'base', 'data integration', 'deep learning', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'interest', 'learning strategy', 'model design', 'novel', 'research and development', 'software systems', 'systematic review', 'text searching', 'tool']",NIEHS,"SCIOME, LLC",R43,2018,225000,-0.024512611084049504
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9527181,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,545116,-0.029085621524836524
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9532186,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2018,259358,-0.031474850643843415
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9614770,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genetic Diseases', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,538700,0.01644675272617869
"Machine Learning for Generalized Multiscale Modeling Project Summary/Abstract  This project develops machine learning approaches that describe statistical systems in biology. By combining analytic results calculated from the exact probabilistic description of the system with machine learning inference, our new methods present exciting opportunities to model previously inaccessible complex dynamics. The resulting Boltzmann machine-like learning algorithms present a new class of modeling techniques based on the powerful in- ference of arti cial neural networks. Further development of this approach will bring the groundbreaking advances from the surge of recent interest in machine learning into the biological modeling eld. The mathematical methods we develop will be used to derive e cient algorithms for multiscale simulation, directly applicable to large scale biological modeling. In particular, the algorithms will be used to study the dynamics of stochastic biochemistry at synapses, with direct relevance to learning and memory formation in the brain. Current studies of these processes are limited by the long timescales involved and the highly spatially organized structures featured. In addition to leveraging the machine learning expertise we are developing, we also employ new electron microscopy datasets to produce 3D reconstructions of neural tissue with unprecedented accuracy. Consequentially, we will be able to study the fundamental mechanisms underlying synaptic plasticity, as well as the biochemical basis of oscillatory behavior in networks of neurons that occurs during sleep. Furthermore, the interactions of these highly stochastic ion channels with electrical in neurons will be explored through groundbreaking hybrid simulation environments. The software that we will develop combines existing popular simulation tools into multiscale approaches, and will be distributed as a powerful tool to the broader biological modeling community. Its usage in further computational experiments can present a key advancement in the development of pharmaceuticals, allowing the direct study of the interactions of biochemistry and whole neuron electrophysiology without making limiting assumptions to sim- plify the simulations. This has promising implications for intervening in age-related learning de cits, as well as in neurological disorders such as Alzheimers. Finally, this proposal will bring together our existing multiscale modeling community, the National Center for Multi-scale Modeling of Biological Systems (MMBioS), with the MSM consortium. The interactions of these organizations and their communities of expert researchers will foster new collaborative work on exciting multiscale problems in biology, including applications of the machine learning frameworks and software we are developing. 1 Project Narrative  A wide variety of biological systems can be described statistically, from molecular biochemistry up to the network level activity of neurons. This work develops machine learning approaches to approximate these systems, enabling new simulation methods that bridge di erent levels of description. The resulting computational studies aim to shed light on the basis of learning and computation in the brain, and will enable the development of pharmaceutical targets for learning de cits associated with aging and neurological disorders such as Alzheimers. 1",Machine Learning for Generalized Multiscale Modeling,9791802,R56AG059602,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Area', 'Behavior', 'Biochemical', 'Biochemistry', 'Biological Models', 'Biological Neural Networks', 'Biology', 'Brain', 'Calcium', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consequentialism', 'Coupling', 'Data Set', 'Development', 'Dimensions', 'Electron Microscopy', 'Electrophysiology (science)', 'Environment', 'Equation', 'Equilibrium', 'Evolution', 'Fostering', 'Hybrids', 'Image', 'Investigation', 'Ion Channel', 'Learning', 'Libraries', 'Light', 'Machine Learning', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Morphology', 'National Institute of General Medical Sciences', 'Neurons', 'Neuropil', 'Neurosciences', 'Pharmacologic Substance', 'Physics', 'Population', 'Potassium Channel', 'Process', 'Pythons', 'Reaction', 'Research Personnel', 'Sleep', 'Structure', 'Synapses', 'Synaptic plasticity', 'System', 'Techniques', 'Time', 'Tissues', 'United States National Institutes of Health', 'Vertebral column', 'Work', 'age related', 'base', 'biological systems', 'calmodulin-dependent protein kinase II', 'computer studies', 'experimental study', 'information processing', 'insight', 'interest', 'mathematical methods', 'men who have sex with men', 'microscopic imaging', 'multi-scale modeling', 'nervous system disorder', 'particle', 'postsynaptic', 'reconstruction', 'relating to nervous system', 'simulation', 'software development', 'success', 'tool', 'working group']",NIA,UNIVERSITY OF CALIFORNIA-IRVINE,R56,2018,619053,-0.023474974266655138
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9523638,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2018,478806,0.007600649464473578
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9467327,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Algorithms', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Image Analysis', 'Kinetics', 'Laboratories', 'Locomotion', 'Machine Learning', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2018,510448,-0.010477663330421055
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9503469,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Work', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial and ethnic', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2018,505649,-0.029803465997532384
"A Modular Automated Platform for Large-scale Drosophila Experiments and Handling PROJECT SUMMARY / ABSTRACT Animal model systems are a powerful tool researchers use to investigate almost all aspects of biology: genetics, development, neuroscience, disease, and more. And fruit flies – Drosophila melanogaster – with their small size, easy care, and remarkable array of available genetic toolkits, occupy a sweet spot on the model organism spectrum. Over 75% of human diseases with a genetic basis have an analogue in the fly, and Drosophila have been a part of the research for six Nobel prizes. Furthermore, the advent of CRISPR/cas9 and other modern genetic tools has opened the door to modeling other diseases and pathways, leading to greater use of Drosophila for drug screens. A great deal of the work (and the majority of the budget) involved in fly experiments is tedious manual labor, and with advances in computer vision, machine learning, and other analytic techniques, the stage is set to automate many phenotypic screens. In this Phase I SBIR, we propose a robotic system – modular automated platform for large-scale experiments (MAPLE) – that can accomplish a wide variety of fly-handling tasks in Drosophila labs. This robot is the fruit fly version of a liquid handling robot, with a large, open workspace that can house a plethora of modules and several manipulators that can move small parts and animals around that workspace. Building on a collaboration between the de Bivort Lab and FlySorter completed in 2017, we will design, fabricate and validate a commercial system that can collect virgin flies, run behavioral assays, conduct drug screens, and adapt to the needs of fly labs through easy-to-code Python scripts. By strategically combining modules and instructions to the robot, MAPLE can perform a wide variety of tasks in a fly lab, saving experimentalists from repetitive chores, cutting labor costs, and increasing scientific output. Just as pipette robots have become standard equipment in wet labs, we envision our fly handling robot will be the engine that powers Drosophila labs in academia and pharma, enabling new kinds of experiments and freeing researchers from the drudgery of fly pushing. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are a powerful model organism used in the study of disease, neuroscience, development, genetics, and recently in drug screens, too, largely through phenotypic screening. This labor-intensive work is time consuming and expensive, and ripe for automation. We propose a fly-handling robot – analogous to a liquid pipetting robot in a wet lab – that can perform a variety of tasks in Drosophila labs, free researchers from the drudgery of fly pushing, and enable a broader spectrum of experiments that will increase scientific knowledge.",A Modular Automated Platform for Large-scale Drosophila Experiments and Handling,9623017,R43MH119092,"['Academia', 'Address', 'Affect', 'Air', 'Anesthesia procedures', 'Animal Model', 'Animals', 'Architecture', 'Automation', 'Basic Science', 'Behavior', 'Behavioral Assay', 'Biological Models', 'Biology', 'Budgets', 'CRISPR/Cas technology', 'Carbon Dioxide', 'Caring', 'Code', 'Collaborations', 'Computer Vision Systems', 'Computer software', 'Computers', 'Custom', 'Data Collection', 'Deposition', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Screening', 'Drug usage', 'Ensure', 'Equipment', 'Feedback', 'Genetic', 'Genetic Screening', 'Genetic study', 'Grant', 'Hand', 'Human', 'Instruction', 'Knowledge', 'Libraries', 'Liquid substance', 'Machine Learning', 'Manuals', 'Modeling', 'Modernization', 'Neurosciences', 'Nobel Prize', 'Organism', 'Output', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Protocols documentation', 'Pythons', 'Reagent', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Running', 'Savings', 'Scanning', 'Small Business Innovation Research Grant', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Transgenic Organisms', 'Travel', 'Universities', 'Update', 'Vacuum', 'Work', 'analog', 'bone', 'cost', 'design', 'drug discovery', 'experimental study', 'flexibility', 'fly', 'graduate student', 'health science research', 'human disease', 'improved', 'operation', 'programs', 'repository', 'robot control', 'screening', 'tool', 'touchscreen']",NIMH,"FLYSORTER, LLC",R43,2018,348007,-0.02440754684263598
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,-0.008129686452775012
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,-0.014799574920723653
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9525704,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,599947,-0.005166595680152352
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9749413,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,155743,-0.011268172344396055
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9560825,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,674602,-0.011268172344396055
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9545836,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'deep learning', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2018,569784,-0.007437386151048284
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9577591,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Supervision', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'forest', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'tool', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2018,243750,-0.02564741919380322
"A computational approach to early sepsis detection Abstract Significance: In this SBIR project, we propose to improve the performance of InSight, a machine-learning- based sepsis screening system, in situations of limited training data from the target clinical site. The proposed work will make possible prospective clinical deployments to sites which are smaller or lack clinical data repositories, by significantly reducing the amount of training data necessary down to a few weeks of clinical observation. Classically, a machine-learning-based system like InSight requires complete retraining for each new clinical setting, in turn requiring a new and large collection of data from each target deployment site. We will circumvent this requirement via transfer learning techniques, which transfer knowledge acquired previously in a source clinical setting to a new, target setting. Research Questions: Which transfer learning methods and paired classification algorithms are most suitable for use with InSight, requiring minimal target-site training data while maintaining strong performance? Are these methods and algorithms robust across the several common sepsis-spectrum definitions? Prior Work: We have developed InSight using the MIMIC-III retrospective data set, on which it attains an area under the receiver operating characteristic curve (AUROC) of 0.88 for sepsis detection, and 0.74 for 4-hour early sepsis prediction. We have also conducted pilot transfer learning  ≥ experiments in a different clinical task, mortality forecasting, in which transfer learning yields a 10-fold reduction in the amount of target-site training data required to achieve AUROC 0.80. Specific Aims: Aim 1 - to implement and assess side-by-side four diverse transfer learning methods for a retrospective clinical sepsis prediction task, where the source data set is MIMIC-III and the simulated clinical target is a data set drawn from UCSF. Aim 2 - to determine which among the best methods from Aim 1 also provide robust performance when applied to two additional sepsis-spectrum gold standards. Methods: We will prepare implementations of transfer learning methods which use instance transfer, residual learning and/or feature augmentation, kernel length scale transfer, and feature transfer. We will test these methods with applicable classifiers on subsets of the UCSF set, using cross-validation and quantifying discrimination performance in terms of AUROC. The best method/classifier pairs will require no more than 30 examples of septic patients from the target set and attain AUROC superiorities of 0.05 in 0- and 4-hour pre-onset sepsis prediction/detection, relative to the best tested alternative screening systems (Aim 1). The top three pairs will then be tested for robustness to gold standard choice, using septic shock (0- and 4-hour) and SIRS-based sepsis (0-hour) gold standards; in these tests, at least one pair must again attain 0.05 margin of superiority in AUROC versus the alternative screening systems (Aim 2). Future Directions: The results of these experiments will enable InSight to be robustly deployed to diverse clinical sites, yielding high performance without the need for extensive target-site data acquisition. Narrative Clinical decision support (CDS) systems present critical information to medical professionals by examining patient data and providing relevant information. Machine learning is a powerful method for creating CDS tools, but accessing its full strength requires re-training with retrospective data from each target clinical site. We will use transfer learning techniques to dramatically reduce the amount of target-site training data required by InSight, our machine-learning-based CDS tool for sepsis prediction, and empirically evaluate several such methods on a patient data set, using three different sepsis-related gold standards.",A computational approach to early sepsis detection,9557664,R43TR002221,"['Address', 'Age', 'Algorithms', 'Area', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Decision Support Systems', 'Collection', 'Custom', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Discrimination', 'Drops', 'Early Diagnosis', 'Early Intervention', 'Future', 'Gold', 'Healthcare', 'Healthcare Systems', 'Hour', 'Image', 'Immune response', 'Institution', 'Knowledge', 'Learning', 'Length', 'Machine Learning', 'Medical', 'Methods', 'Multicenter Studies', 'Nature', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Psychological Transfer', 'Receiver Operating Characteristics', 'Research', 'Residual state', 'Risk', 'SCAP2 gene', 'Sensitivity and Specificity', 'Sepsis', 'Septic Shock', 'Severities', 'Side', 'Site', 'Small Business Innovation Research Grant', 'Source', 'Survival Rate', 'System', 'Techniques', 'Testing', 'Training', 'Validation', 'Work', 'base', 'clinical data warehouse', 'clinical decision support', 'clinical research site', 'cost', 'data acquisition', 'experimental study', 'improved', 'insight', 'learning strategy', 'mortality', 'performance site', 'portability', 'prospective', 'screening', 'septic', 'septic patients', 'success', 'support tools']",NCATS,"DASCENA, INC.",R43,2018,310782,-0.009586741803022462
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9549126,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2018,81977,-0.001815144945396715
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9422475,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,356646,-0.040986075511068226
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,-0.0021585394389371936
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9635840,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2018,854309,-0.0063645418313463984
"Device for real-time streaming of preclinical research data into a central cloud-based platform Project Summary BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. Traditionally researchers collect data from many disparate instruments and store data on PCs running single license software. Raw data is stored across several locations, analysis is restricted to the PC used to collect the data, and opportunities for collaboration, remote-participation, and data sharing are limited. BehaviorCloud is leveraging cloud data streaming and storage to overcome these barriers to discovery. In 2017 BehaviorCloud released a first version of the underlying cloud platform as well as BehaviorCloud Camera, an open-source “reference implementation” that demonstrates automated video tracking of animal behavior on the BehaviorCloud platform using a consumer-grade smartphone. This tool and the underlying platform are both in active use across academic and pharmaceutical labs. The aim of this Phase I SBIR application is to develop patent-pending “Bridge” technology that allows data streaming from third-party instrumentation into the central web platform. Researchers will bypass the original software and PCs associated with their instruments to control trials through their BehaviorCloud account and receive data back in real-time. BehaviorCloud will provide a public repository to aggregate all of these data and accelerate discovery by providing computational tools for large-scale meta-analysis and machine learning based predictive analytics. Project Narrative BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. The goal of this Phase I SBIR application is to develop the technology to enable streaming of data from all kinds of behavioral and phenotyping instrumentation into the BehaviorCloud platform. BehaviorCloud will aggregate these data into a repository and accelerate discovery by providing tools for collaboration and meta-analysis.",Device for real-time streaming of preclinical research data into a central cloud-based platform,9621228,R43OD025448,"['Adoption', 'Animal Behavior', 'Animal Experimentation', 'Animal Model', 'Area', 'Back', 'Behavioral', 'Bypass', 'Carbon Dioxide', 'Cellular Phone', 'Collaborations', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Devices', 'Goals', 'Heart Rate', 'Information Systems', 'Internet', 'Intervention', 'Legal patent', 'Licensing', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Meta-Analysis', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Process', 'Research', 'Research Contracts', 'Research Personnel', 'Resources', 'Running', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Standardization', 'Stimulus', 'Stream', 'System', 'Technology', 'Temperature', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'cloud based', 'cloud platform', 'computerized tools', 'control trial', 'data management', 'data sharing', 'data warehouse', 'design', 'experimental study', 'instrument', 'instrumentation', 'laptop', 'open source', 'phenotypic data', 'pre-clinical', 'pre-clinical research', 'prototype', 'repository', 'tool', 'wasting', 'web interface']",OD,"BEHAVIORCLOUD, LLC",R43,2018,220420,-0.00140061044865266
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9593406,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2018,1420000,-0.01713388486205485
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9770622,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,380000,-0.025439978737569813
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9589711,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2018,439996,-0.016500509255939714
"Robust Control of the Stem Cell Niche Diana Arguijo has a unique background with a double major in biomedical engineering (BME) and electrical and computer engineering (ECE). Leveraging her strong mathematical background, she will develop computational techniques to identify patterns of epigenetic reprogramming during epithelial development and patterns of real- time electrical recoding of the GI tract reflective of sacral nerve modulation. Her work will provide insights into the robustness and plasticity of underlying biological control schemes. Diana Arguijo will develop machine-learning based computational techniques to analyze epigenetic reprogramming of epithelial development and electrical activities of the enteric nervous system. Her analyses will provide insights into the robustness and plasticity of tissue regulation.",Robust Control of the Stem Cell Niche,9731853,R35GM122465,"['Biological', 'Biomedical Engineering', 'Computational Technique', 'Computers', 'Development', 'Engineering', 'Enteric Nervous System', 'Epigenetic Process', 'Epithelial', 'Gastrointestinal tract structure', 'Machine Learning', 'Mathematics', 'Pattern', 'Regulation', 'Sacral nerve', 'Scheme', 'Time', 'Tissues', 'Work', 'base', 'insight', 'stem cell niche']",NIGMS,DUKE UNIVERSITY,R35,2018,42822,-0.04759153474251848
"Vaccine Beliefs and Decision Making Project Summary This project will use methods from quantitative anthropology to describe the social space of vaccine beliefs that circulate among the general public and to provide an initial assessment of how different belief variations influence decisions to vaccinate. The results will establish, for the first time, the patterns of co-variation in the wide variety of pro- and anti- vaccine beliefs, and which axes of this variation appear associated with decisions to vaccinate. Vaccination is a key public health defense against infectious disease, but the lay public largely does not fully appreciate scientific evidence when making decisions for or against vaccination. Understanding the inter-correlations of these beliefs, therefore, is imperative for designing effective educational interventions that can directly interface with the cultural beliefs that surround vaccination and influence the public's decision making on this issue. The project will leverage insights from two very different but complementary data sources: responses to a nationally representative survey (fielded on the RAND American Life Panel) and social media data from Twitter. Our analytic approach will begin with systematic coding techniques from mixed-methods research to classify vaccine beliefs into a comprehensive set of belief variants. Manual coding will be validated through inter-observer reliability checks and replicated at scale with machine-learning algorithms. Having systematically coded the data, we will then assess whether nationally representative survey data and data mined from Twitter produce similar results using Cultural Consensus Analysis, a technique from quantitative cultural anthropology. From the survey data we will test whether vaccine beliefs are correlated with decisions to vaccinate after controlling for demographic attributes. To ensure completion of this innovative and methodologically expansive project, the project team combines expertise from anthropology, decision science, clinical medicine, and biomathematics. The principal investigator brings to this project multiple years of both academic and industry experience in statistical modelling of cultural data. Project Narrative Vaccination is a key public health defense against infectious disease, but patients' vaccination decisions may be more influenced by broadly circulated cultural beliefs than they are influenced by scientific evidence. This proposed research will systematically map the diversity of the publics' vaccination beliefs, assess how these beliefs influence vaccination decisions, and advise policy makers how to interface more directly with these popular belief systems that are critical to effective vaccination efforts.",Vaccine Beliefs and Decision Making,9557573,R21HD087749,"['Achievement', 'Adolescent', 'Adopted', 'Adult', 'Algorithms', 'American', 'Anthropology', 'Autistic Disorder', 'Behavior', 'Belief', 'Belief System', 'Childhood', 'Clinical Medicine', 'Code', 'Cognitive', 'Communicable Diseases', 'Communities', 'Consensus', 'Cultural Anthropology', 'Data', 'Data Sources', 'Decision Making', 'Disease', 'Educational Intervention', 'Ensure', 'Environment', 'Fright', 'General Population', 'Health Communication', 'Health behavior', 'Immunization', 'Individual', 'Industry', 'International', 'Intervention', 'Lead', 'Life', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Maps', 'Measles', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Parents', 'Patients', 'Pattern', 'Persons', 'Policies', 'Policy Maker', 'Positioning Attribute', 'Principal Component Analysis', 'Principal Investigator', 'Probability', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Methodology', 'Resistance', 'Role', 'Safety', 'Sampling', 'Science', 'Statistical Models', 'Structure', 'Survey Methodology', 'Surveys', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'Vaccinated', 'Vaccination', 'Vaccines', 'Variant', 'Work', 'authority', 'biomathematics', 'cognitive process', 'design', 'efficacy testing', 'experience', 'innovation', 'insight', 'interest', 'prevent', 'response', 'social', 'social media', 'social space', 'theories', 'therapy design', 'vaccine safety']",NICHD,RAND CORPORATION,R21,2018,242266,-0.015893276400846743
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9527186,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2018,516810,-0.02038992505473483
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9474101,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'human microbiota', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'multidisciplinary', 'novel', 'open source', 'operational taxonomic units', 'oral behavior', 'oral microbial community', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,350321,-0.011986432245268396
"2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Summary /Abstract:  The 2018 OSA Biophotonics Congress: Biomedical Optics, 3-6 April 2018, Hollywood, FL, consists of four topical meetings. Two of these meetings, Optical Tomography and Spectroscopy (OTS) and Microscopy, Histopathology and Analytics (Microscopy) provide broad exposure to a very active multidisciplinary field in biomedical imaging and bioengineering focused on illness treatment and health enhancement. The interdisciplinary nature of the co- located meetings will provide cross-fertilization of concepts and techniques between fields with the resulting synergies obtained from such interactions. This proposal is to provide registration and travel support for students and early career professionals presenting at one of these topical meetings.  OTS will focus on new developments in diffuse optics, spectroscopy and other non- invasive tomographic imaging approaches, including the fields of diffuse optical tomography (DOT), photoacoustic tomography (PAT), optical coherence tomography (OCT), wavefront engineering to overcome scattering, as well as new developments in spectroscopic technologies.  Microscopy will include topics central to the development of optical microscopy and in vitro optical sensing for the clinic. Areas such as novel optical approaches, including computational optics, new image processing and segmentation techniques, development of decision-assistance algorithms via machine-learning and other strategies, testing technologies in pre-clinical models, applications to clinical samples, and validation in the clinic will be discussed. Optically enabled microfluidics are included in this track as well. The goal of these efforts should be towards clinical translation.  The general purpose of these meetings is to create an inclusive, open forum for the presentation of high-quality scientific research through plenary and technical sessions, short courses, panels, networking and special events. This method of face-to-face information sharing allows researchers to learn what others in their field and related disciplines are doing and to efficiently learn about new research, tools, and techniques that might be relevant to their work. It allows conversations with colleagues from different institutions around the world and engenders far reaching scientific collaborations – both domestic and international. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering. Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Narrative The 2018 OSA Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics Topical Meetings will discuss important, highly interdisciplinary areas that focus on technological solutions to medical challenges and medical applications, and will cover a diversity of cutting-edge research and innovative new tools and techniques, especially in biomedical imaging and bioengineering. These Topical Meetings will bring together researchers working in all aspects of this field and will serve as a forum for discussion of existing and emerging techniques as well as future directions.","2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering.",9543795,R13EB026325,"['Academic Training', 'Algorithms', 'Area', 'Biomedical Engineering', 'Biophotonics', 'Birds', 'Career Mobility', 'Clinic', 'Clinical', 'Collaborations', 'Congresses', 'Development', 'Diffuse', 'Digital Libraries', 'Discipline', 'Doctor of Philosophy', 'Engineering', 'Ensure', 'Event', 'Exhibits', 'Exposure to', 'Fertilization', 'Fostering', 'Future', 'Goals', 'Grant', 'Health', 'Hearing', 'Histopathology', 'In Vitro', 'Industry', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Massachusetts', 'Medical', 'Methods', 'Microfluidics', 'Microscopy', 'National Institute of Biomedical Imaging and Bioengineering', 'Nature', 'Optical Coherence Tomography', 'Optical Tomography', 'Optics', 'Outcome', 'Paper', 'Participant', 'Peer Review', 'Physicians', 'Pre-Clinical Model', 'Principal Investigator', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Services', 'Societies', 'Special Event', 'Spectrum Analysis', 'Students', 'Techniques', 'Technology', 'Testing', 'Time', 'Translating', 'Travel', 'Underrepresented Minority', 'United States National Institutes of Health', 'Validation', 'Washington', 'Work', 'academic standard', 'base', 'bioimaging', 'career', 'clinical application', 'clinical translation', 'diffuse optical tomography', 'graduate student', 'image processing', 'imaging Segmentation', 'imaging approach', 'indexing', 'innovation', 'meetings', 'multidisciplinary', 'novel', 'optoacoustic tomography', 'posters', 'programs', 'symposium', 'synergism', 'technique development', 'tomography', 'tool']",NIBIB,OPTICAL SOCIETY OF AMERICA,R13,2018,10000,-0.012643687436022085
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9639469,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Research Infrastructure', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computer infrastructure', 'digital', 'disease transmission', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'predictive modeling', 'social', 'social media', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector', 'vector control']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2018,407175,-0.003539252339474563
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9453640,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2018,305500,-0.012811118118361092
"The Blackfynn Platform for Rapid Data Integration and Collaboration Summary One in seven people worldwide suffers from a brain disorder, e.g., epilepsy, Parkinson's, stroke, or dementia. Development of future treatments depends on improving our understanding of brain function and disease, and validating new treatments critically depends on identifying the underlying biomarkers associated with different conditions. Biomarker discovery requires volume, quality, richness, and diversity of data. This Direct-to-Phase II project extends Blackfynn's cloud data management platform for team science, in order to support interactive data curation and integration and to facilitate biomarker discovery. Our first technical aim develops tools to help select, curate, assess, and regularize datasets: we develop novel “live” query capabilities to ensure users discover relevant data, develop mechanisms for using data's provenance to decide on trustworthiness, and build tools for mapping fields to common data elements. These capabilities address the critical, under-served problem of selecting the data to analyze. Our second technical aim develops techniques for incorporating algorithms to link and co-register across multi-modal data and metadata. Using ranking and machine learning, we can incorporate and combine state-of-the-art algorithms for finding data relationships, and we can link to remote data sources. These capabilities enable scientists to analyze richer datasets with multiple data modalities and properties – thus enabling them to discover more complex correlations and biomarkers. In our third aim, Blackfynn's new technical capabilities will be applied to challenges faced by Blackfynn partners, including problems assessing trustworthiness of data annotations, conducting image analysis, modeling epileptic networks, and identifying biomarkers for neuro-oncology indications. As part of this validation we will also develop HIPAA-compliant mechanisms for working with protected and de-identified data together. Together, these three thrusts will ensure that development of the Blackfynn platform results in tools and technologies that meaningfully accelerate scientific understanding and discovery over rich and complex data, leading to improved treatments for neurologic disease.   Narrative This Direct-to-Phase II project extends the Blackfynn cloud data management platform to enable biomarker discovery for research and development of improved drugs, devices and clinical care for patients with neurologic disease: it develops tools for assembling, evaluating, and rating data, and linking it across modalities and to external systems. It also validates the techniques' effectiveness using real challenges faced by Blackfynn partners, in imaging, epilepsy, and brain tumor research.",The Blackfynn Platform for Rapid Data Integration and Collaboration,9468362,R44DA044929,"['Address', 'Algorithms', 'Benchmarking', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain Neoplasms', 'Case Study', 'Clinical Pharmacology', 'Collaborations', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Provenance', 'Data Science', 'Data Set', 'Data Sources', 'Dementia', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Ensure', 'Epilepsy', 'Funding', 'Future', 'Health', 'Health Insurance Portability and Accountability Act', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mental Depression', 'Metadata', 'Modality', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Neurosciences Research', 'Notification', 'Ontology', 'Output', 'Parkinson Disease', 'Patient Care', 'Pharmaceutical Preparations', 'Phase', 'Plug-in', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Science', 'Scientist', 'Semantics', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Standardization', 'Stroke', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Trust', 'Use Effectiveness', 'Validation', 'Work', 'base', 'biomarker discovery', 'clinical application', 'clinical care', 'cloud platform', 'computer science', 'data access', 'data integration', 'data management', 'improved', 'indexing', 'nervous system disorder', 'neuro-oncology', 'neuroimaging', 'novel', 'novel therapeutics', 'open source', 'research and development', 'tool']",NIDA,"BLACKFYNN, INC.",R44,2018,696602,-0.0159304997230244
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),9527792,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug Addiction', 'Drug Receptors', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Epigenetic Process', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'algorithmic methodologies', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'cloud platform', 'computational chemistry', 'computer science', 'data mining', 'design', 'distinguished professor', 'dopamine transporter', 'drug abuse prevention', 'falls', 'genome-wide', 'genome-wide analysis', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'novel therapeutics', 'operation', 'prevent', 'professor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2018,1080816,-0.019728781611598748
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9672008,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,347221,-0.006493846784118702
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9559873,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,300000,-0.006493846784118702
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9621771,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2018,748748,-0.024215923127083334
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9588814,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2018,906891,-0.01015259411262775
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9534738,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'ontology development', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2018,431097,-0.017628856522066097
"Mental, measurement, and model complexity in neuroscience PROJECT SUMMARY Neuroscience is producing increasingly complex data sets, including measures and manipulations of sub- cellular, cellular, and multi-cellular mechanisms operating over multiple timescales and in the context of different behaviors and task conditions. These data sets pose several fundamental challenges. First, for a given data set, what are the relevant spatial, temporal, and computational scales in which the underlying information-processing dynamics are best understood? Second, what are the best ways to design and select models to account for these dynamics, given the inevitably limited, noisy, and uneven spatial and temporal sampling used to collect the data? Third, what can increasingly complex data sets, collected under increasingly complex conditions, tells us about how the brain itself processes complex information? The goal of this project is to develop and disseminate new, theoretically grounded methods to help researchers to overcome these challenges. Our primary hypothesis is that resolving, modeling, and interpreting relevant information- processing dynamics from complex data sets depends critically on approaches that are built upon understanding the notion of complexity itself. A key insight driving this proposal is that definitions of complexity that come from different fields, and often with different interpretations, in fact have a common mathematical foundation. This common foundation implies that different approaches, from direct analyses of empirical data to model fitting, can extract statistical features related to computational complexity that can be compared directly to each other and interpreted in the context of ideal-observer benchmarks. Starting with this idea, we will pursue three specific aims: 1) establish a common theoretical foundation for analyzing both data and model complexity; 2) develop practical, complexity-based tools for data analysis and model selection; and 3) establish the usefulness of complexity-based metrics for understanding how the brain processes complex information. Together, these Aims provide new theoretical and practical tools for understanding how the brain integrates information across large temporal and spatial scales, using formal, universal definitions of complexity to facilitate the analysis and interpretation of complex neural and behavioral data sets. PROJECT NARRATIVE The proposed work will establish new, theoretically grounded computational tools to help neuroscience researchers design and analyze studies of brain function. These tools, which will be made widely available to the neuroscience research community, will help support a broad range of studies of the brain, enhance scientific discovery, and promote rigor and reproducibility.","Mental, measurement, and model complexity in neuroscience",9615324,R01EB026945,"['Address', 'Algorithms', 'Automobile Driving', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Benchmarking', 'Brain', 'Characteristics', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Decision Making', 'Dimensions', 'Foundations', 'Goals', 'Guidelines', 'Human', 'Individual', 'Information Theory', 'Length', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Pattern', 'Physics', 'Process', 'Psyche structure', 'Reproducibility', 'Research Personnel', 'Rodent', 'Sampling', 'Series', 'Stream', 'System', 'Techniques', 'Time', 'Work', 'base', 'computer science', 'computerized tools', 'data modeling', 'design', 'information processing', 'insight', 'nonhuman primate', 'relating to nervous system', 'statistics', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2018,21379,-0.02151146091545368
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9583854,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2018,74515,-0.06158344709267789
"A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers PROJECT SUMMARY/ABSTRACT  This application, “A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers,” is in response to PAR-16-238, Dissemination and Implementation Research in Health (R01). Acute respiratory distress syndrome (ARDS) has high prevalence (10% of intensive care unit admissions) and mortality up to 46%. Low tidal volume ventilation (LTVV) is the most effective therapy for ARDS, lowering mortality by 20-25%, and is part of standard practice. However, use of LTVV is as low as 19% of ARDS patients. There is a poor understanding of the barriers to LTVV adoption: current approaches are deficient because they incorporate biases, lack consistency and comprehensiveness, ignore the influence of interpersonal network- or team- based factors, and do not address setting-specific variation. Our research team has previously identified some patient- and clinician-specific facilitators of and barriers to LTVV adoption. We have used two state-of-the-art data driven methods—data science and network analysis—to preliminarily quantify the impact of a diverse array of potential factors affecting LTVV adoption, including network- and team-based factors. The proposed research is guided by the Consolidated Framework for Implementation Research (CFIR) and Rogers' Diffusion of Innovations theory. The overall goals of the proposed research are to understand the differences in facilitators and barriers to LTVV adoption between academic and community settings through a definitive, systematic study in a large, diverse consortium of medical centers, and to advance implementation science by providing a model for how data science and network analysis can be applied to understand the adoption of a complex intervention. The overarching hypothesis is that there are different patient-, clinician-, network-, and team-based facilitators and barriers to LTVV adoption in academic and community settings. We will determine whether different patient- and clinician- (Aim 1 cohort study, clinician survey, and data science analysis), clinician interpersonal network- (Aim 2 network analysis), and team structure and dynamics-based (Aim 3 team construction and modeling) facilitators of and barriers to LTVV adoption exist between academic and community hospital settings. Successful completion of the proposed research will provide a comprehensive understanding of the differences in the facilitators of and barriers to LTVV adoption between academic and community settings, and will advance implementation science by serving as a model of how data science and network analysis can be applied to complex implementation problems. Implementation strategies that account for all these factors may be more likely to lead to significant practice change. PROJECT NARRATIVE  Acute respiratory distress syndrome (ARDS) has high prevalence and mortality among critically ill patients; low tidal volume ventilation is the most effective therapy for ARDS but is used infrequently. Successful completion of the proposed research will identify differences in the facilitators of and barriers to adoption of low tidal volume ventilation between academic and community hospital settings in a large and diverse consortium of medical centers. The proposed research will generate a model of how data science and network analysis can be used to understand the implementation of a complex evidence-based practice.",A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers,9594103,R01HL140362,"['Acute', 'Admission activity', 'Adoption', 'Adult Respiratory Distress Syndrome', 'Affect', 'Attitude', 'Caring', 'Characteristics', 'Cohort Studies', 'Community Hospitals', 'Complex', 'Critical Illness', 'Data', 'Data Science', 'Diffusion of Innovation', 'Environment', 'Environmental air flow', 'Evidence based practice', 'Goals', 'Health', 'Healthcare Systems', 'Height', 'High Prevalence', 'Hypoxemia', 'Individual', 'Inflammatory', 'Intensive Care Units', 'International', 'Intervention', 'Investigation', 'Knowledge', 'Lead', 'Machine Learning', 'Measurement', 'Medical center', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'Nurses', 'Pathway Analysis', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Physicians', 'Professional Organizations', 'Pulmonary Edema', 'Research', 'Severities', 'Speed', 'Structure', 'Surveys', 'Syndrome', 'System', 'Testing', 'Tidal Volume', 'Variant', 'base', 'community setting', 'computer science', 'dissemination research', 'effective therapy', 'experimental study', 'health care settings', 'implementation research', 'implementation science', 'implementation strategy', 'learning strategy', 'lung injury', 'mortality', 'multidisciplinary', 'novel', 'research data dissemination', 'respiratory', 'response', 'theories']",NHLBI,NORTHSHORE UNIVERSITY HEALTHSYSTEM,R01,2018,793267,-0.014892793805822162
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9532909,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Exposure to', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'bioinformatics resource', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'individualized medicine', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2018,365327,-0.011147379533386745
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9531327,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2018,773018,-0.015035066211128307
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),9571275,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Privacy', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Research Infrastructure', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data management', 'data portal', 'data sharing', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'social', 'success', 'theories', 'therapy design', 'tool']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2018,220751,-0.0010061296907460879
"COINSTAC: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community be- comes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2). The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates a dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informat- ics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an inde- pendent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented stor- age vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and pri- vacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix fac- torization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. 4 Project Narrative  Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don’t have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this ‘missing data’ and allow for pooling of both open and ‘closed’ repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed compu- tational solution for a large toolkit of widely used algorithms. 3","COINSTAC: decentralized, scalable analysis of loosely coupled data",9717051,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,282320,-0.008908629297388802
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9473021,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,649098,-0.00837913592731085
"Integrating Bioinformatics and Clustering Analysis for Disease Surveillance ﻿    DESCRIPTION (provided by applicant):  There has been a tremendous focus in bioinformatics on translation of data from the bench into information and knowledge for clinical decision-making. This includes analysis of human genetics for personalized medicine and treatment. However, there has been much less attention on translational bioinformatics for public health practice such as surveillance of emerging/re-emerging viruses. This involves data acquisition, integration, and analyses of viral genetics to infer origin, spread, and evolution suc as the emergence of new strains. The relevant scientific fields for this practice include certain aspects of molecular epidemiology and phylogeography. Recent attention has focused on viruses of zoonotic origin, which are defined as pathogens that are transmittable between animals and humans. In addition to seasonal influenza and West Nile virus, this classification of pathogens includes novel viruses such as Middle Eastern Respiratory Syndrome and influenza A H7N9. Despite the successes highlighted in the literature, there has been little utilization of bioinformatics resources and tools among state public health, agriculture, and wildlife agencies for zoonotic surveillance. Previously this type of resource has been restricted primarily to those in academia.       While bioinformatics has been sparsely used for surveillance of zoonotic viruses, other applications such as Geospatial Information Systems (GIS) have been employed by state health agencies to analyze spatial patterns of infection. This includes software to produce disease maps using an array of data types such as clinical, geographical, or human mobility data for tasks such as, geocoding, clustering, or outbreak detection. In addition, advances in geospatial statistics have enabled health agencies to perform more powerful space-time analyses to infer spatiotemporal patterns. However, these GIS consider only traditional epidemiological data such as location and timing of reported cases and not the genetics of the virus that causes the disease. This prevents health agencies from understanding how changes in the genome of the virus and the associated environment in which it disseminates impacts disease risk.      The long-term goal of this proposal is to enhance the identification of geospatial hotspots of zoonotic viruses by applying bioinformatics principles to access, integrate, and analyze viral genetics and spatiotemporal reportable disease data. This project will include approaches from bioinformatics, genetics, spatial statistics, GIS, and epidemiology. To do this, I will first measue the utilization of bioinformatics resources and tools as well as the current approaches and limitations identified by state agencies of public health, agriculture, and wildlife for detecting nd predicting hotspots (clusters) of zoonotic viruses (Aim 1). I will then use this feedback to develo a spatial decision support system for detecting and predicting zoonotic hotspots that applies bioinformatics principles to access, integrate, and analyze viral genetics, environmental, and spatiotemporal reportable disease data (Aim 2). In Aim 3, I will then evaluate my system for cluster detection and prediction against a system that does not consider viral genetics and relies on traditional spatiotemporal data, and perform validation of the predictive capability. Additional evaluation of the user's satisfaction and system usability will be evaluated. Project Narrative I will develop and evaluate a spatial decision support system to support surveillance of zoonotic viruses in both human and animal populations. I will use approaches from bioinformatics and public health to integrate genetic sequence data from the virus with data from cases of reported infectious diseases and associated environmental data. A surveillance system that considers the genetics and environment of the virus along with public health data will assist public health officials in making informed decisions regarding risk of infectious diseases.",Integrating Bioinformatics and Clustering Analysis for Disease Surveillance,9406513,F31LM012176,"['Academia', 'Address', 'Agriculture', 'Algorithm Design', 'Algorithms', 'Animals', 'Area', 'Attention', 'Biodiversity', 'Bioinformatics', 'Case Study', 'Clinical', 'Cluster Analysis', 'Communicable Diseases', 'Computer software', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Disease', 'Disease Notification', 'Disease Outbreaks', 'Disease Surveillance', 'Ecology', 'Environment', 'Epidemiology', 'Evaluation', 'Evolution', 'Feedback', 'Future', 'Genbank', 'Genetic', 'Genetic Diseases', 'Geographic Information Systems', 'Geography', 'Goals', 'Health', 'Human', 'Human Genetics', 'Infection', 'Influenza', 'Influenza A Virus, H7N9 Subtype', 'Influenza A virus', 'Knowledge', 'Literature', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Metadata', 'Modeling', 'Molecular Epidemiology', 'Molecular Evolution', 'Pattern', 'Population', 'Public Health', 'Public Health Practice', 'Questionnaire Designs', 'Questionnaires', 'Research', 'Resources', 'Retrospective Studies', 'Risk', 'Scanning', 'Sequence Alignment', 'Syndrome', 'System', 'Time', 'Translations', 'Validation', 'Validity and Reliability', 'Viral', 'Viral Genome', 'Virus', 'Virus Integration', 'West Nile virus', 'Work', 'Zoonoses', 'bioinformatics resource', 'clinical decision-making', 'data acquisition', 'data warehouse', 'disorder risk', 'epidemiologic data', 'health data', 'high risk', 'novel virus', 'pathogen', 'personalized medicine', 'prevent', 'respiratory', 'satisfaction', 'seasonal influenza', 'spatiotemporal', 'statistics', 'success', 'tool', 'usability', 'virus classification', 'virus genetics']",NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,F31,2018,40590,-0.012734327349550495
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9699855,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,75000,-0.018974457614949855
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9537614,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,329257,-0.018974457614949855
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9612854,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2018,10000,-0.0034292281389684052
"UTMB Clinical and Translational Science Award UTMB's CTSA-linked KL2 Scholars Program addresses the significant need for developing future clinical and translational (C&T) investigators, with emphases on team-based research, leadership development, and mentorship training. In our new CTSA hub, we will build on our current CTSA's successes over the past 5½ years, with unique innovative integration of exceptional training approaches, including: 1) mentored career development within CTSA-supported multidisciplinary translational teams (MTTs); 2) an emphasis on individual development plans (IDPs) and pre-established C&T research competencies tailored to the needs of the individual scholar, but also emphasizing team-based competencies; 3) development of leadership competencies through participation in a new Leadership Development Academy; 4) an Academy of Research Mentors (ARM), based within our Institute for Translational Sciences (ITS), to foster mentoring and mentor training; 5) a linked Translational Research Scholars Program (TRSP), which provides an expanded peer group and a wider impact across the institution, with twice-monthly career development seminars and a focus on scholars' research; 6) development of entrepreneurial skills and leadership through participation in a UT System-funded Healthcare Entrepreneurship Program; and 7) collaboration and dissemination of program experience to other CTSA hubs in the Texas Regional CTSA Consortium, and the national CTSA Consortium. An important feature of our KL2 Scholars Program is its linked position within the continuum of career development, from graduate student training in our CTSA TL1 Training Core to the production of independently funded C&T faculty members. Our program is enhanced with by financial support from our Provost's office for scholar expenses, ITS education administrators, the ARM, and the Office of Faculty Affairs, which is focused on faculty development at UTMB. UTMB also ranks nationally in promoting diversity, which will enhance our recruitment of underrepresented minority scholars by the participation of faculty from our Hispanic Center of Excellence, and our Medical School Enrichment Program. Our KL2 Program is well-integrated with other institutional education activities, including our Human Pathophysiology and Translational Medicine graduate program, and courses targeted toward scientific writing, biostatistics, and study design. Early-career faculty as Phase 1 TRSP scholars can compete to become CTSA-supported KL2 Scholars, and with acquisition of their first mentored K-level grants (e.g., K08), advance to a Phase 2 TRSP Scholar, then focusing on the transition from mentored to independent grant funding. Upon acquisition of independent (e.g. R01) funding, scholars advance to become Phase 3 TRSP Scholars and Associate Members in the ARM, to facilitate their development of mentoring skills, and eventually to full ARM membership, with demonstrated success in C&T science mentoring. Our KL2 Scholars Program thus integrates novel team-based training, leadership, and mentoring approaches to foster the career development of future leaders in C&T research. n/a",UTMB Clinical and Translational Science Award,9479717,KL2TR001441,"['Academy', 'Address', 'Administrator', 'Biological', 'Biological Markers', 'Biometry', 'Biopsy', 'Body Surface Area', 'Burn injury', 'Characteristics', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Competence', 'Data', 'Dependence', 'Development', 'Development Plans', 'Down-Regulation', 'Education', 'Enrollment', 'Entrepreneurship', 'Evolution', 'Expression Profiling', 'Faculty', 'Failure', 'Financial Support', 'Fostering', 'Freezing', 'Functional disorder', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genomics', 'Grant', 'Healthcare', 'Hepatitis C virus', 'Hispanics', 'Human', 'Individual', 'Infection', 'Inflammation', 'Institutes', 'Institution', 'Interferons', 'Intervention', 'Leadership', 'Link', 'Liver Fibrosis', 'Longitudinal Studies', 'Machine Learning', 'Measures', 'Medicare', 'Medicare claim', 'Medicine', 'Mentors', 'Mentorship', 'MicroRNAs', 'Modeling', 'NCI Scholars Program', 'Participant', 'Pathway interactions', 'Patients', 'Peer Group', 'Phase', 'Physicians', 'Positioning Attribute', 'Process', 'Production', 'Protocols documentation', 'Regimen', 'Research', 'Research Design', 'Rice', 'Structure', 'System', 'TNFSF15 gene', 'Teacher Professional Development', 'Techniques', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underrepresented Minority', 'Variant', 'Writing', 'base', 'career development', 'cohort', 'college', 'demographics', 'early-career faculty', 'experience', 'graduate student', 'innovation', 'leadership development', 'liver biopsy', 'medical schools', 'member', 'microbial', 'multidisciplinary', 'novel', 'outcome forecast', 'peripheral blood', 'predict clinical outcome', 'program dissemination', 'programs', 'recruit', 'response', 'science education', 'skills', 'student training', 'success', 'translational medicine', 'translational scientist']",NCATS,UNIVERSITY OF TEXAS MED BR GALVESTON,KL2,2018,423431,-0.02221991486094918
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9515974,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health care facility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'implementation strategy', 'innovation', 'inpatient service', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2018,871212,-0.01643137914222039
"Phamarcogenomics of Statin Therapy (SUPPLEMENT) PARENT ABSTRACT The overall objective of the Center ""Pharmacogenomics of Statin Therapy"" (POST) is to apply genomic, transcriptomic, and metabolomic analyses, together with studies in cellular and animal models, and innovative informatic tools, to identify and validate biomarkers for efficacy of statin drugs in reducing risk of cardiovascular disease (CVD), and for adverse effects of statins, specifically myopathy and type 2 diabetes. This multidisciplinary approach is enabled by a team of investigators with expertise in genomics (human, mouse, and molecular), statistics and informatics, and clinical medicine and pharmacology. The Center is comprised of three Projects, two Research Cores, and an Administrative Core. A major aim of Project 1 is the identification of cellular transcriptomic and metabolomic markers for clinical efficacy and adverse effects of statins. This will be accomplished by analyses in statin-exposed lymphoblast cell lines derived from patients with major adverse coronary events, or onset of myopathy or type 2 diabetes on statin treatment, compared with unaffected statin-treated controls. In addition, using genome wide genotypes from these patients, DNA variants will be identified that are associated with statin-induced changes in the transcripts and metabolites that most strongly discriminate affected patients and controls. Project 2 will use a unique, well- characterized panel of 100 inbred mouse strains to discover genetic variation associated with statin- induced myopathy and dysglycemia. Mechanisms underlying these effects will be investigated, with emphasis on the role of dysregulation of autophagy by statin treatment. Projects 1 and 2 will also use relevant cellular and mouse models, respectively, to perform functional studies to validate effects of genes identified in all POST projects as strong candidates for modulating statin efficacy or adverse effects. In Project 3, information derived from genome-wide genotypes, electronic health records, and pharmacy data in a very large and diverse population-based patient cohort will be leveraged to identify and replicate genetic associations with statin efficacy (lipid lowering and CVD event reduction) and adverse effects (myopathy and type 2 diabetes), as well as to assess the overall heritability of these responses. The Clinical Core, based in Kaiser Permanente of Northern California, will provide the clinical information and biologic materials for both Projects 1 and 3. Investigators in the Informatics Core will optimize data analysis and integration of results across all projects. The Administrative Core will provide scientific leadership and management of the Center, and foster scientific interactions and training opportunities. Overall, the research program of this Center provides an innovative model for a ""systems"" approach to pharmacogenomics that incorporates complementary investigative tools to discover and validate genetically influenced determinants of drug response. Moreover, the findings have the potential for guiding more effective use of statins for reducing CVD risk and minimizing adverse effects, and identifying biomarkers of pathways that modulate the multiple actions of this widely used class of drugs. ADMINISTRATIVE SUPPLEMENT ABSTRACT In response to NOT-AG-18-008, our goal is to extend the validation and application of our data integration methodologies into Alzheimer’s disease research. This administrative supplement is designed to extend the work of the existing subaward to the University of Pennsylvania subcontract for the POST Informatics Core. The PGRN POST Informatics Core serves as the central hub for data sharing and coordination across the three POST projects in the PGRN P50 award. One of our jobs is annotating the extensive information that will be collected and providing analysis expertise to the projects as needed. However, to make great strides in scientific progress and ensure that the collective whole of the Center is greater than the sum of the parts, a key function of the Informatics Core is to serve as ‘The Integrator’ to combine these data and information. We and others have shown that integration of complementary omics-based data can provide emergent insights into biological processes compared to what can be learned through any single approach alone. The methods that we are developing to integrate data for statin pharmacogenomic phenotypes will be equally applicable in the area of Alzheimer’s disease. Additionally, recent emphasis on open data science by Alzheimer’s disease researchers provides ample data for us to interrogate our method. We have developed novel statistical analysis tools such as the Analysis Tool for Heritable and Environmental Network Associations (ATHENA), and data visualization tools, such as PhenoGram, both of which are designed to collect and combine information from diverse data sources. With these tools, we will leverage publicly available Alzheimer’s disease datasets to maximize the knowledge gleaned about disease risk for Alzheimer’s diseases. The methodologies that we have been developing as part of the PGRN POST award for the past 2.5 years are clearly applicable to the study of Alzheimer’s disease risk. An important validation step of the application of our methodologies is to apply them to different types of datasets and in different phenotypic areas. This administrative supplement focused on extended research into having an Alzheimer’s disease focus is a great mechanism to simultaneously allow us to validate our methodologies with different types of data and potentially identify important risk factors and pathways toward a better understanding of the etiology of Alzheimer’s disease. Finally, we may have the opportunity to identify cross biological implications due to the known pleiotropic relationships between Alzheimer’s disease and cardiovascular disease. PROJECT NARRATIVE This administrative supplement is focused on extended our data integration research into Alzheimer’s disease. This funding mechanism will simultaneously allow us to validate our methodologies with different types of data and also potentially identify important risk factors and pathways toward a better understanding of the etiology of Alzheimer’s disease.",Phamarcogenomics of Statin Therapy (SUPPLEMENT),9719267,P50GM115318,"['Administrative Supplement', 'Adverse effects', 'Affect', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Animal Model', 'Area', 'Autophagocytosis', 'Award', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'California', 'Cardiovascular Diseases', 'Cell Line', 'Cell model', 'Clinical', 'Clinical Markers', 'Clinical Medicine', 'Clinical Pharmacology', 'DNA', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Electronic Health Record', 'Ensure', 'Etiology', 'Event', 'Fostering', 'Funding Mechanisms', 'Genes', 'Genetic Variation', 'Genomics', 'Genotype', 'Glean', 'Goals', 'Heritability', 'Human', 'Inbred Strains Mice', 'Informatics', 'Knowledge', 'Leadership', 'Learning', 'Lipids', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Myopathy', 'Non-Insulin-Dependent Diabetes Mellitus', 'Occupations', 'Parents', 'Pathway interactions', 'Patients', 'Pennsylvania', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Pharmacy facility', 'Phenotype', 'Population Heterogeneity', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Risk Factors', 'Role', 'Sampling', 'Source', 'Statistical Data Interpretation', 'Sum', 'System', 'Transcript', 'Universities', 'Validation', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'cardiovascular disorder risk', 'clinical efficacy', 'cohort', 'coronary event', 'data access', 'data hub', 'data integration', 'data sharing', 'data visualization', 'design', 'experimental study', 'genetic association', 'genome wide association study', 'genome-wide', 'genotyped patients', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'knowledge integration', 'lymphoblast', 'metabolomics', 'mouse model', 'multiple omics', 'novel', 'novel marker', 'open data', 'population based', 'predictive marker', 'programs', 'response', 'risk minimization', 'statistics', 'tool', 'training opportunity', 'transcriptomics']",NIGMS,CHILDREN'S HOSPITAL & RES CTR AT OAKLAND,P50,2018,395986,-0.062025884145079764
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9404042,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2018,564487,-0.01451183588858766
"Innovative text-mining tools to accelerate citation screening: comparative efficiency and impact on conclusions Project Summary Systematic reviews (SRs) synthesize and critically assess bodies of evidence to produce a comprehensive unbiased assessment of what is known. As such, SRs are vital for evidence-based decision-making. However, given the pace of new research, the process of developing an SR remains too slow. One particularly time-consuming step in the process is citation screening, which requires manual review of thousands of abstracts to identify only a small number of relevant studies. Screening such large numbers of studies is necessary because systematic reviewers place a high priority on identifying all relevant studies to avoid bias. Innovative citation screening tools, which utilize text-mining and new sophisticated machine learning methods, represent one potential solution. Abstrackr (Brown University) and EPPI-Reviewer (University College London) are off- the-shelf, web-based citation screening tools designed to improve screening efficiency. Both programs utilize machine- learning techniques to semi-automate the screening process by modeling the probability that each citation will meet criteria for inclusion. This allows efficiency gains through screening prioritization and screening truncation. With screening prioritization, citations are organized for screening from highest to lowest likelihood of inclusion. This allows earlier retrieval of full-text articles and facilitates workflow planning. Organizing citations by likelihood of inclusion also allows reviewers the option of truncating the screening process when remaining citations fall below a certain threshold. While promising, existing studies have predominantly been performed by computer scientists testing individual tools or comparing different modeling algorithms (e.g., various classifiers). To date, no studies have performed a direct comparison of citation screening tools. Similarly, although automatically excluding citations that fall below particular thresholds could substantively improve efficiency, adoption has been low due to concerns that relevant studies could be missed. However, how often studies would be missed and how important such omissions would be remains unknown. To address these knowledge gaps, this project will (1) Compare screening efficiency for two citation-screening tools, Abstrackr and EPPI-reviewer, and (2) Characterize the potential impact of using thresholds to exclude low probability studies automatically. To address aim 1, using citations from 3 large and 6 small completed evidence reports, we will compare Abstrackr to EPPI-Reviewer for citation screening. Using screening prioritization, we will assess what proportion of articles must be screened to identify all included studies (e.g., to achieve 100% sensitivity). For Aim 2, we will explore the potential impact of excluding all citations that fall below particular thresholds during the screening process. We will also assess to what extent missing these studies would alter report conclusions. By characterizing potential efficiency gains from new, innovative, and widely accessible tools, this project can facilitate wider adoption by evidence based practice centers seeking to speed systematic review production. Project Narrative Systematic reviews provide a comprehensive, unbiased assessment of a body of literature and are vital for timely, evidence-based decision-making. However, development of systematic reviews remains too slow, with one particularly time-consuming step being citation screening, in which thousands of research abstracts are manually reviewed to identify a small number of relevant studies. By testing potential gains in citation screening efficiency offered by two innovative, widely-accessible machine- learning tools (Abstrackr and EPPI–Reviewer), and determining if automatically excluding studies to improve speed compromises report conclusions, we hope to enable more rapid production of systematic reviews to inform clinicians and policy-makers and promote high quality, evidence-based patient care.",Innovative text-mining tools to accelerate citation screening: comparative efficiency and impact on conclusions,9435231,R03HS025859,[' '],AHRQ,ECRI INSTITUTE,R03,2018,95324,-0.009115619674658248
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9407024,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2018,395628,-0.00033933777463023294
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9536962,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2018,264913,-0.003918137454347447
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9593059,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Simulation', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data management', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2018,750000,-0.007560771926925439
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9403171,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Learning', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,548068,-0.029085621524836524
"CRCNS: Representational foundations of adaptive behavior in natural and artificial  ﻿    DESCRIPTION (provided by applicant): Overview: Among the most celebrated success stories in computational neuroscience is the discovery that many aspects of decision-making can be understood in terms of the formal framework of reinforcement learning (RL). Ideas drawn from RL have shed light on many behavioral phenomena in learning and action selection, on the functional anatomy and neural processes underlying reward-driven behavior, and on fundamental aspects of neuromodulatory function. However, for all these successes, RL-based work is haunted by an inconvenient truth: Standard RL algorithms scale poorly to large, complex problems. If human learning and decision-making are driven by RL-like mechanisms, how is it that we cope with the kinds of rich, large-scale tasks that are typical of everyday life? Existing research in both psychology and neuroscience hints at one answer to this question: Complex problems can be conquered if the decision-maker is equipped with compact, intelligently formatted representations of the task. This principle is seen in studies of expert play in chess, which show that chess masters leverage highly integrative internal representations of board configurations; in studies of frontal and parietal lobe function, which have revealed receptive fields strongly shaped by task contingencies; and studies on the hippocampus, which point to the role of this structure in supporting a hierarchically organized 'cognitive map,' of task space. Not coincidentally, the critical role of representation has come increasingly to the fore in RL-based research in machine learning and robotics, with growing interest in techniques for dimensionality reduction, hierarchy and deep learning.            The present project aims toward a systematic, empirically validated account of the role of representation in supporting RL and goal-directed behavior at large. The project brings together three investigators with complementary expertise in cognitive and computational neuroscience (Botvinick, Gershman) and machine learning and robotics (Konidaris). Together, we propose an integrative, interdisciplinary program of research, applying behavioral and neuroimaging work with human subjects, computational modeling of neurophysiological and behavioral data, formal mathematical work and simulations with artificial agents. The proposed studies are diverse in theme and method, but work together toward a theory that is both formally grounded and empirically constrained. At a more concrete level, our research focuses on four specific classes of representation, considering the computational impact of each for RL, as well as the relevance of each to neuroscience and human behavior. As detailed in our Project Description, these include (1) metric embedding, (2) spectral decomposition, (3) hierarchical representation and (4) symbolic representation. In addition to investigating the implications of each of these four forms of representation individually, we hypothesize that they fit together into a tiered system, which works as a whole to support the sometimes competing demands of learning and action control.         Intellectual Merit (provided by applicant): Understanding how representational structure impacts learning and decision making is a core challenge in cognitive science, behavioral neuroscience and, artificial intelligence. Success in establishing a computationally explicit, empirically validated theory in this area, with a specific focus on the role of representation in R, would represent an important achievement with wide repercussions. The strategy of leveraging conceptual tools from machine learning to investigate human behavior and brain function can offer considerable scientific leverage, as our own previous research illustrates. The proposed work is motivated by and builds upon established lines of research, bringing these together in order to capitalize on opportunities for synergy. In addition to answering specific empirical and computational questions, the proposed work aims to open up new avenues for future research in an important area of inquiry.         Broader Impact (provided by applicant): The proposed work lies at the crossroads of neuroscience, psychology, artificial intelligence and machine learning, and promises to advance the growing exchange among these fields. The project brings together investigators with contrasting disciplinary affiliations, with the explicit goal of bridging between intellectual cultres. The proposed work is likely to find a wide scientific audience, given its relevance to cognitive and developmental psychology, behavioral, cognitive and systems neuroscience, and behavioral economics. However, the work is likely to be of equal interest within artificial intelligence, machine learning, and robotics, where a current challenge is precisely to understand how representation learning can allow RL to scale up to large problems. Representational approaches to RL are already of intense interest within industry, where the present investigators have a record of active engagement. The topic of the proposed work has applicability in other areas as well, including education and training, and military and medical decision support. The plan for the project has a robust training component at both graduate and postdoctoral levels, with a commitment to fostering involvement of underrepresented minorities, as well as international engagement. n/a",CRCNS: Representational foundations of adaptive behavior in natural and artificial ,9292377,R01MH109177,"['Achievement', 'Adaptive Behaviors', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Cognitive Science', 'Complex', 'Computer Simulation', 'Data', 'Decision Making', 'Dimensions', 'Fostering', 'Foundations', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Industry', 'International', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Mathematics', 'Medical', 'Methods', 'Military Personnel', 'Neurosciences', 'Parietal Lobe', 'Play', 'Process', 'Psychological reinforcement', 'Psychology', 'Research', 'Research Personnel', 'Rewards', 'Robotics', 'Role', 'Structure', 'System', 'Techniques', 'Training', 'Training and Education', 'Underrepresented Minority', 'Work', 'base', 'behavioral economics', 'cognitive neuroscience', 'cognitive system', 'computational neuroscience', 'developmental psychology', 'frontal lobe', 'human subject', 'interest', 'neuroimaging', 'neurophysiology', 'neuroregulation', 'programs', 'receptive field', 'relating to nervous system', 'research based learning', 'scale up', 'simulation', 'success', 'synergism', 'theories', 'tool']",NIMH,PRINCETON UNIVERSITY,R01,2017,373529,-0.027737201999924334
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9392642,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2017,262150,-0.031474850643843415
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,-0.008129686452775012
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,-0.014799574920723653
"A high-throughput imaging and classification system for fruit flies PROJECT SUMMARY / ABSTRACT In this Phase I SBIR application, FlySorter proposes to development a high throughput imaging and classification system to aid research with fruit flies, a widely-used model organism relevant to both basic science as well as studies in human health. The use of animal model systems is essential for research in almost all aspects of biology: genetics, development, neuroscience, disease, physiology, and beyond. The fruit fly – Drosophila melanogaster – is small and easy to care for, but is complex enough an organism to provide a wealth of information that directly relates to human biology and health. Over 75% of human diseases with a genetic basis (including depression, alcoholism, certain forms of cancer, and many more) are either present or have an analog in Drosophila. Modern genetic tools, such as CRISPR/cas9, allow the creation of transgenic flies that provide the opportunity to study diseases, pathways and systems that don’t exist naturally in Drosophila. With these advances, fruit flies are becoming more frequently subjects for drugs screens. For all the advances in the biological tools and techniques applicable to flies, however, the limiting factor in many experiments is the manual labor involved in a few common tasks: moving flies from vial to vial or other lab equipment; classifying and sorting flies by sex, eye color and other phenotypes; and collecting virgin female flies before they mate so that they can be used in controlled crosses, etc. FlySorter’s patent-pending fly dispensing mechanism can reliably deliver a single organism from a vial containing hundreds of awake flies, and our novel FlyPlate system allows storage of individual flies in custom 96 well plates. FlySorter’s robotic fly handling system, co-developed with the de Bivort Lab at Harvard, is capable of manipulating and transporting those individual flies between vial, 96 well plate, and experimental apparatus. The next piece of the automation puzzle to solve is high throughput imaging and classification. To accomplish this goal, FlySorter will: 1) complete a prototype automated image capture hardware system; 2) adapt state-of-the-art computer vision and machine learning algorithms for use on Drosophila; and 3) build a module that can physically sort the classified flies into different vials. Once integrated into the existing FlySorter product ecosystem, this imaging and classification module will greatly expand the kinds of experiments and screens that can be automated, allowing for the study of larger populations or a wider variety of flies, reducing the impact of human error, and freeing up valuable time for researchers. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are one of the most widely used model organisms in biology, for research in genetics, development, neuroscience, disease, and much more. One of the most common tasks in Drosophila labs is sorting flies by various markers and phenotypes using a microscope and paintbrush. FlySorter aims to build an automated system for sorting flies using high resolution digital cameras and modern computer vision algorithms, which will obviate the need for such tedious manual labor.",A high-throughput imaging and classification system for fruit flies,9408980,R43OD023302,"['Air', 'Alcoholism', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Animal Model', 'Animals', 'Appearance', 'Automation', 'Basic Science', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Neural Networks', 'Biology', 'CRISPR/Cas technology', 'Caring', 'Classification', 'Code', 'Complex', 'Computer Vision Systems', 'Custom', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Pathway', 'Dorsal', 'Drosophila genus', 'Drosophila melanogaster', 'Ecosystem', 'Ensure', 'Eye', 'Eye Color', 'Female', 'Floor', 'Genes', 'Genetic', 'Genetic Screening', 'Genetic study', 'Genotype', 'Goals', 'Grant', 'Head', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Legal patent', 'Lighting', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mechanics', 'Mental Depression', 'Methodology', 'Microscope', 'Modernization', 'Motor', 'Mutation', 'Names', 'Neurosciences', 'Obesity', 'Optics', 'Organism', 'Partner in relationship', 'Phase', 'Phenotype', 'Physiology', 'Population', 'Preclinical Drug Evaluation', 'Pump', 'Research', 'Research Personnel', 'Resolution', 'Robot', 'Robotics', 'Sampling', 'Sclera', 'Shapes', 'Small Business Innovation Research Grant', 'Sorting - Cell Movement', 'Standardization', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transgenic Organisms', 'Universities', 'Vial device', 'Walking', 'Work', 'analog', 'awake', 'base', 'depression model', 'digital', 'digital imaging', 'experimental study', 'fly', 'genetic strain', 'human disease', 'improved', 'interest', 'laboratory equipment', 'male', 'meter', 'novel', 'phenotypic biomarker', 'prevent', 'prototype', 'sex', 'tool', 'virtual']",OD,"FLYSORTER, LLC",R43,2017,225000,-0.018250448709365836
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9310440,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2017,599995,-0.005166595680152352
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility. PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9270528,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'Supervision', 'System', 'Time', 'Training', 'clinical predictors', 'cohort', 'computer science', 'crowdsourcing', 'data sharing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2017,316000,-0.012769380228101361
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9384200,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2017,685783,-0.011268172344396055
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,0.0051207735848777325
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9371707,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Databases', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'permissiveness', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2017,93824,-0.001815144945396715
"Policy Surveillance: the Future of Scientific Legal Mapping. “Policy surveillance” is the “ongoing, systematic collection, analysis, interpretation and dissemination of information about a given body of public health law and policy” (Chriqui, O'Connor, and Chaloupka 2011). Policy surveillance produces reliable longitudinal data for evaluating the health effects of laws; it can also increase public and policymaker awareness of trends in the law, and support diffusion of promising and evidence-based policy innovations. The National Institutes of Health (NIH) supported the development of policy surveillance programs, through NIAAA’s Alcohol Policy Information System (APIS) and the National Cancer Institute’s CLASS. More recently, NIDA has funded the launch of two policy surveillance programs, the Prescription Drug Abuse Policy System (PDAPS) and the Drug Abuse Policy System (DAPS). CDC has been increasing its support for policy surveillance activities, and the Robert Wood Johnson Foundation has funded a Policy Surveillance Program to provide methodological and technical assistance, and maintain LawAtlas.org, a general health policy surveillance portal. As more and more researchers and consumers enter the field — from the state, federal and international levels and in private industry — harmonization of terms, methodological standards, and consensus on any needed governance mechanisms is valuable to ensure continued growth and adoption of this important scientific process and its valuable products. To meet these needs, we propose to convene one conference of 30-35 invited attendees who represent the practice’s primary stakeholders from across research, technology and practice; develop, publish and disseminate at least one report that summarizes the discussion and consensus reached among attendees; and establish a one-year plan of collaborative work towards further cooperation and cohesion. Project Narrative Policy surveillance produces reliable longitudinal data for evaluating the health effects of laws, can increase public and policymaker awareness of trends in the law, and support diffusion of promising and evidence-based policy innovations to improve health. A conference funded by NIH will harmonize terms and methodological standards, and will build consensus on field leadership and governance needed to ensure continued growth and adoption of this important scientific process and its valuable products.",Policy Surveillance: the Future of Scientific Legal Mapping.,9331919,R13DA043979,"['Adoption', 'Artificial Intelligence', 'Awareness', 'Categories', 'Centers for Disease Control and Prevention (U.S.)', 'Code', 'Collection', 'Consensus', 'Data', 'Development', 'Diffusion', 'Drug abuse', 'Emerging Technologies', 'Ensure', 'Foundations', 'Funding', 'Future', 'Goals', 'Growth', 'Health', 'Health Policy', 'Industry', 'Industry Standard', 'Information Dissemination', 'Information Technology', 'International', 'Laws', 'Leadership', 'Legal', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'National Cancer Institute', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Outcome', 'Policies', 'Privatization', 'Process', 'Public Health', 'Public Health Schools', 'Publishing', 'Quality Control', 'Reporting', 'Research', 'Research Personnel', 'Site', 'Statutes and Laws', 'Sum', 'Surveillance Program', 'System', 'Technology', 'Text', 'United States National Institutes of Health', 'Variant', 'Wood material', 'Work', 'alcohol policy information system', 'cohesion', 'data access', 'data structure', 'development policy', 'evidence base', 'improved', 'innovation', 'innovative technologies', 'prescription drug abuse', 'programs', 'public health intervention', 'symposium', 'tool', 'trend', 'uptake']",NIDA,TEMPLE UNIV OF THE COMMONWEALTH,R13,2017,24772,-0.01809708995913573
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,229691,-0.0021585394389371936
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9270498,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Taxonomy', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,311153,-0.011986432245268396
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9398728,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Development', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2017,546372,-0.02038992505473483
"Vaccine Beliefs and Decision Making Project Summary This project will use methods from quantitative anthropology to describe the social space of vaccine beliefs that circulate among the general public and to provide an initial assessment of how different belief variations influence decisions to vaccinate. The results will establish, for the first time, the patterns of co-variation in the wide variety of pro- and anti- vaccine beliefs, and which axes of this variation appear associated with decisions to vaccinate. Vaccination is a key public health defense against infectious disease, but the lay public largely does not fully appreciate scientific evidence when making decisions for or against vaccination. Understanding the inter-correlations of these beliefs, therefore, is imperative for designing effective educational interventions that can directly interface with the cultural beliefs that surround vaccination and influence the public's decision making on this issue. The project will leverage insights from two very different but complementary data sources: responses to a nationally representative survey (fielded on the RAND American Life Panel) and social media data from Twitter. Our analytic approach will begin with systematic coding techniques from mixed-methods research to classify vaccine beliefs into a comprehensive set of belief variants. Manual coding will be validated through inter-observer reliability checks and replicated at scale with machine-learning algorithms. Having systematically coded the data, we will then assess whether nationally representative survey data and data mined from Twitter produce similar results using Cultural Consensus Analysis, a technique from quantitative cultural anthropology. From the survey data we will test whether vaccine beliefs are correlated with decisions to vaccinate after controlling for demographic attributes. To ensure completion of this innovative and methodologically expansive project, the project team combines expertise from anthropology, decision science, clinical medicine, and biomathematics. The principal investigator brings to this project multiple years of both academic and industry experience in statistical modelling of cultural data. Project Narrative Vaccination is a key public health defense against infectious disease, but patients' vaccination decisions may be more influenced by broadly circulated cultural beliefs than they are influenced by scientific evidence. This proposed research will systematically map the diversity of the publics' vaccination beliefs, assess how these beliefs influence vaccination decisions, and advise policy makers how to interface more directly with these popular belief systems that are critical to effective vaccination efforts.",Vaccine Beliefs and Decision Making,9241259,R21HD087749,"['Achievement', 'Adolescent', 'Adopted', 'Adult', 'Algorithms', 'American', 'Anthropology', 'Autistic Disorder', 'Behavior', 'Belief', 'Belief System', 'Childhood', 'Clinical Medicine', 'Code', 'Cognitive', 'Communicable Diseases', 'Communities', 'Consensus', 'Cultural Anthropology', 'Data', 'Data Sources', 'Decision Making', 'Disease', 'Educational Intervention', 'Ensure', 'Environment', 'Fright', 'General Population', 'Health Communication', 'Health behavior', 'Immunization', 'Individual', 'Industry', 'International', 'Intervention', 'Lead', 'Life', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Maps', 'Measles', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Parents', 'Patients', 'Pattern', 'Persons', 'Policies', 'Policy Maker', 'Positioning Attribute', 'Principal Component Analysis', 'Principal Investigator', 'Probability', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Methodology', 'Resistance', 'Role', 'Safety', 'Sampling', 'Science', 'Statistical Models', 'Structure', 'Survey Methodology', 'Surveys', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'Vaccinated', 'Vaccination', 'Vaccines', 'Variant', 'Work', 'authority', 'biomathematics', 'cognitive process', 'design', 'efficacy testing', 'experience', 'innovation', 'insight', 'interest', 'prevent', 'response', 'social', 'social media', 'social space', 'theories', 'therapy design', 'vaccine safety']",NICHD,RAND CORPORATION,R21,2017,239723,-0.015893276400846743
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9251828,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Modernization', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Standardization', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'interoperability', 'knowledge translation', 'learning progression', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool', 'translational impact']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,428512,-0.029360706706889554
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  As part of its activities, the National Toxicology Program (NTP) at NIEHS conducts literature-based evaluations to identify the state of the science, evaluate hazards, and determine the effectiveness of NTP’s research. NTP lacks an intelligent automated approach to assist with this work. NIEHS has requested assistant from the Oak Ridge National Laboratory (ORNL), Department of Energy to assist with the research and development of publication and web mining tools for use in NTP’s evaluations. This assistance extends to the Division of Extramual Research and Training to automate the approach for capturing outcomes and impacts from NIEHS-funded research noted in scientific publications and grantees’ progress reports. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9492481,ES16002001,"['Applications Grants', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Funding', 'Imagery', 'Internet', 'Laboratories', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Outcome', 'Program Effectiveness', 'Program Evaluation', 'Progress Reports', 'Publications', 'Research', 'Research Training', 'Science', 'Techniques', 'Work', 'base', 'hazard', 'learning strategy', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2017,446000,-0.0058512637085764985
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9285168,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2017,267313,-0.012811118118361092
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9486059,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2017,1408263,-0.02788295357088705
"The Blackfynn Platform for Rapid Data Integration and Collaboration Summary One in seven people worldwide suffers from a brain disorder, e.g., epilepsy, Parkinson's, stroke, or dementia. Development of future treatments depends on improving our understanding of brain function and disease, and validating new treatments critically depends on identifying the underlying biomarkers associated with different conditions. Biomarker discovery requires volume, quality, richness, and diversity of data. This Direct-to-Phase II project extends Blackfynn's cloud data management platform for team science, in order to support interactive data curation and integration and to facilitate biomarker discovery. Our first technical aim develops tools to help select, curate, assess, and regularize datasets: we develop novel “live” query capabilities to ensure users discover relevant data, develop mechanisms for using data's provenance to decide on trustworthiness, and build tools for mapping fields to common data elements. These capabilities address the critical, under-served problem of selecting the data to analyze. Our second technical aim develops techniques for incorporating algorithms to link and co-register across multi-modal data and metadata. Using ranking and machine learning, we can incorporate and combine state-of-the-art algorithms for finding data relationships, and we can link to remote data sources. These capabilities enable scientists to analyze richer datasets with multiple data modalities and properties – thus enabling them to discover more complex correlations and biomarkers. In our third aim, Blackfynn's new technical capabilities will be applied to challenges faced by Blackfynn partners, including problems assessing trustworthiness of data annotations, conducting image analysis, modeling epileptic networks, and identifying biomarkers for neuro-oncology indications. As part of this validation we will also develop HIPAA-compliant mechanisms for working with protected and de-identified data together. Together, these three thrusts will ensure that development of the Blackfynn platform results in tools and technologies that meaningfully accelerate scientific understanding and discovery over rich and complex data, leading to improved treatments for neurologic disease.   Narrative This Direct-to-Phase II project extends the Blackfynn cloud data management platform to enable biomarker discovery for research and development of improved drugs, devices and clinical care for patients with neurologic disease: it develops tools for assembling, evaluating, and rating data, and linking it across modalities and to external systems. It also validates the techniques' effectiveness using real challenges faced by Blackfynn partners, in imaging, epilepsy, and brain tumor research.",The Blackfynn Platform for Rapid Data Integration and Collaboration,9343385,R44DA044929,"['Address', 'Algorithms', 'Benchmarking', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain Neoplasms', 'Case Study', 'Clinical Pharmacology', 'Collaborations', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Provenance', 'Data Science', 'Data Set', 'Data Sources', 'Dementia', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Ensure', 'Epilepsy', 'Funding', 'Future', 'Health', 'Health Insurance Portability and Accountability Act', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mental Depression', 'Metadata', 'Modality', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Neurosciences Research', 'Notification', 'Ontology', 'Output', 'Parkinson Disease', 'Patient Care', 'Pharmaceutical Preparations', 'Phase', 'Plug-in', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Science', 'Scientist', 'Semantics', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Standardization', 'Stroke', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Trust', 'Use Effectiveness', 'Validation', 'Work', 'base', 'biomarker discovery', 'clinical application', 'clinical care', 'cloud platform', 'computer science', 'data access', 'data integration', 'data management', 'improved', 'indexing', 'nervous system disorder', 'neuro-oncology', 'neuroimaging', 'novel', 'novel therapeutics', 'open source', 'research and development', 'tool']",NIDA,"BLACKFYNN, INC.",R44,2017,652921,-0.0159304997230244
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),9321115,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug Addiction', 'Drug Receptors', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Epigenetic Process', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'algorithmic methodologies', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'cloud platform', 'computational chemistry', 'computer science', 'data mining', 'design', 'distinguished professor', 'dopamine transporter', 'drug abuse prevention', 'falls', 'genome-wide', 'genome-wide analysis', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'novel therapeutics', 'operation', 'prevent', 'professor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2017,1080816,-0.019728781611598748
"Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.",Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons,9275674,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Docking', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,119777,-0.004880327533337241
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9301573,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,216422,-0.002716870856423307
"All of Us, Wisconsin Our application for OT-PM-16-003, “One in a Million – Precision Medicine Initiative Wisconsin” represents the collaborative efforts of three fully integrated regional healthcare systems to form a virtual state-wide integrated delivery network. The application originates at the Marshfield Clinic Health System (MCHS), one of the most productive and earliest adopters of what has become Precision Medicine. The work of the MCHS has been cited by NIH Director Dr. Francis Collins as a model for the Precision Medicine Initiative (PMI). To ensure comprehensive nearly state-wide coverage of participant health care, MCHS has partnered with the other academic integrated delivery networks in Wisconsin, (University of Wisconsin (UW) and Medical College of Wisconsin (MCW)), that together include 173 clinics, 13 hospitals, insurer partners and 5 Federally Qualified Health Centers (FQHCs). The Blood Center of Wisconsin (BCW) also partners to operate state-wide mobile units of staff and equipment for blood collection that may be purposed for this effort. Through an independent agreement with Aurora Health, the electronic health records of participants recruited by academic and FQHC sites will be made available, ensuring nearly complete coverage of health care records across an entire state, with special emphasis on those populations that are traditionally the most underserved and understudied. The academic sites and FQHC partners have a long history of research cooperation and robust community engagement demonstrated by Clinical and Translational Science Awards (CTSA) ties, the Wisconsin Genome Initiative, reciprocal IRB arrangements, multiple joint grants and other collaborative studies. We have pioneered and implemented data sharing platforms and have demonstrated their usefulness with numerous high impact publications over many years. All three academic centers are leaders in the details of community engagement, recruitment, tracking, return of results and data sharing across various platforms in national and international consortia using large cohorts (e.g., MCHS’s 20,000 participant Personalized Medicine Research Project). A major innovation of this application is that it aligns the major healthcare systems of an entire state to more fully capture each participant’s data wherever participants get their health care. Indeed, 80% of all health care in Wisconsin is captured by our footprint. The State-wide catchment area also includes the oldest, sickest and poorest regions in the US and represent rural and urban areas, including African Americans, Hispanics and Native Americans, all represented as community champions. Through the FQHC partners, the recruiting institutions will achieve the 40% African American, Hispanic and Native American participant rate, with long standing community engagement and history of successful recruitment, including Wisconsin’s 12 Native American Tribes. Lending active support are Community Champions and Community Advisory Boards. A second innovative approach will employ Machine Learning (ML) techniques to optimize recruitment and retention processes. Our scientists have long collaborated on efforts to enhance the breadth and reliability of information extracted from the electronic health record (EHR), using a range of data to identify elements including family pedigrees or the occurrence of clinical events that are susceptible to unreliable coding. These efforts employed state-of-the-art methods including random forests, support vector machines and statistical relational learning, among others. These advanced computational methods will be used to predict those who are most likely to participate and which methods of approach are most effective. A third innovative approach is the planned use of mobile recruitment labs, previously successful with the Survey of the Health of Wisconsin (SHOW). SHOW, began in 2008 provides a novel infrastructure for population health research recruitment, enabling engagement across the state, longitudinal follow-up of participants and community-specific studies. Key assets of SHOW are two mobile units staffed by research and healthcare professionals to facilitate on-site health studies in randomly selected or community-specific participants. SHOW is predicated on community engagement and the response rate has been outstanding (in one study more than 90% of ~4,000 participants consented to follow-up, DNA testing, or blood work). This is true across all racial/ethnic groups, including non-Hispanic Whites, African Americans, Hispanics and Native Americans. The fourth innovative approach is to work with our FQHC partners to recruit traditionally underserved populations. MCHS, UW and MCW will work with FQHC partners (Marshfield Family Health Center, Access Community Health Center, Milwaukee Health Services, Progressive Health Center and 16th Street Community Health Center). Importantly, we are ready to start. We have already informed and engaged our communities through press releases and newsletters. Processes are in place. Volunteers have already asked us to contact them. All the required personnel are trained and SOPs are in place to begin recruitment. We have a productive history of working with partners already funded for PMI cohort recruitment. For several years, we have worked closely (and published) with our colleagues at Northwestern and Columbia through the eMERGE Network and with the University of Pittsburgh through CTSA. Project Director Murray Brilliant has recently served as an advisor to the University of Arizona’s Precision Medicine Program. Thus, we are team-players who are ready, willing and able to be valuable collaborative partners in the effort to build a national engine to transform healthcare under PMI. n/a","All of Us, Wisconsin",9512099,OT2OD025286,"['African American', 'Agreement', 'Arizona', 'Award', 'Blood', 'Catchment Area', 'Clinic', 'Clinical', 'Clinical Sciences', 'Code', 'Collection', 'Communities', 'Community Health Centers', 'Computing Methodologies', 'Consent', 'DNA', 'Data', 'Electronic Health Record', 'Elements', 'Ensure', 'Equipment', 'Ethnic group', 'Event', 'Family', 'Family health status', 'Federally Qualified Health Center', 'Funding', 'Genome', 'Grant', 'Health', 'Health Professional', 'Health Services', 'Health Surveys', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hispanic Americans', 'Hospitals', 'Human Resources', 'Institution', 'Insurance Carriers', 'International', 'Joints', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Native Americans', 'Newsletter', 'Not Hispanic or Latino', 'PMI cohort', 'Participant', 'Population', 'Precision Medicine Initiative', 'Press Releases', 'Process', 'Publications', 'Publishing', 'Recording of previous events', 'Records', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Scientist', 'Site', 'Techniques', 'Testing', 'Training', 'Translational Research', 'Tribes', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Wisconsin', 'Work', 'cohort', 'data sharing', 'follow-up', 'forest', 'genetic pedigree', 'innovation', 'medical schools', 'novel', 'personalized medicine', 'population health', 'precision medicine', 'programs', 'racial and ethnic', 'response', 'rural area', 'urban area', 'virtual', 'volunteer']",OD,MARSHFIELD CLINIC RESEARCH FOUNDATION,OT2,2017,5360833,-0.013150456957572812
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9357656,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2017,572778,-0.007437386151048284
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9360131,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'permissiveness', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2017,456710,-0.017628856522066097
"Sci-Score, a tool to support rigor and transparency guidelines Project Summary While  standards  in  reporting  of  scientific  methods  are  absolutely  critical  to  producing  reproducible  science,  meeting  such  standards  is  difficult.  Checklists  and  instructions  are  tough  to  follow  often  resulting  in  low  and inconsistent  compliance.  Scientific  journals  and  societies  as  well  as  the  National  Institutes  of  Health  are  now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods  (e.g.,  http://www.cell.com/star-­methods),  but  the  trickier  part  will  be  to  train  the  biomedical  community  to  use these standards to effectively improve how scientific methods are communicated. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency of  Key Biological Resources, we propose to build Sci-­Score a text mining based tool suite to help authors meet the  standard.  Sci-­Score  will  provide  an  automated  check  on  compliance  with  the  RRID  standard  already  implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. The innovation behind Sci-­ score is the provision of a score, which can be obtained by individual investigators, which reflects a numerical  validation of the quality of their methods reporting. We posit that the score will serve as a tool that investigators  and journals can use to compete with themselves and each other, or in the very least allow them to see how  close they are to the average in meeting quality requirements.   Recently, our group has developed a text mining algorithm that has now been successfully been used to detect software tools and databases from the SciCrunch Registry in published papers. Digital tools are one of four resource types that the RRID standard identifies. We propose to extend this approach to the other types of entities: antibodies, cell lines and model organisms. Resource identification along with other quality metrics twill be used to train an algorithm to score the overall quality of the methods document. If successful, the tool could be used by editors, reviewers, and investigators to improve the number of RRIDs, therefore the quality of descriptors of key biological resources in published papers. This SBIR project will build a set of algorithms similar to the resource finding pipeline and develop it into an industrial robust and reconfigurable software system. Our Phase I specific aims include to 1) creating gold sets of data for each resource type and training a set of algorithms for each resource type; 2) designing and evaluating the scoring system; 3) designing and evaluating a report generating system based on the previous aims. In Phase II, we will develop a scalable backend infrastructure to serve the needs of scientific publishers and research community. Standards for scientific methods reporting are absolutely critical to producing reproducible science, but meeting  such standards is difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent  compliance. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency, we propose to build Sci-Score text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. Sci-Score will provide a score rating the quality of  methods reporting in submitted articles, which provides feedback to authors, reviewers and editors on how to improvecompliance with RRIDs and other standards. ","Sci-Score, a tool to support rigor and transparency guidelines",9345707,R43OD024432,"['Address', 'Agreement', 'Algorithms', 'Animal Model', 'Antibodies', 'Area', 'Big Data', 'Biological', 'California', 'Cell Line', 'Cell model', 'Cells', 'Communities', 'Data Set', 'Databases', 'Descriptor', 'Elements', 'Ensure', 'Evaluation', 'Feedback', 'Funding', 'Glare', 'Gold', 'Guidelines', 'Habits', 'Human', 'Individual', 'Industrialization', 'Instruction', 'Journals', 'Learning', 'Literature', 'Machine Learning', 'Manuscripts', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neurosciences', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reading', 'Reagent', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sales', 'Science', 'Scoring Method', 'Services', 'Small Business Innovation Research Grant', 'Societies', 'Software Tools', 'System', 'Technology', 'Text', 'To specify', 'Training', 'United States National Institutes of Health', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'biological systems', 'computerized tools', 'design', 'digital', 'improved', 'innovation', 'interest', 'meetings', 'prototype', 'software systems', 'sound', 'text searching', 'tool', 'vigilance', 'web site']",OD,"SCICRUNCH, INC.",R43,2017,221865,0.0010631267910689289
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9326315,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Modification', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2017,370434,-0.011147379533386745
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9268713,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Data Sources', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2017,655080,-0.00837913592731085
"Integrating Bioinformatics and Clustering Analysis for Disease Surveillance ﻿    DESCRIPTION (provided by applicant):  There has been a tremendous focus in bioinformatics on translation of data from the bench into information and knowledge for clinical decision-making. This includes analysis of human genetics for personalized medicine and treatment. However, there has been much less attention on translational bioinformatics for public health practice such as surveillance of emerging/re-emerging viruses. This involves data acquisition, integration, and analyses of viral genetics to infer origin, spread, and evolution suc as the emergence of new strains. The relevant scientific fields for this practice include certain aspects of molecular epidemiology and phylogeography. Recent attention has focused on viruses of zoonotic origin, which are defined as pathogens that are transmittable between animals and humans. In addition to seasonal influenza and West Nile virus, this classification of pathogens includes novel viruses such as Middle Eastern Respiratory Syndrome and influenza A H7N9. Despite the successes highlighted in the literature, there has been little utilization of bioinformatics resources and tools among state public health, agriculture, and wildlife agencies for zoonotic surveillance. Previously this type of resource has been restricted primarily to those in academia.       While bioinformatics has been sparsely used for surveillance of zoonotic viruses, other applications such as Geospatial Information Systems (GIS) have been employed by state health agencies to analyze spatial patterns of infection. This includes software to produce disease maps using an array of data types such as clinical, geographical, or human mobility data for tasks such as, geocoding, clustering, or outbreak detection. In addition, advances in geospatial statistics have enabled health agencies to perform more powerful space-time analyses to infer spatiotemporal patterns. However, these GIS consider only traditional epidemiological data such as location and timing of reported cases and not the genetics of the virus that causes the disease. This prevents health agencies from understanding how changes in the genome of the virus and the associated environment in which it disseminates impacts disease risk.      The long-term goal of this proposal is to enhance the identification of geospatial hotspots of zoonotic viruses by applying bioinformatics principles to access, integrate, and analyze viral genetics and spatiotemporal reportable disease data. This project will include approaches from bioinformatics, genetics, spatial statistics, GIS, and epidemiology. To do this, I will first measue the utilization of bioinformatics resources and tools as well as the current approaches and limitations identified by state agencies of public health, agriculture, and wildlife for detecting nd predicting hotspots (clusters) of zoonotic viruses (Aim 1). I will then use this feedback to develo a spatial decision support system for detecting and predicting zoonotic hotspots that applies bioinformatics principles to access, integrate, and analyze viral genetics, environmental, and spatiotemporal reportable disease data (Aim 2). In Aim 3, I will then evaluate my system for cluster detection and prediction against a system that does not consider viral genetics and relies on traditional spatiotemporal data, and perform validation of the predictive capability. Additional evaluation of the user's satisfaction and system usability will be evaluated.               Project Narrative I will develop and evaluate a spatial decision support system to support surveillance of zoonotic viruses in both human and animal populations. I will use approaches from bioinformatics and public health to integrate genetic sequence data from the virus with data from cases of reported infectious diseases and associated environmental data. A surveillance system that considers the genetics and environment of the virus along with public health data will assist public health officials in making informed decisions regarding risk of infectious diseases.",Integrating Bioinformatics and Clustering Analysis for Disease Surveillance,9189635,F31LM012176,"['Academia', 'Address', 'Agriculture', 'Algorithm Design', 'Algorithms', 'Animals', 'Area', 'Attention', 'Biodiversity', 'Bioinformatics', 'Case Study', 'Clinical', 'Cluster Analysis', 'Communicable Diseases', 'Computer software', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Disease', 'Disease Notification', 'Disease Outbreaks', 'Ecology', 'Environment', 'Epidemiology', 'Evaluation', 'Evolution', 'Feedback', 'Future', 'Genbank', 'Genetic', 'Geographic Information Systems', 'Geography', 'Goals', 'Health', 'Human', 'Human Genetics', 'Infection', 'Influenza', 'Influenza A Virus, H7N9 Subtype', 'Influenza A virus', 'Knowledge', 'Literature', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Metadata', 'Modeling', 'Molecular Epidemiology', 'Molecular Evolution', 'Pattern', 'Population', 'Public Health', 'Public Health Practice', 'Questionnaire Designs', 'Questionnaires', 'Research', 'Resources', 'Retrospective Studies', 'Risk', 'Scanning', 'Sequence Alignment', 'Syndrome', 'System', 'Time', 'Translations', 'Validation', 'Validity and Reliability', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Virus Integration', 'West Nile virus', 'Work', 'Zoonoses', 'clinical decision-making', 'data acquisition', 'disorder risk', 'epidemiologic data', 'health data', 'high risk', 'novel virus', 'pathogen', 'personalized medicine', 'prevent', 'respiratory', 'satisfaction', 'seasonal influenza', 'spatiotemporal', 'statistics', 'success', 'tool', 'usability', 'virus classification', 'virus genetics']",NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,F31,2017,38800,-0.012734327349550495
"OMOP information model for eMERGE phenotyping ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",OMOP information model for eMERGE phenotyping,9481767,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'information model', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2017,100000,-0.017510553610864826
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9285815,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2017,871212,-0.01643137914222039
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9443736,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,50000,-0.01092896693777857
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9536289,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,589741,-0.01092896693777857
"UTMB Clinical and Translational Science Award UTMB's CTSA-linked KL2 Scholars Program addresses the significant need for developing future clinical and translational (C&T) investigators, with emphases on team-based research, leadership development, and mentorship training. In our new CTSA hub, we will build on our current CTSA's successes over the past 5½ years, with unique innovative integration of exceptional training approaches, including: 1) mentored career development within CTSA-supported multidisciplinary translational teams (MTTs); 2) an emphasis on individual development plans (IDPs) and pre-established C&T research competencies tailored to the needs of the individual scholar, but also emphasizing team-based competencies; 3) development of leadership competencies through participation in a new Leadership Development Academy; 4) an Academy of Research Mentors (ARM), based within our Institute for Translational Sciences (ITS), to foster mentoring and mentor training; 5) a linked Translational Research Scholars Program (TRSP), which provides an expanded peer group and a wider impact across the institution, with twice-monthly career development seminars and a focus on scholars' research; 6) development of entrepreneurial skills and leadership through participation in a UT System-funded Healthcare Entrepreneurship Program; and 7) collaboration and dissemination of program experience to other CTSA hubs in the Texas Regional CTSA Consortium, and the national CTSA Consortium. An important feature of our KL2 Scholars Program is its linked position within the continuum of career development, from graduate student training in our CTSA TL1 Training Core to the production of independently funded C&T faculty members. Our program is enhanced with by financial support from our Provost's office for scholar expenses, ITS education administrators, the ARM, and the Office of Faculty Affairs, which is focused on faculty development at UTMB. UTMB also ranks nationally in promoting diversity, which will enhance our recruitment of underrepresented minority scholars by the participation of faculty from our Hispanic Center of Excellence, and our Medical School Enrichment Program. Our KL2 Program is well-integrated with other institutional education activities, including our Human Pathophysiology and Translational Medicine graduate program, and courses targeted toward scientific writing, biostatistics, and study design. Early-career faculty as Phase 1 TRSP scholars can compete to become CTSA-supported KL2 Scholars, and with acquisition of their first mentored K-level grants (e.g., K08), advance to a Phase 2 TRSP Scholar, then focusing on the transition from mentored to independent grant funding. Upon acquisition of independent (e.g. R01) funding, scholars advance to become Phase 3 TRSP Scholars and Associate Members in the ARM, to facilitate their development of mentoring skills, and eventually to full ARM membership, with demonstrated success in C&T science mentoring. Our KL2 Scholars Program thus integrates novel team-based training, leadership, and mentoring approaches to foster the career development of future leaders in C&T research. n/a",UTMB Clinical and Translational Science Award,9270644,KL2TR001441,"['Academy', 'Address', 'Administrator', 'Award', 'Biological', 'Biological Markers', 'Biometry', 'Biopsy', 'Body Surface Area', 'Burn injury', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Competence', 'Data', 'Dependence', 'Development', 'Development Plans', 'Down-Regulation', 'Education', 'Enrollment', 'Entrepreneurship', 'Evolution', 'Faculty', 'Failure', 'Financial Support', 'Fostering', 'Freezing', 'Functional disorder', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genomics', 'Grant', 'Healthcare', 'Hepatitis C', 'Hispanics', 'Human', 'Individual', 'Infection', 'Inflammation', 'Institutes', 'Institution', 'Interferons', 'Intervention', 'Leadership', 'Link', 'Liver Fibrosis', 'Longitudinal Studies', 'Machine Learning', 'Measures', 'Medicare', 'Medicare claim', 'Medicine', 'Mentors', 'Mentorship', 'MicroRNAs', 'Modeling', 'Molecular Profiling', 'NCI Scholars Program', 'Participant', 'Pathway interactions', 'Patients', 'Peer Group', 'Phase', 'Physicians', 'Positioning Attribute', 'Process', 'Production', 'Protocols documentation', 'Recruitment Activity', 'Regimen', 'Research', 'Research Design', 'Rice', 'Structure', 'System', 'TNFSF15 gene', 'Teacher Professional Development', 'Techniques', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underrepresented Minority', 'Variant', 'Writing', 'base', 'career development', 'cohort', 'college', 'demographics', 'early-career faculty', 'experience', 'graduate student', 'innovation', 'leadership development', 'liver biopsy', 'medical schools', 'member', 'microbial', 'multidisciplinary', 'novel', 'outcome forecast', 'peripheral blood', 'predict clinical outcome', 'program dissemination', 'programs', 'response', 'science education', 'skills', 'student training', 'success', 'translational medicine', 'translational scientist']",NCATS,UNIVERSITY OF TEXAS MED BR GALVESTON,KL2,2017,313812,-0.02221991486094918
An Integrated Biomedical Dataset Discovery System for Immunological Research Databases  The The Contractor will develop a prototype of an integrated biomedical dataset discovery system for Immunological Research Databases (BIRD) to facilitate integrated search for data/knowledge/tools of interest from DAIT bioinformatics resources. n/a,An Integrated Biomedical Dataset Discovery System for Immunological Research Databases ,9574416,72201700019C,"['Bioinformatics', 'Contractor', 'Data', 'Data Discovery', 'Data Reporting', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Immune system', 'Immunology', 'Ingestion', 'Knowledge', 'Maps', 'Metadata', 'Modeling', 'Natural Language Processing', 'Ontology', 'Research', 'Resources', 'System', 'Techniques', 'Technology', 'Terminology', 'base', 'data integration', 'indexing', 'information organization', 'interest', 'prototype', 'tool', 'user-friendly']",NIAID,"TECHWAVE INTERNATIONAL, INC.",N43,2017,224960,0.00931976734617323
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9360448,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biological Assay', 'Biology', 'Biometry', 'Cells', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2017,906404,-0.015035066211128307
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9217457,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'clinical development', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2017,578512,-0.01451183588858766
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9193091,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2017,396248,-0.00033933777463023294
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,9307936,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Databases', 'Disease', 'Drug Exposure', 'Drug Modelings', 'Drug toxicity', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'cost', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'genomic data', 'improved', 'longitudinal dataset', 'novel', 'open source', 'personalized medicine', 'phenotypic data', 'public health relevance', 'rapid growth', 'rare variant', 'response', 'study population', 'success', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2017,600474,-0.015250265442887156
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9302929,R25OD021880,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",OD,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2017,271803,-0.003918137454347447
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9325275,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Plasticizers', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2017,324508,-0.018974457614949855
"CRCNS: Representational foundations of adaptive behavior in natural and artificial  ﻿    DESCRIPTION (provided by applicant): Overview: Among the most celebrated success stories in computational neuroscience is the discovery that many aspects of decision-making can be understood in terms of the formal framework of reinforcement learning (RL). Ideas drawn from RL have shed light on many behavioral phenomena in learning and action selection, on the functional anatomy and neural processes underlying reward-driven behavior, and on fundamental aspects of neuromodulatory function. However, for all these successes, RL-based work is haunted by an inconvenient truth: Standard RL algorithms scale poorly to large, complex problems. If human learning and decision-making are driven by RL-like mechanisms, how is it that we cope with the kinds of rich, large-scale tasks that are typical of everyday life? Existing research in both psychology and neuroscience hints at one answer to this question: Complex problems can be conquered if the decision-maker is equipped with compact, intelligently formatted representations of the task. This principle is seen in studies of expert play in chess, which show that chess masters leverage highly integrative internal representations of board configurations; in studies of frontal and parietal lobe function, which have revealed receptive fields strongly shaped by task contingencies; and studies on the hippocampus, which point to the role of this structure in supporting a hierarchically organized 'cognitive map,' of task space. Not coincidentally, the critical role of representation has come increasingly to the fore in RL-based research in machine learning and robotics, with growing interest in techniques for dimensionality reduction, hierarchy and deep learning.            The present project aims toward a systematic, empirically validated account of the role of representation in supporting RL and goal-directed behavior at large. The project brings together three investigators with complementary expertise in cognitive and computational neuroscience (Botvinick, Gershman) and machine learning and robotics (Konidaris). Together, we propose an integrative, interdisciplinary program of research, applying behavioral and neuroimaging work with human subjects, computational modeling of neurophysiological and behavioral data, formal mathematical work and simulations with artificial agents. The proposed studies are diverse in theme and method, but work together toward a theory that is both formally grounded and empirically constrained. At a more concrete level, our research focuses on four specific classes of representation, considering the computational impact of each for RL, as well as the relevance of each to neuroscience and human behavior. As detailed in our Project Description, these include (1) metric embedding, (2) spectral decomposition, (3) hierarchical representation and (4) symbolic representation. In addition to investigating the implications of each of these four forms of representation individually, we hypothesize that they fit together into a tiered system, which works as a whole to support the sometimes competing demands of learning and action control.         Intellectual Merit (provided by applicant): Understanding how representational structure impacts learning and decision making is a core challenge in cognitive science, behavioral neuroscience and, artificial intelligence. Success in establishing a computationally explicit, empirically validated theory in this area, with a specific focus on the role of representation in R, would represent an important achievement with wide repercussions. The strategy of leveraging conceptual tools from machine learning to investigate human behavior and brain function can offer considerable scientific leverage, as our own previous research illustrates. The proposed work is motivated by and builds upon established lines of research, bringing these together in order to capitalize on opportunities for synergy. In addition to answering specific empirical and computational questions, the proposed work aims to open up new avenues for future research in an important area of inquiry.         Broader Impact (provided by applicant): The proposed work lies at the crossroads of neuroscience, psychology, artificial intelligence and machine learning, and promises to advance the growing exchange among these fields. The project brings together investigators with contrasting disciplinary affiliations, with the explicit goal of bridging between intellectual cultres. The proposed work is likely to find a wide scientific audience, given its relevance to cognitive and developmental psychology, behavioral, cognitive and systems neuroscience, and behavioral economics. However, the work is likely to be of equal interest within artificial intelligence, machine learning, and robotics, where a current challenge is precisely to understand how representation learning can allow RL to scale up to large problems. Representational approaches to RL are already of intense interest within industry, where the present investigators have a record of active engagement. The topic of the proposed work has applicability in other areas as well, including education and training, and military and medical decision support. The plan for the project has a robust training component at both graduate and postdoctoral levels, with a commitment to fostering involvement of underrepresented minorities, as well as international engagement. n/a",CRCNS: Representational foundations of adaptive behavior in natural and artificial ,9126614,R01MH109177,"['Accounting', 'Achievement', 'Adaptive Behaviors', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Cognitive Science', 'Complex', 'Computer Simulation', 'Data', 'Decision Making', 'Fostering', 'Foundations', 'Goals', 'Hippocampus (Brain)', 'Human', 'Industry', 'International', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Military Personnel', 'Neurosciences', 'Parietal Lobe', 'Play', 'Process', 'Psychological reinforcement', 'Psychology', 'Research', 'Research Personnel', 'Rewards', 'Robotics', 'Role', 'Shapes', 'Structure', 'System', 'Techniques', 'Training', 'Training and Education', 'Underrepresented Minority', 'Work', 'base', 'behavioral economics', 'cognitive neuroscience', 'cognitive system', 'computational neuroscience', 'coping', 'developmental psychology', 'driving behavior', 'frontal lobe', 'human subject', 'interest', 'neuroimaging', 'neurophysiology', 'neuroregulation', 'programs', 'receptive field', 'relating to nervous system', 'research based learning', 'scale up', 'simulation', 'success', 'theories', 'tool']",NIMH,PRINCETON UNIVERSITY,R01,2016,384420,-0.027737201999924334
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9176354,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Staging', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2016,599995,-0.005166595680152352
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,-0.008129686452775012
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility.         PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.        ",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9076555,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'System', 'Time', 'Training', 'cohort', 'computer science', 'crowdsourcing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2016,316000,-0.012769380228101361
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,-0.0021585394389371936
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,-0.0021585394389371936
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9158909,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Process', 'Reading', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'Technology', 'Testing', 'Time', 'Work', 'abstracting', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'research study', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,311803,-0.011986432245268396
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,9005867,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'mobile computing', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2016,1271107,-0.005923053922110792
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338982,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Basic Science', 'Biological', 'Biomedical Research', 'Cell physiology', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2016,1183132,-0.0063645418313463984
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9049511,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Health', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'knowledge translation', 'learning progression', 'learning strategy', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428512,-0.029360706706889554
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338977,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'improved', 'innovation', 'inter-individual variation', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2016,1071976,-0.02788295357088705
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),9118144,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'cloud platform', 'computational chemistry', 'computer science', 'data mining', 'design', 'distinguished professor', 'dopamine transporter', 'drug abuse prevention', 'epigenetic marker', 'falls', 'genome-wide', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'operation', 'predictive modeling', 'prevent', 'professor', 'receptor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2016,1070007,-0.019728781611598748
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9096856,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,2201640,-0.00248327791456111
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9288931,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,224176,-0.00248327791456111
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9161233,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'Work', 'base', 'dashboard', 'empowered', 'genome sequencing', 'improved', 'insight', 'meetings', 'microbial community', 'peer', 'repository', 'social', 'tool', 'transcriptomics', 'web portal']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2016,575532,-0.007437386151048284
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,0.0023509916965505813
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9161167,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Maps', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'information processing', 'knowledge base', 'novel', 'personalized medicine', 'repository', 'response', 'stem', 'tool']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2016,471848,-0.017628856522066097
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9123422,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Natural Products', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'novel therapeutics', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2016,419732,-0.0026077493999520926
"An Open Source Precision Medicine Platform for Cloud Operating Systems ﻿    DESCRIPTION (provided by applicant):  Rapid improvements in DNA sequencing and synthesis have the potential to usher in a new era of precision medicine. To realize this vision, however, we must re-imagine the computational and storage infrastructure used to manage and extract actionable results from the massive data sets made possible by widely available advances in DNA sequencing and synthetic biology. In conjunction with the Global Alliance for Genomics and Health (GA4GH), we propose to build the Arvados platform so that a new ecosystem of clinical decision support applications will be able to navigate petabytes of global biomedical data and search millions of genomes in real-time (seconds). Our team has a proven track record of commercial success and high impact scientific research. Commercialization of this free and open-source software (FOSS) platform, which will be greatly accelerated by this grant, will permit organizations to seamlessly span on-premise & hosted cloud- operating systems and vastly simplify data-management & computation, all while facilitating compliance with institutional policies and regulatory requirements.         PUBLIC HEALTH RELEVANCE:  The delivery of healthcare based on molecular data specific to an individual patient (i.e. precision medicine) will require the creation of a new ecosystem of Clinical Decision Support (CDS) applications. This work will provide a platform that will make the development of such applications faster, easier, and less expensive.        ",An Open Source Precision Medicine Platform for Cloud Operating Systems,9140741,R44GM109737,"['Address', 'Adopted', 'Big Data', 'Bioinformatics', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Collaborations', 'Communities', 'Computer software', 'Contractor', 'DNA Sequence', 'DNA biosynthesis', 'Data', 'Data Set', 'Databases', 'Development', 'Distributed Systems', 'Ecosystem', 'Feedback', 'Fostering', 'Funding', 'Galaxy', 'Genome', 'Genomics', 'Grant', 'Health', 'Healthcare', 'Human', 'Industry', 'Information Technology', 'Institutional Policy', 'International', 'Internet', 'Language', 'Length', 'Letters', 'Machine Learning', 'Maintenance', 'Manuscripts', 'Measures', 'Medicine', 'Memory', 'Molecular', 'Operating System', 'Phase', 'Policies', 'Production', 'Publications', 'Reproducibility', 'Research', 'Research Infrastructure', 'Resources', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Source Code', 'System', 'Technology', 'Time', 'Training Support', 'Vision', 'Work', 'base', 'big biomedical data', 'cloud platform', 'commercialization', 'data management', 'genome sequencing', 'genomic data', 'health care delivery', 'individual patient', 'meetings', 'new technology', 'next generation sequencing', 'open source', 'operation', 'petabyte', 'portability', 'precision medicine', 'public health relevance', 'repository', 'screening', 'success', 'symposium', 'synthetic biology', 'terabyte', 'web services', 'whole genome']",NIGMS,"CUROVERSE, INC.",R44,2016,985339,-0.0024551492473342414
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9195864,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphorylation Site', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Databases', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Regulation', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Staging', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'abstracting', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'kinase inhibitor', 'knowledge base', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2016,374400,-0.011147379533386745
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9100683,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2016,680020,-0.00837913592731085
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9200905,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Decision Modeling', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Immigration', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Legal', 'Legal patent', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2016,221175,-0.01092896693777857
"UTMB Clinical and Translational Science Award UTMB's CTSA-linked KL2 Scholars Program addresses the significant need for developing future clinical and translational (C&T) investigators, with emphases on team-based research, leadership development, and mentorship training. In our new CTSA hub, we will build on our current CTSA's successes over the past 5½ years, with unique innovative integration of exceptional training approaches, including: 1) mentored career development within CTSA-supported multidisciplinary translational teams (MTTs); 2) an emphasis on individual development plans (IDPs) and pre-established C&T research competencies tailored to the needs of the individual scholar, but also emphasizing team-based competencies; 3) development of leadership competencies through participation in a new Leadership Development Academy; 4) an Academy of Research Mentors (ARM), based within our Institute for Translational Sciences (ITS), to foster mentoring and mentor training; 5) a linked Translational Research Scholars Program (TRSP), which provides an expanded peer group and a wider impact across the institution, with twice-monthly career development seminars and a focus on scholars' research; 6) development of entrepreneurial skills and leadership through participation in a UT System-funded Healthcare Entrepreneurship Program; and 7) collaboration and dissemination of program experience to other CTSA hubs in the Texas Regional CTSA Consortium, and the national CTSA Consortium. An important feature of our KL2 Scholars Program is its linked position within the continuum of career development, from graduate student training in our CTSA TL1 Training Core to the production of independently funded C&T faculty members. Our program is enhanced with by financial support from our Provost's office for scholar expenses, ITS education administrators, the ARM, and the Office of Faculty Affairs, which is focused on faculty development at UTMB. UTMB also ranks nationally in promoting diversity, which will enhance our recruitment of underrepresented minority scholars by the participation of faculty from our Hispanic Center of Excellence, and our Medical School Enrichment Program. Our KL2 Program is well-integrated with other institutional education activities, including our Human Pathophysiology and Translational Medicine graduate program, and courses targeted toward scientific writing, biostatistics, and study design. Early-career faculty as Phase 1 TRSP scholars can compete to become CTSA-supported KL2 Scholars, and with acquisition of their first mentored K-level grants (e.g., K08), advance to a Phase 2 TRSP Scholar, then focusing on the transition from mentored to independent grant funding. Upon acquisition of independent (e.g. R01) funding, scholars advance to become Phase 3 TRSP Scholars and Associate Members in the ARM, to facilitate their development of mentoring skills, and eventually to full ARM membership, with demonstrated success in C&T science mentoring. Our KL2 Scholars Program thus integrates novel team-based training, leadership, and mentoring approaches to foster the career development of future leaders in C&T research. n/a",UTMB Clinical and Translational Science Award,9128789,KL2TR001441,"['Academy', 'Address', 'Administrator', 'Award', 'Biological', 'Biological Markers', 'Biometry', 'Biopsy', 'Body Surface Area', 'Burn injury', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Collaborations', 'Data', 'Dependence', 'Development', 'Development Plans', 'Down-Regulation', 'Education', 'Enrollment', 'Entrepreneurship', 'Evolution', 'Faculty', 'Financial Support', 'Fostering', 'Freezing', 'Functional disorder', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Healthcare', 'Hepatitis C virus', 'Hispanics', 'Human', 'Individual', 'Infection', 'Inflammation', 'Institutes', 'Institution', 'Interferons', 'Intervention', 'Lead', 'Leadership', 'Link', 'Liver Fibrosis', 'Longitudinal Studies', 'Machine Learning', 'Measures', 'Medicare', 'Medicare claim', 'Medicine', 'Mentors', 'Mentorship', 'MicroRNAs', 'Modeling', 'Molecular Profiling', 'NCI Scholars Program', 'Organ failure', 'Participant', 'Pathway interactions', 'Patients', 'Peer Group', 'Phase', 'Physicians', 'Positioning Attribute', 'Process', 'Production', 'Protocols documentation', 'Regimen', 'Research', 'Research Design', 'Research Personnel', 'Rice', 'Structure', 'System', 'TNFSF15 gene', 'Teacher Professional Development', 'Techniques', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underrepresented Minority', 'Variant', 'Writing', 'base', 'career development', 'cohort', 'college', 'demographics', 'early-career faculty', 'experience', 'graduate student', 'innovation', 'leadership development', 'liver biopsy', 'medical schools', 'member', 'microbial', 'multidisciplinary', 'novel', 'outcome forecast', 'peripheral blood', 'predict clinical outcome', 'program dissemination', 'programs', 'response', 'science education', 'skills', 'student training', 'success', 'translational medicine']",NCATS,UNIVERSITY OF TEXAS MED BR GALVESTON,KL2,2016,423432,-0.02221991486094918
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans. PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8989994,R00EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Imaging Device', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'mathematical sciences', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'spectrograph', 'symposium', 'time use']",NEI,UNIVERSITY OF MEMPHIS,R00,2016,230028,-0.009424868581275316
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9248725,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2016,98294,-0.01643137914222039
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9134825,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2016,873141,-0.01643137914222039
"Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr) EMPIRICAL TESTING AND ENHANCEMENT OF WEB-BASED ABSTRACT SCREENING TOOL (ABSTRACKR)  In this year-long project, we aim to empirically assess the performance and efficiency of state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine and stakeholder-driven comparative effectiveness reviews. We have developed AbstrackrTM (hereon, Abstrackr), a human-guided computerized abstract screening tool that aims to reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. Abstrackr makes use of machine learning techniques, and is offered as a free web-based tool that enables management of the screening process.  We also aim to revise the web-interface of Abstrackr to make it more intuitive, user friendly, and add documentation and functionalities requested by users; and to revise Abstrackr’s back-end, which includes the way the software parses and analyses citations, fits machine learning models, and makes computations, to make it more efficient. These revisions will ensure that the tool becomes more robust, and that it remains usable for larger projects and for many teams.  The proposed work will be carried out by the developers of Abstrackr, comprising a highly experienced team of systematic review investigators and computer scientists at Brown University and the University of Texas at Austin, who have been working together for at least seven years. We will pursue dissemination of the findings of this assessment and of the revised tool through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its wider adoption by the Agency for Healthcare Research and Quality Evidence-based Practice Center Program, Cochrane Collaboration, and other groups conducting systematic reviews. We will also continue to make all code available online. Our aims are to: Aim 1. Empirically measure the efficiency and accuracy of the prediction algorithms in Abstrackr in the computer-assisted semi-automated screening of citations for eligibility in systematic reviews. Aim 2. Improve and add to the functionality of the Web-based Abstrackr software, based in part on enhancements suggested by a panel of identified heavy users. Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making and systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to assess the performance and efficiency of a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care, and to augment the functionality of its public implementation.",Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr),9168247,R03HS024812,[' '],AHRQ,BROWN UNIVERSITY,R03,2016,99999,0.0020631725014744018
"Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience DESCRIPTION (provided by applicant): This Mid-Career Investigator Award in Patient-Oriented Research will permit the Principal Investigator to expand his mentorship, career development, and research on the integration of patient-oriented psychiatric genetics and neuroscience. The Principal Investigator is an established clinician-scientist in psychiatric genetics and clinical research. This application includes three components: 1) a mentoring plan designed to enhance the Principal Investigator's ability to mentor young investigators in psychiatric genetics and neuroscience; 2) a career development plan designed to expand the Principal Investigator's expertise in the integration of genetics and neuroimaging; and 3) a research plan that leverages the Principal Investigator's existing investigations on the genetic, behavioral and neural basis of psychiatric disorders and their relationship to normal variation in brain structure and function. The Principal Investigator has a substantial track record of mentoring and cultivating trainees and junior faculty in the area of genetic, clinical and epidemiologic research. This award would provide protected time for the Principal Investigator to expand and enhance these efforts. The mentoring plan capitalizes on his current mentoring activities and numerous collaborative research projects to develop a career development path for a new generation of investigators capable of integrating cutting-edge methods at the interface of patient-oriented genetics and neuroscience. Mentees will gain hands-on experience in patient-oriented research and will develop their own independent research directions. The clinical and translational research resources at MGH and the broader Harvard Medical School community will provide a rich training environment for mentees. The new research component of the application will include genomewide analyses of brain phenotypes to examine the genetic and neural architecture of anxiety disorders. This application was designed to address objectives identified by NIMH's Strategic Plan to fuel research on the causes of mental disorders and the National Advisory Mental Health Council Workgroup on Research Training to expand mentoring of young investigators capable of conducting the interdisciplinary research needed to achieve those objectives. Recent advances in genetics and neuroscience have created unprecedented opportunities to unravel the causes of mental illness and ameliorate the public health burden and suffering they incur. This application for a Midcareer Investigator Award in Patient-Oriented Research will allow Dr. Jordan Smoller to enhance his research and mentorship of young scientists to bring together genetics and brain imaging in the service of discovering the causes of mental illness.",Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience,9070764,K24MH094614,"['Address', 'Adolescent', 'Age', 'American', 'Anxiety', 'Anxiety Disorders', 'Architecture', 'Area', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Award', 'Base of the Brain', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biology', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Research', 'Cognition', 'Collection', 'Communities', 'DNA', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnostic', 'Disease', 'Environment', 'Epidemiologic Studies', 'European', 'Faculty', 'Fruit', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Genetic', 'Genetic Research', 'Genetic Variation', 'Genetic study', 'Genomic Library', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Hereditary Disease', 'Individual', 'Infant', 'Interdisciplinary Study', 'International', 'Investigation', 'Jordan', 'Laboratories', 'Leadership', 'Longitudinal Studies', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Midcareer Investigator Award in Patient-Oriented Research', 'National Institute of Mental Health', 'Neurosciences', 'Pathology', 'Personality', 'Phenotype', 'Principal Investigator', 'Psychotic Disorders', 'Public Health', 'Quality Control', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Scientist', 'Services', 'Strategic Planning', 'Structure', 'Suicide', 'Susceptibility Gene', 'Technology', 'Temperament', 'Time', 'Training', 'Transcend', 'Translational Research', 'Variant', 'Work', 'anxious temperament', 'base', 'career', 'career development', 'cohort', 'design', 'disorder risk', 'experience', 'functional disability', 'gene discovery', 'genetic analysis', 'genetic variant', 'genome wide association study', 'genome-wide', 'imaging genetics', 'innovation', 'learning strategy', 'medical schools', 'neuroimaging', 'neuropsychiatric disorder', 'patient oriented', 'patient oriented research', 'phenotypic data', 'programs', 'psychogenetics', 'relating to nervous system', 'research and development', 'resilience', 'risk variant', 'trait']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K24,2016,183275,-0.013460416801307419
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8997510,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2016,510376,-0.00018993057135567687
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8987580,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'Thinking', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'light weight', 'next generation', 'novel', 'online community', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2016,526540,-0.00833269667026724
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,9068953,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Health', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'genomic data', 'improved', 'large-scale database', 'novel', 'open source', 'personalized medicine', 'rapid growth', 'rare variant', 'response', 'study population', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2016,536892,-0.015250265442887156
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8985684,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2016,405708,-0.00033933777463023294
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,0.005668019155035405
"CRCNS: Representational foundations of adaptive behavior in natural and artificial  ﻿    DESCRIPTION (provided by applicant): Overview: Among the most celebrated success stories in computational neuroscience is the discovery that many aspects of decision-making can be understood in terms of the formal framework of reinforcement learning (RL). Ideas drawn from RL have shed light on many behavioral phenomena in learning and action selection, on the functional anatomy and neural processes underlying reward-driven behavior, and on fundamental aspects of neuromodulatory function. However, for all these successes, RL-based work is haunted by an inconvenient truth: Standard RL algorithms scale poorly to large, complex problems. If human learning and decision-making are driven by RL-like mechanisms, how is it that we cope with the kinds of rich, large-scale tasks that are typical of everyday life? Existing research in both psychology and neuroscience hints at one answer to this question: Complex problems can be conquered if the decision-maker is equipped with compact, intelligently formatted representations of the task. This principle is seen in studies of expert play in chess, which show that chess masters leverage highly integrative internal representations of board configurations; in studies of frontal and parietal lobe function, which have revealed receptive fields strongly shaped by task contingencies; and studies on the hippocampus, which point to the role of this structure in supporting a hierarchically organized 'cognitive map,' of task space. Not coincidentally, the critical role of representation has come increasingly to the fore in RL-based research in machine learning and robotics, with growing interest in techniques for dimensionality reduction, hierarchy and deep learning.            The present project aims toward a systematic, empirically validated account of the role of representation in supporting RL and goal-directed behavior at large. The project brings together three investigators with complementary expertise in cognitive and computational neuroscience (Botvinick, Gershman) and machine learning and robotics (Konidaris). Together, we propose an integrative, interdisciplinary program of research, applying behavioral and neuroimaging work with human subjects, computational modeling of neurophysiological and behavioral data, formal mathematical work and simulations with artificial agents. The proposed studies are diverse in theme and method, but work together toward a theory that is both formally grounded and empirically constrained. At a more concrete level, our research focuses on four specific classes of representation, considering the computational impact of each for RL, as well as the relevance of each to neuroscience and human behavior. As detailed in our Project Description, these include (1) metric embedding, (2) spectral decomposition, (3) hierarchical representation and (4) symbolic representation. In addition to investigating the implications of each of these four forms of representation individually, we hypothesize that they fit together into a tiered system, which works as a whole to support the sometimes competing demands of learning and action control.         Intellectual Merit (provided by applicant): Understanding how representational structure impacts learning and decision making is a core challenge in cognitive science, behavioral neuroscience and, artificial intelligence. Success in establishing a computationally explicit, empirically validated theory in this area, with a specific focus on the role of representation in R, would represent an important achievement with wide repercussions. The strategy of leveraging conceptual tools from machine learning to investigate human behavior and brain function can offer considerable scientific leverage, as our own previous research illustrates. The proposed work is motivated by and builds upon established lines of research, bringing these together in order to capitalize on opportunities for synergy. In addition to answering specific empirical and computational questions, the proposed work aims to open up new avenues for future research in an important area of inquiry.         Broader Impact (provided by applicant): The proposed work lies at the crossroads of neuroscience, psychology, artificial intelligence and machine learning, and promises to advance the growing exchange among these fields. The project brings together investigators with contrasting disciplinary affiliations, with the explicit goal of bridging between intellectual cultres. The proposed work is likely to find a wide scientific audience, given its relevance to cognitive and developmental psychology, behavioral, cognitive and systems neuroscience, and behavioral economics. However, the work is likely to be of equal interest within artificial intelligence, machine learning, and robotics, where a current challenge is precisely to understand how representation learning can allow RL to scale up to large problems. Representational approaches to RL are already of intense interest within industry, where the present investigators have a record of active engagement. The topic of the proposed work has applicability in other areas as well, including education and training, and military and medical decision support. The plan for the project has a robust training component at both graduate and postdoctoral levels, with a commitment to fostering involvement of underrepresented minorities, as well as international engagement.                 n/a",CRCNS: Representational foundations of adaptive behavior in natural and artificial ,9052441,R01MH109177,"['Accounting', 'Achievement', 'Adaptive Behaviors', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Cognitive Science', 'Complex', 'Computer Simulation', 'Data', 'Decision Making', 'Fostering', 'Foundations', 'Goals', 'Hippocampus (Brain)', 'Human', 'Industry', 'International', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Military Personnel', 'Neurosciences', 'Parietal Lobe', 'Play', 'Process', 'Psychological reinforcement', 'Psychology', 'Research', 'Research Personnel', 'Rewards', 'Robotics', 'Role', 'Shapes', 'Structure', 'System', 'Techniques', 'Training', 'Training and Education', 'Underrepresented Minority', 'Work', 'base', 'behavioral economics', 'cognitive neuroscience', 'cognitive system', 'computational neuroscience', 'coping', 'developmental psychology', 'driving behavior', 'frontal lobe', 'human subject', 'interest', 'neuroimaging', 'neurophysiology', 'neuroregulation', 'programs', 'receptive field', 'relating to nervous system', 'scale up', 'simulation', 'success', 'theories', 'tool']",NIMH,PRINCETON UNIVERSITY,R01,2015,425468,-0.027737201999924334
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,-0.008129686452775012
"Informatics Tools for High-Throughput Sequences Data Analysis DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK. The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.",Informatics Tools for High-Throughput Sequences Data Analysis,8788050,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2015,967608,-0.012984389458757314
"Reproducibility Assessment for Multivariate Assays DESCRIPTION (provided by applicant): This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of features, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combination of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penalization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools availble for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.",Reproducibility Assessment for Multivariate Assays,8828718,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'Health', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'research study']",NIGMS,INSILICOS,R43,2015,64005,-0.01818029208214393
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,-0.0021585394389371936
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,8840984,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2015,1243799,-0.005923053922110792
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"".         PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.                 ",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,8864679,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Solutions', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'design', 'innovation', 'interest', 'knowledge translation', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2015,445887,-0.029360706706889554
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9147033,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Health', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,489338,-0.00248327791456111
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8935854,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,2116462,-0.00248327791456111
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),8896676,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'computational chemistry', 'computer science', 'data mining', 'design', 'dopamine transporter', 'drug abuse prevention', 'epigenetic marker', 'falls', 'genome-wide', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'operation', 'predictive modeling', 'prevent', 'professor', 'receptor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2015,1064603,-0.019728781611598748
"Protect Privacy of Healthcare Data in the Cloud DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier. Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8925916,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Laplacian', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'privacy protection', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2015,192613,0.0007690728496764642
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9126755,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,40625,-0.0026077493999520926
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9117880,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,379655,-0.0026077493999520926
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities.         PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.                ","COINSTAC: decentralized, scalable analysis of loosely coupled data",8975906,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Solutions', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2015,727692,-0.00837913592731085
"Integrating Bioinformatics and Clustering Analysis for Disease Surveillance ﻿    DESCRIPTION (provided by applicant):  There has been a tremendous focus in bioinformatics on translation of data from the bench into information and knowledge for clinical decision-making. This includes analysis of human genetics for personalized medicine and treatment. However, there has been much less attention on translational bioinformatics for public health practice such as surveillance of emerging/re-emerging viruses. This involves data acquisition, integration, and analyses of viral genetics to infer origin, spread, and evolution suc as the emergence of new strains. The relevant scientific fields for this practice include certain aspects of molecular epidemiology and phylogeography. Recent attention has focused on viruses of zoonotic origin, which are defined as pathogens that are transmittable between animals and humans. In addition to seasonal influenza and West Nile virus, this classification of pathogens includes novel viruses such as Middle Eastern Respiratory Syndrome and influenza A H7N9. Despite the successes highlighted in the literature, there has been little utilization of bioinformatics resources and tools among state public health, agriculture, and wildlife agencies for zoonotic surveillance. Previously this type of resource has been restricted primarily to those in academia.       While bioinformatics has been sparsely used for surveillance of zoonotic viruses, other applications such as Geospatial Information Systems (GIS) have been employed by state health agencies to analyze spatial patterns of infection. This includes software to produce disease maps using an array of data types such as clinical, geographical, or human mobility data for tasks such as, geocoding, clustering, or outbreak detection. In addition, advances in geospatial statistics have enabled health agencies to perform more powerful space-time analyses to infer spatiotemporal patterns. However, these GIS consider only traditional epidemiological data such as location and timing of reported cases and not the genetics of the virus that causes the disease. This prevents health agencies from understanding how changes in the genome of the virus and the associated environment in which it disseminates impacts disease risk.      The long-term goal of this proposal is to enhance the identification of geospatial hotspots of zoonotic viruses by applying bioinformatics principles to access, integrate, and analyze viral genetics and spatiotemporal reportable disease data. This project will include approaches from bioinformatics, genetics, spatial statistics, GIS, and epidemiology. To do this, I will first measue the utilization of bioinformatics resources and tools as well as the current approaches and limitations identified by state agencies of public health, agriculture, and wildlife for detecting nd predicting hotspots (clusters) of zoonotic viruses (Aim 1). I will then use this feedback to develo a spatial decision support system for detecting and predicting zoonotic hotspots that applies bioinformatics principles to access, integrate, and analyze viral genetics, environmental, and spatiotemporal reportable disease data (Aim 2). In Aim 3, I will then evaluate my system for cluster detection and prediction against a system that does not consider viral genetics and relies on traditional spatiotemporal data, and perform validation of the predictive capability. Additional evaluation of the user's satisfaction and system usability will be evaluated.               Project Narrative I will develop and evaluate a spatial decision support system to support surveillance of zoonotic viruses in both human and animal populations. I will use approaches from bioinformatics and public health to integrate genetic sequence data from the virus with data from cases of reported infectious diseases and associated environmental data. A surveillance system that considers the genetics and environment of the virus along with public health data will assist public health officials in making informed decisions regarding risk of infectious diseases.",Integrating Bioinformatics and Clustering Analysis for Disease Surveillance,9050106,F31LM012176,"['Academia', 'Address', 'Agriculture', 'Algorithm Design', 'Algorithms', 'Animals', 'Area', 'Attention', 'Biodiversity', 'Bioinformatics', 'Case Study', 'Clinical', 'Cluster Analysis', 'Communicable Diseases', 'Computer software', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Disease', 'Disease Outbreaks', 'Ecology', 'Environment', 'Epidemiology', 'Evaluation', 'Evolution', 'Feedback', 'Future', 'Genbank', 'Genetic', 'Geographic Information Systems', 'Goals', 'Health', 'Human', 'Human Genetics', 'Infection', 'Influenza', 'Influenza A Virus, H7N9 Subtype', 'Influenza A virus', 'Knowledge', 'Literature', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Metadata', 'Modeling', 'Molecular Epidemiology', 'Molecular Evolution', 'Pattern', 'Population', 'Public Health', 'Public Health Practice', 'Questionnaire Designs', 'Questionnaires', 'Research', 'Resources', 'Retrospective Studies', 'Risk', 'Satellite Viruses', 'Scanning', 'Sequence Alignment', 'Syndrome', 'System', 'Time', 'Translations', 'Validation', 'Validity and Reliability', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'West Nile virus', 'Work', 'clinical decision-making', 'data acquisition', 'disorder risk', 'health data', 'high risk', 'novel virus', 'pathogen', 'personalized medicine', 'prevent', 'respiratory', 'satisfaction', 'seasonal influenza', 'spatiotemporal', 'statistics', 'success', 'tool', 'usability', 'virus classification', 'virus genetics']",NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,F31,2015,36613,-0.012734327349550495
"UTMB Clinical and Translational Science Award UTMB's CTSA-linked KL2 Scholars Program addresses the significant need for developing future clinical and translational (C&T) investigators, with emphases on team-based research, leadership development, and mentorship training. In our new CTSA hub, we will build on our current CTSA's successes over the past 5½ years, with unique innovative integration of exceptional training approaches, including: 1) mentored career development within CTSA-supported multidisciplinary translational teams (MTTs); 2) an emphasis on individual development plans (IDPs) and pre-established C&T research competencies tailored to the needs of the individual scholar, but also emphasizing team-based competencies; 3) development of leadership competencies through participation in a new Leadership Development Academy; 4) an Academy of Research Mentors (ARM), based within our Institute for Translational Sciences (ITS), to foster mentoring and mentor training; 5) a linked Translational Research Scholars Program (TRSP), which provides an expanded peer group and a wider impact across the institution, with twice-monthly career development seminars and a focus on scholars' research; 6) development of entrepreneurial skills and leadership through participation in a UT System-funded Healthcare Entrepreneurship Program; and 7) collaboration and dissemination of program experience to other CTSA hubs in the Texas Regional CTSA Consortium, and the national CTSA Consortium. An important feature of our KL2 Scholars Program is its linked position within the continuum of career development, from graduate student training in our CTSA TL1 Training Core to the production of independently funded C&T faculty members. Our program is enhanced with by financial support from our Provost's office for scholar expenses, ITS education administrators, the ARM, and the Office of Faculty Affairs, which is focused on faculty development at UTMB. UTMB also ranks nationally in promoting diversity, which will enhance our recruitment of underrepresented minority scholars by the participation of faculty from our Hispanic Center of Excellence, and our Medical School Enrichment Program. Our KL2 Program is well-integrated with other institutional education activities, including our Human Pathophysiology and Translational Medicine graduate program, and courses targeted toward scientific writing, biostatistics, and study design. Early-career faculty as Phase 1 TRSP scholars can compete to become CTSA-supported KL2 Scholars, and with acquisition of their first mentored K-level grants (e.g., K08), advance to a Phase 2 TRSP Scholar, then focusing on the transition from mentored to independent grant funding. Upon acquisition of independent (e.g. R01) funding, scholars advance to become Phase 3 TRSP Scholars and Associate Members in the ARM, to facilitate their development of mentoring skills, and eventually to full ARM membership, with demonstrated success in C&T science mentoring. Our KL2 Scholars Program thus integrates novel team-based training, leadership, and mentoring approaches to foster the career development of future leaders in C&T research. n/a",UTMB Clinical and Translational Science Award,9085705,KL2TR001441,"['Academy', 'Address', 'Administrator', 'Award', 'Biological', 'Biological Markers', 'Biometry', 'Biopsy', 'Body Surface Area', 'Burn injury', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Collaborations', 'Data', 'Dependence', 'Development', 'Development Plans', 'Down-Regulation', 'Education', 'Enrollment', 'Entrepreneurship', 'Evolution', 'Faculty', 'Financial Support', 'Fostering', 'Freezing', 'Functional disorder', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Healthcare', 'Hepatitis C virus', 'Hispanics', 'Human', 'Individual', 'Infection', 'Inflammation', 'Institutes', 'Institution', 'Interferons', 'Intervention', 'Lead', 'Leadership', 'Link', 'Liver Fibrosis', 'Longitudinal Studies', 'Machine Learning', 'Measures', 'Medicare', 'Medicare claim', 'Medicine', 'Mentors', 'Mentorship', 'MicroRNAs', 'Modeling', 'Molecular Profiling', 'NCI Scholars Program', 'Organ failure', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Peer Group', 'Phase', 'Physicians', 'Positioning Attribute', 'Process', 'Production', 'Protocols documentation', 'Regimen', 'Research', 'Research Design', 'Research Personnel', 'Rice', 'Structure', 'System', 'Techniques', 'Testing', 'Texas', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underrepresented Minority', 'Variant', 'Writing', 'base', 'career', 'career development', 'clinical predictors', 'cohort', 'college', 'demographics', 'experience', 'graduate student', 'innovation', 'liver biopsy', 'medical schools', 'member', 'microbial', 'multidisciplinary', 'novel', 'outcome forecast', 'peripheral blood', 'programs', 'response', 'science education', 'skills', 'success', 'translational medicine']",NCATS,UNIVERSITY OF TEXAS MED BR GALVESTON,KL2,2015,388145,-0.02221991486094918
IGF::OT::IGF TASK ORDER 1 - CORE & ADMINISTRATION; MAINTENANCE & INSTALLATION FOR THE SURVEILLANCE EPIDEMIOLOGY AND END RESULTS (SEER) ELECTRONIC DATA CAPTURE SOFTWARE SUPPORT AND INSTALLATIONS. Surveillance Epidemiology and End Results (SEER) Electronic Data Capture Software Support and Installations. n/a,IGF::OT::IGF TASK ORDER 1 - CORE & ADMINISTRATION; MAINTENANCE & INSTALLATION FOR THE SURVEILLANCE EPIDEMIOLOGY AND END RESULTS (SEER) ELECTRONIC DATA CAPTURE SOFTWARE SUPPORT AND INSTALLATIONS.,9162089,61201500033I,"['Artificial Intelligence', 'Award', 'Computer software', 'Contracts', 'Data', 'Diagnostic Imaging', 'Electronics', 'Hour', 'Laboratories', 'Maintenance', 'Malignant Neoplasms', 'Medical Surveillance', 'Medicine', 'Pathology', 'Reporting', 'Update', 'electronic data']",NCI,"ARTIFICIAL INTELLIGENCE IN MEDICINE, INC",N03,2015,1135265,-0.013959823406967373
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality.         PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results                ",EMR-Linked Biobank for Translational Genomics,8968014,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2015,899776,-0.01643137914222039
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG. RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8786588,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cardiovascular visualization', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2015,2177431,0.015520901189558347
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans. PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8798661,R00EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'mathematical sciences', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'spectrograph', 'symposium', 'time use']",NEI,UNIVERSITY OF MEMPHIS,R00,2015,228148,-0.009424868581275316
"Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience DESCRIPTION (provided by applicant): This Mid-Career Investigator Award in Patient-Oriented Research will permit the Principal Investigator to expand his mentorship, career development, and research on the integration of patient-oriented psychiatric genetics and neuroscience. The Principal Investigator is an established clinician-scientist in psychiatric genetics and clinical research. This application includes three components: 1) a mentoring plan designed to enhance the Principal Investigator's ability to mentor young investigators in psychiatric genetics and neuroscience; 2) a career development plan designed to expand the Principal Investigator's expertise in the integration of genetics and neuroimaging; and 3) a research plan that leverages the Principal Investigator's existing investigations on the genetic, behavioral and neural basis of psychiatric disorders and their relationship to normal variation in brain structure and function. The Principal Investigator has a substantial track record of mentoring and cultivating trainees and junior faculty in the area of genetic, clinical and epidemiologic research. This award would provide protected time for the Principal Investigator to expand and enhance these efforts. The mentoring plan capitalizes on his current mentoring activities and numerous collaborative research projects to develop a career development path for a new generation of investigators capable of integrating cutting-edge methods at the interface of patient-oriented genetics and neuroscience. Mentees will gain hands-on experience in patient-oriented research and will develop their own independent research directions. The clinical and translational research resources at MGH and the broader Harvard Medical School community will provide a rich training environment for mentees. The new research component of the application will include genomewide analyses of brain phenotypes to examine the genetic and neural architecture of anxiety disorders. This application was designed to address objectives identified by NIMH's Strategic Plan to fuel research on the causes of mental disorders and the National Advisory Mental Health Council Workgroup on Research Training to expand mentoring of young investigators capable of conducting the interdisciplinary research needed to achieve those objectives. Recent advances in genetics and neuroscience have created unprecedented opportunities to unravel the causes of mental illness and ameliorate the public health burden and suffering they incur. This application for a Midcareer Investigator Award in Patient-Oriented Research will allow Dr. Jordan Smoller to enhance his research and mentorship of young scientists to bring together genetics and brain imaging in the service of discovering the causes of mental illness.",Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience,8851679,K24MH094614,"['Address', 'Adolescent', 'Age', 'American', 'Anxiety', 'Anxiety Disorders', 'Architecture', 'Area', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Award', 'Base of the Brain', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biology', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Research', 'Cognition', 'Collection', 'Communities', 'DNA', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnostic', 'Disease', 'Environment', 'Epidemiologic Studies', 'European', 'Faculty', 'Fruit', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Genetic', 'Genetic Research', 'Genetic Variation', 'Genetic study', 'Genomic Library', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Hereditary Disease', 'Image', 'Individual', 'Infant', 'Interdisciplinary Study', 'International', 'Investigation', 'Jordan', 'Laboratories', 'Leadership', 'Longitudinal Studies', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Midcareer Investigator Award in Patient-Oriented Research', 'National Institute of Mental Health', 'Neurosciences', 'Pathology', 'Personality', 'Phenotype', 'Principal Investigator', 'Psychotic Disorders', 'Public Health', 'Quality Control', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Scientist', 'Services', 'Strategic Planning', 'Structure', 'Suicide', 'Susceptibility Gene', 'Technology', 'Temperament', 'Time', 'Training', 'Transcend', 'Translational Research', 'Variant', 'Work', 'anxious temperament', 'base', 'career', 'career development', 'cohort', 'design', 'disorder risk', 'experience', 'functional disability', 'gene discovery', 'genetic analysis', 'genetic variant', 'genome wide association study', 'genome-wide', 'innovation', 'medical schools', 'neuroimaging', 'neuropsychiatry', 'patient oriented', 'patient oriented research', 'programs', 'psychogenetics', 'relating to nervous system', 'research and development', 'resilience', 'risk variant', 'trait']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K24,2015,184179,-0.013460416801307419
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8803385,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2015,523965,-0.00018993057135567687
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8788417,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2015,526540,-0.00833269667026724
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,-0.004268561181197057
"Informatics to enable routine personalized cancer therapy     DESCRIPTION (provided by applicant): Genomic analysis of individual patients is now affordable and therapies targeted to specific molecular aberrations are being tested in clinical trials. However, even highly-specialized physicians at leading academic centers are not equipped to apply genomic information available in publically available sources to clinical- decision-making concerning individual patients. Our central hypothesis is that we can develop informatics tools to support personalized cancer treatment as ""standard of care"" rather than ""one off"" exceptions. We will: 1) implement a bioinformatics pipeline for processing molecular data into actionable profiles, 2) create and maintain a database of therapeutic implications of common genomic aberrations using automated processing of publically-available sources and 3) develop tools to summarize and present patient-specific advice to clinicians. These tools will be based on existing technologies and publicly available data sources. Once tested, we will make these tools available via appropriate open source license.          PUBLIC HEALTH RELEVANCE: Genomic analysis of individual patients is now affordable and therapies targeted to specific molecular aberrations are being tested in clinical trials. In this project, we will develop informatics tools to support personalized cancer treatment as ""standard of care"" rather than ""one off"" exception.             ",Informatics to enable routine personalized cancer therapy,8904309,U01CA180964,"['Address', 'Adoption', 'Algorithms', 'Bioinformatics', 'Biological Factors', 'Cancer Biology', 'Cancer Center', 'Cancer Patient', 'Cataloging', 'Catalogs', 'Characteristics', 'Classification', 'Clinical', 'Clinical Medicine', 'Clinical Trials', 'DNA Sequence Alteration', 'Data', 'Data Sources', 'Databases', 'Drug Compounding', 'Enrollment', 'Evaluation', 'FDA approved', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Gold', 'Hot Spot', 'Human', 'Individual', 'Informatics', 'Institutes', 'Internet', 'Ions', 'Licensing', 'Link', 'Literature', 'MEDLINE', 'Malignant Neoplasms', 'Manuals', 'Molecular', 'Molecular Profiling', 'Mutation', 'Natural Language Processing', 'Nucleotides', 'Oncogenes', 'Oncologist', 'PTPRC gene', 'Paper', 'Pathologic', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Review Literature', 'Source', 'Source Code', 'Technology', 'Testing', 'Therapeutic', 'Therapy trial', 'Time', 'Update', 'Validation', 'Variant', 'base', 'biomedical informatics', 'clinical decision-making', 'clinically relevant', 'exome sequencing', 'open source', 'personalized cancer therapy', 'personalized medicine', 'programs', 'public health relevance', 'standard of care', 'success', 'targeted treatment', 'tool', 'tumor', 'tumor DNA', 'usability', 'web site']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2015,316190,-0.03177620680144851
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,-0.004268561181197057
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,-0.004268561181197057
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS     DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans.         PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.            ",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8817212,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Books', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2015,406247,-0.00033933777463023294
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,8929257,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Health', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Population Study', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'improved', 'large-scale database', 'novel', 'open source', 'personalized medicine', 'rapid growth', 'rare variant', 'response', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY,R01,2015,598996,-0.015250265442887156
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8683213,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2014,221252,-0.020010339804886326
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8601147,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2014,989800,-0.012984389458757314
"GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY     DESCRIPTION:  Many complex diseases such as cancer, cardiovascular disorders, and schizophrenia may be understood as failures in the functioning of nested hierarchies of biomolecular and cellular networks. These nested hierarchies control a range of processes including the differentiation and migration of cells, remodeling of extracellular matrices and tissues, and information encoding in neuronal subsystems. Washington University has established expertise in cutting edge imaging, molecular biology and genomic technologies synergistic with computational approaches such as machine learning and unraveling the principles of hierarchical organization and dynamics of complex systems. This collective expertise is being leveraged to develop new drugs, improve our ability to interpret sophisticated imaging data, understand how populations of neurons act collectively to accomplish complex tasks, and model the onset and progression of complex diseases as dynamical rewiring of hierarchical, multi-scale networks. Biological network analyses provide a rich set of tools for organizing and interpreting the vast quantities of data produced by state-of-the-art experimental protocols. The rapid advancement of computationally intensive research in these areas is outstripping the capabilities of CPU-based high performance computing (HPC) systems. This application would support the acquisition and integration of a large-scale IBM high performance cluster of Graphics Processor Units (GPUs) to be added as an upgrade to the existing IBM-designed Heterogeneous High Performance Computing environment to form a state-of-the-art hybrid computing capability. Such a resource is essential to match the growing need for high performance computing at Washington University and to support state of the art research software applications that are optimized for GPU computing. The acquisition and integration of a high performance GPU cluster will solve critical computing challenges that exist within Washington University's growing NIH research portfolio. The proposed state-of-the-art hybrid GPU/CPU computing capabilities will be deployed within the framework of a stable, productive and rapidly growing resource center. The addition of high-capacity GPU computing capabilities will allow critical calculation to be performed in hours instead of days and enable substantial increases in productivity for existing projects covering a broad range of application areas as well as enabling new research directions.             n/a",GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY,8640341,S10OD018091,"['Area', 'Biological', 'Biology', 'Cardiovascular Diseases', 'Complex', 'Computer Systems', 'Computer software', 'Data', 'Disease', 'Environment', 'Extracellular Matrix', 'Failure', 'Genomics', 'High Performance Computing', 'Hour', 'Hybrids', 'Image', 'Machine Learning', 'Malignant Neoplasms', 'Modeling', 'Molecular Biology', 'Neurons', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Productivity', 'Protocols documentation', 'Research', 'Resources', 'Schizophrenia', 'System', 'Technology', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'cell motility', 'computing resources', 'design', 'improved', 'innovation', 'tool']",OD,WASHINGTON UNIVERSITY,S10,2014,597700,-0.0013536991701727907
"Reproducibility Assessment for Multivariate Assays  Project Summary. This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of fea- tures, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combina- tion of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penal- ization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools available for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.            ",Reproducibility Assessment for Multivariate Assays,8647816,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Simulate', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'public health relevance', 'research study']",NIGMS,INSILICOS,R43,2014,131071,-0.018736053282841843
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,-0.0021585394389371936
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8661774,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2014,1248956,-0.005923053922110792
"Scalable Biomedical Pattern Recognition Via Deep Learning     DESCRIPTION (provided by applicant):  Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.      The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.      Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.      In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes.             Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,8689173,R21LM011664,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Architecture', 'Area', 'Biomedical Research', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computerized Medical Record', 'Couples', 'Data', 'Data Set', 'Data Sources', 'Dependence', 'Development', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Engineering', 'Evaluation', 'Exhibits', 'Feedback', 'Goals', 'Gold', 'Healthcare Systems', 'Human', 'Image', 'Individual', 'Judgment', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Literature', 'Manuals', 'Measures', 'Medical', 'Metformin', 'Methods', 'Modeling', 'Myocardial Infarction', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'PTGS2 gene', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Population', 'Probability', 'Process', 'ROC Curve', 'Records', 'Research', 'Risk', 'Sampling', 'Sea', 'Serum', 'Single Nucleotide Polymorphism', 'Source', 'Specific qualifier value', 'Structure', 'Testing', 'Time', 'To specify', 'Uncertainty', 'Uric Acid', 'Work', 'cell growth', 'clinical care', 'clinical phenotype', 'density', 'diabetic', 'genetic association', 'genetic variant', 'inhibitor/antagonist', 'interest', 'meetings', 'neoplastic cell', 'non-diabetic', 'outcome forecast', 'prevent', 'speech recognition', 'success', 'type I and type II diabetes']",NLM,VANDERBILT UNIVERSITY,R21,2014,202714,-0.011609777693415098
"Scalable Biomedical Pattern Recognition Via Deep Learning DESCRIPTION (provided by applicant): Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.  The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is  being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.  Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.  In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes. Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,9302040,R21LM011664,[' '],NLM,VANDERBILT UNIVERSITY MEDICAL CENTER,R21,2014,7696,-0.011609777693415098
"Protect Privacy of Healthcare Data in the Cloud     DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier.             Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8810023,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2014,183155,0.0007690728496764642
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8774407,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2014,1623265,-0.00248327791456111
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),8743368,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'computational chemistry', 'computer science', 'data mining', 'design', 'dopamine transporter', 'drug abuse prevention', 'epigenetic marker', 'falls', 'genome-wide', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'operation', 'predictive modeling', 'prevent', 'professor', 'receptor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2014,1094316,-0.019728781611598748
"Building an open-source cloud-based computational platform to improve data access   We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members.  One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. This proposal aims to fill this gap by developing a robust platform that integrates state-of-the-art open-source technologies for data storage, data access, data mining and analysis, annotation, visualization and reporting.  We previously developed a cloud-based BioDatomics platform for Next Generation Sequencing (NGS), BioDX, which has been successful and has been used commercially by several clients. This proposal aims to develop a new platform leveraging our experience with the BioDX platform that integrates: data storage and real-time data querying using Cloudera Impala; powerful and customizable analytics tools using R and its derivative Bioconductor suite of programs for bioinformatics; annotation integration and reporting which is an existing feature of BioDX; and a visual programming interface that will simplify and enhance the development and maintenance of reproducible analytics workflows. We believe this powerful integrated data platform, if successful, will enable real-time collaboration, dramatically reduce data repository costs, and increase the efficiency and efficacy of data analyses for translating experimental data into actionable research products.  We are committed to analyzing stakeholder needs and optimizing hardware, software and information technology systems to meet their demands. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools.  The initial phase of work will involve development of the platform, optimizing performance on the cloud and testing the integration of new technology. BioDatomics is committed to funding the next phase of work which will include usability testing and finalizing a commercial product, following which full commercialization will proceed. Preliminary commercialization plans have demonstrated that the project has the capacity to generate a million dollars in revenue during the first full year after commercial release.  The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This open source platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders. PUBLIC HEALTH RELEVANCE: One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools. The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders.            ",Building an open-source cloud-based computational platform to improve data access,8647860,R43DA036970,"['Academia', 'Apache Indians', 'Bioconductor', 'Bioinformatics', 'Biological', 'Businesses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Clinical', 'Clinical Research', 'Collaborations', 'Commit', 'Complex', 'Computer software', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Distributed Databases', 'Drug Industry', 'Environment', 'Expenditure', 'Funding', 'Genomics', 'Government Agencies', 'Growth', 'Imagery', 'Industry', 'Information Dissemination', 'Information Systems', 'Java', 'Language', 'Licensing', 'Link', 'Machine Learning', 'Maintenance', 'Marketing', 'Messenger RNA', 'Modeling', 'Molecular', 'Molecular Target', 'Mutation', 'Online Systems', 'Patients', 'Performance', 'Pharmacologic Substance', 'Phase', 'Programming Languages', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk Behaviors', 'Savings', 'Services', 'Software Engineering', 'Software Tools', 'Solutions', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Substance Addiction', 'Substance Use Disorder', 'Substance abuse problem', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Visual', 'Work', 'base', 'beneficiary', 'cloud based', 'commercialization', 'computer infrastructure', 'cost', 'cost effective', 'data mining', 'drug discovery', 'experience', 'improved', 'mathematical model', 'meetings', 'member', 'models and simulation', 'new technology', 'next generation sequencing', 'novel', 'open source', 'programs', 'public health relevance', 'substance abuse treatment', 'success', 'tool', 'usability', 'user-friendly']",NIDA,"BIODATOMICS, LLC",R43,2014,195584,-0.00011685465610165155
"In silico identification of phyto-therapies     DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits.                RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.  ",In silico identification of phyto-therapies,8749705,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R01,2014,389869,-0.0026077493999520926
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG. RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8928685,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2014,159202,0.015520901189558347
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8588958,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2014,2177431,0.015520901189558347
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans. PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8786916,R00EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'mathematical sciences', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'symposium', 'time use']",NEI,UNIVERSITY OF MEMPHIS,R00,2014,248999,-0.009424868581275316
"Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience     DESCRIPTION (provided by applicant): This Mid-Career Investigator Award in Patient-Oriented Research will permit the Principal Investigator to expand his mentorship, career development, and research on the integration of patient-oriented psychiatric genetics and neuroscience. The Principal Investigator is an established clinician-scientist in psychiatric genetics and clinical research. This application includes three components: 1) a mentoring plan designed to enhance the Principal Investigator's ability to mentor young investigators in psychiatric genetics and neuroscience; 2) a career development plan designed to expand the Principal Investigator's expertise in the integration of genetics and neuroimaging; and 3) a research plan that leverages the Principal Investigator's existing investigations on the genetic, behavioral and neural basis of psychiatric disorders and their relationship to normal variation in brain structure and function. The Principal Investigator has a substantial track record of mentoring and cultivating trainees and junior faculty in the area of genetic, clinical and epidemiologic research. This award would provide protected time for the Principal Investigator to expand and enhance these efforts. The mentoring plan capitalizes on his current mentoring activities and numerous collaborative research projects to develop a career development path for a new generation of investigators capable of integrating cutting-edge methods at the interface of patient-oriented genetics and neuroscience. Mentees will gain hands-on experience in patient-oriented research and will develop their own independent research directions. The clinical and translational research resources at MGH and the broader Harvard Medical School community will provide a rich training environment for mentees. The new research component of the application will include genomewide analyses of brain phenotypes to examine the genetic and neural architecture of anxiety disorders. This application was designed to address objectives identified by NIMH's Strategic Plan to fuel research on the causes of mental disorders and the National Advisory Mental Health Council Workgroup on Research Training to expand mentoring of young investigators capable of conducting the interdisciplinary research needed to achieve those objectives.          Recent advances in genetics and neuroscience have created unprecedented opportunities to unravel the causes of mental illness and ameliorate the public health burden and suffering they incur. This application for a Midcareer Investigator Award in Patient-Oriented Research will allow Dr. Jordan Smoller to enhance his research and mentorship of young scientists to bring together genetics and brain imaging in the service of discovering the causes of mental illness.            ",Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience,8659505,K24MH094614,"['Address', 'Adolescent', 'Age', 'American', 'Anxiety', 'Anxiety Disorders', 'Architecture', 'Area', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Award', 'Base of the Brain', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biology', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Research', 'Cognition', 'Collection', 'Communities', 'DNA', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnostic', 'Disease', 'Environment', 'Epidemiologic Studies', 'European', 'Faculty', 'Fruit', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Genetic', 'Genetic Research', 'Genetic Variation', 'Genomic Library', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Hereditary Disease', 'Image', 'Individual', 'Infant', 'Interdisciplinary Study', 'International', 'Investigation', 'Jordan', 'Laboratories', 'Leadership', 'Longitudinal Studies', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Midcareer Investigator Award in Patient-Oriented Research', 'National Institute of Mental Health', 'Neurosciences', 'Pathology', 'Personality', 'Phenotype', 'Predisposition', 'Principal Investigator', 'Psychotic Disorders', 'Public Health', 'Quality Control', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Scientist', 'Services', 'Strategic Planning', 'Structure', 'Suicide', 'Technology', 'Temperament', 'Time', 'Training', 'Transcend', 'Translational Research', 'Variant', 'Work', 'base', 'career', 'career development', 'cohort', 'design', 'disorder risk', 'experience', 'functional disability', 'gene discovery', 'genetic analysis', 'genetic variant', 'genome wide association study', 'genome-wide', 'innovation', 'medical schools', 'neuroimaging', 'neuropsychiatry', 'patient oriented', 'patient oriented research', 'programs', 'psychogenetics', 'relating to nervous system', 'research and development', 'resilience', 'risk variant', 'trait']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K24,2014,185064,-0.013460416801307419
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8714053,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,307471,-0.019860647757736438
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8628132,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2014,525880,-0.00018993057135567687
"Informatics to enable routine personalized cancer therapy     DESCRIPTION (provided by applicant): Genomic analysis of individual patients is now affordable and therapies targeted to specific molecular aberrations are being tested in clinical trials. However, even highly-specialized physicians at leading academic centers are not equipped to apply genomic information available in publically available sources to clinical- decision-making concerning individual patients. Our central hypothesis is that we can develop informatics tools to support personalized cancer treatment as ""standard of care"" rather than ""one off"" exceptions. We will: 1) implement a bioinformatics pipeline for processing molecular data into actionable profiles, 2) create and maintain a database of therapeutic implications of common genomic aberrations using automated processing of publically-available sources and 3) develop tools to summarize and present patient-specific advice to clinicians. These tools will be based on existing technologies and publicly available data sources. Once tested, we will make these tools available via appropriate open source license.          PUBLIC HEALTH RELEVANCE: Genomic analysis of individual patients is now affordable and therapies targeted to specific molecular aberrations are being tested in clinical trials. In this project, we will develop informatics tools to support personalized cancer treatment as ""standard of care"" rather than ""one off"" exception.             ",Informatics to enable routine personalized cancer therapy,8741711,U01CA180964,"['Address', 'Adoption', 'Algorithms', 'Bioinformatics', 'Biological Factors', 'Cancer Biology', 'Cancer Center', 'Cancer Patient', 'Cataloging', 'Catalogs', 'Characteristics', 'Classification', 'Clinical', 'Clinical Medicine', 'Clinical Trials', 'DNA', 'Data', 'Data Sources', 'Databases', 'Drug Compounding', 'Enrollment', 'Evaluation', 'FDA approved', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Gold', 'Hot Spot', 'Human', 'Individual', 'Informatics', 'Institutes', 'Internet', 'Ions', 'Licensing', 'Link', 'Literature', 'MEDLINE', 'Malignant Neoplasms', 'Manuals', 'Metric', 'Molecular', 'Molecular Profiling', 'Mutation', 'Natural Language Processing', 'Nucleotides', 'Oncogenes', 'Oncologist', 'PTPRC gene', 'Paper', 'Pathologic', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Review Literature', 'Source', 'Source Code', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Update', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cancer therapy', 'clinical decision-making', 'clinically relevant', 'exome sequencing', 'open source', 'programs', 'public health relevance', 'standard of care', 'success', 'tool', 'tumor', 'usability', 'web site']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2014,312379,-0.03177620680144851
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,-0.004268561181197057
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8597446,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2014,533554,-0.00833269667026724
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers.         PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.            ",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,8629996,R01GM103859,"['Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Medicine', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Population Study', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'improved', 'large-scale database', 'novel', 'open source', 'public health relevance', 'rapid growth', 'rare variant', 'response', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY,R01,2014,648591,-0.015250265442887156
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8737919,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2014,2941548,0.01000274565074541
"Informatics for Integrating Biology & the Bedside (i2b2) DESCRIPTION (PROVIDED BY APPLICANT): The large and growing size of the healthcare system makes it imperative to understand what is happening to us, the recipients of healthcare, to be able to efficiently conduct research to improve healthcare delivery and to improve the state of biomedicine by advancing its science. i2b2, ""Informatics for Integrating Biology and the Bedside"" seeks to provide this instrumentation using the informational by products of healthcare and the biological materials accumulated through the delivery of healthcare. This complements existing efforts to create prospective cohort studies or trials outside the delivery of routine healthcare. In the first round of i2b2, we demonstrated that we could identify known adverse events and phenotypically select and then genotype patients for genetic association at approximately 1/10* of the price and less than l/10 of the time usually entailed to develop such populations for study. The challenge we have set ourselves for the next methodological challenge in i2b2 is the development of Virtual Cohort Studies (VCS) encompassing the population of a healthcare system as study subjects and asking questions of comparative effectiveness, unforeseen adverse events and identification of clinically relevant subpopulations including both clinical and genome-scale measures. We will be comparing the results of the VCS to those of carefully planned and executed cohort studies such as the Framingham Heart Study. VCS will require multiple methodological advances and tools development including in the disciplines of natural language processing, temporal reasoning, predictive modeling, biostatistics and machine learning. VCS methods will be tested by two driving biology projects, the first studying a collection of autoimmune diseases and the second type 2 diabetes. In both projects, VCS methods will be applied to investigate the components of cardiovascular risk from the genetic to the epigenetic and including the full range of clinical history including medications exposure. A systems/integrative approach will be taken to identify commonalities in these risk profiles across these disparate disease domains. VCS methods will be shared with i2b2 user community under open source governance while i2b2 user community contributions are folded into the i2b2 toolkit. Public Health Relevance: The i2b2 model of using existing clinical data for high throughput, cost effective, and timely research promises to rapidly leverage our nation's ability to better understand the existing disease burden and how best to achieve cost-effective clinical effectiveness, including new drug targets, new uses of existing drugs and improved management of existing disease.",Informatics for Integrating Biology & the Bedside (i2b2),8519533,U54LM008748,"['Adverse event', 'Autoimmune Diseases', 'Automobile Driving', 'Biocompatible Materials', 'Biology', 'Biometry', 'Clinical', 'Clinical Data', 'Clinical effectiveness', 'Cohort Studies', 'Collection', 'Communities', 'Complement', 'Development', 'Discipline', 'Disease', 'Drug Targeting', 'Epigenetic Process', 'Framingham Heart Study', 'Genetic', 'Genome', 'Genotype', 'Healthcare', 'Healthcare Systems', 'Informatics', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Natural Language Processing', 'Non-Insulin-Dependent Diabetes Mellitus', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Population Study', 'Price', 'Recording of previous events', 'Research', 'Risk', 'Science', 'Study Subject', 'System', 'Testing', 'Time', 'burden of illness', 'cardiovascular risk factor', 'clinically relevant', 'comparative effectiveness', 'cost effective', 'genetic association', 'health care delivery', 'improved', 'instrumentation', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'tool development', 'virtual']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,U54,2013,3017637,-0.02410155458720698
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8518436,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2013,220626,-0.020010339804886326
"Brain Science Computer Cluster     DESCRIPTION (provided by applicant): Computational requirements of contemporary brain science research often exceed financial and resource management limits of individual investigator laboratories. Many contemporary neuroscience research projects require analysis of large data sets with advanced statistical methods and anatomical reconstruction techniques. These methods require high speed computational and graphics engines operating in a multiple processor environments equipped with large capacity, high-speed storage devices. A limitation in the Brown brain science effort at understanding neural processing is the lack of a readily accessible high-speed computational resource. A central computational resource based on a unified cluster of contemporary Linux CPUs and GPUs will serve the computational needs of a core group of brain science investigators at Brown without compromising individual access common to stand-alone workstations. The requested computer cluster has system software that automatically balances CPU and GPU usage, thereby ensuring maximum access to the computational resource for all users. Intensive 3D graphics are off-loaded either to GPUs or to client workstations, thereby further reducing the central computational load. Commercial or open-source software with an open operating environment will be used for analysis using standard and novel statistical and machine learning approaches to assess significance of large data sets. This proposal details the architecture and benefits of a contemporary computational resource for the major and minor users, and more generally the Brown brain science community. The resource was designed to fill immediate and near-term computational and storage needs of a core group of Brown brain scientists. The system can be readily expansion as needs, either computational, storage, or new users, arise. Expansion of the existing core investigators group can occur easily since the computational power or storage capacity of the system can be readily enhanced at relatively low cost. The flexible nature of the system will serve a variety of research needs of the Brown brain science community. The computational resource is expected to bring together researchers at Brown working on the common problem of neural processing.             n/a",Brain Science Computer Cluster,8447697,S10OD016366,"['Architecture', 'Brain', 'Client', 'Communities', 'Computer software', 'Data Set', 'Devices', 'Ensure', 'Environment', 'Equilibrium', 'Individual', 'Laboratories', 'Linux', 'Machine Learning', 'Methods', 'Minor', 'Nature', 'Neurosciences Research', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Speed', 'Statistical Methods', 'System', 'Techniques', 'Work', 'base', 'computer cluster', 'computing resources', 'cost', 'design', 'flexibility', 'novel', 'open source', 'reconstruction', 'relating to nervous system', 'software systems']",OD,BROWN UNIVERSITY,S10,2013,599598,-0.021503162090515327
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8416349,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2013,964551,-0.012984389458757314
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8473164,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2013,1216983,-0.005923053922110792
"Scalable Biomedical Pattern Recognition Via Deep Learning     DESCRIPTION (provided by applicant):  Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.      The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.      Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.      In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes.             Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,8566062,R21LM011664,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Architecture', 'Area', 'Biomedical Research', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computerized Medical Record', 'Couples', 'Data', 'Data Set', 'Data Sources', 'Dependence', 'Development', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Engineering', 'Evaluation', 'Exhibits', 'Feedback', 'Goals', 'Gold', 'Healthcare Systems', 'Human', 'Image', 'Individual', 'Judgment', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Literature', 'Manuals', 'Measures', 'Medical', 'Metformin', 'Methods', 'Modeling', 'Myocardial Infarction', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'PTGS2 gene', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Population', 'Probability', 'Process', 'ROC Curve', 'Records', 'Research', 'Risk', 'Sampling', 'Sea', 'Serum', 'Single Nucleotide Polymorphism', 'Source', 'Specific qualifier value', 'Structure', 'Testing', 'Time', 'To specify', 'Uncertainty', 'Uric Acid', 'Work', 'cell growth', 'clinical care', 'clinical phenotype', 'density', 'diabetic', 'genetic association', 'genetic variant', 'inhibitor/antagonist', 'interest', 'meetings', 'neoplastic cell', 'non-diabetic', 'outcome forecast', 'prevent', 'speech recognition', 'success', 'type I and type II diabetes']",NLM,VANDERBILT UNIVERSITY,R21,2013,175500,-0.011609777693415098
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8426190,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'stress disorder', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,315672,-0.017933586461158335
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8424997,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2013,2177404,0.015520901189558347
"Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience     DESCRIPTION (provided by applicant): This Mid-Career Investigator Award in Patient-Oriented Research will permit the Principal Investigator to expand his mentorship, career development, and research on the integration of patient-oriented psychiatric genetics and neuroscience. The Principal Investigator is an established clinician-scientist in psychiatric genetics and clinical research. This application includes three components: 1) a mentoring plan designed to enhance the Principal Investigator's ability to mentor young investigators in psychiatric genetics and neuroscience; 2) a career development plan designed to expand the Principal Investigator's expertise in the integration of genetics and neuroimaging; and 3) a research plan that leverages the Principal Investigator's existing investigations on the genetic, behavioral and neural basis of psychiatric disorders and their relationship to normal variation in brain structure and function. The Principal Investigator has a substantial track record of mentoring and cultivating trainees and junior faculty in the area of genetic, clinical and epidemiologic research. This award would provide protected time for the Principal Investigator to expand and enhance these efforts. The mentoring plan capitalizes on his current mentoring activities and numerous collaborative research projects to develop a career development path for a new generation of investigators capable of integrating cutting-edge methods at the interface of patient-oriented genetics and neuroscience. Mentees will gain hands-on experience in patient-oriented research and will develop their own independent research directions. The clinical and translational research resources at MGH and the broader Harvard Medical School community will provide a rich training environment for mentees. The new research component of the application will include genomewide analyses of brain phenotypes to examine the genetic and neural architecture of anxiety disorders. This application was designed to address objectives identified by NIMH's Strategic Plan to fuel research on the causes of mental disorders and the National Advisory Mental Health Council Workgroup on Research Training to expand mentoring of young investigators capable of conducting the interdisciplinary research needed to achieve those objectives.          Recent advances in genetics and neuroscience have created unprecedented opportunities to unravel the causes of mental illness and ameliorate the public health burden and suffering they incur. This application for a Midcareer Investigator Award in Patient-Oriented Research will allow Dr. Jordan Smoller to enhance his research and mentorship of young scientists to bring together genetics and brain imaging in the service of discovering the causes of mental illness.            ",Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience,8467053,K24MH094614,"['Address', 'Adolescent', 'Age', 'American', 'Anxiety', 'Anxiety Disorders', 'Architecture', 'Area', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Award', 'Base of the Brain', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biology', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Research', 'Cognition', 'Collection', 'Communities', 'DNA', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnostic', 'Disease', 'Environment', 'Epidemiologic Studies', 'European', 'Faculty', 'Fruit', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Genetic', 'Genetic Research', 'Genetic Variation', 'Genomic Library', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Hereditary Disease', 'Image', 'Individual', 'Infant', 'Interdisciplinary Study', 'International', 'Investigation', 'Jordan', 'Laboratories', 'Leadership', 'Longitudinal Studies', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Midcareer Investigator Award in Patient-Oriented Research', 'National Institute of Mental Health', 'Neurosciences', 'Pathology', 'Personality', 'Phenotype', 'Predisposition', 'Principal Investigator', 'Psychotic Disorders', 'Public Health', 'Quality Control', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Scientist', 'Services', 'Strategic Planning', 'Structure', 'Suicide', 'Technology', 'Temperament', 'Time', 'Training', 'Transcend', 'Translational Research', 'Variant', 'Work', 'base', 'career', 'career development', 'cohort', 'design', 'disorder risk', 'experience', 'functional disability', 'gene discovery', 'genetic analysis', 'genetic variant', 'genome wide association study', 'genome-wide', 'innovation', 'medical schools', 'neuroimaging', 'neuropsychiatry', 'patient oriented', 'patient oriented research', 'programs', 'psychogenetics', 'relating to nervous system', 'research and development', 'resilience', 'risk variant', 'trait']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K24,2013,185934,-0.013460416801307419
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8535824,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,291730,-0.019860647757736438
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8504843,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2013,527736,-0.00018993057135567687
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,-0.004268561181197057
"Informatics to enable routine personalized cancer therapy  Program Director/Principal Investigator (Bernstam, Elmer Victor) Abstract Genomic analysis of individual patients is now affordable and therapies targeted to specific molecular aberrations are being tested in clinical trials. However, even highly-specialized physicians at leading academic centers are not equipped to apply genomic information available in publically available sources to clinical- decision-making concerning individual patients. Our central hypothesis is that we can develop informatics tools to support personalized cancer treatment as ""standard of care"" rather than ""one off"" exceptions. We will: 1) implement a bioinformatics pipeline for processing molecular data into actionable profiles, 2) create and maintain a database of therapeutic implications of common genomic aberrations using automated processing of publically-available sources and 3) develop tools to summarize and present patient-specific advice to clinicians. These tools will be based on existing technologies and publicly available data sources. Once tested, we will make these tools available via appropriate open source license. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: Genomic analysis of individual patients is now affordable and therapies targeted to specific molecular aberrations are being tested in clinical trials. In this project, we will develop informatics tools to support personalized cancer treatment as ""standard of care"" rather than ""one off"" exception.             ",Informatics to enable routine personalized cancer therapy,8607017,U01CA180964,"['Address', 'Adoption', 'Algorithms', 'Bioinformatics', 'Biological Factors', 'Cancer Biology', 'Cancer Center', 'Cancer Patient', 'Cataloging', 'Catalogs', 'Characteristics', 'Classification', 'Clinical', 'Clinical Medicine', 'Clinical Trials', 'DNA', 'Data', 'Data Sources', 'Databases', 'Drug Compounding', 'Enrollment', 'Evaluation', 'FDA approved', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Gold', 'Hot Spot', 'Human', 'Individual', 'Informatics', 'Institutes', 'Internet', 'Ions', 'Licensing', 'Link', 'Literature', 'MEDLINE', 'Malignant Neoplasms', 'Manuals', 'Metric', 'Molecular', 'Molecular Profiling', 'Mutation', 'Natural Language Processing', 'Nucleotides', 'Oncogenes', 'Oncologist', 'PTPRC gene', 'Paper', 'Pathologic', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Review Literature', 'Source', 'Source Code', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Update', 'Validation', 'Variant', 'base', 'biomedical informatics', 'cancer therapy', 'clinical decision-making', 'clinically relevant', 'exome sequencing', 'open source', 'programs', 'public health relevance', 'standard of care', 'success', 'tool', 'tumor', 'usability', 'web site']",NCI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2013,335040,-0.031662533421416804
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8438361,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2013,533554,-0.00833269667026724
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541872,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2013,872488,0.01000274565074541
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8545224,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,4024095,0.009182509348360048
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known  as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network  (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5  years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases  Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and  procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on  32 current studies, contingent upon the successful re-competition of their associated clinical research  consortia, addition of new studies reflecting the growth of the network, accommodation of federated  databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and  registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per  year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical  trials. We will continue development of new technologies to support scalability and generalizability and tools  for cross-disease data mining. Our international clinical information network is secure providing coordinated  data management services for collection, storage and analysis of diverse data types from multiple diseases  and geographically disparate locations and a portal for the general public and larger community of clinical  investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8734648,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,304154,0.009182509348360048
"Informatics for Integrating Biology & the Bedside (i2b2) DESCRIPTION (PROVIDED BY APPLICANT): The large and growing size of the healthcare system makes it imperative to understand what is happening to us, the recipients of healthcare, to be able to efficiently conduct research to improve healthcare delivery and to improve the state of biomedicine by advancing its science. i2b2, ""Informatics for Integrating Biology and the Bedside"" seeks to provide this instrumentation using the informational by products of healthcare and the biological materials accumulated through the delivery of healthcare. This complements existing efforts to create prospective cohort studies or trials outside the delivery of routine healthcare. In the first round of i2b2, we demonstrated that we could identify known adverse events and phenotypically select and then genotype patients for genetic association at approximately 1/10* of the price and less than l/10 of the time usually entailed to develop such populations for study. The challenge we have set ourselves for the next methodological challenge in i2b2 is the development of Virtual Cohort Studies (VCS) encompassing the population of a healthcare system as study subjects and asking questions of comparative effectiveness, unforeseen adverse events and identification of clinically relevant subpopulations including both clinical and genome-scale measures. We will be comparing the results of the VCS to those of carefully planned and executed cohort studies such as the Framingham Heart Study. VCS will require multiple methodological advances and tools development including in the disciplines of natural language processing, temporal reasoning, predictive modeling, biostatistics and machine learning. VCS methods will be tested by two driving biology projects, the first studying a collection of autoimmune diseases and the second type 2 diabetes. In both projects, VCS methods will be applied to investigate the components of cardiovascular risk from the genetic to the epigenetic and including the full range of clinical history including medications exposure. A systems/integrative approach will be taken to identify commonalities in these risk profiles across these disparate disease domains. VCS methods will be shared with i2b2 user community under open source governance while i2b2 user community contributions are folded into the i2b2 toolkit. Public Health Relevance: The i2b2 model of using existing clinical data for high throughput, cost effective, and timely research promises to rapidly leverage our nation's ability to better understand the existing disease burden and how best to achieve cost-effective clinical effectiveness, including new drug targets, new uses of existing drugs and improved management of existing disease.",Informatics for Integrating Biology & the Bedside (i2b2),8514121,U54LM008748,"['Adverse event', 'Autoimmune Diseases', 'Automobile Driving', 'Biocompatible Materials', 'Biology', 'Biometry', 'Clinical', 'Clinical Data', 'Clinical effectiveness', 'Cohort Studies', 'Collection', 'Communities', 'Complement', 'Development', 'Discipline', 'Disease', 'Drug Delivery Systems', 'Epigenetic Process', 'Framingham Heart Study', 'Genetic', 'Genome', 'Genotype', 'Healthcare', 'Healthcare Systems', 'Informatics', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Natural Language Processing', 'Non-Insulin-Dependent Diabetes Mellitus', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Population Study', 'Price', 'Recording of previous events', 'Research', 'Risk', 'Science', 'Study Subject', 'System', 'Testing', 'Time', 'burden of illness', 'cardiovascular risk factor', 'clinically relevant', 'comparative effectiveness', 'cost effective', 'genetic association', 'health care delivery', 'improved', 'instrumentation', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'tool development', 'virtual']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,U54,2012,50000,-0.02410155458720698
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.        PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.              DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8350385,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Base Sequence', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'RNA', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'tool', 'transcription factor', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2012,220000,-0.012619400671215027
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.      PUBLIC HEALTH RELEVANCE: The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.              The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8237596,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Delivery Systems', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2012,1010000,-0.028202061160340096
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.        PUBLIC HEALTH RELEVANCE: RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                  RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8268588,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2012,1300000,3.097844840065465e-05
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.         The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8331495,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2012,103535,-0.03279889625687251
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8238371,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,326844,-0.017933586461158335
"Informatics for Integrating Biology & the Bedside (i2b2) DESCRIPTION (PROVIDED BY APPLICANT): The large and growing size of the healthcare system makes it imperative to understand what is happening to us, the recipients of healthcare, to be able to efficiently conduct research to improve healthcare delivery and to improve the state of biomedicine by advancing its science. i2b2, ""Informatics for Integrating Biology and the Bedside"" seeks to provide this instrumentation using the informational by products of healthcare and the biological materials accumulated through the delivery of healthcare. This complements existing efforts to create prospective cohort studies or trials outside the delivery of routine healthcare. In the first round of i2b2, we demonstrated that we could identify known adverse events and phenotypically select and then genotype patients for genetic association at approximately 1/10* of the price and less than l/10 of the time usually entailed to develop such populations for study. The challenge we have set ourselves for the next methodological challenge in i2b2 is the development of Virtual Cohort Studies (VCS) encompassing the population of a healthcare system as study subjects and asking questions of comparative effectiveness, unforeseen adverse events and identification of clinically relevant subpopulations including both clinical and genome-scale measures. We will be comparing the results of the VCS to those of carefully planned and executed cohort studies such as the Framingham Heart Study. VCS will require multiple methodological advances and tools development including in the disciplines of natural language processing, temporal reasoning, predictive modeling, biostatistics and machine learning. VCS methods will be tested by two driving biology projects, the first studying a collection of autoimmune diseases and the second type 2 diabetes. In both projects, VCS methods will be applied to investigate the components of cardiovascular risk from the genetic to the epigenetic and including the full range of clinical history including medications exposure. A systems/integrative approach will be taken to identify commonalities in these risk profiles across these disparate disease domains. VCS methods will be shared with i2b2 user community under open source governance while i2b2 user community contributions are folded into the i2b2 toolkit. Public Health Relevance: The i2b2 model of using existing clinical data for high throughput, cost effective, and timely research promises to rapidly leverage our nation's ability to better understand the existing disease burden and how best to achieve cost-effective clinical effectiveness, including new drug targets, new uses of existing drugs and improved management of existing disease.",Informatics for Integrating Biology & the Bedside (i2b2),8326732,U54LM008748,[' '],NLM,BRIGHAM AND WOMEN'S HOSPITAL,U54,2012,1282800,-0.02410155458720698
"Ontology-based Information Network to Support Vaccine Research  Project Summary (Abstract):  Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.  Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8311060,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'abstracting', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2012,264994,-0.009602783718005095
"The Cardiovascular Research Grid    DESCRIPTION (provided by applicant):       The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinafions of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotafing ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and mofion that can predict the early presence of developing heart disease in fime for therapeufic intervenfion. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informafics system that allows clinical informafion to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease. (End of Abstract)          The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store,  manage, and analyze data on the structure and function of the cardiovascular system in health and disease.  The CVRG Project has developed and deployed unique technology that is now being used in a broad range  of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to  explore and analvze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8240702,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Time', 'Ultrasonography', 'Work', 'abstracting', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2012,2194299,0.012516855849738552
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma  ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans.  PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8209138,K99EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'symposium', 'time use']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2012,89325,-0.009424868581275316
"Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience     DESCRIPTION (provided by applicant): This Mid-Career Investigator Award in Patient-Oriented Research will permit the Principal Investigator to expand his mentorship, career development, and research on the integration of patient-oriented psychiatric genetics and neuroscience. The Principal Investigator is an established clinician-scientist in psychiatric genetics and clinical research. This application includes three components: 1) a mentoring plan designed to enhance the Principal Investigator's ability to mentor young investigators in psychiatric genetics and neuroscience; 2) a career development plan designed to expand the Principal Investigator's expertise in the integration of genetics and neuroimaging; and 3) a research plan that leverages the Principal Investigator's existing investigations on the genetic, behavioral and neural basis of psychiatric disorders and their relationship to normal variation in brain structure and function. The Principal Investigator has a substantial track record of mentoring and cultivating trainees and junior faculty in the area of genetic, clinical and epidemiologic research. This award would provide protected time for the Principal Investigator to expand and enhance these efforts. The mentoring plan capitalizes on his current mentoring activities and numerous collaborative research projects to develop a career development path for a new generation of investigators capable of integrating cutting-edge methods at the interface of patient-oriented genetics and neuroscience. Mentees will gain hands-on experience in patient-oriented research and will develop their own independent research directions. The clinical and translational research resources at MGH and the broader Harvard Medical School community will provide a rich training environment for mentees. The new research component of the application will include genomewide analyses of brain phenotypes to examine the genetic and neural architecture of anxiety disorders. This application was designed to address objectives identified by NIMH's Strategic Plan to fuel research on the causes of mental disorders and the National Advisory Mental Health Council Workgroup on Research Training to expand mentoring of young investigators capable of conducting the interdisciplinary research needed to achieve those objectives.        PUBLIC HEALTH RELEVANCE: Recent advances in genetics and neuroscience have created unprecedented opportunities to unravel the causes of mental illness and ameliorate the public health burden and suffering they incur. This application for a Midcareer Investigator Award in Patient-Oriented Research will allow Dr. Jordan Smoller to enhance his research and mentorship of young scientists to bring together genetics and brain imaging in the service of discovering the causes of mental illness.              Recent advances in genetics and neuroscience have created unprecedented opportunities to unravel the causes of mental illness and ameliorate the public health burden and suffering they incur. This application for a Midcareer Investigator Award in Patient-Oriented Research will allow Dr. Jordan Smoller to enhance his research and mentorship of young scientists to bring together genetics and brain imaging in the service of discovering the causes of mental illness.            ",Research and Mentoring on Integrating Psychiatric Genetics and Neuroscience,8299909,K24MH094614,"['Address', 'Adolescent', 'Age', 'American', 'Anxiety', 'Anxiety Disorders', 'Architecture', 'Area', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Award', 'Base of the Brain', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Biology', 'Bipolar Disorder', 'Brain', 'Brain imaging', 'Clinical', 'Clinical Research', 'Cognition', 'Collection', 'Communities', 'DNA', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnostic', 'Disease', 'Environment', 'Epidemiologic Studies', 'European', 'Faculty', 'Fruit', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Genetic', 'Genetic Research', 'Genetic Variation', 'Genomic Library', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Hereditary Disease', 'Image', 'Individual', 'Infant', 'Interdisciplinary Study', 'International', 'Investigation', 'Jordan', 'Laboratories', 'Leadership', 'Longitudinal Studies', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Midcareer Investigator Award in Patient-Oriented Research', 'National Institute of Mental Health', 'Neurosciences', 'Pathology', 'Personality', 'Phenotype', 'Predisposition', 'Principal Investigator', 'Psychotic Disorders', 'Public Health', 'Quality Control', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Risk', 'Role', 'Sampling', 'Schizophrenia', 'Scientist', 'Services', 'Strategic Planning', 'Structure', 'Suicide', 'Technology', 'Temperament', 'Time', 'Training', 'Transcend', 'Translational Research', 'Variant', 'Work', 'base', 'career', 'career development', 'cohort', 'design', 'disorder risk', 'experience', 'functional disability', 'gene discovery', 'genetic analysis', 'genetic variant', 'genome wide association study', 'genome-wide', 'innovation', 'medical schools', 'neuroimaging', 'neuropsychiatry', 'patient oriented', 'patient oriented research', 'programs', 'psychogenetics', 'relating to nervous system', 'research and development', 'resilience', 'trait']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K24,2012,186942,-0.01372661577860408
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8326650,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,317212,-0.019860647757736438
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8242742,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2012,392767,0.015478873795701288
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,-0.004268561181197057
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8496251,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2012,40000,0.005635261875240009
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8330927,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,1821611,0.01000274565074541
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541935,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,100000,0.01000274565074541
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8330246,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2012,2827142,0.009182509348360048
"CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING    DESCRIPTION (provided by applicant): Case Studies in Bayesian Statistics and Machine Learning I continues in the tradition of the Case Studies in Bayesian Statistics series. The original series of workshops were held in odd years at Carnegie Mellon University in the early fall. The first edition of the new workshop will be held at Carnegie Mellon University on October 14-15, 2011. The highest level goal of the workshop series is to generate and present successful solutions to difficult substantive problems in a wide variety of areas. The specific objectives of the workshop are to 1. Present and discuss solutions to challenging scientific problems that illustrate the potential for statistical machine learning approaches in substantive research; 2. Present an opportunity for statisticians and computer scientists to present applications-oriented research  that changes the way that data are analyzed in scientific fields; 3. Stimulate discussion of the challenges of the analysis of high-dimensional and complex datasets in a scientifically useful manner; 4. Encourage young researchers, including graduate students, to present their applied work; 5. Provide a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; 6. Expose young researchers to important challenges and opportunities in collaborative research; 7. Include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment; 8. Encourage dissemination of the findings presented at the workshop via well-documented and peer- reviewed journal articles.      PUBLIC HEALTH RELEVANCE: Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.           Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.         ",CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING,8203089,R13CA144626,"['Area', 'Bioinformatics', 'Case Study', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Data Analyses', 'Data Set', 'Disabled Persons', 'Discipline', 'Disease', 'Educational workshop', 'Environment', 'Fostering', 'Goals', 'Hand', 'Health Sciences', 'Image', 'Institutes', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Human Genome Research Institute', 'Participant', 'Peer Review', 'Research', 'Research Personnel', 'Scientist', 'Series', 'Solutions', 'Underrepresented Minority', 'Universities', 'Woman', 'Work', 'anticancer research', 'computer science', 'data modeling', 'falls', 'graduate student', 'interest', 'journal article', 'meetings', 'peer', 'planetary Atmosphere', 'statistics', 'symposium']",NCI,CARNEGIE-MELLON UNIVERSITY,R13,2011,7500,-0.023287881083480268
"A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology    DESCRIPTION (provided by applicant): The non-medical use of pharmaceutical opioids has been identified as one of the fastest growing forms of drug abuse in the U.S. There is a critical need to enhance current epidemiological monitoring, early warning, and post-marketing surveillance systems by providing additional and more timely data. The World Wide Web has been identified as one of the ""leading edge"" data sources for detecting patterns and changes in drug use practices. Many websites provide a venue for individuals to freely share their own experiences, post questions, and offer comments about different drugs. Such User Generated Content (UGC) can be used as a very rich data source to study knowledge, attitudes, and behaviors related to illicit drugs. To harness the full potential of the Web for drug abuse research, the field needs to develop a highly automated way of accessing, extracting, and analyzing Web-based data related to illicit drug use. This exploratory R21 is a multi-principal investigator, collaborative effort between researchers at the Center for Interventions, Treatment and Addictions Research (CITAR) and the Center for Knowledge-Enabled Information Services and Science (Kno.e.sis) at Wright State University. The purpose of this Web-based study is to apply cutting-edge information processing techniques, such as the Semantic Web, Natural Language Processing, and Machine Learning, to qualitative and quantitative content analysis of user generated content to achieve the following aims: 1) Describe drug users' knowledge, attitudes, and behaviors related to the illicit use of Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine); 2) Identify and describe temporal patterns of the illicit use of these drugs as reflected on web-based forums. To collect data, the study will use websites that allow for the free discussion of illicit drugs, contain information on illicit prescription drug use, and are accessible for public viewing. The study will generate new information about the practices of buprenorphine abuse and will contribute to the advancement of public health and substance abuse research by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data. Automated information extraction methods applied in this study will enhance current early warning and epidemiological surveillance systems and could advance qualitative and Web-based research methods in other areas of public health.      PUBLIC HEALTH RELEVANCE: Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.              Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.            ",A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology,8190799,R21DA030571,"['Accident and Emergency department', 'Archives', 'Area', 'Behavior', 'Buprenorphine', 'Code', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Data', 'Data Sources', 'Drug Rehabilitation Centers', 'Drug abuse', 'Drug usage', 'Drug user', 'Early identification', 'Epidemiologic Monitoring', 'Epidemiologic Studies', 'Epidemiology', 'Face', 'Health', 'Health Knowledge, Attitudes, Practice', 'Health Professional', 'Heroin Dependence', 'Hospitals', 'Illicit Drugs', 'Individual', 'Information Sciences', 'Information Services', 'Information Systems', 'Internet', 'Intervention', 'Intervention Studies', 'Interview', 'Knowledge', 'Label', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Monitor', 'NIH Program Announcements', 'Naloxone', 'Natural Language Processing', 'Online Systems', 'Opioid', 'Overdose', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Poison Control Centers', 'Policies', 'Population', 'Prevention', 'Principal Investigator', 'Process', 'Public Health', 'Published Comment', 'Qualitative Research', 'Reading', 'Reliance', 'Research', 'Research Methodology', 'Research Personnel', 'Sampling', 'Selection Bias', 'Self Disclosure', 'Semantics', 'Source', 'Substance abuse problem', 'Subutex', 'Surveillance Methods', 'Surveys', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'addiction', 'buprenorphine abuse', 'computer based Semantic Analysis', 'design', 'emotional disclosure', 'empowered', 'experience', 'informant', 'information processing', 'misuse of prescription only drugs', 'population survey', 'post-market', 'prescription drug abuse', 'programs', 'response', 'social', 'tool', 'trend', 'web site', 'working group']",NIDA,WRIGHT STATE UNIVERSITY,R21,2011,219000,-0.034682294333491986
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,8062287,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Comorbidity', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Health Sciences', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Schools', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2011,140381,-0.0008826377759913067
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,8075593,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2011,274386,-0.006775271348992179
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.           Project Narrative The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.",Scalable Learning with Ensemble Techniques and Parallel Computing,8045486,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Biological Sciences', 'Biomedical Research', 'Classification', 'Communication', 'Communities', 'Community Financing', 'Companions', 'Complex', 'Computer software', 'Consult', 'Crowding', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Health', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'new technology', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2011,374673,0.008141762788406682
"Optimizing Peripheral Nerve Regeneration using Computational Intelligence based T    DESCRIPTION (provided by applicant):  Peripheral nerve injuries are common diseases that affect a large amount of patients every year.  Tissue engineering has emerged as a powerful approach for developing alternative nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  Some research groups have applied artificial neural networks and decision trees to obtain the best model configuration for the prediction of the tissue engineering strategies.  For the decision trees based methods, it is hard to tell which classification tree is better than the other.  Furthermore, the prediction system using the decision tree algorithm lacks the capability of accumulating the learning experience over time.  On the other hand, Artificial Neural Networks (ANNs) exhibit some remarkable properties, but only the connection weights are trained with fixed topology.  It is hard to find the best fixed topology in advance for each specific tissue engineering strategy.  In this proposal, swarm intelligence (SI) based evolving ANNs technique is proposed to tackle this challenge.  Two swarm intelligence based methods, Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO), will be applied in this project to train the ANN model.  More specifically, ACO will be used to optimize the topology structure of the ANN models, while the PSO is used to adjust the connection weights of the ANN models based on the optimized topology structure.  For this SWarm Intelligence based Reinforcement Learning method for ANNs (SWIRL-ANN) system, both topology and connection weight of artificial neural networks can be evolved automatically and simultaneously so that an optimal classifier for tissue engineering strategies in peripheral nerve regeneration can be achieved.  The research project will include the following phases:  Aim 1:  Predict tissue engineering strategies in peripheral nerve regeneration using SWarm Intelligence based Reinforcement Learning method for ANNs (SWIRL-ANN) analytical and prediction system.  Aim 2:  Validate the efficacy of novel unknown tissue engineered nerve grafts as predicted by using SWIRL-ANN based analytical and prediction system for bridging peripheral nerve gaps in rat sciatic nerve injury model in vivo.      PUBLIC HEALTH RELEVANCE:  Tissue engineering has emerged as a powerful approach for developing nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  In this proposal, swarm intelligence (SI) based evolving artificial neural networks (ANNs) technique is proposed to tackle this challenge.  The proposed research will be helpful to efficiently develop tissue engineered products for tissue and organ replacement.               Tissue engineering has emerged as a powerful approach for developing nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  In this proposal, swarm intelligence (SI) based evolving artificial neural networks (ANNs) technique is proposed to tackle this challenge.  The proposed research will be helpful to efficiently develop tissue engineered products for tissue and organ replacement.            ",Optimizing Peripheral Nerve Regeneration using Computational Intelligence based T,8232817,R15NS074404,"['Advanced Development', 'Affect', 'Algorithms', 'Animal Experiments', 'Animals', 'Ants', 'Area', 'Artificial Intelligence', 'Autologous Transplantation', 'Biocompatible Materials', 'Biological', 'Biological Neural Networks', 'Biomedical Research', 'Breathing', 'Cells', 'Characteristics', 'Classification', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Defect', 'Disease', 'Exhibits', 'Foundations', 'Future', 'Generic Drugs', 'Individual', 'Insecta', 'Intelligence', 'Knowledge', 'Learning', 'Maps', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Nerve', 'Nerve Regeneration', 'Network-based', 'Neural Network Simulation', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Perception', 'Peripheral Nerves', 'Peripheral nerve injury', 'Phase', 'Physicians', 'Process', 'Property', 'Psychological reinforcement', 'Rattus', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Social Behavior', 'Structure', 'Surgical incisions', 'System', 'Techniques', 'Time', 'Tissue Engineering', 'Tissues', 'Training', 'Treatment Protocols', 'Trees', 'Vertebrates', 'Weight', 'Work', 'base', 'experience', 'in vivo', 'in vivo Model', 'insight', 'nerve gap', 'nerve injury', 'novel', 'particle', 'relating to nervous system', 'sciatic nerve', 'social', 'tool', 'vector']",NINDS,STEVENS INSTITUTE OF TECHNOLOGY,R15,2011,423840,-0.021509118501748803
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,8133946,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computational algorithm', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2011,297497,-0.013787275848051084
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,8112574,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'spatiotemporal', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2011,176524,-0.016359203963363912
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.       PUBLIC HEALTH RELEVANCE: The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.              The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8164533,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2011,102709,-0.028453002307883894
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,8113231,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2011,119416,-0.030524897703237935
"Informatics for Integrating Biology & the Bedside (i2b2) DESCRIPTION (PROVIDED BY APPLICANT): The large and growing size of the healthcare system makes it imperative to understand what is happening to us, the recipients of healthcare, to be able to efficiently conduct research to improve healthcare delivery and to improve the state of biomedicine by advancing its science. i2b2, ""Informatics for Integrating Biology and the Bedside"" seeks to provide this instrumentation using the informational by products of healthcare and the biological materials accumulated through the delivery of healthcare. This complements existing efforts to create prospective cohort studies or trials outside the delivery of routine healthcare. In the first round of i2b2, we demonstrated that we could identify known adverse events and phenotypically select and then genotype patients for genetic association at approximately 1/10* of the price and less than l/10 of the time usually entailed to develop such populations for study. The challenge we have set ourselves for the next methodological challenge in i2b2 is the development of Virtual Cohort Studies (VCS) encompassing the population of a healthcare system as study subjects and asking questions of comparative effectiveness, unforeseen adverse events and identification of clinically relevant subpopulations including both clinical and genome-scale measures. We will be comparing the results of the VCS to those of carefully planned and executed cohort studies such as the Framingham Heart Study. VCS will require multiple methodological advances and tools development including in the disciplines of natural language processing, temporal reasoning, predictive modeling, biostatistics and machine learning. VCS methods will be tested by two driving biology projects, the first studying a collection of autoimmune diseases and the second type 2 diabetes. In both projects, VCS methods will be applied to investigate the components of cardiovascular risk from the genetic to the epigenetic and including the full range of clinical history including medications exposure. A systems/integrative approach will be taken to identify commonalities in these risk profiles across these disparate disease domains. VCS methods will be shared with i2b2 user community under open source governance while i2b2 user community contributions are folded into the i2b2 toolkit. Public Health Relevance: The i2b2 model of using existing clinical data for high throughput, cost effective, and timely research promises to rapidly leverage our nation's ability to better understand the existing disease burden and how best to achieve cost-effective clinical effectiveness, including new drug targets, new uses of existing drugs and improved management of existing disease.",Informatics for Integrating Biology & the Bedside (i2b2),8144250,U54LM008748,[' '],NLM,BRIGHAM AND WOMEN'S HOSPITAL,U54,2011,2657626,-0.02410155458720698
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8044673,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,324859,-0.010806908488033366
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8120230,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2011,264994,-0.010001230670858985
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,8055527,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Traumatic Stress Disorders', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,248610,-0.020400891839231425
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,8138486,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2011,266112,-0.012061652339260936
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma    DESCRIPTION (provided by applicant) This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Computational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, and progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans.      PUBLIC HEALTH RELEVANCE:  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.            Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.         ",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,7871151,K99EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Science', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'symposium', 'time use']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2011,89922,-0.009491950732491594
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,8139155,R44RR024094,"['AIDS/HIV problem', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cell physiology', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2011,449663,-0.03603383459604477
"The Cardiovascular Research Grid    DESCRIPTION (provided by applicant):       The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinafions of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotafing ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and mofion that can predict the early presence of developing heart disease in fime for therapeufic intervenfion. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informafics system that allows clinical informafion to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease. (End of Abstract)          The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store,  manage, and analyze data on the structure and function of the cardiovascular system in health and disease.  The CVRG Project has developed and deployed unique technology that is now being used in a broad range  of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to  explore and analvze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8017600,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Time', 'Ultrasonography', 'Work', 'abstracting', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2011,2241978,0.012516855849738552
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,8143297,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'animal tissue', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2011,132786,-0.013460630828542266
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,0.006557532413048082
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8202896,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,312599,-0.019860647757736438
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8039246,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2011,526649,0.015478873795701288
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,-0.005578951530501415
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8138357,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2011,128304,0.005635261875240009
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8325815,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2011,100000,0.005635261875240009
"COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems.  To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati,8173654,76201000029C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Health Care Costs', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data mining', 'flexibility', 'innovation', 'novel', 'novel strategies', 'rapid growth']",NLM,NORTHEASTERN UNIVERSITY,N03,2010,377968,0.01805241190844751
"COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible,8170612,76201000023C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Medical Informatics', 'Patients', 'Population', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'prototype', 'rapid growth']",NLM,"CYCORP, INC.",N03,2010,377967,0.011520407299498901
"COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat  In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat ,8172635,76201000024C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Disease', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Human', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patients', 'Population', 'Research', 'Research Personnel', 'Research Project Grants', 'Statistical Methods', 'Structure', 'System', 'Text', 'Thinking', 'biomedical scientist', 'clinical care', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,N03,2010,373073,0.009364736431112311
"COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical,8173644,76201000025C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computer Simulation', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patient Care', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'clinical application', 'evidence based guidelines', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,STANFORD UNIVERSITY,N03,2010,378000,0.01181881406407796
"COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que,8173663,76201000032C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Communities', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'rapid growth', 'social']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,N03,2010,301251,0.018026067851251652
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,8049892,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2010,5742,-0.017659494165545724
"COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica,8173959,76201000034C,"['Caring', 'Clinical', 'Clinical Data', 'Clinical Decision Support Systems', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'clinical decision-making', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SAMSUNG SDS AMERICA, INC.",N03,2010,377925,0.01653354413158759
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173956,76201000033C,"['Biology', 'Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computer software', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Molecular Biology', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data modeling', 'flexibility', 'innovation', 'novel strategies', 'open source', 'prototype', 'rapid growth']",NLM,UNIVERSITY OF COLORADO DENVER,N03,2010,377982,0.014987800102179195
"COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a","COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec",8173645,76201000026C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Population', 'Prevention', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'cost effectiveness', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SIMULCONSULT, INC.",N03,2010,377991,0.016560228159354404
"COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization,8173653,76201000028C,"['Age', 'Caring', 'Chronic Disease', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Complex', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Disease Management', 'Face', 'Family', 'Funding', 'Human', 'Imagery', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Methods', 'Patients', 'Population', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'Visual', 'biomedical scientist', 'design', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"RUTGERS, THE STATE UNIV OF N.J.",N03,2010,300216,0.014640603912418655
"COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De,8173961,76201000035C,"['Caring', 'Childhood', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Disease', 'Expert Systems', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Lead', 'Machine Learning', 'Medical', 'Memory', 'Mental Depression', 'Outcome', 'Patients', 'Probability', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'evidence base', 'flexibility', 'health application', 'innovation', 'novel strategies', 'rapid growth', 'tool', 'treatment planning']",NLM,SRI INTERNATIONAL,N03,2010,377997,0.015584382097098629
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7802898,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Comorbidity', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2010,136911,-0.0008826377759913067
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7851323,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2010,278345,-0.006775271348992179
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,8013208,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2010,376899,0.008658932021163265
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7881671,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,264640,-0.011564507240101328
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7912919,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,304151,-0.013787275848051084
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,7961101,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2010,221192,-0.016359203963363912
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7945336,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'abstracting', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs']",NICHD,SEQUOIA FOUNDATION,RC2,2010,1397584,-0.0024746130693482117
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173647,76201000027C,[' '],NLM,STANFORD UNIVERSITY,N03,2010,378000,0.014987800102179195
"COMPUTATIONAL THINKING-Text Mining of clinical narratives: In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Text Mining of clinical narratives:,8173660,76201000031C,[' '],NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,N03,2010,336943,0.008728249140988803
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7754089,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2010,2037327,0.0003808877120557354
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7895558,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2010,116662,-0.030524897703237935
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,8129905,R43HG005046,"['Address', 'Benefits and Risks', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2010,35000,-0.027146454653858197
"Informatics for Integrating Biology & the Bedside (i2b2) DESCRIPTION (PROVIDED BY APPLICANT): The large and growing size of the healthcare system makes it imperative to understand what is happening to us, the recipients of healthcare, to be able to efficiently conduct research to improve healthcare delivery and to improve the state of biomedicine by advancing its science. i2b2, ""Informatics for Integrating Biology and the Bedside"" seeks to provide this instrumentation using the informational by products of healthcare and the biological materials accumulated through the delivery of healthcare. This complements existing efforts to create prospective cohort studies or trials outside the delivery of routine healthcare. In the first round of i2b2, we demonstrated that we could identify known adverse events and phenotypically select and then genotype patients for genetic association at approximately 1/10* of the price and less than l/10 of the time usually entailed to develop such populations for study. The challenge we have set ourselves for the next methodological challenge in i2b2 is the development of Virtual Cohort Studies (VCS) encompassing the population of a healthcare system as study subjects and asking questions of comparative effectiveness, unforeseen adverse events and identification of clinically relevant subpopulations including both clinical and genome-scale measures. We will be comparing the results of the VCS to those of carefully planned and executed cohort studies such as the Framingham Heart Study. VCS will require multiple methodological advances and tools development including in the disciplines of natural language processing, temporal reasoning, predictive modeling, biostatistics and machine learning. VCS methods will be tested by two driving biology projects, the first studying a collection of autoimmune diseases and the second type 2 diabetes. In both projects, VCS methods will be applied to investigate the components of cardiovascular risk from the genetic to the epigenetic and including the full range of clinical history including medications exposure. A systems/integrative approach will be taken to identify commonalities in these risk profiles across these disparate disease domains. VCS methods will be shared with i2b2 user community under open source governance while i2b2 user community contributions are folded into the i2b2 toolkit. Public Health Relevance: The i2b2 model of using existing clinical data for high throughput, cost effective, and timely research promises to rapidly leverage our nation's ability to better understand the existing disease burden and how best to achieve cost-effective clinical effectiveness, including new drug targets, new uses of existing drugs and improved management of existing disease.",Informatics for Integrating Biology & the Bedside (i2b2),8012931,U54LM008748,[' '],NLM,BRIGHAM AND WOMEN'S HOSPITAL,U54,2010,3966606,-0.02410155458720698
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7846105,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,326254,-0.010806908488033366
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7935464,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2010,267671,-0.010001230670858985
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7799875,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,250650,-0.020400891839231425
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7929664,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2010,277200,-0.012061652339260936
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7941984,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2010,303186,-0.013460630828542266
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,7999420,R44RR024094,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'HIV', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2010,449663,-0.03603383459604477
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,0.006557532413048082
"Enhancing 3dsvm to improve its interoperability and dissemination    DESCRIPTION (provided by applicant): This research plan outlines crucial software enhancements to a program called 3dsvm, which is a command line program and graphical user interface (gui) plugin for AFNI (Cox, 1996). 3dsvm performs support vector machine (SVM) analysis on fMRI data, which constitutes one important approach to performing multivariate supervised learning of neuroimaging data. 3dsvm originally provided the ability to analyze fMRI data as described in (LaConte et al., 2005). Since its first distribution as a part of AFNI, it has been steadily extended to provide new functionality including regression and non-linear kernels, as well as multiclass classification capabilities. In addition to its integration into AFNI, features that make 3dsvm particularly well suited for fMRI analysis are that it is easy to spatially mask voxels (to include/exclude them in the SVM analysis) as well as to flexibly select subsets of a dataset to use as training or testing samples. It has been used to generate results for our own work and for collaborative efforts and has been cited as a resource by others (Mur et al. 2009; Hanke et al. 2009). Despite many positive aspects of 3dsvm, the priorities of PAR-07-417 address a genuine need that this software project has - the ability to focus on improvements that will increase its dissemination and interoperability. A major motivation for PAR-07-417 is to facilitate the improved interface, characterization, and documentation to enhance the extent of sharing and to provide the groundwork for future extensions. Our aims are well aligned with this program announcement. Further, there is a growing need in the neuroimaging community for tools such as 3dsvm. Since 3dsvm is not a new project, is tightly integrated into the software environment of AFNI, and can be further integrated to enable better functionality to support needs as diverse as NIfTI format capabilities to rtFMRI, this proposed project will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.      PUBLIC HEALTH RELEVANCE: This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.           NARRATIVE This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.",Enhancing 3dsvm to improve its interoperability and dissemination,8278135,R03EB012464,[' '],NIBIB,VIRGINIA POLYTECHNIC INST AND ST UNIV,R03,2010,156500,-0.012588658508338634
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,8076789,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2010,956625,0.02862268565147817
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7774343,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2010,525262,0.015478873795701288
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7945368,R01HG004836,"['Anatomy', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Communities', 'Competence', 'Complex', 'Computer software', 'Consultations', 'Controlled Vocabulary', 'Dana-Farber Cancer Institute', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Fostering', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'In Vitro', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Molecular', 'National Cancer Institute', 'Nature', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'Validation', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'design', 'empowered', 'gene function', 'graphical user interface', 'information organization', 'instrument', 'interoperability', 'novel', 'open source', 'repository', 'research study', 'response', 'scale up', 'sound', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2010,428079,0.005706847248697948
"Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor    DESCRIPTION (provided by applicant): The Informatics Core is part of the Consortium for the ""Collaborative Initiative on Fetal Alcohol Spectrum Disorders"" (CIFASD). The theme of this collaborative initiative is a cross-cultural assessment of ""fetal alcohol spectrum disorder"" (FASD). The CIFASD will coordinate basic, behavioral, and clinical investigators in a multidisciplinary research project to better inform approaches aimed at developing effective intervention and treatment approaches for FASD. It will involve the input and contributions from basic researchers, behavioral scientists, and clinical investigators with the willingness to utilize novel and cutting-edge techniques so as not to simply replicate previous or ongoing work, but rather to try and move it forward in a rigorous fashion. The Informatics Core will develop and maintain the CIFASD Data Repository, which will be used to collect, maintain and distribute data generated by the various participants in the consortium. The Informatics Core will be responsible for working with the other consortium participants to define a Data Dictionary to be used in standardizing data collection, enabling the transfer of data to and from the CIFASD Data Repository, consulting on how to establish local data management systems, providing both software tools and consulting to consortium participants, and producing status reports about the progress of the various projects within the consortium. The Informatics Core draws on a wealth of resources, experience, and expertise at Indiana University in information technology infrastructure and data management, The CIFASD Data Repository will be developed on Indiana University's state-of-the-art central supercomputing facilities, taking advantage of Indiana University's strong commitment to institutional computational resources. Resources that will be used to implement the CIFASD Data Repository include multiple supercomputers, an array of readily available database and statistical software, and a high- speed, secure, robust data archiving system capable of storing duplicate copies in multiple physical locations separated by more than fifty miles. The Informatics Core will use these extraordinary computational resources to provide a single, highly secure location for consortium participants to obtain the cross-cultural data that will enable the CIFASD to meet its goals of developing novel techniques for intervention and treatment of FASD.           n/a",Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor,7900050,U24AA014818,"['Address', 'Adolescent', 'Affect', 'Africa', 'Alcohol consumption', 'Alcohol-Related Neurodevelopmental Disorder', 'Alcohols', 'Animal Model', 'Animals', 'Archives', 'Arts', 'Basic Science', 'Behavioral', 'Binding Sites', 'Biology', 'Brain', 'Brain imaging', 'Cells', 'Characteristics', 'Child', 'Choline', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Collaborations', 'Computer Systems', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Collection', 'Data Storage and Retrieval', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Dictionary', 'Dietary Intervention', 'Drug Design', 'Dysmorphology', 'Early Diagnosis', 'Early treatment', 'Elements', 'Emotional', 'Ethanol', 'Ethnic group', 'Face', 'Fetal Alcohol Exposure', 'Fetal Alcohol Spectrum Disorder', 'Fetal Alcohol Syndrome', 'Frequencies', 'Future', 'Genetic Polymorphism', 'Goals', 'Human', 'Image', 'Image Analysis', 'Indiana', 'Individual', 'Infant', 'Informal Social Control', 'Informatics', 'Information Technology', 'Interdisciplinary Study', 'Internet', 'Intervention', 'Knowledge', 'Label', 'Language', 'Language Development', 'Life', 'Location', 'Los Angeles', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methods', 'Mission', 'Modeling', 'Moscow', 'Mothers', 'Mus', 'Names', 'Neonatal', 'Neural Cell Adhesion Molecule L1', 'Neurobiology', 'New Mexico', 'Outcome', 'Participant', 'Pattern', 'Performance', 'Phenotype', 'Pilot Projects', 'Play', 'Population', 'Pregnancy', 'Rattus', 'Recording of previous events', 'Reporting', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Schools', 'Scientist', 'Secure', 'Sensitivity and Specificity', 'Sheep', 'Signal Pathway', 'Signal Transduction', 'Site', 'Software Tools', 'Specificity', 'Speed', 'Structure', 'Sum', 'Supercomputing', 'Supplementation', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Three-Dimensional Image', 'Time', 'Toddler', 'Ukraine', 'Ultrasonography', 'Universities', 'Woman', 'Work', 'alcohol exposure', 'alcohol sensitivity', 'analog', 'brain behavior', 'brain morphology', 'cognitive control', 'computing resources', 'critical period', 'data exchange', 'data integration', 'data management', 'data sharing', 'distributed data', 'drinking', 'effective intervention', 'experience', 'fetal', 'flexibility', 'follow-up', 'imaging modality', 'improved', 'in utero', 'infancy', 'literacy', 'meetings', 'mouse model', 'multidisciplinary', 'neurobehavioral', 'northern plains', 'novel', 'offspring', 'prenatal', 'prospective', 'quality assurance', 'reconstruction', 'repository', 'social', 'supercomputer', 'three dimensional structure', 'willingness']",NIAAA,INDIANA UNIVERSITY BLOOMINGTON,U24,2010,192573,-0.06898257102120521
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7851333,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Belief', 'Bioinformatics', 'Biological Assay', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Phenotype', 'Physiological', 'Protocols documentation', 'Public Health', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly']",NHLBI,WASHINGTON UNIVERSITY,R01,2010,474912,-0.009188704279595719
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8150047,U54NS064808,"['Address', 'Adherence', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Bite', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials Cooperative Group', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Computer software', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Quality', 'Data Reporting', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Disasters', 'Disease', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family history of', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Image', 'Individual', 'Industry', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Manuals', 'Maps', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patients', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Positron-Emission Tomography', 'Principal Investigator', 'Printing', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Reader', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Scanning', 'Scientist', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Support Groups', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'computerized data processing', 'data management', 'data mining', 'electronic data', 'follow-up', 'improved', 'interest', 'meetings', 'patient advocacy group', 'programs', 'quality assurance', 'radiologist', 'sample collection', 'statistics', 'tool', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2010,959195,0.009182509348360048
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7921043,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2010,133651,0.005635261875240009
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,0.006542819027175385
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7670456,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,74750,-0.017659494165545724
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7664538,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'systematic review', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,318898,-0.015439731637364328
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7597080,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2009,138161,-0.0008826377759913067
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7920591,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2009,54000,-0.0008826377759913067
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7656692,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,282304,-0.006775271348992179
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7651387,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,267801,-0.011564507240101328
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7685518,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Series', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'new therapeutic target', 'novel', 'pathway tools', 'programs', 'reconstruction']",NLM,SRI INTERNATIONAL,R01,2009,175647,-0.015525943479884816
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7670408,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,311541,-0.013787275848051084
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7853378,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs', 'public health relevance']",NICHD,SEQUOIA FOUNDATION,RC2,2009,2003191,-0.0024746130693482117
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7582301,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2009,2057843,0.0003808877120557354
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7652367,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2009,124786,-0.030524897703237935
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7913595,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2009,108000,-0.030524897703237935
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7633119,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2009,18437,-0.0081654752817338
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,0.006542819027175385
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,0.006542819027175385
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,7748337,R43HG005046,"['Address', 'Benefits and Risks', 'Body of uterus', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2009,119499,-0.027146454653858197
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7582189,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Body of uterus', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,312453,-0.010806908488033366
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7735790,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2009,270375,-0.010001230670858985
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7663288,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Computer Systems Development', 'Computer software', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Source', 'Structure', 'System', 'Systems Integration', 'Technology', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'meetings', 'models and simulation', 'open source', 'outreach', 'protein complex', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2009,437938,-0.007228588660677277
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7591237,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,272818,-0.020400891839231425
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7790821,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Support', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'flexibility', 'innovation', 'interest', 'meetings', 'prevent', 'product development', 'public health relevance', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2009,4047,-0.015601179137337214
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7693803,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2009,280000,-0.012061652339260936
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of  computer scientists, software engineers, and medical investigators who develop computational tools for the  analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure  and environment for the development of computational algorithms and open source technologies, and then  oversee the training and dissemination of these tools to the medical research community. This world-class  software and development environment serves as a foundation for accelerating the development and  deployment of computational tools that are readily accessible to the medical research community. The team  combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state  of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed,  open-source environment) to enable computational examination of both basic neurosience and neurologicat  disorders. In developing this infrastructure resource, the team will significantly expand upon proven open  systems technology and platforms. The driving biological projects will come initially from the study of  schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open  systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures  and connectivity patterns in the brain, derangements of which have long been thought to play a role in the  etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range  of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially  including microscopic, genomic, and other image data. It will apply to image data from individual  )atients,and to studies executed across large poplulations. The data will be taken from subjects across a  Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7915998,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Genomics', 'Goals', 'Healthcare', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2009,3720664,-0.013976657121543121
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7804332,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2009,363247,-0.013460630828542266
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,0.006557532413048082
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7660538,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2009,688362,0.02862268565147817
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7558468,R01HG004836,"['Anatomy', 'Architecture', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Competence', 'Complex', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Side', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'computerized data processing', 'design', 'empowered', 'graphical user interface', 'insight', 'instrument', 'open source', 'programs', 'repository', 'research study', 'response', 'scale up', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2009,428078,0.005706847248697948
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7565504,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Conflict (Psychology)', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2009,529858,0.015478873795701288
"Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor    DESCRIPTION (provided by applicant): The Informatics Core is part of the Consortium for the ""Collaborative Initiative on Fetal Alcohol Spectrum Disorders"" (CIFASD). The theme of this collaborative initiative is a cross-cultural assessment of ""fetal alcohol spectrum disorder"" (FASD). The CIFASD will coordinate basic, behavioral, and clinical investigators in a multidisciplinary research project to better inform approaches aimed at developing effective intervention and treatment approaches for FASD. It will involve the input and contributions from basic researchers, behavioral scientists, and clinical investigators with the willingness to utilize novel and cutting-edge techniques so as not to simply replicate previous or ongoing work, but rather to try and move it forward in a rigorous fashion. The Informatics Core will develop and maintain the CIFASD Data Repository, which will be used to collect, maintain and distribute data generated by the various participants in the consortium. The Informatics Core will be responsible for working with the other consortium participants to define a Data Dictionary to be used in standardizing data collection, enabling the transfer of data to and from the CIFASD Data Repository, consulting on how to establish local data management systems, providing both software tools and consulting to consortium participants, and producing status reports about the progress of the various projects within the consortium. The Informatics Core draws on a wealth of resources, experience, and expertise at Indiana University in information technology infrastructure and data management, The CIFASD Data Repository will be developed on Indiana University's state-of-the-art central supercomputing facilities, taking advantage of Indiana University's strong commitment to institutional computational resources. Resources that will be used to implement the CIFASD Data Repository include multiple supercomputers, an array of readily available database and statistical software, and a high- speed, secure, robust data archiving system capable of storing duplicate copies in multiple physical locations separated by more than fifty miles. The Informatics Core will use these extraordinary computational resources to provide a single, highly secure location for consortium participants to obtain the cross-cultural data that will enable the CIFASD to meet its goals of developing novel techniques for intervention and treatment of FASD.           n/a",Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor,7668731,U24AA014818,"['Address', 'Adolescent', 'Affect', 'Africa', 'Alcohol consumption', 'Alcohol-Related Neurodevelopmental Disorder', 'Alcohols', 'Animal Model', 'Animals', 'Archives', 'Arts', 'Basic Science', 'Behavioral', 'Binding Sites', 'Biology', 'Brain', 'Brain imaging', 'Cells', 'Characteristics', 'Child', 'Choline', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Collaborations', 'Computer Systems', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Collection', 'Data Storage and Retrieval', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Dictionary', 'Dietary Intervention', 'Drug Design', 'Dysmorphology', 'Early Diagnosis', 'Early treatment', 'Elements', 'Emotional', 'Ethanol', 'Ethnic group', 'Face', 'Fetal Alcohol Exposure', 'Fetal Alcohol Spectrum Disorder', 'Fetal Alcohol Syndrome', 'Frequencies', 'Future', 'Genetic Polymorphism', 'Goals', 'Human', 'Image', 'Image Analysis', 'Indiana', 'Individual', 'Infant', 'Informal Social Control', 'Informatics', 'Information Technology', 'Interdisciplinary Study', 'Internet', 'Intervention', 'Knowledge', 'Label', 'Language', 'Language Development', 'Life', 'Location', 'Los Angeles', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methods', 'Mission', 'Modeling', 'Moscow', 'Mothers', 'Mus', 'Names', 'Neonatal', 'Neural Cell Adhesion Molecule L1', 'Neurobiology', 'New Mexico', 'Outcome', 'Participant', 'Pattern', 'Performance', 'Phenotype', 'Pilot Projects', 'Play', 'Population', 'Pregnancy', 'Rattus', 'Recording of previous events', 'Reporting', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Schools', 'Scientist', 'Secure', 'Sensitivity and Specificity', 'Sheep', 'Signal Pathway', 'Signal Transduction', 'Site', 'Software Tools', 'Specificity', 'Speed', 'Structure', 'Sum', 'Supercomputing', 'Supplementation', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Three-Dimensional Image', 'Time', 'Toddler', 'Ukraine', 'Ultrasonography', 'Universities', 'Woman', 'Work', 'alcohol exposure', 'alcohol sensitivity', 'analog', 'brain behavior', 'brain morphology', 'cognitive control', 'computing resources', 'critical period', 'data exchange', 'data integration', 'data management', 'data sharing', 'distributed data', 'drinking', 'effective intervention', 'experience', 'fetal', 'flexibility', 'follow-up', 'imaging modality', 'improved', 'in utero', 'infancy', 'literacy', 'meetings', 'mouse model', 'multidisciplinary', 'neurobehavioral', 'northern plains', 'novel', 'offspring', 'prenatal', 'prospective', 'quality assurance', 'reconstruction', 'repository', 'social', 'supercomputer', 'three dimensional structure', 'willingness']",NIAAA,INDIANA UNIVERSITY BLOOMINGTON,U24,2009,185188,-0.06898257102120521
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7692401,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2009,135000,0.005635261875240009
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7928674,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2009,55179,0.005635261875240009
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7558424,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Area', 'Belief', 'Bioinformatics', 'Biological Assay', 'Budgets', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Research', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Electrocardiogram', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Grant', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Length', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Peer Review', 'Phenotype', 'Physiological', 'Preparation', 'Protocols documentation', 'Public Health', 'Published Comment', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'base', 'bench to bedside', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'graphical user interface', 'interest', 'meetings', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly', 'working group']",NHLBI,WASHINGTON UNIVERSITY,R01,2009,488000,-0.009188704279595719
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7470967,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Class', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Numbers', 'Outcome', 'Performance', 'Polymerase Chain Reaction', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Standards of Weights and Measures', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2008,74521,-0.017659494165545724
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7498449,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2008,153203,-0.006018847025268777
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7468470,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2008,286582,-0.015439731637364328
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7364629,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physical Dialysis', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Range', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Standards of Weights and Measures', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'computer science', 'cost effectiveness', 'data management', 'desire', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'prescription document', 'prescription procedure', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2008,134890,-0.0008826377759913067
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7432910,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Eukaryotic Cell', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'desire', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,273906,-0.006775271348992179
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7433144,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Class', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Computers', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Numbers', 'Performance', 'Personal Satisfaction', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Public Health', 'Randomized', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Today', 'Training', 'Voting', 'Work', 'base', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R44,2008,25548,0.008658932021163265
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7468497,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,268274,-0.011564507240101328
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7504002,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2008,176002,-0.015525943479884816
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7596501,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Chromosome Pairing', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Numbers', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Purpose', 'Rate', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'concept', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,319129,-0.013787275848051084
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7367958,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2008,2037396,0.0003808877120557354
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7473948,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Condition', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Range', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2008,122190,-0.030524897703237935
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7489320,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2008,52898,-0.0081654752817338
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,7371085,K12MH069281,"['Anatomy', 'Area', 'Artificial Intelligence', 'Base of the Brain', 'Biomedical Engineering', 'Biomedical Informatics Research Network', 'Brain', 'Brain imaging', 'Caliber', 'Class', 'Clinical', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Development', 'Discipline', 'Educational Activities', 'Exposure to', 'Foundations', 'Functional disorder', 'Future', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Laboratories', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Monitor', 'Neurosciences', 'Numbers', 'Operative Surgical Procedures', 'Physicians', 'Procedures', 'Program Development', 'Psyche structure', 'Range', 'Rate', 'Recruitment Activity', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'SECTM1 gene', 'Science', 'Scientist', 'Surface', 'Techniques', 'Technology', 'Training', 'Training Programs', 'Woman', 'base', 'bioimaging', 'biomedical informatics', 'brain morphology', 'career', 'cationic antimicrobial protein CAP 37', 'clinical application', 'cognitive neuroscience', 'computer science', 'imaging informatics', 'multidisciplinary', 'nervous system disorder', 'neuroimaging', 'neuroinformatics', 'post-doctoral training', 'programs', 'research and development', 'response', 'skills']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2008,462488,-0.01161063163428598
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7459910,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2008,118789,-0.023690698237579536
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7457647,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2008,437938,-0.007228588660677277
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7426246,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Depth', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Facility Construction Funding Category', 'Funding', 'GDF15 gene', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Numbers', 'Ontology', 'PLAB Protein', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,266213,-0.020400891839231425
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7535414,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Analysis', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Purpose', 'Records', 'Research', 'Research Methodology', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'Support of Research', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'desire', 'innovation', 'interest', 'prevent', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2008,99843,-0.015601179137337214
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7467204,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Personal Satisfaction', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Today', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'concept', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2008,280000,-0.012061652339260936
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neurosience and neurologicat disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual )atients,and to studies executed across large poplulations. The data will be taken from subjects across a Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7688808,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Class', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Future', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Link', 'Localized', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Property', 'Purpose', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Services', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Textiles', 'Thinking', 'Time', 'Tissues', 'Today', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,100000,-0.013976657121543121
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neurosience and neurologicat disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual )atients,and to studies executed across large poplulations. The data will be taken from subjects across a Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7688368,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Class', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Future', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Link', 'Localized', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Purpose', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Services', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Textiles', 'Thinking', 'Time', 'Tissues', 'Today', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,50000,-0.013976657121543121
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7748401,R44GM083965,"['Learning', 'Techniques', 'parallel computing']",NIGMS,INSILICOS,R44,2008,143361,0.008658932021163265
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7433931,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2008,322087,-0.004615189581098885
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.       n/a",Bioconductor: an open computing resource for genomics,7495201,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2008,805222,0.014727103202718717
"Causal Discovery Algorithms for Translational Research with High-Throughput Data Project Summary Causal Discovery Algorithms for Translational Research with High-Throughput Data The long-term goal of this project is to provide to the biomedical community next-generation causal algorithms to facilitate discovery of disease molecular pathways and causative as well as predictive biomarkers and molecular signatures from high-throughput data. Such knowledge and methods are necessary toward earlier and more accurate diagnosis and prognosis, personalized medicine, and rational drug design. If successful, the proposed research will have significant and wide methodological and practical implications spanning several areas of biomedicine with a primary focus and immediate benefits in high-throughput diagnostics and personalized medicine. It will provide significantly improved computational methods and deeper theoretical understanding related to producing molecular signatures and understanding mechanisms of disease and concomitant leads for new drugs. It will provide evidence about applicability of novel causal methods in other types of data. It will generate insights in specific pathways of lung cancer in humans. It will deepen our understanding and solutions to the Rashomon effect in ¿omics¿ data. The proposed research will also shed light on the operational value of the stability heuristic. Finally the research will engage the international research community to address open computational causal discovery problems relevant to high-throughput and other biomedical data. ¿ Aim 1. Evaluate and characterize several novel causal algorithms for biomarker selection, molecular signature creation and reverse network engineering using real, simulated, resimulated, and experimental datasets. Study generality of the methods by means of applicability to non-¿omics¿ datasets. ¿ Aim 2. Evaluate and characterize, novel and state of the art causal algorithms against state-of-the-art non-causal and quasi-causal algorithms. ¿ Aim 3. Systematically investigate the Rashomon effect as it applies to biomarker and signature multiplicity. ¿ Aim 4. Systematically investigate the utility of applying the stability heuristic for causal discovery. ¿ Aim 5. Derive novel biomarkers, pathways and hypotheses for lung cancer. ¿ Aim 6. Induce novel solutions through an international causal discovery competition. ¿ Aim 7. Disseminate findings. n/a",Causal Discovery Algorithms for Translational Research with High-Throughput Data,7643514,R56LM007948,"['AKT1 gene', 'AKT2 gene', 'AKT3 gene', 'Address', 'Affect', 'Algorithms', 'Area', 'Arts', 'Benchmarking', 'Bioinformatics', 'Biologic Characteristic', 'Biological Markers', 'Biology', 'Biometry', 'Book Chapters', 'Books', 'Cancer cell line', 'Causations', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Communities', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Set', 'Depth', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discipline', 'Disease', 'Drug Design', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Ensure', 'Epidermal Growth Factor Receptor', 'European', 'Evaluation', 'Event', 'Excision', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Hereditary Disease', 'Home environment', 'Human', 'Human Cell Line', 'Inferior', 'Information Retrieval', 'Institution', 'International', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Malignant neoplasm of lung', 'Marker Discovery', 'Medicine', 'Methods', 'Modality', 'Molecular', 'Molecular Profiling', 'Neighborhoods', 'Noise', 'Numbers', 'Online Systems', 'Outcome', 'Output', 'Paper', 'Pathway interactions', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteomics', 'Protocols documentation', 'Public Domains', 'Publishing', 'Quality Control', 'Random Allocation', 'Randomized', 'Rate', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Sample Size', 'Sampling', 'Schedule', 'Score', 'Services', 'Simulate', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Text', 'Thinking', 'Tissues', 'Translational Research', 'Variant', 'Work', 'base', 'c-erbB-1 Proto-Oncogenes', 'clinically relevant', 'computer based statistical methods', 'computer science', 'contextual factors', 'coping', 'data mining', 'design', 'drug development', 'heuristics', 'human data', 'human tissue', 'improved', 'innovation', 'insight', 'journal article', 'member', 'new technology', 'next generation', 'novel', 'novel diagnostics', 'outcome forecast', 'reconstruction', 'research study', 'software systems', 'symposium', 'theories', 'tool']",NLM,VANDERBILT UNIVERSITY,R56,2008,4434,-0.047098528953973456
"Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor    DESCRIPTION (provided by applicant): The Informatics Core is part of the Consortium for the ""Collaborative Initiative on Fetal Alcohol Spectrum Disorders"" (CIFASD). The theme of this collaborative initiative is a cross-cultural assessment of ""fetal alcohol spectrum disorder"" (FASD). The CIFASD will coordinate basic, behavioral, and clinical investigators in a multidisciplinary research project to better inform approaches aimed at developing effective intervention and treatment approaches for FASD. It will involve the input and contributions from basic researchers, behavioral scientists, and clinical investigators with the willingness to utilize novel and cutting-edge techniques so as not to simply replicate previous or ongoing work, but rather to try and move it forward in a rigorous fashion. The Informatics Core will develop and maintain the CIFASD Data Repository, which will be used to collect, maintain and distribute data generated by the various participants in the consortium. The Informatics Core will be responsible for working with the other consortium participants to define a Data Dictionary to be used in standardizing data collection, enabling the transfer of data to and from the CIFASD Data Repository, consulting on how to establish local data management systems, providing both software tools and consulting to consortium participants, and producing status reports about the progress of the various projects within the consortium. The Informatics Core draws on a wealth of resources, experience, and expertise at Indiana University in information technology infrastructure and data management, The CIFASD Data Repository will be developed on Indiana University's state-of-the-art central supercomputing facilities, taking advantage of Indiana University's strong commitment to institutional computational resources. Resources that will be used to implement the CIFASD Data Repository include multiple supercomputers, an array of readily available database and statistical software, and a high- speed, secure, robust data archiving system capable of storing duplicate copies in multiple physical locations separated by more than fifty miles. The Informatics Core will use these extraordinary computational resources to provide a single, highly secure location for consortium participants to obtain the cross-cultural data that will enable the CIFASD to meet its goals of developing novel techniques for intervention and treatment of FASD.           n/a",Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor,7502758,U24AA014818,"['Address', 'Adolescent', 'Affect', 'Africa', 'Alcohol consumption', 'Alcohol-Related Neurodevelopmental Disorder', 'Alcohols', 'Animal Model', 'Animals', 'Archives', 'Arts', 'Basic Science', 'Behavioral', 'Binding Sites', 'Biology', 'Brain', 'Brain imaging', 'Cells', 'Characteristics', 'Child', 'Choline', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Collaborations', 'Computer software', 'Condition', 'Consult', 'Data', 'Data Analyses', 'Data Collection', 'Data Storage and Retrieval', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Dictionary', 'Dietary Intervention', 'Drug Design', 'Dysmorphology', 'Early Diagnosis', 'Early Intervention', 'Elements', 'Emotional', 'Ethanol', 'Ethnic group', 'Face', 'Fetal Alcohol Exposure', 'Fetal Alcohol Spectrum Disorder', 'Fetal Alcohol Syndrome', 'Frequencies', 'Future', 'Genetic Polymorphism', 'Goals', 'Human', 'Image', 'Image Analysis', 'Indiana', 'Individual', 'Infant', 'Infant Development', 'Informal Social Control', 'Informatics', 'Information Storage', 'Information Technology', 'Interdisciplinary Study', 'Internet', 'Intervention', 'Knowledge', 'Label', 'Language', 'Language Development', 'Life', 'Location', 'Los Angeles', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methods', 'Mission', 'Modeling', 'Moscow', 'Mothers', 'Mus', 'Names', 'Neonatal', 'Neural Cell Adhesion Molecule L1', 'Neurobiology', 'New Mexico', 'Outcome', 'Participant', 'Pattern', 'Performance', 'Phenotype', 'Pilot Projects', 'Play', 'Population', 'Pregnancy', 'Principal Investigator', 'Range', 'Rattus', 'Recording of previous events', 'Reporting', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Schools', 'Scientist', 'Secure', 'Sensitivity and Specificity', 'Sheep', 'Signal Pathway', 'Signal Transduction', 'Site', 'Software Tools', 'Specificity', 'Speed', 'Structure', 'Sum', 'Supercomputing', 'Supplementation', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Time', 'Toddler', 'Ukraine', 'Ultrasonography', 'Universities', 'Woman', 'Work', 'alcohol exposure', 'alcohol sensitivity', 'analog', 'brain behavior', 'brain morphology', 'cognitive control', 'critical developmental period', 'data integration', 'data management', 'distributed data', 'drinking', 'experience', 'fetal', 'follow-up', 'improved', 'in utero', 'infancy', 'literacy', 'mouse model', 'multidisciplinary', 'neurobehavioral', 'northern plains', 'novel', 'prenatal', 'programs', 'prospective', 'quality assurance', 'reconstruction', 'repository', 'social', 'supercomputer', 'three dimensional structure', 'transmission process', 'willingness']",NIAAA,INDIANA UNIVERSITY BLOOMINGTON,U24,2008,186589,-0.06898257102120521
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7479786,U54EB005149,[' '],NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,3534631,-0.012678727794285442
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7458835,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,503603,-0.01765182568786898
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7688793,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,58299,-0.01765182568786898
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,-0.020355613066453327
"Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi    DESCRIPTION (provided by applicant): The study of complex biological systems increasingly depends on vast amounts of dynamic information from diverse sources. The scientific analysis of the parasite Trypanosoma cruzi (T.cruzi), the principal causative agent of human Chagas disease, is the driving biological application of this proposal. Approximately 18 million people, predominantly in Latin America, are infected with the T.cruzi parasite. As many as 40 percent of these are predicted eventually to suffer from Chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. Research on T. cruzi is therefore an important human disease related effort. It has reached a critical juncture with the quantities of experimental data being generated by labs around the world, due in large part to the publication of the T.cruzi genome in 2005. Although this research has the potential to improve human health significantly, the data being generated exist in independent heterogeneous databases with poor integration and accessibility. The scientific objectives of this research proposal are to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for T.cruzi. This is in collaboration with the National Center for Biomedical Ontologies (NCBO) and will leverage its resources to achieve the objectives of this proposal as well as effectively to disseminate results to the broader life science community, including researchers in human pathogens. The PSE allows the dynamic integration of local and public data to answer biological questions at multiple levels of granularity. The PSE will utilize state-of- the-art semantic technologies for effective querying of multiple databases and, just as important, feature an intuitive and comprehensive set of interfaces for usability and easy adoption by biologists. Included in the multimodal datasets will be the genomic data and the associated bioinformatics predictions, functional information from metabolic pathways, experimental data from mass spectrometry and microarray experiments, and textual information from Pubmed. Researchers will be able to use and contribute to a rigorously curated T.cruzi knowledge base that will make it reusable and extensible. The resources developed as part of this proposal will be also useful to researchers in T.cruzi related kinetoplastids, Trypanosoma brucei and Leishmania major (among other pathogenic organisms), which use similar research protocols and face similar informatics challenges. PUBLIC HEALTH RELEVANCE: The scientific objective of this proposal is to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for Trypanosoma cruzi, a parasite that infects approximately 18 million people, predominantly in Latin America. As many as 40 percent of those infected are predicted to eventually suffer from Chagas disease, the leading cause of heart disease and sudden death in middle-aged adults in the region. Facilitating T.cruzi research through the PSE, with the aim of identifying vaccine, diagnostic, and therapeutic targets, is an important human disease related endeavor.          n/a",Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi,7428761,R01HL087795,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Animal Model', 'Anti-Retroviral Agents', 'Architecture', 'Archives', 'Area', 'Arts', 'Automobile Driving', 'Beds', 'Behavior', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Body of uterus', 'Buffaloes', 'California', 'Caring', 'Chagas Disease', 'Childhood', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Computers', 'Controlled Vocabulary', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Doctor of Public Health', 'Drops', 'Drosophila genus', 'Educational Activities', 'Educational workshop', 'Electronics', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evolution', 'Face', 'Feedback', 'Foundations', 'Future', 'Gene Mutation', 'Generations', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Geographic Locations', 'Goals', 'HIV', 'HIV Infections', 'Health', 'Heart Diseases', 'Homologous Gene', 'Human', 'Human Resources', 'Imagery', 'Immunologic Deficiency Syndromes', 'Immunology', 'Individual', 'Infection', 'Informatics', 'Information Management', 'Information Resources', 'Information Services', 'Information Technology', 'International', 'Internet', 'Interruption', 'Knowledge', 'Laboratories', 'Laboratory Organism', 'Language', 'Latin America', 'Lead', 'Learning', 'Leishmania major', 'Libraries', 'Link', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Medical Informatics', 'Medicine', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Mind', 'Mining', 'Modeling', 'Mutation', 'Natural Language Processing', 'Nature', 'Online Mendelian Inheritance In Man', 'Online Systems', 'Ontology', 'Operative Surgical Procedures', 'Oregon', 'Organism', 'Orthologous Gene', 'Outcome', 'Parasites', 'Pathogenesis', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Philosophy', 'Physiology', 'Prevention strategy', 'Principal Investigator', 'Problem Solving', 'Process', 'Proteomics', 'Protocols documentation', 'PubMed', 'Public Health', 'Publications', 'Publishing', 'Purpose', 'Randomized Clinical Trials', 'Range', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Resources', 'San Francisco', 'Science', 'Scientist', 'Semantics', 'Services', 'Site', 'Software Tools', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Structure', 'Study models', 'Sudden Death', 'Sum', 'System', 'TAF8 gene', 'Talents', 'Techniques', 'Technology', 'Terminology', 'Testing', 'Thinking', 'Training', 'Treatment Protocols', 'Trypanosoma brucei brucei', 'Trypanosoma cruzi', 'USA Georgia', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Vaccines', 'Vertical Disease Transmission', 'Victoria Austrailia', 'Virtual Library', 'Virus', 'Western Asia Georgia', 'Work', 'Zebrafish', 'abstracting', 'base', 'biocomputing', 'biomedical scientist', 'college', 'computer based Semantic Analysis', 'computer science', 'concept', 'data integration', 'design', 'desire', 'fundamental research', 'human disease', 'improved', 'indexing', 'innovative technologies', 'knowledge base', 'member', 'metabolomics', 'middle age', 'novel', 'novel strategies', 'open source', 'outreach', 'pandemic disease', 'pathogen', 'prevent', 'programs', 'protein protein interaction', 'repository', 'research and development', 'research study', 'syntax', 'theories', 'therapeutic target', 'tool', 'usability']",NHLBI,WRIGHT STATE UNIVERSITY,R01,2008,393930,-0.022308288685093394
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7299922,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2007,181125,-0.006018847025268777
"Technology Development for a MolBio Knowledge-Base Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges adsing from the proliferation of high- throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent iadvances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB. n/a",Technology Development for a MolBio Knowledge-Base,7473405,R01LM008111,"['Address', 'Area', 'Biology', 'Class', 'Clinical', 'Collection', 'Data', 'Databases', 'Evaluation', 'Facility Construction Funding Category', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Human', 'Indium', 'Informatics', 'Information Resources', 'Information Resources Management', 'Investments', 'Knowledge', 'Knowledge Base (Computer)', 'Laboratories', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Metabolism', 'Methods', 'Metric System', 'Molecular', 'Molecular Biology', 'Mus', 'Names', 'Natural Language Processing', 'Numbers', 'Ontology', 'Persons', 'Pharmaceutical Preparations', 'Plug-in', 'Proteins', 'Purpose', 'Semantics', 'Source', 'SwissProt', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'base', 'concept', 'data integration', 'gene function', 'heuristics', 'instrumentation', 'knowledge base', 'success', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,234058,-0.0004130908623428818
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7242352,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH AND SCI UNIVERSITY,R01,2007,292133,-0.015439731637364328
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7209599,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physical Dialysis', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Range', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Standards of Weights and Measures', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'computer science', 'cost effectiveness', 'data management', 'desire', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'prescription document', 'prescription procedure', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2007,112534,-0.0008826377759913067
"A RuleFit Product for Classification and Regression Prediction and data exploration are important aspects of modern commercial and scientific life. Regression methods predict dependent variables (e.g., tumor growth, severity of disease), while classification methods predict class membership (e.g., tumor or disease type). Both use a vector of independent variables to make the predictions. Because they are often superior predictors, can handle large numbers observations and large numbers of variables, can often yield insight into the data not provided by other methods, and because they can adapt to arbitrarily complex relationships, modern machine learning methods based on tree ensembles such as RANDOM FORESTS and MART have become leading modern analytical methods. Here we propose to commercially implement RULEFIT, a recent innovative method extending the RANDOM FORESTS and MART approaches, that shows strong evidence of being consistently more accurate than either ensemble. RULEFIT also includes groundbreaking new methods for variable selection in the face of huge numbers of predictors, and for identifying interactions, and ranking their importance. Optionally, RULEFIT extracts ""rules"" of special interest: succinct statements of conditions under which an outcome is especially likely or unlikely, or especially large or small. The primary output of RULEFIT is a numeric value reecting a prediction of the value of the dependent variable or the probability of a class membership. RULEFIT is likely to become a leading technique in the machine learning and statistics. It builds on RANDOM FORESTS and MART and includes all their useful benefits such as variable selection, data exploration, data reduction, outlier detection, and missing value imputation, while enhancing and extending these benefits.  COMMERCIAL POTENTIAL The market for advanced analytical tools has been growing strongly over the last decade and the growth shows no signs of diminishing. Modelers and data analysts in both university- based and commercial settings are increasingly aware of the power and value of new analytical tools derived from modern statistics and machine learning research. The increased accuracy of the new methods and the acceleration they provide to the analysis of complex data are fueling demand for this new technology. The advances embedded in the proposed product represent substantial improvements to existing technology and include methods to solve vexing problems in contemporary data analysis, and thus should find a welcoming market.  There are further reasons to forecast robust commercial potential for this product. The applicant organization has a strong track record in the industry and is widely recognized as a developer of high quality software. We have been working with consultant Friedman since 1990 and have gained exclusive rights to the proprietary sourcecode for a number of his innovations. These include CART, MARS, MART and PRIM. With the addition of RULEFIT and its associated sub-components, these products represent a unique collection of pedigreed tools. We have also forged a similar relationship with the (late) Leo Breiman and have the exclusive rights to commercialization of Breiman's Random Forests sourcecode. Our proposed package thus occupies a distinctive position in machine learning software which cannot be replicated by other vendors. Keywords: machine learning; classi?cation; prediction; supervised learning; variable importance; inter- action detection; Justi?cation Dr. Steinberg has extensive experience in software development for advanced statistical and machine learning methods, particularly in the area of classi?cation and regression trees, sur- vival analysis, adaptive modeling, RANDOM FORESTS and MART. He will oversee all aspects of the project. He will will work with Dr. Cardell, Professor Friedman, Mr. Colla, and with the Salford Systems software development engineer in creating and studying the software and methods used in this proposal. He will also be responsible for the architecture of the Phase I software. Professor Friedman and Dr. Cardell will provide technical support as follows: Dr. Fried- man is an expert on machine learning methods and is one of the developers of the RULEFIT technique. Regular consultation with him will be in this area. Dr. Cardell is an expert in asymptotic theory, and in the design of Monte Carlo and other tests for the evaluation of ma- chine learning algorithms. He also has extensive experience in machine learning, including adaptive modeling, neural networks, logistic regression, and classi?cation methods. He will review core algorithms of RULEFIT for possible improvement and extension and design the Monte Carlo tests. Mr. Colla has extensive experience in software development and with machine learning methods, including work on the commercial implementations of CART, MARS, RANDOM FORESTS, and MART. Working with Dr. Cardell, he will be responsible for much of the new software coding. 5 Project Description Page 7 Principal Investigator/Program Director (Last, first, middle): Steinberg, Dan Prediction models based upon classification and regression tree ensembles have become important in medical and other research. There are currently no commercial products available that implement the proposed RuleFit methodology. These methods have significant advantages over existing techniques, and will aid researchers in obtaining the best possible predictions.   n/a",A RuleFit Product for Classification and Regression,7268612,R43CA124294,"['Acceleration', 'Agreement', 'Algorithms', 'Architecture', 'Area', 'Beds', 'Build-it', 'Cations', 'Class', 'Classification', 'Code', 'Collection', 'Comparative Study', 'Complex', 'Computer software', 'Condition', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Detection', 'Disease', 'Disease regression', 'Engineering', 'Evaluation', 'Face', 'Generations', 'Growth', 'Industry', 'Information Systems', 'Investigation', 'Learning', 'Left', 'Life', 'Linear Models', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Neural Network Simulation', 'Numbers', 'Outcome', 'Output', 'Painless', 'Pattern', 'Performance', 'Phase', 'Plant Leaves', 'Play', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Rate', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Rights', 'Role', 'Sampling', 'Severity of illness', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Trees', 'Universities', 'Variant', 'Vendor', 'Work', 'analytical method', 'analytical tool', 'base', 'commercialization', 'data mining', 'data structure', 'design', 'evaluation/testing', 'experience', 'forest', 'forging', 'graphical user interface', 'innovation', 'insight', 'interest', 'loss of function', 'man', 'new technology', 'novel', 'professor', 'programs', 'prototype', 'relating to nervous system', 'research study', 'software development', 'statistics', 'theories', 'tool', 'tumor', 'tumor growth', 'vector']",NCI,SALFORD SYSTEMS,R43,2007,91700,-0.01952436693597752
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7299383,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2007,266852,-0.011564507240101328
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7301424,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2007,173307,-0.015525943479884816
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7246847,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2007,2183988,0.0003808877120557354
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7259194,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Condition', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Range', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2007,102389,-0.030524897703237935
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7275769,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2007,51278,-0.0081654752817338
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,0.006542819027175385
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,7189144,K12MH069281,"['Anatomy', 'Area', 'Artificial Intelligence', 'Base of the Brain', 'Biomedical Engineering', 'Biomedical Informatics Research Network', 'Brain', 'Brain imaging', 'Caliber', 'Class', 'Clinical', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Development', 'Discipline', 'Educational Activities', 'Exposure to', 'Foundations', 'Functional disorder', 'Future', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Laboratories', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Monitor', 'Neurosciences', 'Numbers', 'Operative Surgical Procedures', 'Physicians', 'Procedures', 'Program Development', 'Psyche structure', 'Range', 'Rate', 'Recruitment Activity', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'SECTM1 gene', 'Science', 'Scientist', 'Surface', 'Techniques', 'Technology', 'Training', 'Training Programs', 'Woman', 'base', 'bioimaging', 'biomedical informatics', 'brain morphology', 'career', 'cationic antimicrobial protein CAP 37', 'clinical application', 'cognitive neuroscience', 'computer science', 'imaging informatics', 'multidisciplinary', 'nervous system disorder', 'neuroimaging', 'neuroinformatics', 'post-doctoral training', 'programs', 'research and development', 'response', 'skills']",NIMH,MASSACHUSETTS GENERAL HOSP,K12,2007,450967,-0.01161063163428598
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7269383,R01RR014477,[' '],NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2007,307022,-0.031863620261843514
"Comparative Visualization and Analysis for GCxGC    DESCRIPTION (provided by applicant): Project Summary. This project will investigate and develop effective information technologies for comparative analysis and visualization of complex data generated by comprehensive two-dimensional gas chromatography (GCxGC). GCxGC is an emerging technology that provides an order-of-magnitude greater separation capacity, significantly better signal-to-noise ratio, and higher dimensional retention-structure relations than traditional GC. The principal challenge for utilization of GCxGC, in a wide range of public-health and other applications, is the difficulty of analyzing and interpreting the large, complex data it generates. The quantity and complexity of GCxGC data necessitates the investigation and development of new information technologies. This project will develop and demonstrate innovative methods and tools for comparative analysis of GCxGC datasets. The expected results of this research and development include a PCA-based method for chemical fingerprinting, decision trees with chemical constraints for sample classification, genetic programming for template and constraint-based matching and classification, and visualization methods for comparative GCxGC analyses. These methods will be implemented in commercial software that will support researchers and laboratory analysts in a wide range of commercial applications, including health care, environmental monitoring, and chemical processing. The power of GCxGC, supported by effective information technologies, will enable better understanding of chemical compositions and processes, a foundation for future scientific advances and discoveries. Relevance to Public Health. Today, a few advanced laboratories are pioneering GCxGC for a variety of applications such as environmental monitoring of exposure profiles in air, soil, food, and water; identification and quantification of toxic products in blood, urine, milk, and breath samples; and qualitative and quantitative metabolomics to provide a holistic view of the biochemical status or biochemical phenotype of an organism. Many analyses in these applications require detailed chemical comparisons of samples, e.g..monitoring changes, comparison to reference standards, chemical matching or ""fingerprinting"", and classification. GCxGC is a powerful new technology for such comparative analyses. This proposal will provide innovative information technologies to support users in these applications.           n/a",Comparative Visualization and Analysis for GCxGC,7270029,R44RR020256,"['Air', 'Archives', 'Biochemical', 'Blood', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Emerging Technologies', 'Environmental Monitoring', 'Fingerprint', 'Food', 'Foundations', 'Future', 'Gas Chromatography', 'Genetic Programming', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Information Technology', 'Investigation', 'Laboratories', 'Language', 'Machine Learning', 'Marketing', 'Methods', 'Milk', 'Monitor', 'Noise', 'Organism', 'Pattern', 'Phase', 'Phenotype', 'Principal Component Analysis', 'Process', 'Public Health', 'Range', 'Reference Standards', 'Reporting', 'Research Personnel', 'Sales', 'Sampling', 'Schedule', 'Scientific Advances and Accomplishments', 'Signal Transduction', 'Software Tools', 'Soil', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Today', 'Trademark', 'Urine', 'Water', 'base', 'chemical fingerprinting', 'commercial application', 'comparative', 'innovation', 'innovative technologies', 'instrument', 'metabolomics', 'new technology', 'research and development', 'tool', 'two-dimensional']",NCRR,"GC IMAGE, LLC",R44,2007,239373,-0.014239514716093431
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7252088,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2007,276104,-0.023690698237579536
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7287965,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2007,446875,-0.007228588660677277
Comparative and Web-Enabled Virtual Screening No abstract available n/a,Comparative and Web-Enabled Virtual Screening,7472716,P20HG003900,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'comparative', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project', 'virtual', 'web-enabled']",NHGRI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,P20,2007,363833,-0.022260397008372616
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7244058,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2007,328325,-0.004615189581098885
"Bayesian Methods and Experimental Design for Molecular Biology Experiments    DESCRIPTION (provided by applicant): The goal of this proposal is to provide a suite of software tools for bioinformatics and systems biology researchers who are using molecular biology (Omics) data to identify the best experimental design and to analyze the resulting experimental data using Bayesian tools. A common problem for most bioinformatics experiments is low power due to low replication. This problem can be alleviated economically when an increase in adoption and use of a specific platform leads to a decrease in associated costs, thereby enabling an increase in samples allocated per treatment. Yet, many bioinformatics experiments remain underpowered as researchers use the offsets of decreased costs to explore more complex questions. When designing an experiment, the allocation of samples to treatment regimens, and the choice of treatments to test, are traditionally the only variables to manipulate. Bayesian experimental design provides a framework to find the optimal design out of n possible designs subject to a utility function that can include such items as time and material costs.      Bayesian statistical methods have been gaining substantial favor in bioinformatics and systems biology as they provide a highly flexible framework for fitting and exploring complex models. Bayesian models also provides to domain experts such as biologists and physicians easily interpretable models through posterior probabilities which are more naturally understood than the traditional p-value. While a number of open source tools based on Bayesian models are available, most are applied best in the context of a specific research data analysis problem or model and are not integrated into a single, complete system for data analysis.      We propose to research and develop a statistical analysis software package S+OBAYES (for S-PLUS and R) with generalized tools for Bayesian design of experiments, empirical and fully Bayesian analysis, and modeling and simulation using modern commercial software development practices. These tools will provide functionality for finding the optimal choice and layout of experimental treatments for molecular biology experiments and for fitting Bayesian linear and non-linear models to a variety of data types including time series. We propose to validate the software in molecular biology research problems such as the detection of differential gene, protein, and metabolite abundance. The benefits of this work will be a commercial-quality software package with validated statistical methodology and interactive visualization tools that will appeal to molecular biologists and systems biology investigators. The results of the proposed work will expedite discoveries in basic science, early disease detection, and drug discovery and development.          n/a",Bayesian Methods and Experimental Design for Molecular Biology Experiments,7325828,R43GM083023,"['Address', 'Adoption', 'Algorithms', 'Animal Genetics', 'Arizona', 'Basic Science', 'Bayesian Analysis', 'Bayesian Method', 'Bioconductor', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biometry', 'Biotechnology', 'Cations', 'Chromosome Mapping', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Department of Defense', 'Depth', 'Detection', 'Development', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Employment', 'Ensure', 'Experimental Designs', 'Exposure to', 'Factor Analysis', 'Foundations', 'Funding', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Government', 'Government Agencies', 'Health', 'Imagery', 'Industry', 'Information Systems', 'Institution', 'Iowa', 'Libraries', 'Linear Models', 'Machine Learning', 'Manuals', 'Manuscripts', 'Maps', 'Marketing', 'Mass Spectrum Analysis', 'Measures', 'Medical Informatics', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Biology', 'Non-linear Models', 'Numbers', 'Pathway interactions', 'Phase', 'Physicians', 'Population Study', 'Principal Investigator', 'Probability', 'Property', 'Proteome', 'Proteomics', 'Proxy', 'Quantitative Trait Loci', 'Research', 'Research Personnel', 'Rice', 'Risk Factors', 'SNP genotyping', 'Sampling', 'Science', 'Scientist', 'Series', 'Services', 'Simulate', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Software Tools', 'Software Validation', 'Solutions', 'Speed', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Systems Biology', 'Techniques', 'Telecommunications', 'Testing', 'Time', 'Time Series Analysis', 'Training', 'Treatment Protocols', 'Universities', 'Validation', 'Washington', 'Wisconsin', 'Work', 'animal breeding', 'base', 'cost', 'design', 'drug discovery', 'experience', 'human subject', 'improved', 'interest', 'lecturer', 'models and simulation', 'open source', 'professor', 'programs', 'protein metabolite', 'research and development', 'research study', 'skills', 'software development', 'statistics', 'success', 'theories', 'tool', 'treatment effect']",NIGMS,INSIGHTFUL CORPORATION,R43,2007,103995,-0.013491505155858598
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methodswithin the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data,  earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user  nterface to support access to information resources for enhanced decision-making by practitioners. The ong-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3  Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technologywork; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C). n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7284424,P01HK000027,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2007,1889501,-0.0018071886131799015
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7214118,R42ES013321,"['Accounting', 'Animals', 'Architecture', 'Biological Assay', 'Biological Neural Networks', 'Chemicals', 'Clinical', 'Clinical Trials', 'Computer Simulation', 'Computer software', 'Contracts', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Drug Formulations', 'Drug Industry', 'Drug toxicity', 'End Point', 'Expert Systems', 'Funding', 'Future', 'Fuzzy Logic', 'Gene Expression', 'Guidelines', 'Health Care Costs', 'Hepatotoxicity', 'Investments', 'Learning', 'Liver', 'Marketing', 'Methods', 'Network-based', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Proteomics', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Rate', 'Relative (related person)', 'Reliance', 'Research', 'Research Personnel', 'Screening procedure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Training', 'Validation', 'base', 'computational chemistry', 'cost', 'data acquisition', 'design', 'highly advanced system', 'improved', 'innovation', 'knowledge base', 'metabolomics', 'quantum', 'serial analysis of gene expression', 'subtraction hybridization', 'tool']",NIEHS,"YAHSGS, LLC",R42,2007,257269,-0.013912900178092769
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7475421,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,160000,0.02862268565147817
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7248464,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,693808,0.02862268565147817
"Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor    DESCRIPTION (provided by applicant): The Informatics Core is part of the Consortium for the ""Collaborative Initiative on Fetal Alcohol Spectrum Disorders"" (CIFASD). The theme of this collaborative initiative is a cross-cultural assessment of ""fetal alcohol spectrum disorder"" (FASD). The CIFASD will coordinate basic, behavioral, and clinical investigators in a multidisciplinary research project to better inform approaches aimed at developing effective intervention and treatment approaches for FASD. It will involve the input and contributions from basic researchers, behavioral scientists, and clinical investigators with the willingness to utilize novel and cutting-edge techniques so as not to simply replicate previous or ongoing work, but rather to try and move it forward in a rigorous fashion. The Informatics Core will develop and maintain the CIFASD Data Repository, which will be used to collect, maintain and distribute data generated by the various participants in the consortium. The Informatics Core will be responsible for working with the other consortium participants to define a Data Dictionary to be used in standardizing data collection, enabling the transfer of data to and from the CIFASD Data Repository, consulting on how to establish local data management systems, providing both software tools and consulting to consortium participants, and producing status reports about the progress of the various projects within the consortium. The Informatics Core draws on a wealth of resources, experience, and expertise at Indiana University in information technology infrastructure and data management, The CIFASD Data Repository will be developed on Indiana University's state-of-the-art central supercomputing facilities, taking advantage of Indiana University's strong commitment to institutional computational resources. Resources that will be used to implement the CIFASD Data Repository include multiple supercomputers, an array of readily available database and statistical software, and a high- speed, secure, robust data archiving system capable of storing duplicate copies in multiple physical locations separated by more than fifty miles. The Informatics Core will use these extraordinary computational resources to provide a single, highly secure location for consortium participants to obtain the cross-cultural data that will enable the CIFASD to meet its goals of developing novel techniques for intervention and treatment of FASD.           n/a",Informatics Core for the Collaborative Initiative on Fetal Alcohol Spectrum disor,7343355,U24AA014818,"['Address', 'Adolescent', 'Affect', 'Africa', 'Alcohol consumption', 'Alcohol-Related Neurodevelopmental Disorder', 'Alcohols', 'Animal Model', 'Animals', 'Archives', 'Arts', 'Basic Science', 'Behavioral', 'Binding Sites', 'Biology', 'Brain', 'Brain imaging', 'Cells', 'Characteristics', 'Child', 'Choline', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Collaborations', 'Computer software', 'Condition', 'Consult', 'Data', 'Data Analyses', 'Data Collection', 'Data Storage and Retrieval', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Dictionary', 'Dietary Intervention', 'Drug Design', 'Dysmorphology', 'Early Diagnosis', 'Early Intervention', 'Elements', 'Emotional', 'Ethanol', 'Ethnic group', 'Face', 'Fetal Alcohol Exposure', 'Fetal Alcohol Spectrum Disorder', 'Fetal Alcohol Syndrome', 'Frequencies', 'Future', 'Genetic Polymorphism', 'Goals', 'Human', 'Image', 'Image Analysis', 'Indiana', 'Individual', 'Infant', 'Infant Development', 'Informal Social Control', 'Informatics', 'Information Storage', 'Information Technology', 'Interdisciplinary Study', 'Internet', 'Intervention', 'Knowledge', 'Label', 'Language', 'Language Development', 'Life', 'Location', 'Los Angeles', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methods', 'Mission', 'Modeling', 'Moscow', 'Mothers', 'Mus', 'Names', 'Neonatal', 'Neural Cell Adhesion Molecule L1', 'Neurobiology', 'New Mexico', 'Outcome', 'Participant', 'Pattern', 'Performance', 'Phenotype', 'Pilot Projects', 'Play', 'Population', 'Pregnancy', 'Principal Investigator', 'Range', 'Rattus', 'Recording of previous events', 'Reporting', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Schools', 'Scientist', 'Secure', 'Sensitivity and Specificity', 'Sheep', 'Signal Pathway', 'Signal Transduction', 'Site', 'Software Tools', 'Specificity', 'Speed', 'Structure', 'Sum', 'Supercomputing', 'Supplementation', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Time', 'Toddler', 'Ukraine', 'Ultrasonography', 'Universities', 'Woman', 'Work', 'alcohol exposure', 'alcohol sensitivity', 'analog', 'brain behavior', 'brain morphology', 'cognitive control', 'critical developmental period', 'data integration', 'data management', 'distributed data', 'drinking', 'experience', 'fetal', 'follow-up', 'improved', 'in utero', 'infancy', 'literacy', 'mouse model', 'multidisciplinary', 'neurobehavioral', 'northern plains', 'novel', 'prenatal', 'programs', 'prospective', 'quality assurance', 'reconstruction', 'repository', 'social', 'supercomputer', 'three dimensional structure', 'transmission process', 'willingness']",NIAAA,INDIANA UNIVERSITY BLOOMINGTON,U24,2007,178873,-0.06898257102120521
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7494181,U54EB005149,"['Address', 'Affect', 'Alcohols', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Appearance', 'Area', 'Atlases', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Cells', 'Characteristics', 'Clinical', 'Clinical Data', 'Cognitive', 'Collaborations', 'Collection', 'Commit', 'Complex', 'Computational algorithm', 'Computer software', 'Computer-Assisted Image Analysis', 'Computing Methodologies', 'Coupling', 'Data', 'Data Correlations', 'Data Sources', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Elements', 'Ensure', 'Epilepsy', 'Feedback', 'Fiber', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Image-Guided Surgery', 'Imagery', 'Imaging Techniques', 'Individual', 'Knowledge', 'Life', 'Link', 'Localized', 'Location', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Medical', 'Medical Imaging', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modification', 'Morphology', 'Multiple Sclerosis', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Organ', 'Other Imaging Modalities', 'Output', 'Participant', 'Patients', 'Pattern', 'Performance', 'Physiological', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Range', 'Recording of previous events', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Schizophrenia', 'Science', 'Services', 'Shapes', 'Software Engineering', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Distributions', 'Structure', 'Syndrome', 'System', 'Systems Analysis', 'Techniques', 'Testing', 'Textiles', 'Time', 'Tissues', 'Today', 'USA Georgia', 'Utah', 'Visible Radiation', 'Vision', 'Western Asia Georgia', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'design', 'disability', 'experience', 'image registration', 'insight', 'interest', 'mathematical model', 'neuroimaging', 'novel', 'prenatal', 'research study', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion', 'white matter']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2007,87500,-0.012678727794285442
"National Alliance for Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance for Medical Imaging Computing (NAMIC)(RMI),7271955,U54EB005149,[' '],NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2007,3888915,-0.012678727794285442
"Accelerating metabolic discovery using characterization data    DESCRIPTION (provided by applicant): The long term goal of this project is to develop methods that will allow researchers to gain insight into the metabolic networks of organisms for which we have little or no high-throughput data. Such metabolic networks can reveal aspects of the organism's metabolism that might make it vulnerable to new or existing therapies. A core data set using genomic and other omic data from data-rich bacteria that are related to the organisms of interest will be assembled. The statistical tools needed to integrate these data and to infer metabolic networks using these core data plus characterization (phenotypic) data will then be built. Using the statistical inference algorithms, the characterization data can be leveraged to reveal the metabolic networks of data-poor bacteria for which we have only characterization data. This approach can eliminate the need for genome sequencing, gene expression experiments and the like for thousands of Gram-negative facultative rod bacteria (GNF). There are five tasks in the project: (1) assemble the data sets from data-rich organisms that will be used to inform the inference algorithm. These data include (a) the genomic sequences and annotation information, (b) extant pathway data and (c) gene expression data. All these data contain some level of information about the connectivity within the metabolic network; (2) process the genomic data to enhance its predictive value; (3) develop a data integration algorithm; (4) investigate modeling frameworks to be used for Bayesian data fusion and network inference; (5) validate the metabolic networks. Deliverables from this project should include: (1) a set of pathway genome databases for 35 GNF, This group includes 20 strains classified as category A or B biothreat agents, (2) a core dataset that integrates all the information we have relevant to the metabolic pathways in the 35 sequenced GNF, (3) a probabilistic graphical modeling framework capable of integrating disparate types of data and inferring networks from the integrated data, (4) a method for using characterization data, along with deliverables 2 and 3, to infer metabolic networks for bacterial strains for which we have only characterization data. The ability to rapidly construct models of metabolic networks means researchers will be able to respond to emerging infectious agents or biothreats more quickly. Relevance The methods developed as part of this proposal will allow us to quickly make metabolic maps for thousands of bacteria. Such maps can guide researchers to promising new targets for therapeutic or preventative measures against pathogenic bacteria. The fight against well-known pathogens and biothreat agents, as well as against new, emerging pathogens will be greatly aided by these tools.              n/a",Accelerating metabolic discovery using characterization data,7267998,R21AI067543,"['Adopted', 'Algorithms', 'American Type Culture Collection', 'Artificial Intelligence', 'Bacteria', 'Bacteriology', 'Biochemical', 'Biochemical Pathway', 'Biological Models', 'Biology', 'Bypass', 'Categories', 'Cholera', 'Code', 'Data', 'Data Collection', 'Data Set', 'Depth', 'Disease', 'Electronics', 'Facility Construction Funding Category', 'Gammaproteobacteria', 'Gene Expression', 'Genomics', 'Goals', 'Gram&apos', 's stain', 'Infectious Agent', 'Information Networks', 'Manuals', 'Maps', 'Measures', 'Meta-Analysis', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Nosocomial Infections', 'Numbers', 'Nutritional', 'Organism', 'Outcome', 'Oxygen', 'Pathway interactions', 'Plague', 'Predictive Value', 'Process', 'Prophylactic treatment', 'Proteomics', 'Research Personnel', 'Salmonella typhi', 'Shapes', 'Shigella', 'Shigella Infections', 'Signal Transduction', 'Source', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Typhoid Fever', 'Variant', 'Vibrio cholerae', 'Work', 'Writing', 'Yersinia pestis', 'biothreat', 'computerized data processing', 'data integration', 'design', 'falls', 'fight against', 'genome database', 'genome sequencing', 'gram negative facultative rods', 'innovation', 'insight', 'interest', 'network models', 'novel', 'pathogen', 'pathogenic bacteria', 'programs', 'research study', 'retinal rods', 'routine Bacterial stain', 'sound', 'success', 'therapeutic target', 'tool', 'transcriptomics']",NIAID,AMERICAN TYPE CULTURE COLLECTION,R21,2007,185677,-0.024382817220377453
"Dopaminergic Mechanisms of Reward in Schizophrenia    DESCRIPTION (provided by applicant): This revised proposal has been developed in response to PAR-02-062 (Building Translational Research in Behavioral Science). The goal of the proposed work is to demonstrate that an explicitly translational, multidisciplinary approach to defining the role of dopamine (DA) in schizophrenia (SC), is capable of illuminating some of the most poorly understood, most treatment-resistant cognitive and motivational aspects of the illness. Specifically, recent basic research has shown that DA cell firing plays a critical role in the encoding of reward prediction and reward based learning. The proposed work will provide a rigorous test of the applicability of this hypothesis as a framework for understanding the motivational and learning impairments of SC with important implications for schizophrenia therapeutics. To test this hypothesis, we have organized a group of basic and clinical investigators into three scientific modules:1) electrophysiology, 2) neuroimaging, and 3) behavior, with the overall program designed to address different aspects of same theory of DA function. These modules are designed to develop collaborative partnerships with the results of initial pilot experiments used to refine hypothesis for further experimental testing. Initial studies are proposed to document the specific behavioral, electrophysiological, and fMRI Bold signal abnormalities of SC patients during the performance of reward learning paradigms, the impact of different antipsychotics on reward processing in animals, and the physiological sequela associated with reward-driven changes in DA cell firing. Computational modeling will be used to determine whether the experimental results in animals and observed deficits in patients are consistent with the hypothesized role of DA in reward processing. The overall program is designed to develop resources, paradigms, and proof of principle studies that illuminate the nature of DA dysfunction in SC through the application of translational paradigms and models demonstrating the critical role of DA in reward and reinforcement learning. If successful, the proposed work is designed to establish the conceptual foundation and preliminary data needed to support individual RO1 applications and a Translational Center.         n/a",Dopaminergic Mechanisms of Reward in Schizophrenia,7278791,R24MH072647,"['Acute', 'Address', 'Algorithms', 'Animal Experiments', 'Animals', 'Anterior', 'Antipsychotic Agents', 'Appendix', 'Area', 'Artificial Intelligence', 'Basic Science', 'Behavior', 'Behavior Control', 'Behavioral', 'Behavioral Assay', 'Behavioral Paradigm', 'Behavioral Sciences', 'Brain', 'Cells', 'Clinical', 'Clinical Investigator', 'Code', 'Cognitive', 'Computer Simulation', 'Condition', 'Cues', 'Data', 'Depth', 'Dopamine', 'Dose', 'Electric Stimulation', 'Electrophysiology (science)', 'Environment', 'Event', 'Evoked Potentials', 'Feedback', 'Fire - disasters', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Generations', 'Goals', 'Gold', 'Grant', 'Habenula', 'Haloperidol', 'Human', 'Impairment', 'Individual', 'Lateral', 'Lead', 'Learning', 'Literature', 'Methodology', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Outcome Study', 'Patients', 'Pattern', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Pilot Projects', 'Play', 'Procedures', 'Process', 'Psychological reinforcement', 'Psychopathology', 'Rattus', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Rewards', 'Rodent', 'Role', 'Schizophrenia', 'Scientist', 'Series', 'Signal Transduction', 'Simulate', 'Stimulus', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Thinking', 'Translational Research', 'Work', 'base', 'behavioral pharmacology', 'blood oxygenation level dependent response', 'cingulate cortex', 'classical conditioning', 'clinical application', 'cognitive neuroscience', 'design', 'experience', 'hedonic', 'heuristics', 'human study', 'insight', 'interdisciplinary approach', 'neural circuit', 'neuroimaging', 'novel', 'olanzapine', 'programs', 'quetiapine', 'research study', 'response', 'reward circuitry', 'reward processing', 'theories', 'translational approach', 'volunteer']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2007,342842,-0.02227012176772211
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7284239,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2007,588968,-0.01765182568786898
"AMAUTA HEALTH INFORMATICS RESEARCH AND TRAINING PROGRAM DESCRIPTION (provided by applicant):     The proposed training program in Informatics for Global Health will consist of several activities carried out over a five year period 1) two short courses for 30-50 participants in basic informatics to be held in Peru 2) short term skills based training in informatics in Seattle with emphasis on genomics and resource access skills building 3) long term training in Informatics at the post doctoral or Masters of Science level through the Biomedical and Health Informatics program at the School of Medicine. Six scholars will be trained in addition to the short course participants. Four scholars now engaged in research will receive focused, skills building technical training and six scholars will be recruited and enrolled for longer term (2-3 year) post doctoral or Masters of Science training at the University of Washington. It is envisioned that this training program will foster the development of the capacity of UPCH to continue its role as a leading biomedical research institution and also to create a home to health informatics research activity in Peru. n/a",AMAUTA HEALTH INFORMATICS RESEARCH AND TRAINING PROGRAM,7249492,D43TW007551,"['1 year old', 'AIDS prevention', 'AIDS/HIV problem', 'Academic Detailing', 'Academic Medical Centers', 'Achievement', 'Acquired Immunodeficiency Syndrome', 'Acyclovir', 'Address', 'Admission activity', 'Adopted', 'Adult', 'Advisory Committees', 'Aeromonas', 'Affect', 'Age', 'Age Reporting', 'Agreement', 'AlamarBlue', 'Algorithms', 'Alleles', 'Am 80', 'Amauta', 'American', 'Americas', 'Amino Acid Sequence', 'Aminoglycosides', 'Anal Sex', 'Anogenital venereal warts', 'Anti-Retroviral Agents', 'Antibiotic susceptibility', 'Antigens', 'Antimicrobial susceptibility', 'Antitubercular Agents', 'Appendix', 'Appointment', 'Area', 'Artificial Intelligence', 'Arts', 'Asia', 'Aspirate substance', 'Australia', 'Automation', 'Award', 'Bacillus (bacterium)', 'Back', 'Bacteria', 'Bacterial Infections', 'Bacterial Vaginosis', 'Baseline Surveys', 'Basic Science', 'Behavior', 'Behavior Control', 'Behavior Therapy', 'Behavioral Research', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Engineering', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Birth', 'Bisexual', 'Blinded', 'Blood Tests', 'Bolivia', 'Books', 'Businesses', 'CD4 Lymphocyte Count', 'Calmette-Guerin Bacillus', 'Campylobacter', 'Canada', 'Candida', 'Caring', 'Case-Control Studies', 'Cataloging', 'Catalogs', 'Categories', 'Cause of Death', 'Cell physiology', 'Cells', 'Centers for Disease Control and Prevention (U.S.)', 'Certification', 'Cervical', 'Characteristics', 'Chest', 'Child', 'Child health care', 'Childhood', 'Chlamydia', 'Chlamydia Infections', 'Chromosome Mapping', 'Chronic', 'Ciprofloxacin', 'Cities', 'Class', 'Classification', 'Client', 'Climate', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Clinical trial protocol document', 'Code', 'Cohort Studies', 'Collaborations', 'Collection', 'Color', 'Commit', 'Committee Membership', 'Communicable Diseases', 'Communication', 'Communities', 'Community Medicine', 'Competence', 'Complex', 'Computer Assisted', 'Computer software', 'Computerized Medical Record', 'Computers', 'Congenital Syphilis', 'Consultations', 'Contact Tracing', 'Costa Rica', 'Coughing', 'Counseling', 'Country', 'County', 'Coupled', 'Critiques', 'Cross-Sectional Studies', 'Cryptosporidium', 'DNA', 'DNA Microarray Chip', 'DNA Microarray format', 'DNA amplification', 'Daily', 'Data', 'Data Collection', 'Data Sources', 'Databases', 'Decision Analysis', 'Decision Making', 'Decision Support Systems', 'Dental Schools', 'Depth', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Diagnostic tests', 'Diarrhea', 'Dimensions', 'Discipline', 'Discipline of Nursing', 'Disease', 'Disease remission', 'Distance Education', 'Distance Learning', 'Doctor of Philosophy', 'Dose', 'Drug Formulations', 'Drug usage', 'Early Diagnosis', 'Ecology', 'Economics', 'Education', 'Educational Curriculum', 'Educational Intervention', 'Educational Status', 'Educational process of instructing', 'Effectiveness', 'Electronic Mail', 'Electronics', 'Elements', 'Emergency Situation', 'Engineering', 'English Language', 'Enrollment', 'Ensure', 'Environment', 'Environmental Health', 'Epidemiologist', 'Epidemiology', 'Ethambutol', 'Ethics', 'Etiology', 'Evaluation', 'Event', 'Evolution', 'Expert Systems', 'Exposure to', 'Face', 'Faculty', 'Familiarity', 'Family', 'Feedback', 'Fellowship', 'Fellowship Program', 'Female', 'Female Condoms', 'Fertility Rates', 'Fever', 'Figs - dietary', 'Financial Support', 'Floor', 'Focus Groups', 'Fostering', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Future', 'Gene Conversion', 'Gene Expression', 'General Population', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Recombination', 'Genetic screening method', 'Genome', 'Genomics', 'Genus Mycobacterium', 'Geographic Information Systems', 'Geography', 'Giardia lamblia', 'Globus Pallidus', 'Glues', 'Goals', 'Gonorrhea', 'Government', 'Grant', 'Grenada', 'Guanosine Diphosphate', 'Guidelines', 'HIV', 'HIV Infections', 'HIV Seropositivity', 'HIV prevention trials network', 'HIV vaccine', 'HIV-1', 'HIV-2 vaccine', 'Hand', 'Head', 'Health', 'Health Information System', 'Health Personnel', 'Health Policy', 'Health Professional', 'Health Science Library', 'Health Sciences', 'Health Services', 'Health Services Research', 'Health Status', 'Health care facility', 'Health education', 'Healthcare', 'Heterogeneity', 'Heterosexuals', 'High Prevalence', 'Hispanics', 'Home Page', 'Home environment', 'Hospital Administration', 'Hospital Information Systems', 'Hospitalization', 'Hospitals', 'Hour', 'Housing', 'Human', 'Human Herpesvirus 2', 'Human Papilloma Virus Vaccine', 'Human Papillomavirus', 'Human Resources', 'Human immunodeficiency virus test', 'Human papillomavirus 16', 'Hybrids', 'Hyphae', 'IL2 gene', 'Image', 'Immersion Investigative Technique', 'Immune', 'Incidence', 'Individual', 'Induration', 'Infant', 'Infection', 'Infection Control', 'Informatics', 'Information Centers', 'Information Management', 'Information Networks', 'Information Resources', 'Information Sciences', 'Information Systems', 'Institutes', 'Institution', 'Instruction', 'Interdisciplinary Study', 'Interleukin-2', 'Internal Medicine', 'International', 'International AIDS', 'International Classification of Diseases', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Intervention', 'Intervention Trial', 'Interview', 'Isoniazid resistance', 'Japan', 'Job Description', 'Journals', 'Kenya', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Laboratory Finding', 'Laboratory Research', 'Lamivudine/Zidovudine', 'Language', 'Latin America', 'Laws', 'Lead', 'Leadership', 'Learning', 'Left', 'Leishmaniasis', 'Lesion', 'Librarians', 'Libraries', 'Library Science', 'Licensing', 'Life', 'Life Expectancy', 'Light', 'Link', 'Linux', 'Literature', 'Local Area Networks', 'Location', 'London', 'Low income', 'Lung diseases', 'MEDLINE', 'Machine Learning', 'Malaria', 'Manuscripts', 'Maps', 'Marketing', 'Master of Science', 'Master&apos', 's Degree', 'Measures', 'Mediating', 'Mediation', 'Medical', 'Medical Education', 'Medical Informatics', 'Medical Libraries', 'Medical Research', 'Medical Students', 'Medical Surveillance', 'Medical Technology', 'Medical center', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Metronidazole', 'Microbiology', 'Mining', 'Minority', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Monitor', 'Montenegro', 'Mothers', 'Motivation', 'Multi-Drug Resistance', 'Multidrug-Resistant Tuberculosis', 'Mutation', 'Mycobacterium tuberculosis', 'N.I.H. Research Support', 'Names', 'Nature', 'Needs Assessment', 'Neighborhoods', 'Neisseria', 'Neonatology', 'Nested Case-Control Study', 'Nested PCR', 'Network-based', 'Nevirapine', 'Nuclear Energy', 'Nucleic Acids', 'Numbers', 'Nurses', 'Nursing Faculty', 'Nursing Schools', 'Occupational', 'Occupations', 'Online Systems', 'Ontology', 'Operating System', 'Oral', 'Outcome', 'Outcome Measure', 'Oxidation-Reduction', 'Oxidative Stress', 'Pacific Northwest', 'Pan American Health Organization', 'Paper', 'Parasites', 'Parasitic infection', 'Parasitology', 'Participant', 'Pathogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patient Education', 'Patient currently pregnant', 'Patients', 'Pattern', 'Pediatrics', 'Peer Review', 'Pelvic Examination', 'Pelvic Inflammatory Disease', 'Peptide Sequence Determination', 'Performance', 'Perinatal', 'Peripheral Blood Mononuclear Cell', 'Peroxidase', 'Peroxidases', 'Personal Satisfaction', 'Persons', 'Peru', 'Pharmaceutical Preparations', 'Pharmacists', 'Pharmacologic Substance', 'Pharmacy Schools', 'Pharmacy facility', 'Phase', 'Phenotype', 'Philosophy', 'Physicians', 'Physics', 'Pilot Projects', 'Placebos', 'Placement', 'Plasma', 'Play', 'Pliability', 'Policies', 'Polymerase Chain Reaction', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Practice Management', 'Pre-Post Tests', 'Predictive Value', 'Predisposition', 'Pregnancy', 'Pregnant Women', 'Prenatal care', 'Preparation', 'Prevalence', 'Prevention', 'Prevention strategy', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Printing', 'Private Practice', 'Prize', 'Problem Solving', 'Procedures', 'Process', 'Production', 'Professional counselor', 'Program Development', 'Program Evaluation', 'Programmed Learning', 'Progress Reports', 'Prophylactic treatment', 'Proteins', 'Proteomics', 'Protocols documentation', 'Protozoa', 'Provider', 'Province', 'Psychological reinforcement', 'Public Health', 'Public Health Administration', 'Public Health Education', 'Public Health Informatics', 'Public Health Practice', 'Public Health Schools', 'Public Hospitals', 'Public Policy', 'Publications', 'Publishing', 'Pulmonary Tuberculosis', 'Purpose', 'Pyrazinamide', 'Qualifying', 'Qualitative Methods', 'Questionnaires', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Randomized', 'Randomized Controlled Clinical Trials', 'Randomized Controlled Trials', 'Range', 'Rate', 'Reaction', 'Readiness', 'Recommendation', 'Recording of previous events', 'Recruitment Activity', 'Recurrence', 'Regulation', 'Regulatory Element', 'Relative (related person)', 'Reporting', 'Representations, Knowledge (Computer)', 'Reproductive Tract Infections', 'Research', 'Research Activity', 'Research Design', 'Research Ethics Committees', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Research Training', 'Resistance', 'Resources', 'Restriction fragment length polymorphism', 'Retrieval', 'Review Literature', 'Rheumatology', 'Rifampin', 'Risk', 'Risk Behaviors', 'Risk Factors', 'Robotics', 'Role', 'Rotavirus', 'Route', 'Running', 'Rural', 'Rural Health', 'Rural Population', 'Safe Sex', 'Salmonella', 'Sampling', 'Scholarship', 'School Nursing', 'Schools', 'Science', 'Scientist', 'Score', 'Screening procedure', 'Second Pregnancy Trimester', 'Secure', 'Security', 'Seeds', 'Senegal', 'Sentinel', 'Sentinel Surveillance', 'Sequence Alignment', 'Sequence Analysis', 'Series', 'Seroepidemiologic Studies', 'Serological', 'Seroprevalences', 'Services', 'Sex Behavior', 'Sexual Partners', 'Sexually Transmitted Diseases', 'Side', 'Signs and Symptoms', 'Simulate', 'Singapore', 'Sister', 'Site', 'Social Development', 'Social Psychology', 'Software Engineering', 'Software Tools', 'Solid', 'Solutions', 'Source', 'Specificity', 'Specimen', 'Spectinomycin', 'Speed', 'Sputum', 'Staining method', 'Stains', 'Standards of Weights and Measures', 'Stomach', 'Streptomycin', 'Structure', 'Students', 'Supplementation', 'Support System', 'Surrogate Markers', 'Surveillance Program', 'Surveys', 'Swab', 'Switzerland', 'Symptoms', 'Syndrome', 'Syphilis', 'System', 'T-Lymphocyte', 'TAF8 gene', 'Taiwan', 'Tanzania', 'Teaching Materials', 'Techniques', 'Technology', 'Teleconferences', 'Telemedicine', 'Television', 'Test Result', 'Testing', 'Textbooks', 'Therapeutic', 'Thinking', 'Time', 'TimeLine', 'Title', 'Touch sensation', 'Trainers Training', 'Training', 'Training Activity', 'Training Programs', 'Training and Infrastructure', 'Treatment Efficacy', 'Treponema pallidum', 'Triad Acrylic Resin', 'Trichomonas Infections', 'Tropical Disease', 'Trust', 'Tuberculin', 'Tuberculosis', 'U-Series Cooperative Agreements', 'USAID', 'Ulcer', 'Underemployment', 'United States', 'United States Dept. of Health and Human Services', 'United States National Institutes of Health', 'Universities', 'Unmarried', 'Update', 'Urban Population', 'Urethritis', 'Vaccination', 'Vaccines', 'Vagina', 'Variant', 'Venezuela', 'Vertebral column', 'Vibrio', 'Videoconferences', 'Videoconferencing', 'Virulence', 'Virulence Factors', 'Vision', 'Visit', 'Visual', 'Voice', 'Walking', 'Washington', 'Week', 'Western Europe', 'Wing', 'Wolves', 'Woman', 'Work', 'Workplace', 'World Health Organization', 'Writing', 'Zidovudine', 'Zinc', 'abstracting', 'base', 'behavior change', 'biomedical informatics', 'blood filter', 'cancer therapy', 'career', 'case control', 'case-based', 'catalase', 'clinically relevant', 'cohort', 'college', 'computer center', 'computer science', 'computing resources', 'concept', 'condoms', 'contextual factors', 'coping', 'cost', 'cytokine', 'data integration', 'data management', 'data mining', 'database design', 'day', 'design', 'desire', 'digital', 'epidemiology study', 'evaluation/testing', 'expectation', 'experience', 'falls', 'fetal', 'fly', 'follow-up', 'forging', 'genital herpes', 'genome sequencing', 'global environment', 'health administration', 'health application', 'high risk behavior', 'human subject', 'image processing', 'imaging informatics', 'immunopathology', 'improved', 'informatics training', 'information gathering', 'information organization', 'innovation', 'insight', 'instrumentation', 'interest', 'international center', 'isoniazid', 'knowledge base', 'laboratory facility', 'lectures', 'macrophage', 'male', 'mathematical model', 'medical schools', 'member', 'men', 'microbial', 'mortality', 'mouse Gdi2 protein', 'mycobacterial', 'neglect', 'nevirapine resistance', 'new growth', 'next generation', 'older patient', 'older women', 'oncology', 'open source', 'outreach', 'pathogen', 'pediatric AIDS', 'pediatrician', 'peer', 'point of care', 'prenatal', 'prevent', 'professor', 'programs', 'prospective', 'prototype', 'recombinase', 'rectal', 'research and development', 'research study', 'response', 'satisfaction', 'sex', 'size', 'skills', 'skills training', 'software development', 'sound', 'statistics', 'success', 'symposium', 'syndromic surveillance', 'teacher', 'tool', 'tool development', 'trafficking', 'transcription factor', 'transmission process', 'treatment planning', 'trend', 'tumor', 'web based interface', 'wide area network', 'willingness', 'young adult']",FIC,UNIVERSITY OF WASHINGTON,D43,2007,73250,-0.024437971619651225
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7111722,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2006,314029,-0.031863620261843514
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,7123058,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,611872,-0.0007654371420069235
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7125135,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2006,387181,-0.013912900178092769
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,-0.006326860786558946
"Computer System for Functional Analysis of Genomic Data    DESCRIPTION (provided by applicant): In two previous stages of this project, both funded by the National Institute of General Medical Sciences and carried out successfully, we developed GeneWays, a completely automated system that efficiently distills information about molecular interactions from an astronomical number of full-text biomedical articles. The next logical stage of the project is to carry this system from the computational laboratory into a practical, useful, and even indispensable tool that researchers can use to solve complex problems currently posed in experimental medicine and biology. The central hypothesis of our work on GeneWays has been that our computational tools will generate biological predictions of a quality sufficiently high that the biomedical community will invest in serious experimental validation. Specifically, we propose the following. 1. We will improve significantly the precision and recall of the GeneWays system. 2. We will develop and implement a probabilistic belief-network formalism?a belief-graph relative of the Bayesian network formalism that allows us to place and update beliefs on both the vertices and the edges of the graph for probabilistic reasoning over the large collection of facts in the GeneWays database. We will develop and implement a coordinated collection of methods for computing and updating beliefs on individual nodes and edges of the belief graph. 3. We will develop and implement a mathematical framework for incorporating pathway information into a genetic- linkage analysis formalism in such a way that each piece of pathway knowledge includes a specified degree of confidence. 4. We will process an enormous collection of texts, such as open-access biomedical journals, PubMed abstracts, and the GeneWays corpus, and thus will build a comprehensive GeneHighWays database. We will make the GeneHighWays database easily and freely accessible to academic researchers through a web interface. We will evaluate the new version of the GeneWays system and the GeneHighWays database for the quality of data, performance of the mathematical methods, and quality of the interface.           n/a",Computer System for Functional Analysis of Genomic Data,7148274,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,303688,0.0087399377405953
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7007706,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2006,153843,0.007271204483544945
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,-0.010386035587726567
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,7068069,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2006,19532,-0.021385487678203623
"10th International Fragile X Conference    DESCRIPTION (Provided by the Applicant):  10th International Fragile X Conference: The National Fragile X Foundation's 10th International Fragile X Conference in Atlanta, Georgia, at the OMNI Hotel - CNN Center, July 19-23, 2006, will bring together the world's leading researchers in molecular biology and genetics as well as leading clinicians and treatment specialists, selected by its Scientific and Clinical Advisory Committee, with hundreds of parents, extended family members and students engaged in research training. Both scientific and family-friendly sessions covering the three conditions resulting from the fragile X gene mutation will be addressed: fragile X syndrome; fragile X associated tremor ataxia syndrome; fragile X related premature ovarian failure. Keynote presentations, breakout session lectures, research abstract sessions, panels and posters will present the latest knowledge regarding the underlying mechanisms for the fragile X related conditions, plus evidence-based medical, therapeutic and educational interventions. The conference will benefit multiple disciplines including those engaged in clinical practice, epidemiology and delivery system organization. The National Fragile X Foundation will publish and disseminate the results in conference proceedings as well as other formats and utilize the recommendations as the basis for advancing the research and treatment fields.    n/a",10th International Fragile X Conference,7166760,R13HS016448,"['fragile X syndromes', 'meeting /conference /symposium', 'travel']",AHRQ,NATIONAL FRAGILE X FOUNDATION,R13,2006,25000,-0.03538307237296612
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,7056141,K12MH069281,"['bioimaging /biomedical imaging', 'bioinformatics', 'brain imaging /visualization /scanning', 'brain morphology', 'career', 'image processing', 'morphometry', 'neuroimaging', 'neurosciences', 'training']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2006,439784,-0.01161063163428598
"The RPI Exploratory Center for Cheminformatics (RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics (RMI),7125575,P20HG003899,"['Internet', 'NIH Roadmap Initiative tag', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2006,377226,-0.008026453250919765
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,7284550,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2006,498959,-0.02963222161762935
"24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS    DESCRIPTION (provided by applicant):    This conference grant (R13) application requests funds to partially cover the cost of planning, organizing, publicizing and hosting the 24th Annual Symposium on Nonhuman Primate Models for AIDS. The symposium will be held October 4-7, 2006, at the Omni Hotel at CNN Center in downtown Atlanta, Georgia, and will be hosted by the Yerkes National Primate Research Center, Emory University. This meeting is the premier forum for the presentation and exchange of the most recent scientific advances in AIDS research utilizing the nonhuman primate model. The latest findings in primate pathogenesis, immunology, genomics, virology, vaccines and therapeutics will be presented. It is anticipated more than 300 scientists from the United States and other countries will attend. The symposium will encompass five half-day scientific sessions and an evening poster session. The scientific sessions will be: Virology, Pathogenesis, Immunology, Vaccines and Therapeutics/Genomics. Each session will have an invited Chair, a scientific leader in the field, who will give a 30-minute state-of-the-field presentation to open the session, and a Co-Chair from the Scientific Committee, who will moderate the session and entertain questions. In addition, there will be an invited keynote speaker and a banquet speaker, who will address scientific approaches and concerns regarding the global AIDS crisis and related issues of public health. A Scientific Program Committee consisting of eight-ten members drawn from the Yerkes/Emory community and other institutions will review abstracts and assign oral or poster presentations for each of the scientific sessions. Committee members will include leaders in the field from a variety of scientific disciplines. Criteria for selection of oral presentations will include relevance of the topic as well as originality and quality of the information contained in the abstract. Those giving talks will be invited to submit their presentations in manuscript form for publication in the Journal of Medical Primatology. A poster session will include meritorious presentations that cannot be accommodated in one of the platform sessions. A local Organizing Committee will handle arrangements and logistics for the symposium. Feedback from the participants will be obtained through written questionnaires or oral comments to members of the organizing committee. This format has been successfully followed using NCRR support for the previous Annual symposium.           n/a",24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS,7114527,R13RR022961,"['AIDS', 'Primates', 'disease /disorder model', 'meeting /conference /symposium', 'travel']",NCRR,EMORY UNIVERSITY,R13,2006,63089,0.004443857423931704
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7104243,U54EB005149,"['NIH Roadmap Initiative tag', 'bioimaging /biomedical imaging', 'bioinformatics', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2006,3809481,-0.012678727794285442
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,7033080,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2006,133459,-0.017483691611175967
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6916483,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,321788,-0.031863620261843514
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6953701,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2005,613495,-0.0007654371420069235
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7052491,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2005,180862,-0.013912900178092769
"BIOROBOTICS TOOLKIT FOR ENABLING RAPID EXPERIEMENTATION    DESCRIPTION (provided by applicant): We propose to create a biorobotic toolkit for rapid experimentation in the life sciences, medicine, and bioengineering. This toolkit will allow the rapid creation of biorobots derived from reference designs. These reference designs are contributed by the community of researchers. The anticipated outcome will be a vast improvement in methodology in this field. The specific aims of phase I are: (1) The design of 1 reference model (2) Demonstration of a modular plug and play sensor that will be part of a biorobot derived from the reference model (3)Demonstration of a modular plug and play Actuator that will be part of a biorobot derived from the reference model (4) Assemble a robot derived from the reference model, and using the plug and play sensors and actuators achieved in aims 2,3. (5) Quantify the closed loop performance of the sensor-actuator network. (6) Layout a preliminary specification of the architecture.         n/a",BIOROBOTICS TOOLKIT FOR ENABLING RAPID EXPERIEMENTATION,6975326,R43EB004827,"['artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment', 'biomedical equipment development', 'computational biology', 'model design /development', 'neuropsychology', 'psychological models', 'robotics']",NIBIB,"IGUANA ROBOTICS, INC.",R43,2005,150419,0.000983248650050595
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6929696,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2005,206378,0.007159483637073082
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,-0.006326860786558946
"Shifting Conceptions of Human Identity DESCRIPTION (provided by applicant):  . One of the most important questions raised by the ongoing achievements of the Human Genome Project is how this new biological knowledge - and the powers it confers - will affect our identity and self-understanding as human beings. This book project focuses on one key aspect of this complex issue: exploring the extent to which human identity can be reconciled with deliberate design or partial redesign. The author proposes to shed new light on this question by comparing the debates surrounding two areas of scientific innovation that are not normally associated with each other, but that are in fact deeply related: the enterprise of human genetic intervention and the enterprise of building intelligent machines. Both these enterprises entail ""pushing the limits"" of traditional concepts of what it means to be human; and both ultimately confront their makers with the same core ""family"" of questions: What are the defining features of human personhood? To what extent can those features be modified or extended, before human personhood begins to break down? Can some (or all) of those features find embodiment in an entity other than a human being? These kinds of questions are no longer the sole province of science fiction writers, but have been taken up with increasing seriousness by mainstream scientists and technologists, as well as by a wide array of ""science watchers"" in academia, legislative circles, and the news media.   . Through documentary research and interviews, this project aims to deepen our understanding of the history and sociology of the debates surrounding these powerful new technologies, electro-mechanical and biological, that are perceived as destabilizing human identity. The intended audience for the book is a broad one: scientists and technological practitioners interested in the social and cultural reception of their research; legislators and other policymakers with a stake in the governance of science; general educated readers who are concerned about the role of science and technology in shaping our collective future. n/a",Shifting Conceptions of Human Identity,6915830,R03HG003298,"['adult human (21+)', 'artificial intelligence', 'behavioral /social science research tag', 'biotechnology', 'books', 'clinical research', 'ethics', 'genetic manipulation', 'history of life science', 'human subject', 'identity', 'interview', 'robotics', 'self concept', 'sociology /anthropology']",NHGRI,VANDERBILT UNIVERSITY,R03,2005,75833,-0.02992967527120515
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,-0.005078477793019919
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,7000653,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,SPELMAN COLLEGE,R33,2005,260030,-0.011309827224277426
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6908174,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2005,375000,-0.02287334247519165
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7125331,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2005,152083,0.007271204483544945
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,-0.010386035587726567
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6937143,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2005,20000,-0.021385487678203623
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6931476,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2005,392500,-0.02395445676117786
"The RPI Exploratory Center for Cheminformatics(RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics(RMI),7032113,P20HG003899,"['Internet', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2005,375639,-0.008026453250919765
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,6870217,K12MH069281,"['bioimaging /biomedical imaging', 'bioinformatics', 'brain imaging /visualization /scanning', 'brain morphology', 'career', 'image processing', 'morphometry', 'neuroimaging', 'neurosciences', 'training']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2005,428924,-0.01161063163428598
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS    DESCRIPTION (provided by applicant): The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methods within the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data, earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user interface to support access to information resources for enhanced decision-making by practitioners. The long-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3 Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technology work; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C).             n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7084856,P01CD000261,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2005,1270432,-0.001983712994700241
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6848697,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2005,1446246,-0.02963222161762935
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6943136,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2005,467823,-0.028015714179985184
"Computer cluster for computational biology DESCRIPTION (provided by applicant):    The present application aims to establish a computer Cluster for Computational Biology and Bioinformatic (CCBB). The cluster will consists of 256 dual nodes connected with Giganet switches to enable rapid communication between the processors. The cluster will enable the integration of the two approaches and make it possible to effectively address the highly demanding computational tasks of the field. It will serve a small group of investigators, supported by the NIH, and their close collaborators. The hardware needs of computational biology and bioinformatic applications, and of the team of investigators listed in this application can be summarized as follows:   1. Significant computer power for complex and expensive simulations.   2. Large storage capacity for the whole cluster (shared) and (separately) for the individual nodes.   3. Large and rapidly accessible memory for effective statistical analysis, application of machine learning techniques, and biological discovery.   4. Fast network for information updates across the network.   In addition CCBB will have high level of databases and software integration including   1. Updates of important ""mirrors"" of shared databases (such as NR, swissprot, human EST, human genome, protein databank, etc.)   2. Local installation and frequent upgrade of widely used software packages (e.g. BLAST, Pfam, CHARMm etc.)   3. Help in porting novel software for optimal use on the CCBB hardware platform.   The combined unification of optimal hardware and software for computational biology and bioinformatic will make the new cluster; an outstanding resource for NIH related research n/a",Computer cluster for computational biology,6877645,S10RR020889,"['bioinformatics', 'biomedical equipment purchase', 'computational biology', 'computer network', 'computer program /software', 'computer system hardware', 'computers']",NCRR,CORNELL UNIVERSITY ITHACA,S10,2005,500000,-0.02185509774527621
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),6950028,U54EB005149,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2005,3800000,-0.012678727794285442
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6865478,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2005,162750,-0.017483691611175967
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6799187,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,321983,-0.031863620261843514
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6822280,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2004,577307,-0.0007654371420069235
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6912979,R44HG002244,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data management', 'genotype', 'informatics', 'mathematical model', 'nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2004,191986,-0.05636206521464775
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6703756,R44CA093112,"['artificial intelligence', 'clinical research', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'mathematics', 'statistics /biometry']",NCI,"CYTEL, INC",R44,2004,411387,-0.012756279762444117
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,-0.006326860786558946
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6783420,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2004,200515,0.007159483637073082
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,-0.005078477793019919
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,-0.005078477793019919
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6788945,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2004,375000,-0.02287334247519165
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6697243,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,UNIVERSITY OF IOWA,R33,2004,252586,-0.011309827224277426
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,337653,-0.00121116048873992
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,-0.010386035587726567
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6787778,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2004,392500,-0.02395445676117786
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6803809,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2004,20000,-0.021385487678203623
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6733529,R01LM007273,"['Internet', 'behavioral /social science research tag', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'confidentiality', 'data management', 'decision making', 'health care facility information system', 'health care policy', 'human data', 'human rights', 'information dissemination', 'information retrieval', 'mathematical model', 'medical records', 'model design /development', 'patient oriented research', 'statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2004,406979,-0.005819982033211368
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,6700018,K12MH069281,"['bioimaging /biomedical imaging', 'bioinformatics', 'brain imaging /visualization /scanning', 'brain morphology', 'career', 'image processing', 'morphometry', 'neuroimaging', 'neurosciences', 'training']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2004,311472,-0.01161063163428598
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6795323,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2004,754129,-0.028015714179985184
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6699330,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2004,1405759,-0.02963222161762935
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6777517,R43EB000608,"['archives', 'clinical research', 'computer data analysis', 'computer system design /evaluation', 'confidentiality', 'data management', 'health care policy', 'health related legal', 'human data', 'imaging /visualization /scanning', 'information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2004,149200,-0.01835026860229881
"National Alliance for Medical Imaging Computing (RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance for Medical Imaging Computing (RMI),6847712,U54EB005149,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2004,100000,-0.012678727794285442
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6709988,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2004,161668,-0.017483691611175967
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6682996,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' automated data processing', ' chemical structure', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' crystallization', ' data collection methodology /evaluation', ' image processing', ' mathematics', ' method development', ' protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,298672,-0.031863620261843514
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6637557,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2003,148818,-0.0220954154990258
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6622259,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2003,560392,-0.05636206521464775
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6587476,R44CA093112,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' human data', ' mathematical model', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R44,2003,400084,-0.012756279762444117
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6658916,R33GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2003,200824,0.007159483637073082
"Software to Handle Missing Values in Large Data DESCRIPTION (provided by applicant):    This SBIR aims to produce commercial software for handling missing data in large data sets, where the goal is data mining and knowledge discovery. There may be a large number of subjects, variables, or both. Examples include microarray data, surveys, genomic data, and high throughput screening data.      Handling missing data is one important step of careful data preparation, which is key to the success of an entire project. Missing values often arise in medical data. This is an obstacle because many data mining tools either require complete data or are not robust to missing data.      Principled methods of handling missing data are computationally intensive. Therefore computational feasibility is a challenge to handling missing values in large data sets.      Phase I work will explore strategies such as sampling, constraining parameters, and monotone data algorithms for model based techniques. Factor analysis and multivariate linear mixed effects models will be used to reduce the number of parameters. A variable-by-variable approach using a popular data mining technique, recursive partitioning, will also be used to impute missing values.      For each of the methods, we will write prototype software and test performance on missing data patterns simulated on real data. Several ad hoc techniques will serve as a baseline for comparison.   Experience writing prototypes and using them in simulations will lead to preliminary software design that will serve as the foundation of Phase II work.       This proposed software will enable medical researchers to gain more from their data mining efforts: maximally extracting information and achieving unbiased predictions, despite missing data. n/a",Software to Handle Missing Values in Large Data,6690119,R43RR017862,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' human data', ' mathematical model', ' statistics /biometry']",NCRR,INSIGHTFUL CORPORATION,R43,2003,99847,-0.004231251109335096
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,-0.005078477793019919
"Smart Power Assistance Module for Manual Wheelchairs    DESCRIPTION (provided by applicant): We propose to use power assistance as the basis for a Smart Power Assistance Module (SPAM) that provides independent power assistance to the right and left rear wheels of a manual wheelchair. The SPAM will detect obstacles near the wheelchair, and modify the forces applied to each wheel to avoid obstacles.  For individuals with visual impairments that are unable to walk with a long cane or walker, the SPAM will provide safe travel by assisting the user to avoid obstacles. This research will build on the investigative team's previous experience with power assistance for manual wheelchairs and obstacle avoidance for power wheelchairs and rollators. Extensive outside evaluation of the SPAM will be provided throughout the course of the project by clinicians active in wheelchair seating and mobility.         n/a",Smart Power Assistance Module for Manual Wheelchairs,6667132,R43EY014490,"['artificial intelligence', ' assistive device /technology', ' biomedical device power system', ' biomedical equipment development', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' field study', ' human subject', ' medical rehabilitation related tag', ' vision aid', ' vision disorders']",NEI,AT SCIENCES,R43,2003,209800,-0.021007261752742538
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6744998,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2003,191306,-0.011277913270460816
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6626641,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2003,482862,-0.010507136440499143
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6549357,R33EY013688,"['artificial intelligence', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' human subject', ' informatics', ' interdisciplinary collaboration', ' macular degeneration', ' mathematics', ' pathologic process', ' phenotype']",NEI,UNIVERSITY OF IOWA,R33,2003,259228,-0.011309827224277426
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6636465,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,327818,-0.00121116048873992
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6738628,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,159000,-0.00121116048873992
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6646557,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2003,237000,-0.015214284206119137
"MCASE QSAR Expert System for Salmonella Mutagenicity   DESCRIPTION (provided by applicant): Computational expert systems provide an         inexpensive and fast alternative to short term genotoxicity assays such as the       Ames test. Validation studies show the predictive capability of the MCASE            system is about 85 percent. That is, 85 percent concordance is expected between      experiment and computational genotoxicity predictions for new chemicals. The         strong correlation between chemical structure and genotoxicity is particularly       useful for 'in silico' prescreening of new drugs in the pharmaceutical               industry.                                                                                                                                                                 The new Salmonella database modules being developed in this work will be made        available online to the public through the InfoTox web site                          (http://www.l-tox.com). Additionally, NIH grantees will be allowed unlimited         access to the Salmonella modules through InfoTox at no cost.                                                                                                              Collaboration will be sought with large drug companies, with mutual exchange of      data. Thus the databases will evolve and improve over time as new data are           submitted to form a centralized pool of mutagenicity data, that will provide a       resource for avoiding unneeded testing of chemicals structurally similarly to        those that are already thoroughly understood. Our collaborators at the FDA/CDER      will lead the effort to build this industrial consortium.                            PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                                                   n/a",MCASE QSAR Expert System for Salmonella Mutagenicity,6625981,R44CA090178,"['Salmonella typhimurium', ' alternatives to animals in research', ' artificial intelligence', ' chemical information system', ' chemical property', ' chemical structure function', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data collection', ' gene mutation', ' high throughput technology', ' informatics', ' mathematical model', ' mutagen testing', ' mutagens', ' physical property', ' statistics /biometry', ' toxicology']",NCI,"MULTICASE, INC.",R44,2003,278750,-0.0230655827511295
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6690235,P20GM067650,"['animal tissue', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer simulation', ' computer system design /evaluation', ' data management', ' disease /disorder etiology', ' epidemiology', ' functional /structural genomics', ' gene expression', ' human subject', ' informatics', ' interdisciplinary collaboration', ' mathematical model', ' pharmacokinetics', ' science education', ' statistics /biometry', ' technology /technique development', ' therapy', ' training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2003,392500,-0.02395445676117786
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6620783,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2003,380761,-0.005819982033211368
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6626535,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2003,1357166,-0.02963222161762935
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6647589,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2003,729100,-0.028015714179985184
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6694270,R43EB000608,"['archives', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data management', ' health care policy', ' health related legal', ' human data', ' imaging /visualization /scanning', ' information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2003,250800,-0.01835026860229881
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6636516,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2003,374063,-0.013469507648520597
"DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES A real-time clinical repository contains a wealth of detailed information useful for clinical care, research, and administration.  In their raw form, however, the data are difficult to use there is too much volume, too much detail, missing values, and inaccuracies. Clinicians, researchers, and administrators require higher level interpretations that address their questions. For example, a clinician may need to know whether a patient is at sufficient risk for having active tuberculosis to warrant respiratory isolation. The answer to the question may be spread around the clinical repository in chest radiographs, laboratory tests, medication histories, vital signs, and physician's notes. Translating from these raw data to the interpretation (at risk or not) is a difficult and laborious task. The hypothesis of this proposal is that data mining techniques can be applied to a real-time clinical repository to discover knowledge and generate accurate clinical interpretations, and that these interpretations can be automated. The project differs from earlier machine learning studies in its emphasis on a real clinical repository and the use of natural language processing to supply coded clinical data. The specific aims are: (l) Select clinical domains--Several clinical domains with interesting, non-trivial clinical problems will be selected. Problems for which a gold standard answer can or has been assembled for a retrospective cohort will be chosen. (2) Prepare raw clinical data for mining--The raw data from a clinical repository will be transformed into a structure that facilitates data mining. The data will be flattened, pivoted, summarized, and mapped as needed for the domains. Narrative data will be coded using the MedLEE natural language processor. The preparation process will be automated. (3) Use data mining algorithms to discover knowledge- Several data mining algorithms will be applied to the selected clinical domains. Algorithms will include decision tree generation, rule discovery, neural networks, nearest neighbor, logistic regression, and composite algorithms (for variable reduction). The algorithms will be trained on a training set for each domain, and their predictive accuracy will be measured and compared to each other and to expert-written rules. The performance of human experts writing rules using manual data mining visualization techniques (which does not require an explicit training set) will also be measured. (4) Study the dependence of data mining on the training set--The performance of data mining algorithms depends on the data used the train them. The sensitivity of the algorithms to noise (inaccurate data), missing data, and training set size will be measured. (5) Use the discovered knowledge to generate real-time interpretations-- The output of the algorithms (decision tree, rules, neural network equation, or logistic regression equation, but not nearest neighbor) along with the necessary data preparation steps will be encoded in Arden Syntax Medical Logic Modules. They will be run against the clinical repository to verify that the interpretation can be automated in real time. (6) Disseminate the methods and results--The methods and results will be disseminated via publications and a Web site, and tools will be made available.  n/a",DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES,6538211,R01LM006910,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information dissemination', ' information system analysis', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2002,403006,-0.025242707339332846
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6530779,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2002,144484,-0.0220954154990258
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6444292,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2002,531259,-0.05636206521464775
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6549345,R21GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R21,2002,99510,0.007159483637073082
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6490198,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2002,331492,-0.03720126695197187
"Smart Power Assistance Module for Manual Wheelchairs    DESCRIPTION (provided by applicant): We propose to use power assistance as the basis for a Smart Power Assistance Module (SPAM) that provides independent power assistance to the right and left rear wheels of a manual wheelchair. The SPAM will detect obstacles near the wheelchair, and modify the forces applied to each wheel to avoid obstacles.  For individuals with visual impairments that are unable to walk with a long cane or walker, the SPAM will provide safe travel by assisting the user to avoid obstacles. This research will build on the investigative team's previous experience with power assistance for manual wheelchairs and obstacle avoidance for power wheelchairs and rollators. Extensive outside evaluation of the SPAM will be provided throughout the course of the project by clinicians active in wheelchair seating and mobility.         n/a",Smart Power Assistance Module for Manual Wheelchairs,6581049,R43EY014490,"['artificial intelligence', ' assistive device /technology', ' biomedical device power system', ' biomedical equipment development', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' field study', ' human subject', ' medical rehabilitation related tag', ' vision aid', ' vision disorders']",NEI,AT SCIENCES,R43,2002,249727,-0.021007261752742538
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6489213,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2002,474210,-0.010507136440499143
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6520265,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,318270,-0.00121116048873992
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6649647,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,232118,-0.00121116048873992
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6526274,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2002,237000,-0.015214284206119137
"MCASE QSAR Expert System for Salmonella Mutagenicity   DESCRIPTION (provided by applicant): Computational expert systems provide an         inexpensive and fast alternative to short term genotoxicity assays such as the       Ames test. Validation studies show the predictive capability of the MCASE            system is about 85 percent. That is, 85 percent concordance is expected between      experiment and computational genotoxicity predictions for new chemicals. The         strong correlation between chemical structure and genotoxicity is particularly       useful for 'in silico' prescreening of new drugs in the pharmaceutical               industry.                                                                                                                                                                 The new Salmonella database modules being developed in this work will be made        available online to the public through the InfoTox web site                          (http://www.l-tox.com). Additionally, NIH grantees will be allowed unlimited         access to the Salmonella modules through InfoTox at no cost.                                                                                                              Collaboration will be sought with large drug companies, with mutual exchange of      data. Thus the databases will evolve and improve over time as new data are           submitted to form a centralized pool of mutagenicity data, that will provide a       resource for avoiding unneeded testing of chemicals structurally similarly to        those that are already thoroughly understood. Our collaborators at the FDA/CDER      will lead the effort to build this industrial consortium.                            PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                                                   n/a",MCASE QSAR Expert System for Salmonella Mutagenicity,6481789,R44CA090178,"['Salmonella typhimurium', ' alternatives to animals in research', ' artificial intelligence', ' chemical information system', ' chemical property', ' chemical structure function', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data collection', ' gene mutation', ' high throughput technology', ' informatics', ' mathematical model', ' mutagen testing', ' mutagens', ' physical property', ' statistics /biometry', ' toxicology']",NCI,"MULTICASE, INC.",R44,2002,359495,-0.0230655827511295
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6421732,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2002,384388,-0.005819982033211368
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6533705,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2002,688297,-0.028015714179985184
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6489015,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2002,1329187,-0.02963222161762935
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6520333,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2002,363261,-0.013469507648520597
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6525584,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2002,128860,-0.014131405755938128
"DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES A real-time clinical repository contains a wealth of detailed information useful for clinical care, research, and administration.  In their raw form, however, the data are difficult to use there is too much volume, too much detail, missing values, and inaccuracies. Clinicians, researchers, and administrators require higher level interpretations that address their questions. For example, a clinician may need to know whether a patient is at sufficient risk for having active tuberculosis to warrant respiratory isolation. The answer to the question may be spread around the clinical repository in chest radiographs, laboratory tests, medication histories, vital signs, and physician's notes. Translating from these raw data to the interpretation (at risk or not) is a difficult and laborious task. The hypothesis of this proposal is that data mining techniques can be applied to a real-time clinical repository to discover knowledge and generate accurate clinical interpretations, and that these interpretations can be automated. The project differs from earlier machine learning studies in its emphasis on a real clinical repository and the use of natural language processing to supply coded clinical data. The specific aims are: (l) Select clinical domains--Several clinical domains with interesting, non-trivial clinical problems will be selected. Problems for which a gold standard answer can or has been assembled for a retrospective cohort will be chosen. (2) Prepare raw clinical data for mining--The raw data from a clinical repository will be transformed into a structure that facilitates data mining. The data will be flattened, pivoted, summarized, and mapped as needed for the domains. Narrative data will be coded using the MedLEE natural language processor. The preparation process will be automated. (3) Use data mining algorithms to discover knowledge- Several data mining algorithms will be applied to the selected clinical domains. Algorithms will include decision tree generation, rule discovery, neural networks, nearest neighbor, logistic regression, and composite algorithms (for variable reduction). The algorithms will be trained on a training set for each domain, and their predictive accuracy will be measured and compared to each other and to expert-written rules. The performance of human experts writing rules using manual data mining visualization techniques (which does not require an explicit training set) will also be measured. (4) Study the dependence of data mining on the training set--The performance of data mining algorithms depends on the data used the train them. The sensitivity of the algorithms to noise (inaccurate data), missing data, and training set size will be measured. (5) Use the discovered knowledge to generate real-time interpretations-- The output of the algorithms (decision tree, rules, neural network equation, or logistic regression equation, but not nearest neighbor) along with the necessary data preparation steps will be encoded in Arden Syntax Medical Logic Modules. They will be run against the clinical repository to verify that the interpretation can be automated in real time. (6) Disseminate the methods and results--The methods and results will be disseminated via publications and a Web site, and tools will be made available.  n/a",DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES,6391286,R01LM006910,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information dissemination', ' information system analysis', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2001,396315,-0.025242707339332846
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6258188,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2001,140274,-0.0220954154990258
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6391275,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,215487,0.0035722454517872174
"Markov Chain Monte Carlo and Exact Logistic Regression   DESCRIPTION (provided by applicant): Logistic regression is a very popular           model for the analysis of binary data with widespread applicability in the           physical, behavioral and biomedical sciences. Parameter inference for this           model is usually based on maximizing the unconditional likelihood function.          However unconditional maximum likelihood inference can produce inconsistent          point estimates, inaccurate p-values and inaccurate confidence intervals for         small or unbalanced data sets and for data sets with a large number of               parameters relative to the number of observations. Sometimes the method fails        entirely as no estimates can be found that maximize the unconditional                likelihood function. A methodologically sound alternative approach that has          none of the aforementioned drawbacks is the exact conditional approach in which      one generates the permutation distributions of the sufficient statistics for         the parameters of interest conditional on fixing the sufficient statistics of        the remaining nuisance parameters at their observed values. The major stumbling      block to this approach is the heavy computational burden it imposes. Monte           Carlo methods attempt to overcome this problem by sampling from the reference        set of possible permutations instead of enumerating them all. Two competing          Monte Carlo methods are network based sampling and Markov Chain Monte Carlo          (MCMC) sampling. Network sampling suffers from memory limitations while MCMC         sampling can produce incorrect results if the Markov chain is not ergodic or if      the process is not in the steady state. We propose a novel approach which            combines the network and MCMC sampling, draws upon the strengths of each of          them and overcomes their individual limitations. We propose to implement this        hybrid network-MCMC method in our LogXact software and as an external procedure      in the SAS system.                                                                   PROPOSED COMMERCIAL APPLICATION:  There is great demand for logistic regression software that can handle small, sparse or  unbalanced data sets by exact methods.  Our LogXact package is the only software that  can provide exact inference for data sets which are not ""toy problems"".  Yet even  LogXact quickly breaks down on moderate sized problems.  The new generation of hybrid  network-MCMC algorithms will handle substantially larger problems that nevertheless need  exact inference.  The commercial potential is considerable since such data sets are common  in scientific studies.                                                                                      n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6404971,R43CA093112,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R43,2001,113111,-0.007824335163328082
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6343026,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2001,376147,-0.03720126695197187
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6286183,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2001,465813,-0.010507136440499143
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6363593,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2001,306158,-0.011277913270460816
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6495900,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,35096,-0.00121116048873992
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6351629,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2001,106893,-0.00987087270998037
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6387173,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,309000,-0.00121116048873992
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6392266,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2001,232139,-0.018298807197734992
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6401728,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2001,237000,-0.015214284206119137
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6449653,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2001,658874,-0.028015714179985184
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6230917,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2001,1291932,-0.02963222161762935
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6484619,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2001,337739,-0.013469507648520597
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6385653,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2001,127154,-0.014131405755938128
"DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES A real-time clinical repository contains a wealth of detailed information useful for clinical care, research, and administration.  In their raw form, however, the data are difficult to use there is too much volume, too much detail, missing values, and inaccuracies. Clinicians, researchers, and administrators require higher level interpretations that address their questions. For example, a clinician may need to know whether a patient is at sufficient risk for having active tuberculosis to warrant respiratory isolation. The answer to the question may be spread around the clinical repository in chest radiographs, laboratory tests, medication histories, vital signs, and physician's notes. Translating from these raw data to the interpretation (at risk or not) is a difficult and laborious task. The hypothesis of this proposal is that data mining techniques can be applied to a real-time clinical repository to discover knowledge and generate accurate clinical interpretations, and that these interpretations can be automated. The project differs from earlier machine learning studies in its emphasis on a real clinical repository and the use of natural language processing to supply coded clinical data. The specific aims are: (l) Select clinical domains--Several clinical domains with interesting, non-trivial clinical problems will be selected. Problems for which a gold standard answer can or has been assembled for a retrospective cohort will be chosen. (2) Prepare raw clinical data for mining--The raw data from a clinical repository will be transformed into a structure that facilitates data mining. The data will be flattened, pivoted, summarized, and mapped as needed for the domains. Narrative data will be coded using the MedLEE natural language processor. The preparation process will be automated. (3) Use data mining algorithms to discover knowledge- Several data mining algorithms will be applied to the selected clinical domains. Algorithms will include decision tree generation, rule discovery, neural networks, nearest neighbor, logistic regression, and composite algorithms (for variable reduction). The algorithms will be trained on a training set for each domain, and their predictive accuracy will be measured and compared to each other and to expert-written rules. The performance of human experts writing rules using manual data mining visualization techniques (which does not require an explicit training set) will also be measured. (4) Study the dependence of data mining on the training set--The performance of data mining algorithms depends on the data used the train them. The sensitivity of the algorithms to noise (inaccurate data), missing data, and training set size will be measured. (5) Use the discovered knowledge to generate real-time interpretations-- The output of the algorithms (decision tree, rules, neural network equation, or logistic regression equation, but not nearest neighbor) along with the necessary data preparation steps will be encoded in Arden Syntax Medical Logic Modules. They will be run against the clinical repository to verify that the interpretation can be automated in real time. (6) Disseminate the methods and results--The methods and results will be disseminated via publications and a Web site, and tools will be made available.  n/a",DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES,6031325,R01LM006910,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information dissemination', ' information system analysis', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2000,433383,-0.025242707339332846
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6185231,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,213046,0.0035722454517872174
ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS No abstract available n/a,ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS,6361843,01LM003528,"['anatomy', ' artificial intelligence', ' vocabulary development for information system']",NLM,UNIVERSITY OF WASHINGTON,N01,2000,228331,-0.012702740537434486
"SELECTING AMONG MATHEMATICAL MODELS OF COGNITION DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling       of cognition, it is important to have well-justified criteria for choosing       among differing explanations (i.e., models) of observed data.  This project      investigates those criteria as well as their instantiation in five model         selection methods.                                                                                                                                                Two lines of research will be undertaken.  In the first, a thorough              investigation of model complexity will be conducted.  Comprehensive              simulations re intended to determine complexity's contribution to model fit      and to model selection.  An analytical solution will also be sought with the     hope of quantifying model complexity.                                                                                                                             The second line of work examines the utility of each of the five selection       methods in choosing among models in three topic areas in cognitive               psychology (information integration, categorization, connectionist               modeling), the end goal being to identify their merits and shortcomings.                                                                                          Findings should provide a better understanding of model selection than           currently available and serve as a useful guide for researchers comparing        the suitability of quantitative models of cognition.                                                                                                               n/a",SELECTING AMONG MATHEMATICAL MODELS OF COGNITION,6185788,R01MH057472,"['artificial intelligence', ' choice', ' cognition', ' computer simulation', ' information dissemination', ' mathematical model', ' psychometrics']",NIMH,OHIO STATE UNIVERSITY,R01,2000,77332,-0.012112917260900592
"NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating             Characteristic (ROC) analysis is recognized widely as the best way of            measuring and specifying the accuracies of diagnostic procedures, because it     is able to distinguish between actual differences in discrimination              capacity, on one hand, and apparent differences that are due only to             decision-threshold effects, on the other.  Key methodological needs remain       to be satisfied before ROC analysis can address all of the practically           important situations that arise in diagnostic applications, however.  This       project employs signal detection theory and computer simulation to address       several of those needs, by:  (1) refining and continuing distribution of         software developed previously by the applicants for fitting ROC curves and       for testing the statistical significance of differences between ROC curve        estimates; (2) developing and evaluating new algorithms for ROC                  curve-Fitting and statistical testing, based on their recently-developed         ""proper"" binormal model, that should provide more meaningful results in          experimental situations that involve small samples of cases; (3)                 investigating the usefulness of a form of ROC methodology that is based on       mixture distributions in order to rduce the need for diagnostic truth in ROC     experiments; (4) investigating the effect of case-saple difficulty on the        statistical power tests for differences between ROC curves, in order to          determine the optimal difficulty of cases that shouldbe studied on rank          diagnostic systems; and (5) developing methods for training artificial           neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC           analysis and signal detection theory.                                             n/a",NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY,6181168,R01GM057622,"['artificial intelligence', ' computer assisted diagnosis', ' computer system design /evaluation', ' method development', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R01,2000,218176,-0.023122130602796026
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,-0.003681701504052094
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,-0.003681701504052094
"KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA Abstractions of time-stamped clinical data are useful for planning               therapy, for monitoring therapy, and for creating high-level summaries of        time-oriented clinical databases.  Temporal abstractions also support            explanations by an intelligent patient-record system and can be used for         representation of the goals and intentions of clinical guidelines and            protocols.                                                                                                                                                        We propose to reengineer and expand the scope of the RESUME system, a            prototype computer program that implements the knowledge-based temporal-         abstraction method, a conceptual and computational framework that we have        developed for abstraction of time-stamped clinical data into clinically          meaningful interval-based concepts. RESUME has been evaluated with highly        encouraging results in several clinical areas. We will address the               practical and theoretical issues of representation, acquisition,                 maintenance, and reuse of temporal-abstraction knowledge. Our specific           aims are defined by a four-step research plan:                                                                                                                    1. We will define formally the knowledge requirements for five                   computational modules (mechanisms) we employ, thus facilitating the              acquisition, maintenance, reuse, and sharing of the required knowledge.                                                                                           2. We will enhance, expand, and redesign five computational temporal-            abstraction mechanisms:                                                          (a) Automatic formation of meaningful contexts for interpretation of             clinical data.                                                                   (b) Classification of clinical data that have equivalent time stamps into        higher-level concepts.                                                           (c) Temporal inference (e.g., the join of certain interval-based clinical        abstractions into longer ones).                                                  (d) Interpolation between temporally disjoint clinical abstractions,             including a development of a probabilistic representation and semantics.         (e) Matching of predefined and runtime temporal patterns, given time-            stamped data and conclusions.                                                                                                                                     3. We will develop a tool for automated acquisition, from expert                 physicians, of temporal-abstraction knowledge, using techniques from the         PROTEGE-II project for designing knowledge-based systems.                                                                                                         4. We will validate and evaluate our methodology and its implementation.         (a) We will assess the value of the knowledge-acquisition tool in several        experiments.                                                                     (b) We will validate the performance of the computational mechanisms in          the domain of therapy of patients who have insulin-dependent diabetes by         collaboration with expert endocrinologists.                                      (c) We will evaluate the overall framework within EON, a project in which        researchers are implementing an integrated architecture for protocol-based       care.                                                                             n/a",KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA,6185217,R29LM006245,"['abstracting', ' artificial intelligence', ' computer assisted medical decision making', ' computer program /software', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' time resolved data']",NLM,STANFORD UNIVERSITY,R29,2000,121082,-0.004483709323403196
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6071498,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2000,364001,-0.03720126695197187
"TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES DESCRIPTION (Adapted from the applicant's abstract):                                                                                                              The proposed research will build, refine, and test in operational use, a set     of software tools designed to help support, maintain, and iteratively            revalidate computer-based clinical guidelines as they evolve over time.  The     project will focus on the domain of childhood immunization, and will build       upon IMM/Serve, a childhood immunization forecasting program that takes as       input a child's immunization history, and produces recommendations as to         which vaccinations are due and which vaccinations should be scheduled next.      The effort required to modify and validate such a program as the clinical        field evolves over time is a challenging task.  It will be extremely             important to have a robust set of tools to assist in this process.  Partial      prototype versions of certain of these tools already exist.                                                                                                       1.  The project will refine and extend computer-based tools for immunization     knowledge maintenance.  These tools will include:  a) IMM/Def, a program         which automatically generates the rule-based logic for the most complex          portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program          which automatically generates a set of test cases to help test the kernel        logic.  The project will also develop an organized set of strategies for         immunization test case generation, and implement those strategies in the         refined version of IMM/Test.                                                                                                                                      2.  The project will build a Web site to support immunization knowledge          maintenance.                                                                                                                                                      3.  The project will keep a detailed record of all modifications and             customization of the knowledge, and will represent all the variations of the     knowledge using a standardized format such as GLIF, the Guideline                Interchange Format being developed as a standard for exchanging guidelines       between sites.                                                                                                                                                    4.  The project will link IMM/Serve to a database designed to hold               IMM/Serve's analysis of a set of cases, so that the resulting package can be     used as a tool to perform compliance assessment.                                                                                                                  5.  A set of evaluation studies will be carried out to help assess the           efficacy of the tools and to help improve their functionality.                    n/a",TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES,6185229,R01LM006682,"['Internet', ' artificial intelligence', ' computer assisted medical decision making', ' computer assisted patient care', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' human data', ' immunization', ' information systems', ' medical records', ' pediatrics']",NLM,YALE UNIVERSITY,R01,2000,317318,0.009066849926037434
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6165092,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2000,297119,-0.011277913270460816
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6151393,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2000,103781,-0.005537543631137279
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6186179,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2000,234591,-0.018298807197734992
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,6185220,R01LM006326,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,2000,117821,-0.01724755235644679
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6132622,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,300000,-0.00121116048873992
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6323962,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,186611,-0.00121116048873992
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6344145,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,141192,-0.00121116048873992
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6193019,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN,R01,2000,478050,-0.013469507648520597
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6180399,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2000,150497,-0.014131405755938128
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10224492,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,233900,-0.008098744503399565
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10017950,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,513041,-0.008098744503399565
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,9952803,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2020,92359,0.0008336614611372541
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9979659,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'MeSH Thesaurus', 'Measures', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'large scale data', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'public repository', 'specific biomarkers']",NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2020,467177,-0.029085621524836524
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9941090,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2020,605875,-0.017519750073882862
"Center for Machine Learning in Urology PROJECT SUMMARY We propose to establish an Exploratory Center for Interdisciplinary Research in Benign Urology at the Children’s Hospital of Philadelphia (CHOP) and the University of Pennsylvania (Penn), the central mission of which is to apply machine learning to improve the understanding of the pathophysiology, diagnosis, risk stratification, and prediction of treatment responses of benign urological disease among children and adults. The proposed CHOP/Penn Center for Machine Learning in Urology (CMLU) addresses critical structural and scientific barriers that impede the development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. Structurally, urologic research occurs in silos, with little interaction among investigators that study different diseases or different populations (e.g. pediatric and adult). Scientifically, analysis of imaging and other types complex data is limited by inter-observer variability, and incomplete utilization of available information. This proposal overcomes these barriers by applying cutting-edge approaches in machine learning to analyze CT images that are routinely obtained for evaluation of individuals with kidney stone disease. Central to the CHOP/Penn CMLU is the partnership of urologists and experts in machine learning, which will bring a new approach to generating knowledge that advances research and clinical care. In addition, the CMLU will expand the urologic research community by providing a research platform and standalone machine learning executables that could be applied to other datasets. The Center’s mission will be achieved through the following Aims, with progress assessed through systematic evaluation: Aim 1. To expand the research base investigating benign urological disease. We will establish a community with the research base, particularly with the KURe, UroEpi programs, other P20 Centers, and O’Brien Centers. We will build this community by providing mini-coaching clinics to facilitate application of machine learning to individual projects, developing an educational hub for synchronous and asynchronous engagement with the research base, and making freely available all source codes and standalone executables for all machine learning tools. Aim 2. To improve prediction of ureteral stone passage using machine learning of CT images. The CMLU has developed deep learning methods that segment and automate measurement of urinary stones and adjacent renal anatomy. In the Research Project, we will compare these methods to existing segmentation methods and the current gold standard of manual measurement. We will then extract informative features from thousands of CT scans to predict the probability of spontaneous passage of ureteral stones for children and adults evaluated in the CHOP and Penn healthcare systems. Aim 3. To foster collaboration in benign urological disease research across levels of training and centers through an Educational Enrichment Program. We will amplify interactions across institutions and engage investigators locally and nationally by providing summer research internships, and interinstitutional exchange program, and an annual research symposium. PROJECT NARRATIVE The proposed CHOP/Penn O’Brien Center for Machine Learning in Urology addresses critical structural and scientific barriers that impede development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. This application overcomes these barriers by applying cutting- edge approaches in machine learning to analyze complex imaging data for individuals with kidney stone disease.The Center’s strategic vision of using machine learning to generate knowledge that improves diagnosis, risk stratification strategies, and prediction of outcomes among children and adults will be achieved through the implementation of a Educational Enrichment Program and a Research Project.",Center for Machine Learning in Urology,10133362,P20DK127488,"['Address', 'Adult', 'Algorithms', 'Anatomy', 'Area', 'Benign', 'Characteristics', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Clinical Investigator', 'Code', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Educational Status', 'Evaluation', 'Fostering', 'Functional disorder', 'Funding', 'Future', 'Gold', 'Healthcare Systems', 'Image', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Internships', 'Interobserver Variability', 'Investigation', 'Kidney', 'Kidney Calculi', 'Knowledge', 'Lead', 'Longevity', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Mission', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pediatric Hospitals', 'Pennsylvania', 'Philadelphia', 'Population', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Probability', 'Publishing', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk stratification', 'Site', 'Source Code', 'Structure', 'Students', 'Techniques', 'United States National Institutes of Health', 'Universities', 'Urinary Calculi', 'Urologic Diseases', 'Urologist', 'Urology', 'Vision', 'Visit', 'X-Ray Computed Tomography', 'base', 'clinical care', 'complex data ', 'deep learning', 'deep neural network', 'design', 'experience', 'feature selection', 'human error', 'improved', 'interdisciplinary collaboration', 'interest', 'learning strategy', 'novel strategies', 'outcome prediction', 'peer', 'programs', 'routine imaging', 'senior faculty', 'skills', 'summer research', 'symposium', 'tool', 'urologic', 'web page']",NIDDK,CHILDREN'S HOSP OF PHILADELPHIA,P20,2020,358890,-0.03948735600300293
"Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points Project Summary Biomedical data science data modeling is relevant to a plethora of informatics research activities, such as natural language processing, machine learning, artificial intelligence, and predictive analytics. As Electronic Health Record systems become more advanced and more mature, with the potential to incorporate a wide and diverse array of data from genomics to mobile health (mHealth) applications, the scope and nature of the biomedical data science questions researchers ask become broader. Concomitantly, the answers to their questions have the potential to impact the care of millions of patients—getting the answers right, proactively, is high stakes. However, in data modeling currently, there is no bioethics framework to guide the process of mapping key decision points and recording the rationale for choices made. Making data modeling decision points, as well as the reasoning behind them, explicit would have a twofold impact on improving biomedical data science by: 1. Enhancing transparency and reproducibility and maximizing the value of data science research and 2. Supporting the ability to assess decision points and rationales in terms of their most crucial ethical ramifications. Research in this area is particularly timely amid the interest in, and enthusiasm for, leveraging Big Data sources in the service of improving patient population health and the health of the general public. The National Institutes of Health (NIH) recently released a strategic plan for data science; there is no better time than now to create an initial bioethical framework to inform common data modeling decision points. The improvements in data quality that will derive from decision point mapping and bioethical review will enhance efforts to apply data models across a range of high-impact areas, from predictive analytics to support clinical decision-making to robust trending models in population health to better inform local, regional, and national health policies and resource allocation. To develop this initial bioethics framework, we will use well- established qualitative research methods (interviews, focus groups, and in-person deliberation) to map the decision points in biomedical data modeling research and document the rationales invoked to support those decisions (Aim 1 key informant interviews); assess those data science decision points and decision-making rationales for their bioethical ramifications (Aim 2 focus groups); and create an initial bioethics data modeling framework (Aim 3 deliberative meeting). This study would be the first to provide a bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe. This approach directly supports core scientific values of inclusivity, transparency, accountability, and reproducibility that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global. Project Narrative This study would be the first to develop an initial bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe—not least because poorly developed data models have the potential to impact adversely the health of individuals, groups, and communities. Currently, there is limited conversation around potential bioethics issues in data modeling, and as yet no implementable guidance on how biomedical data science modeling research activities should occur. The initial ethics framework developed by this study would provide a roadmap to ensure that data modeling decision points are documented and their ethical ramifications considered at the outset of model creation, thus supporting core scientific values of inclusivity, accountability, reproducibility, and transparency that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global.",Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points,10039527,R21HG011277,"['Accountability', 'Address', 'Area', 'Artificial Intelligence', 'Big Data', 'Bioethical Issues', 'Bioethics', 'Bioethics Consultants', 'Caring', 'Clinical', 'Communities', 'Data', 'Data Science', 'Data Scientist', 'Data Sources', 'Decision Making', 'Development', 'Electronic Health Record', 'Ensure', 'Ethical Issues', 'Ethical Review', 'Ethics', 'Focus Groups', 'Fostering', 'General Population', 'Health', 'Health Resources', 'Health system', 'Individual', 'Informatics', 'Interview', 'Machine Learning', 'Maps', 'Methods', 'Mobile Health Application', 'Modeling', 'National Health Policy', 'Natural Language Processing', 'Nature', 'Output', 'Patients', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Qualitative Research', 'Reproducibility', 'Research', 'Research Activity', 'Research Methodology', 'Research Personnel', 'Resource Allocation', 'Role', 'Services', 'Social Environment', 'Strategic Planning', 'Structure', 'System', 'Time', 'Trust', 'United States National Institutes of Health', 'Walking', 'base', 'biomedical data science', 'clinical decision support', 'clinical decision-making', 'data modeling', 'data quality', 'data tools', 'ethical legal social implication', 'genomic data', 'high standard', 'improved', 'individual patient', 'informant', 'interest', 'interoperability', 'meetings', 'model development', 'patient population', 'population health', 'programs', 'public trust', 'tool', 'trend', 'usability']",NHGRI,"HASTINGS CENTER, INC.",R21,2020,100000,-0.012467693872757297
"Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement PROJECT SUMMARY/ABSTRACT Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing. Project Narrative Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing",Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement,10149058,R24MH117179,"['Address', 'Administrative Supplement', 'Archives', 'Artificial Intelligence', 'Award', 'BRAIN initiative', 'Benefits and Risks', 'Consent Forms', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Data Set', 'Ensure', 'Ethics', 'Foundations', 'Funding', 'Future', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Subject Research', 'International', 'Investments', 'Laws', 'Legal', 'Light', 'Machine Learning', 'Magnetic Resonance Imaging', 'Neurosciences', 'Parents', 'Policies', 'Privacy', 'Process', 'Regulation', 'Research', 'Research Subjects', 'Risk', 'Security Measures', 'Series', 'Software Tools', 'Solid', 'Surveys', 'Techniques', 'United States', 'United States National Institutes of Health', 'data archive', 'data privacy', 'data sharing', 'design', 'human subject', 'human subject protection', 'machine learning algorithm', 'neuroethics', 'neuroimaging', 'novel', 'prevent', 'privacy protection', 'research study', 'sharing platform', 'sound', 'stem']",NIMH,STANFORD UNIVERSITY,R24,2020,126592,-0.013683567212958894
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy Project Summary/Abstract Artificial intelligence on genomic/healthcare data that is performed jointly between multiple collaborating institutions relies on a trust model but can accelerate genomic medicine research and facilitate quality improvement. To conduct such machine learning while protecting patient privacy and reducing security risks, we are developing blockchain-based privacy-preserving learning methods in a K99/R00 study supported by the National Human Genome Research Institute (NHGRI). However, our previous design of privacy-preserving learning on private blockchain assumed “semi-honesty” as the underlying adversary assumption. That is, we assume that each participating site is curious yet very careful and honest, such that it would only submit correct predictive models. Nevertheless, in real world this assumption may be too optimistic; the models submitted could be an old one due to network latency or malicious users may try to create fake models, which can in turn lead to bioethical concerns and reduce the incentives for genomic/clinical institutions to participate in the collaborative predictive modeling. Therefore, the capability to detect, assess and prevent “model misconducts” is critical to increase the integrity/reliability of machine learning. To address this issue, we consider the following 3 types of model misconducts: (1) model plagiarism, of which a site becomes a free-rider and just submits a copy of a model from the other sites, trying to hide their own information and inspect models from other sites; (2) model fabrication, of which a site mocks up a model, trying to hide information and disturb the machine learning process; and (3) model falsification, of which a site tweaks its model a bit, trying to just disturb the learning process. For each type of the model misconducts, we are interest in how to detect these misconducts of another site, how to assess the losses of machine learning results due to misconducts, and how to prevent these model misconducts. Our aims include (a) detecting model misconducts using model properties, (b) assessing model misconducts losses via model simulation, and (c) preventing model misconducts based on whole model history. The innovative components to our proposed project include (i) summarizing various types of model misconduct, (ii) developing a complete strategy to handle the model misconduct, and (iii) providing a generalizable approach to mitigate bioethical concerns for collaborative machine learning. Project Narrative Artificial intelligence performed jointly between multiple collaborating institutions can accelerate genomic medicine research and facilitate quality improvement, but relies on a trust model which may be too optimistic in real-world setting. In this project, we plan to develop a comprehensive detection, assessment and prevention mechanism to address the potential bioethical risks brought by misconducts of model plagiarism, fabrication, and falsification. The proposed study can supplement the considerations of model misconducts for our original project of privacy-preserving learning on blockchain.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,10130868,R00HG009680,"['Address', 'Artificial Intelligence', 'Bioethics', 'Budgets', 'Calibration', 'Clinical', 'Data', 'Data Set', 'Detection', 'Digit structure', 'Discrimination', 'Event', 'Genomic medicine', 'Genomics', 'Healthcare', 'Incentives', 'Institution', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pattern', 'Plagiarism', 'Prevention', 'Privacy', 'Privatization', 'Process', 'Property', 'Randomized', 'Recording of previous events', 'Research', 'Risk', 'Security', 'Site', 'Sum', 'Testing', 'Time', 'Trust', 'base', 'blockchain', 'design', 'distributed ledger', 'innovation', 'interest', 'learning strategy', 'models and simulation', 'patient privacy', 'predictive modeling', 'prevent', 'privacy preservation', 'statistics']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,102049,-0.004610204292944652
"SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy The research objective of this proposal, Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy Prediction, with Pl Dominique Duncan from the University of Southern California, is to predict the onset of epileptic seizures following traumatic brain injury (TBI), using innovative analytic tools from machine learning and applied mathematics to identify features of epileptiform activity, from a multimodal dataset collected from both an animal model and human patients. The proposed research will accelerate the discovery of salient and robust features of epileptogenesis following TBI from a rich dataset, collected from the Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx), as it is being acquired by investigating state-of-the-art models, methods, and algorithms from contemporary machine learning theory. This secondary use of data to support automated discovery of reliable knowledge from aggregated records of animal model and human patient data will lead to innovative models to predict post-traumatic epilepsy (PTE). This machine learning based investigation of a rich dataset complements ongoing data acquisition and classical biostatistics-based analyses ongoing in the study and can lead to rigorous outcomes for the development of antiepileptogenic therapies, which can prevent this disease. Identifying salient features in time series and images to help design a predictor of PTE using data from two species and multiple individuals with heterogeneous TBI conditions presents significant theoretical challenges that need to be tackled. In this project, it is proposed to adopt transfer learning and domain adaptation perspectives to accomplish these goals in multimodal biomedical datasets across two populations. Specifically, techniques emerging from d,eep learning literature will be exploited to augment data, share parameters across model components to reduce the number of parameters that need to be optimized, and use state-of-the-art architectures to develop models for feature extraction. These will be compared against established pipelines of hand-crafted feature extraction in rigorous cross-validation analyses. Developed techniques for transfer learning will be able to extract features that generalize across animal and human data. Moreover, these theoretical techniques with associated models and optimization methods will be applicable to other multi-species transfer learning challenges that may arise in the context of health and medicine. Multimodal feature extraction and discriminative model learning for disease onset prediction using novel classifiers also offer insights into biomarker discovery using advanced machine learning techniques through joint multimodal data analysis. A significant percentage of people develop epilepsy after a moderate-severe traumatic brain injury. If we can identify who will develop post-traumatic epilepsy and at what time point after the injury, those patients can be treated with antiepileptogenic therapies and medications to stop or prevent the seizures from occurring. It is likely that biomarkers of epileptogenesis after TBI can only be found by analyzing multimodal data from a large population, which requires advanced mathematical tools and models.",SCH: INT: Collaborative Research: Multimodal Signal Analysis and Data Fusion for Post-traumatic Epilepsy,9921505,R01NS111744,"['Adopted', 'Algorithms', 'Animal Model', 'Antiepileptogenic', 'Architecture', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Brain imaging', 'California', 'Chemicals', 'Complement', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Graph', 'Hand', 'Health', 'High Frequency Oscillation', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Injury', 'Intuition', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Length', 'Limbic System', 'Literature', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'MicroRNAs', 'Modeling', 'Onset of illness', 'Outcome', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Post-Traumatic Epilepsy', 'Property', 'Proteins', 'Psychological Techniques', 'Psychological Transfer', 'Rattus', 'Records', 'Research', 'Rest', 'Scalp structure', 'Seizures', 'Series', 'Signal Transduction', 'Statistical Models', 'Structure', 'Techniques', 'Thalamic structure', 'Time', 'Tissues', 'Traumatic Brain Injury', 'Universities', 'Update', 'Validation', 'Voting', 'Work', 'analytical tool', 'animal data', 'base', 'biomarker discovery', 'data acquisition', 'data fusion', 'deep learning', 'design', 'feature extraction', 'human data', 'imaging modality', 'improved', 'innovation', 'insight', 'laboratory experiment', 'learning strategy', 'multimodal data', 'multimodality', 'neural network', 'neural network classifier', 'neurophysiology', 'novel', 'post-trauma', 'predictive modeling', 'prevent', 'random forest', 'support vector machine', 'theories', 'tool']",NINDS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,245552,-0.014451860133832166
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9884791,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical data science', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'patient health information', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2020,92070,-0.02804690157493218
"Advanced End-to-End Relation Extraction with Deep Neural Networks ABSTRACT Relations linking various biomedical entities constitute a crucial resource that enables biomedical data science applications and knowledge discovery. Relational information spans the translational science spectrum going from biology (e.g., protein–protein interactions) to translational bioinformatics (e.g., gene–disease associations), and eventually to clinical care (e.g., drug–drug interactions). Scientists report newly discovered relations in nat- ural language through peer-reviewed literature and physicians may communicate them in clinical notes. More recently, patients are also reporting side-effects and adverse events on social media. With exponential growth in textual data, advances in biomedical natural language processing (BioNLP) methods are gaining prominence for biomedical relation extraction (BRE) from text. Most current efforts in BRE follow a pipeline approach containing named entity recognition (NER), entity normalization (EN), and relation classiﬁcation (RC) as subtasks. They typically suffer from error snowballing — errors in a component of the pipeline leading to more downstream errors — resulting in lower performance of the overall BRE system. This situation has lead to evaluation of different BRE substaks conducted in isolation. In this proposal we make a strong case for strictly end-to-end evaluations where relations are to be produced from raw text. We propose novel deep neural network architectures that model BRE in an end-to-end fashion and directly identify relations and corresponding entity spans in a single pass. We also extend our architectures to n-ary and cross-sentence settings where more than two entities may need to be linked even as the relation is expressed across multiple sentences. We also propose to create two new gold standard BRE datasets, one for drug–disease treatment relations and another ﬁrst of a kind dataset for combination drug therapies. Our main hypothesis is that our end-to-end extraction models will yield supe- rior performance when compared with traditional pipelines. We test this through (1). intrinsic evaluations based on standard performance measures with several gold standard datasets and (2). extrinsic application oriented assessments of relations extracted with use-cases in information retrieval, question answering, and knowledge base completion. All software and data developed as part of this project will be made available for public use and we hope this will foster rigorous end-to-end benchmarking of BRE systems. NARRATIVE Relations connecting biomedical entities are at the heart of biomedical research given they encapsulate mech- anisms of disease etiology, progression, and treatment. As most such relations are ﬁrst disclosed in textual narratives (scientiﬁc literature or clinical notes), methods to extract and represent them in a structured format are essential to facilitate applications such as hypotheses generation, question answering, and information retrieval. The high level objective of this project is to develop and evaluate novel end-to-end supervised machine learning methods for biomedical relation extraction using latest advances in deep neural networks.",Advanced End-to-End Relation Extraction with Deep Neural Networks,10052028,R01LM013240,"['Adverse event', 'Architecture', 'Area', 'Benchmarking', 'Bioinformatics', 'Biology', 'Biomedical Research', 'Classification', 'Clinical', 'Code', 'Collaborations', 'Combination Drug Therapy', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Disease', 'Distant', 'Drug Interactions', 'Encapsulated', 'Etiology', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Gold', 'Growth', 'Hand', 'Heart', 'Information Retrieval', 'Information Sciences', 'Intramural Research', 'Joints', 'Knowledge Discovery', 'Label', 'Language', 'Lead', 'Link', 'Literature', 'Manuals', 'Maps', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Patients', 'Peer Review', 'Performance', 'Periodicity', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'Scientist', 'Semantics', 'Software Tools', 'Source', 'Standardization', 'Structure', 'Supervision', 'System', 'Terminology', 'Testing', 'Text', 'Training', 'Translational Research', 'Trees', 'base', 'biomedical data science', 'clinical care', 'deep neural network', 'improved', 'insight', 'interest', 'knowledge base', 'machine learning method', 'natural language', 'neural network', 'neural network architecture', 'new therapeutic target', 'novel', 'off-label use', 'protein protein interaction', 'relating to nervous system', 'side effect', 'social media', 'supervised learning', 'syntax']",NLM,UNIVERSITY OF KENTUCKY,R01,2020,358691,-0.024040684315520738
"Novel machine learning approaches for improving structural discrimination in cryo-electron tomography Project Summary Cellular cryo-electron tomography (Cryo-ET) has made possible the observation of cellular organelles and macromolecular complexes at nanometer resolution with native conformations. The rapid increasing amount of Cryo-ET data available however brings along some major challenges for analysis which we will timely ad- dress in this proposal. We will design novel data-driven machine learning algorithms for improving structural discrimination and resolution. In particular, we have the following speciﬁc aims: (1) We will develop a novel Autoencoder and Iterative region Matching (AIM) algorithm for marker-free alignment of image tilt-series to re- construct tomograms with improved resolution; (2) We will develop a saliency-based auto-picking algorithm for better detecting macromolecular complexes, and combine it with an innovative 2D-to-3D framework to further improve structure detection accuracy; (3) We will design an end-to-end convolutional model for pose-invariant clustering of subtomograms. This model will produce an initial clustering which will be reﬁned by a new subto- mogram averaging algorithm that automatically down-weights subtomograms of noise and little contribution; (4) We will perform experimental evaluations by using previously reported bacterial secretion systems and mito- chondrial ultrastructures datasets to improve the ﬁnal resolution. Implementing algorithms in Aims 1-3, we will develop a user-friendly open-source graphical user interface -tom to directly beneﬁt the scientiﬁc community.  -tom will be systematically compared with existing software including IMOD, EMAN2, and Relion on simulated and benchmark datasets. To facilitate distribution, -tom will be integrated into existing software platforms Sci- pion and TomoMiner. Our data-driven algorithms and software not only will facilitate and accelerate the future use of Cryo-ET, but also can be readily used on analyzing the existing large amounts of Cryo-ET data to im- prove our understanding of the structure, function, and spatial organization of macromolecular complexes in situ. Project Narrative This project will create a system of machine learning algorithms to accelerate and facilitate the use and re-use of the rapidly accumulating Cryo-ET datasets. For easy use, we will develop an open-source GUI -Tom (to be disseminated into the Scipion and TomoMiner software platforms) that streamlines the new approaches from the initial tomogram reconstruction step to the ﬁnal subtomogram averaging step. We will validate the performance of our system by applying it on published Cryo-ET datasets and monitor the improvement of the ﬁnal results.",Novel machine learning approaches for improving structural discrimination in cryo-electron tomography,9973462,R01GM134020,"['3-Dimensional', 'Algorithmic Software', 'Algorithms', 'Back', 'Benchmarking', 'Biological Process', 'Cells', 'Communities', 'Computer Analysis', 'Computer software', 'Cryo-electron tomography', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Discrimination', 'Evaluation', 'Future', 'Gaussian model', 'Group Structure', 'Hour', 'Image', 'In Situ', 'Knowledge', 'Laplacian', 'Literature', 'Machine Learning', 'Macromolecular Complexes', 'Manuals', 'Methods', 'Mitochondria', 'Modeling', 'Molecular Conformation', 'Monitor', 'Neurophysiology - biologic function', 'Noise', 'Organelles', 'Performance', 'Process', 'Publishing', 'Reporting', 'Resolution', 'Series', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tomogram', 'Weight', 'Work', 'autoencoder', 'automated algorithm', 'base', 'deep learning', 'design', 'falls', 'feature detection', 'graphical user interface', 'improved', 'innovation', 'insight', 'machine learning algorithm', 'nano', 'nanometer resolution', 'novel', 'novel strategies', 'open source', 'particle', 'pi-Mesons', 'programs', 'reconstruction', 'success', 'user-friendly']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,342970,-0.015109129811987046
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10057158,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'structured data', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2020,795873,-0.00875796450433728
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9955351,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical data science', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'large scale data', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,517554,0.01644675272617869
"Deep Learning Algorithms for FreeSurfer Abstract FreeSurfer is a tool for the analysis of Magnetic Resonance Imaging (MRI) that has proven to be a flexible and powerful technology for quantifying the effects of many conditions, including numerous neurological disorders, on human brain anatomy, connectivity, vasculature, chemical composition, physiology and function. In the past 20 years, these open source tools have been developed to accurately and automatically segment an array of brain structures and have become the core analysis infrastructure for the Alzheimer’s Disease NeuroImaging Initiative (ADNI). In this project, we seek the resources to radically increase the speed, accuracy and flexibility of these tools, taking advantage of exciting new results in Deep Learning. This will enable us to more accurately quantify neuroanatomical changes that are critical to diagnosing, staging and assessing the efficacy of potential therapeutic interventions in diseases such as Alzheimer’s. This includes the generation of documentation, tutorials, unit tests, regression tests and system tests to harden the tools and make them usable by clinicians and neuroscientists, and finally the distribution and support of the data, manual labelings and tools to the more than 40,000 researchers that use FreeSurfer through our existing open source mechanism. In addition, we will analyze the entire Alzheimer’s Disease NeuroImaging Initiative dataset and return it for public release, including a set of manually labeled data that can be used to optimize Deep Learning tools for Alzheimer’s Disease over the next decade. Relevance Successful completion of the proposed project will increase the usability and accuracy of our publicly available segmentation tools, and open up new possibilities, such as integrating them into the MRI scanner and rapidly detecting Alzheimer’s-related changes. These new capabilities well enable other studies to significantly increase their ability to detect AD and other disease effects in research settings as well as phase II and phase III clinical trials due to the radical increase in speed of the new tools, enabling them to be applied to a diverse set of MRI contrasts and much larger datasets, rapidly and accurately. Further, they will allow rapid application of cutting-edge analyses to the ongoing Alzheimer’s Disease NeuroImaging Initiative dataset, improving the ability to extract early biomarkers of this devastating disease.",Deep Learning Algorithms for FreeSurfer,9970009,R01AG064027,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Chemicals', 'Code', 'Communities', 'Data', 'Data Set', 'Diagnosis', 'Disease', 'Documentation', 'Engineering', 'Ensure', 'Excision', 'Functional Magnetic Resonance Imaging', 'Future', 'Generations', 'Hour', 'Human', 'Image', 'Infrastructure', 'Label', 'Licensing', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Memory', 'Modeling', 'Neurobiology', 'Pattern', 'Phase II Clinical Trials', 'Phase III Clinical Trials', 'Physiology', 'Population', 'Procedures', 'Publishing', 'Recording of previous events', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Rest', 'Sensitivity and Specificity', 'Speed', 'Staging', 'Stream', 'Structure', 'Surface', 'System', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Validation', 'Variant', 'Work', 'base', 'contrast imaging', 'convolutional neural network', 'cranium', 'deep learning', 'deep learning algorithm', 'early detection biomarkers', 'flexibility', 'high resolution imaging', 'human disease', 'improved', 'large datasets', 'morphometry', 'nervous system disorder', 'neuroimaging', 'novel', 'open source', 'prevent', 'prototype', 'skills', 'spatial relationship', 'support tools', 'tool', 'usability', 'web site', 'wiki']",NIA,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,649026,-0.08449542235792291
"Large-scale annotation-free disease correlation analysis of the iHMP Project Summary We will work with the iHMP data resource to apply novel tools and data analysis methodologies to the challenge of disease association between large microbiome data sets, Inflammatory Bowel Disease, and the onset of diabetes. We will start with an annotation-free approach using k-mers to preprocess IBD and diabetes cohorts. We then will apply a novel scaling technology implemented in the sourmash software to reduce the data set size by a factor of 2000, rendering it tractable to machine learning approaches. We next will use random forests to determine a subset of predictive k-mers, and will measure their accuracy on validation data sets not used in the initial training. Finally, we will annotate the predictive k-mers using all available genome databases as well as a novel method to infer the metagenomic presence of accessory genomes of known genomes. Our outcomes will include a catalog of microbial genomes that correlate with IBD subtype and the onset of diabetes, as well as automated workflows to apply similar approaches to other data sets. Project Narrative We propose to work with the iHMP data, a large central microbiome resource, to study disease correlations with inflammatory bowel disease and diabetes. We will work to associate specific microbial species with the disease conditions. We will also produce resources that will help other researchers perform similar studies.",Large-scale annotation-free disease correlation analysis of the iHMP,10112077,R03OD030596,"['Catalogs', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diabetes Mellitus', 'Disease', 'Ecology', 'Genbank', 'Genome', 'Human', 'Immunoglobulin Variable Region', 'Inflammatory Bowel Diseases', 'Machine Learning', 'Measures', 'Metadata', 'Metagenomics', 'Methodology', 'Methods', 'Onset of illness', 'Organism', 'Outcome', 'Reporting', 'Research Personnel', 'Resources', 'Technology', 'Training', 'Update', 'Validation', 'Variant', 'Viral', 'Work', 'cohort', 'data resource', 'genome database', 'member', 'metagenome', 'metatranscriptome', 'metatranscriptomics', 'microbial', 'microbial genome', 'microbiome', 'novel', 'random forest', 'tool']",OD,UNIVERSITY OF CALIFORNIA AT DAVIS,R03,2020,304918,-0.0197391287890455
"Center for Critical Assessment of Genome Interpretation Genomic data hold the promise of revolutionizing our understanding and treatment of human disease. Multiple barriers stand between the acquisition of the data and realizing these and other benefits. Rapid accumulation of genomic data far exceeds our capacity to reliably interpret genomic variation. New developments in artificial intelligence and machine learning, combined with increased computing power and domain knowledge, provide hope for the deployment of enhanced computational tools in both basic research and clinical practice. Use of these methods critically depends upon reliable characterization of their performance.  The Center for Critical Assessment of Genome Interpretation (C-CAGI) will address these needs, through objective evaluation of the state of the art in relating human genetic variation and health. CAGI has had five editions since 2010 with 50 challenges posed to the community taken on by hundreds of predictors, leading to scores of publications about prediction methods and their assessment. We propose for C-CAGI to continue to advance the field of variant interpretation through the following Specific Aims: 1. Develop community experiments to evaluate the quality of computational methods for interpreting genomic variation data. C-CAGI will conduct community experiments in which participants make bona fide blinded predictions of disease related phenotypes on the basis of genomic data. We will engage a diverse predictor community to spur innovation. The CAGI Ethics Forum will vet studies to ensure that privacy and sharing maintain the highest standards and will educate the community. 2. Assess the quality of current computational methods for interpreting genomic variation data; highlight innovations and progress at interactive conferences. Predictions will be evaluated by independent assessors, who will be supported by new assessment approaches from C-CAGI. Results will be presented at CAGI experiment conferences with deep technical engagement, which will be interleaved with reflective CAGIâ meetings that create an environment for a comprehensive evaluation of the field, facilitating identification of major bottlenecks and problems faced by the current genome interpretation approaches. 3. Broadly disseminate the results and conclusions from the CAGI experiments and analysis. C-CAGI will outreach to the broader scientific and clinical community through its publications, and the creation of a calibrated reference integrated into the most common workflows for ready adoption. CAGI will also be represented at international meetings with presentations and workshops. 4. Operate effectively and responsively. C-CAGI will operate efficiently as it closely interacts with hundreds of participants. CAGI will build upon a robust information infrastructure that securely facilitates data dissemination, prediction submission, and assessment. Genomic variation is responsible for numerous rare diseases, for propensity for many common traits and diseases, for drug response, and is a key characteristic of cancer evolution. At present, our ability to characterize genetic differences far exceeds our capacity to interpret them either for basic research understanding or for clinical application. The Center for Critical Assessment of Genome Interpretation, operating on robust ethical foundations, will provide an evaluation of the current state of the art and help promote progress in understanding the impact of genomic variation.",Center for Critical Assessment of Genome Interpretation,9937546,U24HG007346,"['Address', 'Adoption', 'Affect', 'Amino Acid Sequence', 'Artificial Intelligence', 'Basic Science', 'Blinded', 'Characteristics', 'Clinical', 'Communities', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Ensure', 'Environment', 'Ethics', 'Evaluation', 'Evolution', 'Foundations', 'Genetic', 'Genetic Variation', 'Genome', 'Health', 'Human Genetics', 'Infrastructure', 'International', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Nucleotides', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Privacy', 'Provider', 'Publications', 'RNA Splicing', 'Rare Diseases', 'Secure', 'Structure', 'Trust', 'Variant', 'Work', 'base', 'clinical application', 'clinical practice', 'computerized tools', 'data acquisition', 'data dissemination', 'exome', 'experimental study', 'genetic information', 'genetic variant', 'genomic data', 'genomic variation', 'high standard', 'human disease', 'innovation', 'meetings', 'multiple omics', 'operation', 'outreach', 'response', 'symposium', 'trait', 'whole genome']",NHGRI,UNIVERSITY OF CALIFORNIA BERKELEY,U24,2020,314933,-0.030676188383464725
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9969467,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Models', 'Computer software', 'Computers', 'Custom', 'Data', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'algorithm development', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'large datasets', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,614363,0.002977694011856563
"Abiotic-Biotic Interfaces for Ophthalmology Symposium ABSTRACT This proposal seeks funding to support a symposium, Abiotic-Biotic Interfaces for Ophthalmology (ABI), which will bring together recognized world experts in clinical, research, vision science, engineering, industrial and pharmaceutical communities as well as junior investigators (i.e., young faculty and those in training) to discuss the current state of ABI, ranging from bioelectronic implantable and wearable devices, to nanoscale scaffolds for stem cell and gene therapies. Given the multidisciplinary nature of this field, it is essential to bring together researchers and clinicians with varying levels of expertise across many domains related to ABI to advance the progress of this novel field, identify challenges of advancement, and develop a strategic action plan to overcome these challenges. The timing to have such a symposium to further the application of implantable and/or wearable bioengineered systems in ophthalmology is now as we focus on precision and personalized medicine and leverage the revolution in deep learning artificial intelligence algorithms. Through symposium talks, sessions, and discussions we will cover the fundamentals and also identify innovative and cutting-edge strategies and methodologies to accelerate the rate of major discoveries and development of novel therapeutics. The specific aims of this symposium are: Specific Aim 1. To bring together both established and junior investigators representing a broad range of disciplines to discuss cutting edge research in this novel field, catalyze the development of cross-disciplinary and translational approaches to advance abiotic-biotic interfaces for ophthalmology, and identify gaps in knowledge and barriers to advancement. We will identify research questions and develop an agenda to guide future research that is consistent with the objectives and interests of NEI. Specific Aim 2. Develop a junior investigator program to motivate a diverse group of students and junior investigators to pursue research careers in vision science and ophthalmologic therapeutic development, who will ultimately submit grant proposals to NEI solicitations and contribute to the scientific literature. Specific Aim 3. Develop a strategic action plan to set priorities for future studies that will encourage inter-agency collaborations (e.g., NEI, NSF, DARPA, etc.). This is critical because often certain engineering tasks are best suited to be supported by NSF or DARPA whereas the biological testing of the engineered systems lends itself to funding from NEI. Hence such inter-agency or cross-agency efforts can help leverage the funding to develop sophisticated abiotic-biotic systems NARRATIVE This meeting is the first on this topic dedicated to the broad use of implantable and/or wearable bioelectronics for ophthalmological applications. It is anticipated that the strategic action plan will significantly impact the field by greatly accelerating the translation of basic science and engineering research findings to stimulate the development of novel treatments and improve clinical practice. Key topics include visual restoration, drug and gene delivery, and sensing intraocular pressure. This meeting will foster training and development of future leaders in this emerging field and promote collaboration and exchange of knowledge and ideas among junior and established investigators.",Abiotic-Biotic Interfaces for Ophthalmology Symposium,10070800,R13EY031988,"['Algorithms', 'Applications Grants', 'Artificial Intelligence', 'Basic Science', 'Biological Testing', 'Biomedical Engineering', 'Cellular Phone', 'Clinical Research', 'Collaborations', 'Communities', 'Computer software', 'Contact Lenses', 'Custom', 'Data', 'Development', 'Devices', 'Diagnosis', 'Discipline', 'Disease', 'Drug Delivery Systems', 'Electronics', 'Engineering', 'Eye', 'Faculty', 'Fostering', 'Funding', 'Future', 'Gene Delivery', 'Glass', 'Industrialization', 'Intraocular lens implant device', 'Knowledge', 'Literature', 'Medicine', 'Methodology', 'Nature', 'Neural Retina', 'Ophthalmology', 'Optics', 'Pharmacologic Substance', 'Physiologic Intraocular Pressure', 'Physiological', 'Research', 'Research Personnel', 'Route', 'Scientific Inquiry', 'Scientist', 'Senior Scientist', 'Students', 'System', 'Time', 'Training', 'Translations', 'Virtual and Augmented reality', 'Visual', 'base', 'career', 'clinical practice', 'deep learning', 'gene therapy', 'implantable device', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'meetings', 'multidisciplinary', 'nanoscale', 'neural network', 'novel', 'novel therapeutics', 'personalized medicine', 'portability', 'precision medicine', 'programs', 'restoration', 'scaffold', 'stem cell therapy', 'symposium', 'therapeutic development', 'translational approach', 'vision science', 'wearable device']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R13,2020,42465,-0.023272157180756065
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10058463,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data reuse', 'data sharing', 'data visualization', 'data warehouse', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,627034,-0.005276702596176724
"SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder  Intellectual Merit: This project will for the first time provide the fundamental tools to integrate unique multimodal data toward screening, diagnosis, and intervention in eating disorders, with an initial focus on children with ARFID and related developmental and health disorders. This work is critical for enriching the understanding of healthy development and for broadening the foundations of behavioral data science. ARFID ·motivates the development of new computer vision and data analysis tools critical for the analysis of multidimensional behavioral data. The main aims are: 1. Develop and user individualized and integrated continuous facial affect coding from videos to discern affective motivations for food avoidance, critical due to the unique sensory aspects of eating disorders, and resulting from active stimulation via friendly and carefully designed images/videos and real food presentation; 2. Use data analysis and machine learning to derive sensory profiles based on patterns of food consumption and preference from existing unique datasets of selective eaters; and 3. Translate the tools developed in Aims 1 and 2 into the clinic and home to assess the capacity of these tools to define a threshold of clinically significant food avoidance, to detect change in acceptability of food with repeated presentations, and to examine and modify the accuracy of our food suggestion algorithms. Broader Impacts: The impact of this application comprises two broad domains. First is the derivation of processes, tools, and strategies to analyze very disparate data across multiple levels of analysis and to codify those strategies to inform similar future work, in particular incorporating automatic behavioral coding. Second is the exploitation of these tools to address questions about the emergence of healthy/unhealthy food selectivity across the lifespan, including recommendation delivery via apps and at-home recordings. The health impact of even partial success in this project is very broad and significant. Undergraduate students will be involved in this project via the 6-weeks summer research program at the Information Initiative at Duke, a center dedicated to the fundamentals of data science and its applications; via the co-Pl's research lab devoted to eating disorders; and via the Pl's project dedicated to training undergraduate students to address eating disorders of their friends via an anonymous app. Outreach and dissemination will follow the broad use of the developed app, both in the clinic and the general population, including the Pl's connections with low-income and under-represented bi-lingual preK. RELEVANCE (See instructions): Eating disorders are potentially life-threatening mental illnesses affecting the general population; -90% of individuals never receive treatment, in part due to lack of awareness and access. Individuals with eating disorders experience a diminished quality of life, high mental and physical illness comorbidities, and an existence marked by profound loneliness and isolation. Combining expertise in eating disorders with computer vision and machine learning, we bring for the first time data science to this health challenge. PROJECT/PERFORMANCE S1TE(S) (If addItIonal space Is needed use Project/Performance Stte Format Page) n/a",SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder ,10022332,R01MH122370,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anxiety', 'Assessment tool', 'Attention', 'Awareness', 'Behavior Therapy', 'Behavioral', 'Caregivers', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Depressed mood', 'Derivation procedure', 'Development', 'Diagnosis', 'Diet', 'Disease', 'Distress', 'Eating', 'Eating Disorders', 'Emotional', 'Emotions', 'Evolution', 'Exposure to', 'Face', 'Family', 'Food', 'Food Patterns', 'Food Preferences', 'Foundations', 'Friends', 'Fright', 'Future', 'General Population', 'Goals', 'Health', 'Health Personnel', 'Home environment', 'Image', 'Impairment', 'Individual', 'Industry', 'Instruction', 'Intervention', 'Life', 'Link', 'Literature', 'Loneliness', 'Longevity', 'Low income', 'Machine Learning', 'Maps', 'Mathematics', 'Measures', 'Mental disorders', 'Monitor', 'Motion', 'Motivation', 'Parents', 'Performance', 'Phenotype', 'Primary Health Care', 'Process', 'Psyche structure', 'Quality of life', 'Reaction', 'Recommendation', 'Research', 'Scientist', 'Sensory', 'Severities', 'Smell Perception', 'Standardization', 'Structure', 'Suggestion', 'System', 'Taste aversion', 'Time', 'Training', 'Translating', 'Uncertainty', 'Work', 'analytical tool', 'base', 'behavior change', 'clinically significant', 'comorbidity', 'computerized tools', 'design', 'experience', 'food avoidance', 'food consumption', 'gaze', 'improved', 'indexing', 'mathematical algorithm', 'multimodal data', 'novel', 'outreach', 'precision medicine', 'preference', 'programs', 'relating to nervous system', 'response', 'screening', 'success', 'summer research', 'tool', 'undergraduate student', 'wasting', 'willingness']",NIMH,DUKE UNIVERSITY,R01,2020,243885,-0.04501427277195322
"SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder  Intellectual Merit: This project will for the first time provide the fundamental tools to integrate unique multimodal data toward screening, diagnosis, and intervention in eating disorders, with an initial focus on children with ARFID and related developmental and health disorders. This work is critical for enriching the understanding of healthy development and for broadening the foundations of behavioral data science. ARFID ·motivates the development of new computer vision and data analysis tools critical for the analysis of multidimensional behavioral data. The main aims are: 1. Develop and user individualized and integrated continuous facial affect coding from videos to discern affective motivations for food avoidance, critical due to the unique sensory aspects of eating disorders, and resulting from active stimulation via friendly and carefully designed images/videos and real food presentation; 2. Use data analysis and machine learning to derive sensory profiles based on patterns of food consumption and preference from existing unique datasets of selective eaters; and 3. Translate the tools developed in Aims 1 and 2 into the clinic and home to assess the capacity of these tools to define a threshold of clinically significant food avoidance, to detect change in acceptability of food with repeated presentations, and to examine and modify the accuracy of our food suggestion algorithms. Broader Impacts: The impact of this application comprises two broad domains. First is the derivation of processes, tools, and strategies to analyze very disparate data across multiple levels of analysis and to codify those strategies to inform similar future work, in particular incorporating automatic behavioral coding. Second is the exploitation of these tools to address questions about the emergence of healthy/unhealthy food selectivity across the lifespan, including recommendation delivery via apps and at-home recordings. The health impact of even partial success in this project is very broad and significant. Undergraduate students will be involved in this project via the 6-weeks summer research program at the Information Initiative at Duke, a center dedicated to the fundamentals of data science and its applications; via the co-Pl's research lab devoted to eating disorders; and via the Pl's project dedicated to training undergraduate students to address eating disorders of their friends via an anonymous app. Outreach and dissemination will follow the broad use of the developed app, both in the clinic and the general population, including the Pl's connections with low-income and under-represented bi-lingual preK. RELEVANCE (See instructions): Eating disorders are potentially life-threatening mental illnesses affecting the general population; -90% of individuals never receive treatment, in part due to lack of awareness and access. Individuals with eating disorders experience a diminished quality of life, high mental and physical illness comorbidities, and an existence marked by profound loneliness and isolation. Combining expertise in eating disorders with computer vision and machine learning, we bring for the first time data science to this health challenge. PROJECT/PERFORMANCE S1TE(S) (If addItIonal space Is needed use Project/Performance Stte Format Page) n/a",SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder ,10228145,R01MH122370,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anxiety', 'Assessment tool', 'Attention', 'Awareness', 'Behavior Therapy', 'Behavioral', 'Caregivers', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Depressed mood', 'Derivation procedure', 'Development', 'Diagnosis', 'Diet', 'Disease', 'Distress', 'Eating', 'Eating Disorders', 'Emotional', 'Emotions', 'Evolution', 'Exposure to', 'Face', 'Family', 'Food', 'Food Patterns', 'Food Preferences', 'Foundations', 'Friends', 'Fright', 'Future', 'General Population', 'Goals', 'Health', 'Health Personnel', 'Home environment', 'Image', 'Impairment', 'Individual', 'Industry', 'Instruction', 'Intervention', 'Life', 'Link', 'Literature', 'Loneliness', 'Longevity', 'Low income', 'Machine Learning', 'Maps', 'Mathematics', 'Measures', 'Mental disorders', 'Monitor', 'Motion', 'Motivation', 'Parents', 'Performance', 'Phenotype', 'Primary Health Care', 'Process', 'Psyche structure', 'Quality of life', 'Reaction', 'Recommendation', 'Research', 'Scientist', 'Sensory', 'Severities', 'Smell Perception', 'Standardization', 'Structure', 'Suggestion', 'System', 'Taste aversion', 'Time', 'Training', 'Translating', 'Uncertainty', 'Work', 'analytical tool', 'base', 'behavior change', 'clinically significant', 'comorbidity', 'computerized tools', 'design', 'experience', 'food avoidance', 'food consumption', 'gaze', 'improved', 'indexing', 'mathematical algorithm', 'multimodal data', 'novel', 'outreach', 'precision medicine', 'preference', 'programs', 'relating to nervous system', 'response', 'screening', 'success', 'summer research', 'tool', 'undergraduate student', 'wasting', 'willingness']",NIMH,DUKE UNIVERSITY,R01,2020,59206,-0.04501427277195322
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9847973,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data cleaning', 'data ecosystem', 'data reuse', 'data sharing', 'data standards', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2020,489919,0.007600649464473578
"Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication,10111658,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'data standards', 'data streams', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF ARKANSAS AT FAYETTEVILLE,R01,2020,491992,-0.030475675242450984
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,-0.003705210241865795
"Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records PROJECT SUMMARY  Today, more patients can access their health records online than ever before. However, clinical acronyms hinder patients' comprehension of their records and decrease the benefits of transparency. An automated system for expanding clinical acronyms should have major clinical significance and far-reaching consequences for improving patient-provider communication, shared decision-making, and health outcomes. Existing systems have limited power to expand clinical acronyms, primarily due to the lack of comprehensiveness (or generali- zability) of existing acronym sense inventories. Because developing comprehensive sense inventories is difficult, existing knowledge engineering methods have primarily focused on developing institution-specific sense inventories. Institution-specific sense inventories may not be generalizable to other geographical regions and medical specialties. Furthermore, developing an institution-specific sense inventory at every US healthcare organization is not feasible, especially without automated methods which currently do not exist.  I developed advanced knowledge engineering methods to overcome these limitations through the use of fully automated techniques to generalize existing sense inventories from different geographical regions and medical specialties. My methods leverage the extensive resources already devoted to developing institution- specific sense inventories in the U.S., and may help generalize existing sense inventories to institutions without the resources to develop them. Although promising, challenges remain with the optimization and evaluation of these methods. The objective of the proposed project is to use knowledge engineering to improve patients' comprehension of their health records, focusing specifically on clinical acronyms. In Aim 1, I will develop new knowledge engineering methods to facilitate the automated integration of sense inventories, using literature- based quality heuristics and a Siamese neural network to establish synonymy. I will evaluate these methods using multiple metrics to assess redundancy, quality, and coverage in two test corpora with over 17 million clinical notes. In Aim 2, I will evaluate whether the knowledge engineering methods improve comprehension of doctors' notes in 60 hospitalized patients with advanced heart failure. With success, I will create novel, automated knowledge engineering methods that can be directly applied to improve patient care. This research is in support of my mentored doctoral training at Columbia University Department of Biomedical Informatics (DBMI) under Drs. David Vawdrey, George Hripcsak, Carol Friedman, Suzanne Bakken, and Chunhua Weng, and will include coursework on deep learning, oral presentations at major annual conferences, and career development planning, among other activities. DBMI is frequently recognized as one of the oldest and best programs of its kind in the world, and provides an exception training environment for my development into an independent and productive academic investigator. PROJECT NARRATIVE Clinical acronyms make it difficult for patients to understand their medical records, decreasing the benefits of transparency. This project applies advanced knowledge engineering methods and machine learning to generate comprehensive acronym sense inventories used to aid consumers' comprehension of their health records. The project is in support of the applicant's mentored doctoral dissertation research.",Automated Knowledge Engineering Methods to Improve Consumers' Comprehension of their Health Records,9895430,F31LM013054,"['Abbreviations', 'Award', 'Clinical', 'Clinical Medicine', 'Comprehension', 'Controlled Vocabulary', 'Development', 'Development Plans', 'Engineering', 'Environment', 'Equipment and supply inventories', 'Evaluation', 'Future', 'Geographic Locations', 'Goals', 'Grant', 'Health', 'Healthcare', 'Heart failure', 'Hospitals', 'Informatics', 'Information Resources', 'Institution', 'Knowledge', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Mentors', 'Mentorship', 'Methods', 'Natural Language Processing', 'Oral', 'Outcome', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Positioning Attribute', 'Publishing', 'Questionnaires', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Safety', 'Source', 'Support System', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Unified Medical Language System', 'Universities', 'acronyms', 'base', 'biomedical informatics', 'career development', 'clinically significant', 'deep learning', 'doctoral student', 'federal policy', 'health care service organization', 'health record', 'heuristics', 'improved', 'information organization', 'medical specialties', 'method development', 'multidisciplinary', 'neural network', 'novel', 'patient portal', 'patient-clinician communication', 'programs', 'shared decision making', 'success', 'symposium', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,F31,2020,47355,-0.01475216602598943
"Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution Project Summary/Abstract It is hard to overstate the importance of monoclonal antibodies in the life sciences. Antibodies are critical tools in biomedical research and diagnostics (e.g. western blotting, immunoprecipitation, cytometry, biomarker discovery, and histology), are one of the most rapidly growing class of therapeutics, and are the basis for myriad new strategies in cancer therapy, such as checkpoint inhibitors that are revolutionizing treatment. Unfortunately, current methods for the generation of custom antibodies, including animal immunization and phage display, are slow, costly, inaccessible to most researchers, and often unsuccessful. We propose Autonomously EvolvinG Yeast-displayed antibodieS (AEGYS), a system for the continuous and rapid evolution of high-quality antibodies against custom antigens that requires only the simple culturing of yeast cells. We believe this can be achieved by combining cutting-edge generative machine learning algorithms for antibody library design with a new technology for in vivo continuous evolution and a yeast antigen-presenting cell that we will engineer. If successful, AEGYS should have a transformative impact across the whole of biomedicine by turning monoclonal antibody generation into a rapid, scalable, and accessible process where any lab with standard molecular biology capabilities can generate custom antibodies on demand simply by “immunizing” a test tube of yeast cells with an antigen. We anticipate that this democratization of antibody generation will also result in an explosion of crowdsourced antibody sequence data that will train our machine learning algorithms to design better antibody libraries for AEGYS, starting a virtuous cycle. We ourselves will use AEGYS to generate a panel of subtype- and conformation-specific nanobodies against biogenic amine receptors including those that respond to acetylcholine, adrenaline, dopamine, and other neurotransmitters, so that we can understand their role in neurobiology and addiction.! Project Narrative This proposal will provide a system for the scalable continuous evolution and computational design of antibodies against user-selected antigens. Antibodies are critical tools in medical research and are the basis for numerous therapies, but the generation of custom antibodies against new targets is a difficult and specialized task. The system proposed will turn antibody generation into a routine and widely accessible process for researchers in almost any field.","Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution",10021311,R01CA260415,"['Acetylcholine', 'Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antibody Formation', 'Antigen Targeting', 'Antigen-Presenting Cells', 'Antigens', 'Architecture', 'Area', 'Back', 'Biogenic Amine Receptors', 'Biological Sciences', 'Biomedical Research', 'Cell Surface Receptors', 'Cells', 'Chemistry', 'Clinic', 'Collection', 'Communities', 'Cultured Cells', 'Custom', 'Cytometry', 'Data', 'Data Set', 'Detergents', 'Diagnostic', 'Directed Molecular Evolution', 'Docking', 'Dopamine', 'Elements', 'Engineering', 'Epidemic', 'Epinephrine', 'Evolution', 'Explosion', 'G-Protein-Coupled Receptors', 'Generations', 'Genes', 'Genetic', 'Histology', 'Human', 'Hybridomas', 'Image', 'Immune checkpoint inhibitor', 'Immune system', 'Immunization', 'Immunize', 'Immunoglobulin Fragments', 'Immunoprecipitation', 'Libraries', 'Machine Learning', 'Medical Research', 'Medicine', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Monoclonal Antibodies', 'Neuraxis', 'Neurobiology', 'Neurosciences', 'Neurotransmitters', 'Nobel Prize', 'Outcome', 'Pathogen detection', 'Phage Display', 'Pharmaceutical Preparations', 'Pheromone', 'Play', 'Problem Solving', 'Process', 'Production', 'Protein Engineering', 'Proteins', 'Proteome', 'Public Health', 'Reagent', 'Research', 'Research Personnel', 'Role', 'Signal Transduction', 'Specificity', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Training', 'Tube', 'Update', 'V(D)J Recombination', 'Western Blotting', 'Yeasts', 'addiction', 'antibody engineering', 'antibody libraries', 'antigen binding', 'base', 'biomarker discovery', 'cancer therapy', 'cost', 'crowdsourcing', 'decision research', 'design', 'empowered', 'experimental study', 'follow-up', 'improved', 'in vivo', 'innovation', 'insight', 'interest', 'machine learning algorithm', 'nanobodies', 'new technology', 'novel', 'receptor', 'response', 'scaffold', 'structural biology', 'tool']",NCI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2020,1690552,-0.018307104148197993
"National Resource for Network Biology (NRNB) OVERALL - PROJECT SUMMARY The mission of the National Resource for Network Biology (NRNB) is to advance the science of biological networks by creating leading-edge bioinformatic methods, software tools and infrastructure, and by engaging the scientific community in a portfolio of collaboration and training opportunities. Much of biomedical research is dependent on knowledge of biological networks of multiple types and scales, including molecular interactions among genes, proteins, metabolites and drugs; cell communication systems; relationships among genotypes and biological and clinical phenotypes; and patient and social networks. NRNB-supported platforms like Cytoscape are among the most widely used software tools in biology, with tens of thousands of active users, enabling researchers to apply network concepts and data to understand biological systems and how they are reprogrammed in disease.  NRNB’s three Technology Research and Development projects introduce innovative concepts with the potential to transform network biology, transitioning it from a static to a dynamic science (TR&D 1); from flat network diagrams to multi-scale hierarchies of biological structure and function (TR&D 2); and from descriptive interaction maps to predictive and interpretable machine learning models (TR&D 3). In previous funding periods our technology projects have produced novel and highly cited approaches, including network-based biomarkers for stratification of disease, data-driven gene ontologies assembled completely from network data, and deep learning models of cell structure and function built using biological networks as a scaffold.  During the next period of support, we introduce dynamic regulatory networks formulated from single-cell transcriptomics and advanced proteomics data (TR&D 1); substantially improved methodology for the study of hierarchical structure and pleiotropy in biological networks (TR&D 2); and procedures for using networks to seed machine learning models of drug response that are both mechanistically interpretable and transferable across biomedical contexts (TR&D 3). These efforts are developed and applied in close collaboration with outside investigators from 19 Driving Biomedical Projects who specialize in experimental generation of network data, disease biology (cancer, neuropsychiatric disorders, diabetes), single-cell developmental biology, and clinical trials. TR&Ds are also bolstered by 7 Technology Partnerships in which NRNB scientists coordinate technology development with leading resource-development groups in gene function prediction, mathematics and algorithm development, and biomedical databases. Beyond these driving collaborations, we continually support a broader portfolio of transient (non-driving) research collaborations; organize and lead international meetings including the popular Network Biology track of the Intelligent Systems for Molecular Biology conference; and deliver a rich set of training opportunities and network analysis protocols. OVERALL - PROJECT NARRATIVE We are all familiar with some of the components of biological systems – DNA, proteins, cells, organs, individuals – but understanding biological systems involves more than just cataloging its component parts. It is critical to understand the many interactions of these parts within systems, and how these systems give rise to biological functions and responses and determine states of health and disease. The National Resource for Network Biology provides the scientific community with a broad platform of computational tools for the study of biological networks and for incorporating network knowledge in biomedical research.",National Resource for Network Biology (NRNB),9937486,P41GM103504,"['Area', 'Automobile Driving', 'Beds', 'Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Biomedical Technology', 'Cataloging', 'Catalogs', 'Cell Communication', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Conceptions', 'DNA', 'Data', 'Data Set', 'Databases', 'Developmental Cell Biology', 'Diabetes Mellitus', 'Disease', 'Disease stratification', 'Drug Modelings', 'Ecosystem', 'Educational workshop', 'Event', 'Expert Systems', 'Feedback', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic Risk', 'Genomics', 'Genotype', 'Goals', 'Health', 'Individual', 'Infrastructure', 'International', 'Knowledge', 'Lead', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mentors', 'Methodological Studies', 'Methods', 'Mission', 'Modeling', 'Molecular Biology', 'National Institute of General Medical Sciences', 'Network-based', 'Ontology', 'Organ', 'Pathway Analysis', 'Patients', 'Pharmaceutical Preparations', 'Phase Transition', 'Phenotype', 'Positioning Attribute', 'Procedures', 'Proteins', 'Proteomics', 'Protocols documentation', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Running', 'Science', 'Scientist', 'Seeds', 'Services', 'Social Network', 'Software Tools', 'Structure', 'Students', 'System', 'Technology', 'Testing', 'Tissues', 'Training', 'Visual', 'Visualization', 'Work', 'algorithm development', 'biological systems', 'clinical phenotype', 'cloud storage', 'computational platform', 'computerized tools', 'deep learning', 'gene function', 'genomics cloud', 'improved', 'innovation', 'interoperability', 'lens', 'mathematical algorithm', 'meetings', 'method development', 'multi-scale modeling', 'neuropsychiatric disorder', 'next generation', 'novel', 'pleiotropism', 'prediction algorithm', 'programs', 'protein metabolite', 'response', 'scaffold', 'single cell analysis', 'software infrastructure', 'symposium', 'technology development', 'technology research and development', 'tool', 'training opportunity', 'transcriptome', 'transcriptomics']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",P41,2020,1995376,-0.011970365781562577
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,10136941,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'learning classifier', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2020,746725,0.0013878550358259747
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,-0.014799574920723653
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,9987133,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,313495,-0.021192055323359358
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9961522,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2020,15000,-0.018150819274557455
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10237828,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Information Retrieval', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2020,500000,-0.025439978737569813
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9874005,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,360227,-0.0077085986250331785
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9920181,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,249000,-0.001815144945396715
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,10016840,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data curation', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'large scale data', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2020,330502,-0.007421085906148131
"Learning Dynamics of Biological Processes from Time Course Omics Datasets Complex biological processes, including organ development, immune response and disease progression, are inherently dynamic. Learning their regulatory architecture requires understanding how components of a large system dynamically interact with each other and give rise to emergent behavior. Recent experimental advances have made ii possible to investigate these biological systems in a data-driven fashion al high temporal resolution, allowing identification of new genes and their regulatory interactions. Longitudinal omics data sets are becoming increasingly common in clinical practice as well. Information on these collections of interacting genes can be integrated to gain systems-level insights into the roles of biological pathways and processes, including progression of diseases. Consequently, developing interpretable methods for learning functional relationships among genes, proteins or metabolites from high-dimensional time series data has become a timely research problem. The nature of these time-course data sets presents exciting opportunities and interesting challenges from a statistical perspective. Typical time-course omics data sets are challenging because of their high-dimensionality and non-linear relationships among system components. To tackle these challenges, one needs sophisticated dimension-reduction techniques that are biologically meaningful, computationally efficient and allow uncertainty quantification. Methods that incorporate prior biological information (e.g., pathway membership, protein-protein interactions) into the data analysis are good candidates for analyzing such high-dimensional systems using small samples. Here, we will develop three core methods to address the above challenges - (Aim 1): an empirical Bayes framework for clustering high-dimensional omics time-course data using prior biological knowledge; (Aim 2): a quantile-based Granger causality framework for learning interactions among genes or metabolites from their lead-lag relationships; and (Aim 3): a decision tree ensemble framework for searching cascades of interactions among genes from their temporal expression profiles. Our interdisciplinary team of statisticians and scientists will analyze time-course omics data from three research projects: (i) innate immune response systems in Drosophila, (ii) developmental process in mouse models, and (ii) longitudinal metabolite profiling of TB patients. These insights will be used to build and validate our methodology, which will be implemented in a publicly available software. This proposal is innovative in its incorporation of prior biological knowledge in the framework of novel dimension reduction techniques for interrogating high-dimensional time-course omics data. This research is significant in that it will impact basic sciences by elucidating data-driven, testable hypotheses on the regulatory architecture of biological processes, and clinical practice by monitoring disease progression and prognosis. n/a",Learning Dynamics of Biological Processes from Time Course Omics Datasets,10021429,R01GM135926,"['Address', 'Algorithms', 'Architecture', 'Basic Science', 'Behavior', 'Biological', 'Biological Process', 'Clinical', 'Collection', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Developmental Process', 'Dimensions', 'Disease Progression', 'Drosophila genus', 'Etiology', 'Expression Profiling', 'Gene Proteins', 'Genes', 'Grouping', 'Immune System Diseases', 'Immune response', 'Innate Immune Response', 'Knowledge', 'Lead', 'Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pattern', 'Process', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Scientist', 'Series', 'Silicon Dioxide', 'Structure', 'System', 'Techniques', 'Time', 'Uncertainty', 'Validation', 'Variant', 'base', 'biological systems', 'clinical practice', 'dynamic system', 'experimental study', 'high dimensionality', 'innovation', 'insight', 'learning algorithm', 'learning strategy', 'mouse model', 'novel', 'open source', 'organ growth', 'outcome forecast', 'protein protein interaction', 'random forest', 'temporal measurement']",NIGMS,CORNELL UNIVERSITY,R01,2020,344345,-0.02528784947977275
"A decentralized macro and micro gene-by-environment interaction analysis of substance use behavior and its brain biomarkers   Abstract Wide-spread data sharing has started to permeate the brain imaging community from funders to researchers. However, in recent years there have also been some concerns raised regarding ethical issues related to privacy and data ownership among others. In the parent award we are leveraging and extending a privacy preserving decentralized data sharing platform called COINSTAC to perform a study of gene-by-environmental effects by pooling together data from across the world, some of which is unable to be openly shared. In this supplement we will study various bioethical issues related to different data sharing strategies. This will include calculating risk scores from existing data to evaluate the effectiveness of machine learning to potentially reidentify from similar or different data types, a detailed survey of various policy makers and stakeholders including researchers, federal employees, IRB members, and more, and finally the development of a forward looking white paper addressing both privacy, policy, and regulatory aspects which attempts to frame the various aspects that arise in the contact of the spectrum of data sharing approaches including fully open, ‘trust’ based via data usage agreements, privacy preserving via tools like COINSTAC, and more. The outcomes of this supplement will provide a useful guide for the field going forward and also provide initial data necessary to develop a larger scale project on these topics going forward. Narrative The era of big data, open science, and deep learning is upon us, and data sharing can be done in various ways ranging from fully open to privacy preserving to fully closed. However bioethical issues related to risk, privacy, and data sharing strategies have not been well studied in the context of combined brain imaging, genomics, and macro-environmental data and can be especially sensitive in the context of information such as substance use. In this supplement we will quantify risk levels, survey a broad community of stakeholders, and develop recommendations for the field going forward.",A decentralized macro and micro gene-by-environment interaction analysis of substance use behavior and its brain biomarkers  ,10131528,R01DA049238,"['Address', 'Adolescence', 'Adolescent', 'Age', 'Agreement', 'Alcohol or Other Drugs use', 'Anxiety', 'Award', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Big Data', 'Bioethical Issues', 'Biological Markers', 'Brain', 'Brain imaging', 'Brain scan', 'China', 'Climate', 'Communities', 'Complex', 'Computer software', 'Consumption', 'Country', 'Data', 'Data Protection', 'Decentralization', 'Development', 'Disease', 'Dropout', 'Economic Burden', 'Employee', 'Environment', 'Environmental Risk Factor', 'Epidemiology', 'Ethical Issues', 'Ethnic group', 'Europe', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heritability', 'Household', 'Image', 'Income', 'India', 'Individual', 'Institutional Review Boards', 'International', 'Intervention', 'Learning', 'Legal', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Mental Depression', 'Modeling', 'Movement', 'Neurobiology', 'Neurons', 'Neurosciences', 'Outcome', 'Ownership', 'Paper', 'Parents', 'Physiological', 'Policies', 'Policy Maker', 'Population', 'Population Density', 'Privacy', 'Quality Control', 'Race', 'Recommendation', 'Regulation', 'Research Personnel', 'Risk', 'Scanning', 'Smoking', 'Source Code', 'Structure', 'Surveys', 'System', 'Time', 'Trust', 'Twin Multiple Birth', 'Update', 'Urbanization', 'Visualization', 'adolescent substance use', 'base', 'cloud based', 'cognitive development', 'cohort', 'computerized tools', 'cost', 'data access', 'data exchange', 'data sharing', 'deep learning', 'drinking', 'early life stress', 'effectiveness evaluation', 'epidemiologic data', 'gene environment interaction', 'genetic profiling', 'human subject', 'imaging genetics', 'insight', 'member', 'neuroimaging', 'open data', 'open source', 'peer', 'privacy preservation', 'psychiatric symptom', 'relating to nervous system', 'rural area', 'sex', 'sharing platform', 'statistics', 'tool', 'tool development', 'urban area', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,149834,-0.030360396695005933
"Nathan Shock Center of Excellence in Basic Biology of Aging OVERALL PROJECT SUMMARY This application is for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington and affiliated institutions. This Center has over the past 25 years provided key resources in support of investigators who study the biology of aging. This application continues a theme that emphasizes outreach and service to the broadest community of investigators in the gerosciences. Of proximal relevance is the characterization of aging-related phenotypes of longevity and healthspan. As our Center services must be easily accessible to outside users, our Longevity and Healthspan Core (Core E) focuses on invertebrate assays, many of them novel. Two other Resources Cores focus on the high dimensional assessments that are closely related to aging phenotypes: Protein Phenotypes of Aging (Core C) and Metabolite Phenotypes of Aging (Core D). Sophisticated computational and bioinformatic tools for data analysis and optimal insight are provided by the Artificial Intelligence and Bioinformatics Core F. Each of these four Resource Cores is led by highly respected experts in that field, including Michael MacCoss and Judit Villen (Core C), Daniel Promislow (Core D), Matt Kaeberlein and Maitreya Dunham (Core E) and Su-In Lee (Core F). Each will push the envelope of appropriate technologies, developing new state-of-the art approaches for assessments that are the most applicable to gerontology and making them accessible to the national aging community. The Research Development Core (Core B) will continue to support pilot and junior faculty studies, with a firm focus on outreach of service to the national geroscience constituency. The Administrative and Program Enrichment Core (Core A) supports administrative management, an external advisory panel, courses, and data sharing and dissemination. Core A’s program of seminars and symposia will continue a focus on sponsorship and organization of national courses, meetings and pre-meetings, as well as workshops in the fields allied to our Resource Core Services. In coordination with other Nathan Shock Centers, we will support a new Geropathology Research initiative. UW NATHAN SHOCK CENTER OVERALL - PROJECT NARRATIVE We apply for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington, which has for 25 years provided key resources supporting investigators who study the biology of aging. The overarching goal of this Center is to have a positive impact on the field by accelerating research discovery and providing research support for investigators nationally and internationally, particularly junior investigators in the process of building their own research programs. We will accomplish this goal through six cores that function synergistically together: four Resource Cores with particular expertise in protein (Core C) and metabolite (Core D) phenotypes of aging, invertebrate longevity and healthspan phenotypes (Core E) and artificial intelligence and bioinformatics (Core F), along with a Research Development Core (Core B) that supports external pilot projects and junior faculty studies, and an Administrative and Program Enrichment Core (Core A) that supports administrative management, an external advisory panel, sponsorship and organization of national meetings and pre-meetings, courses, workshops and seminars, and, in coordination with other Nathan Shock Centers, a Geropathology Research initiative.",Nathan Shock Center of Excellence in Basic Biology of Aging,10042617,P30AG013280,"['Aging', 'Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'Biology of Aging', 'Collaborations', 'Communication', 'Communities', 'Consult', 'Data', 'Data Analyses', 'Development', 'Educational workshop', 'Environment', 'Experimental Designs', 'Faculty', 'Genes', 'Genetic study', 'Gerontology', 'Geroscience', 'Goals', 'Growth', 'Informatics', 'Institution', 'International', 'Invertebrates', 'Leadership', 'Longevity', 'Methodology', 'Methods', 'Microfluidics', 'Molecular Genetics', 'Office of Administrative Management', 'Pathway interactions', 'Phenotype', 'Philosophy', 'Pilot Projects', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Research', 'Research Activity', 'Research Personnel', 'Research Support', 'Resources', 'Robotics', 'Services', 'Shock', 'Statistical Data Interpretation', 'Technology', 'Transcript', 'Universities', 'Variant', 'Washington', 'bioinformatics tool', 'career development', 'cell age', 'computerized tools', 'data dissemination', 'data sharing', 'healthspan', 'high dimensionality', 'insight', 'meetings', 'metabolomics', 'multiple omics', 'novel', 'outreach', 'outreach services', 'programs', 'protein metabolite', 'research and development', 'symposium', 'tool', 'trait']",NIA,UNIVERSITY OF WASHINGTON,P30,2020,962037,-0.005136675425615396
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,10002330,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data curation', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2020,653481,-0.011268172344396055
"Clinical Research Education in Genome Science (CREiGS) Project Summary/Abstract  The sensitivity and availability of omic technologies have enabled the genomic, transcriptomic and proteomic characterization of disease phenotypes, at the tissue and even the single cell level. This has allowed development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. Patients of providers who have participated in these educational initiatives also benefit as it allows for more rapid integration of genomic study findings into the clinical care setting. Thus, in response to PAR-19-185, we propose to develop and implement the Clinical Research Education in Genome Science (CREiGS) program that will not only focus on the analysis of genomic data, but also on gene-expression data, the integration of these two data types, as well as introductory theory and application of statistical and machine learning methods. Specifically we propose to accomplish the following specific aims: 1. Develop and successfully implement the online and in-person phases of CREiGS to increase the methodologic ingenuity by which researchers tackle important genomics-related clinical problems. 2. Establish a Diversity Recruitment External Advisory Board to ensure that the most effective strategies are employed to recruit URM doctoral students, postdoctoral fellows, and faculty from academic institutions nationwide into CREiGS. 3. Enhance the dissemination phase of CREiGS by packaging and uploading the asynchronous lectures and the online critical thinking/problem solving assessments with solutions for publicly available, online teaching resources. 4. Implement effective methods to evaluate the efficacy of CREiGS by examining:1) the participants' grasp of the CREiGS core competencies, 2) the clarity and quality of the curriculum, 3) program logistics and operation, and 4) the participants' short-term and long-term success attributed to participation in CREiGS. In summary, we posit that CREiGS will provide participants with a solid foundation in genomics science to answer complex, clinical questions. We believe that CREiGS supports the mission of the NHGRI by providing researchers with rigorous training to “accelerate medical breakthroughs that improve human health.” Project Narrative The sensitivity and availability of omic technologies have allowed for the development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. The overall goal of the Clinical Research Education in Genome Science program is to increase the methodologic ingenuity of students, postdoctoral fellows, and faculty from academic institutions nationwide through a solid foundation in genomics science to answer complex, clinical research questions and improve patient care.",Clinical Research Education in Genome Science (CREiGS),9934567,R25HG011021,"['Area', 'Biomedical Research', 'Cells', 'Clinical', 'Clinical Data', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Critical Thinking', 'Data', 'Data Analyses', 'Development', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Hour', 'Human', 'Hybrids', 'Institution', 'Knowledge', 'Logistics', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Human Genome Research Institute', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Persons', 'Phase', 'Phenotype', 'Play', 'Postdoctoral Fellow', 'Problem Solving', 'Proteomics', 'Provider', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Role', 'Single Nucleotide Polymorphism', 'Solid', 'Statistical Methods', 'Students', 'Technology', 'Tissues', 'Training', 'Translational Research', 'Treatment outcome', 'Underrepresented Minority', 'Underserved Population', 'Update', 'cancer therapy', 'clinical care', 'clinical efficacy', 'computerized tools', 'data integration', 'data management', 'disease phenotype', 'disorder subtype', 'doctoral student', 'education research', 'genetic analysis', 'genome sciences', 'genomic data', 'grasp', 'health disparity', 'improved', 'individual patient', 'innovation', 'lectures', 'machine learning method', 'operation', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'response', 'statistical and machine learning', 'success', 'theories', 'therapy development', 'tool', 'transcriptomics', 'treatment optimization', 'virtual']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R25,2020,161662,-0.040540533052373345
"Highly Portable and Cloud-Enabled Neuroimaging Research: Confronting Ethics Challenges in Field Research with New Populations Project Summary / Abstract  This 4-year Neuroethics R01 based at the University of Minnesota (UMN) will convene a national Working Group of top neuroethics, neurolaw, and neuroscience experts to conduct empirical research and generate evidence-based consensus recommendations for the ethical conduct of population research using highly portable, cloud-enabled MRI in new and diverse populations in field settings. NIH is supporting the development of both high-field portable MRI (3U01EB025153-02S2, PI: Garwood), and ultra- low field MRI (P41EB015896, PI: Rosen). As portable MRI develops quickly, guidance is urgently needed on unresolved ethical, legal, and social issues (ELSI). This R01 project builds on two NIH Administrative Supplements that have preliminarily identified the most pressing unresolved ELSI issues: (1) informed consent; (2) data security and privacy; (3) establishing local capacity to interpret and communicate neuroimaging data; (4) extensive reliance on cloud-based artificial intelligence (AI) for data analysis; (5) potential bias of interpretive algorithms in diverse populations; (6) return of research results and incidental (or secondary) findings to research participants; and (7) responding to participant requests for access to their data.  Building on this preliminary work, Aim 1 will utilize survey research to inform a systematic Working Group (WG) process described in Aim 2. In Aim 1a, we will survey the U.S. general public, including over- sampling of rural, older adult, non-Hispanic African American, Hispanic/Latino, and economically disadvantaged respondents, to probe likely research use cases, issues they raise, potential solutions, and willingness to participate in research. In Aim 1b, we will survey expert stakeholders to elicit views on current/future research use cases and how to address the ELSI challenges. Expert stakeholders will be from 5 key groups: (1) researchers utilizing brain MRI and scientists developing new MRI technology; (2) neuroethics and legal scholars; (3) industry stakeholders; (4) leaders in regulatory agencies and standard-setting organizations; and (5) leaders in patient advocacy organizations. Aim 2 builds on Aim 1 to generate evidence-based consensus guidance on the ethical conduct of research in the field using highly portable, cloud-enabled neuroimaging. In Aim 2a, we will use a modified Delphi method to elicit initial WG views on issue priorities, research use cases, and potential recommendations, and will develop an Annotated Bibliography. In Aim 2b, the WG will pursue a structured process of analysis and consensus building that is well-established in bioethics and law, in order to identify best practices and formulate recommendations informed by the Aim 1 work. In Aim 2c, we will solicit feedback on our recommendations from expert readers and through a major public conference. Project products will include: an online Annotated Bibliography, WG consensus recommendations, individual targeted articles, published empirical analyses, a webcast public conference, a symposium issue of a peer-reviewed journal, online access to our work, and wide dissemination. Project Narrative This innovative 4-year project based at the University of Minnesota will convene a national Working Group of top neuroethics, neurolaw, and neuroscience experts to conduct empirical research and generate evidence- based consensus recommendations for the ethical conduct of research using highly portable, cloud-enabled MRI in new and diverse populations in field settings. Highly-portable MRI, a transformative technology supported by the NIH BRAIN Initiative, will allow researchers to conduct population-based neuroscience research, including racial and ethnic minorities, rural, and socioeconomically disadvantaged populations that are currently underrepresented in neuroimaging research, and will accelerate research on brain biomarkers for neurodegeneration. The project team will address fundamental challenges in field-based neuroimaging research such as informed consent, data privacy, and return of results; produce Working Group consensus recommendations and targeted individual articles; publish empirical analyses; create a symposium issue of a peer-reviewed journal presenting project publications; create a publicly accessible Annotated Bibliography; produce a webcast and videotaped public conference; build an online portal offering access to our work; and conduct wide dissemination.",Highly Portable and Cloud-Enabled Neuroimaging Research: Confronting Ethics Challenges in Field Research with New Populations,10035136,RF1MH123698,"['Address', 'Administrative Supplement', 'African American', 'Algorithms', 'Artificial Intelligence', 'BRAIN initiative', 'Bibliography', 'Bioethics', 'Biological Markers', 'Brain', 'Computer software', 'Consensus', 'Consultations', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Development', 'Economically Deprived Population', 'Elderly', 'Empirical Research', 'Ethical Issues', 'Ethics', 'Feedback', 'Foundations', 'General Population', 'Group Processes', 'Hispanics', 'Individual', 'Industry', 'Informed Consent', 'International', 'Interview', 'Journals', 'Latino', 'Laws', 'Legal', 'Magnetic Resonance Imaging', 'Measures', 'Mechanics', 'Methods', 'Minnesota', 'Nerve Degeneration', 'Neurology', 'Neurosciences', 'Neurosciences Research', 'Not Hispanic or Latino', 'Participant', 'Patient Representative', 'Patient advocacy', 'Peer Review', 'Policies', 'Population', 'Population Heterogeneity', 'Population Research', 'Process', 'Professional Organizations', 'Publications', 'Publishing', 'Radiology Specialty', 'Reader', 'Recommendation', 'Research', 'Research Design', 'Research Personnel', 'Respondent', 'Review Literature', 'Rural', 'Sampling', 'Scientist', 'Source', 'Structure', 'Subgroup', 'Surveys', 'Technology', 'Testing', 'Travel', 'United States National Institutes of Health', 'Universities', 'Videotape', 'Work', 'advocacy organizations', 'base', 'cloud based', 'data privacy', 'demographics', 'disadvantaged population', 'ethical legal social implication', 'ethnic minority population', 'evidence base', 'health disparity', 'innovation', 'meetings', 'neuroethics', 'neuroimaging', 'population based', 'portability', 'racial minority', 'socioeconomic disadvantage', 'symposium', 'web portal', 'willingness', 'working group']",NIMH,UNIVERSITY OF MINNESOTA,RF1,2020,25577,-0.023189840266480643
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,10124880,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,50000,-0.01713388486205485
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9980967,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,1100000,-0.01713388486205485
"What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives,10162151,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Participant', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,100000,-0.017276771839119263
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,10017004,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'clinical center', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'mHealth', 'machine learning method', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2020,725375,-0.02021415721040003
"S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450 This application is requesting funds to purchase the Aperio™ GT-450 digital pathology slide scanner from Leica Biosystems. The requested instrumentation will be located in the Pathology and Biobanking Core of the Lester and Sue Smith Breast Center at Baylor College of Medicine (BCM). The predominant use of the Aperio scanner will be research-based whole slide imaging (WSI) and analysis of patient specimens, patient-derived xenograft (PDX) cancer models, and pre-clinical investigations on various animal- and cell-line model systems. All user projects have large sample cohorts that require high throughput, high-resolution scanning and image analysis. High capacity and improved scanning with dynamic focusing makes the GT-450 microscope scanner well-suited and the most cost-effective for use in the proposed projects. An underlying theme in the studies selected for Aperio scanner-supported services integrates novel biomarker and molecular pathway discovery with spatial morphological characterization, a necessary process to investigate heterogeneity in disease states. This instrument leverages high-throughput scanning capability with open-source, fully customizable machine-learning analytics to meet the evolving needs of investigators at Baylor College of Medicine, in particular faculty groups studying mechanisms of cancer cell dynamics and the development of new therapeutic targets. Expansion of systems biology and precision medicine research is an essential component of the college’s strategic roadmap. The Aperio GT-450 is critically needed as we modernize our laboratory offerings and capabilities; the acquisition of this digital scanner will strengthen existing research programs underway and establish new, collaborative research opportunities and directions within Baylor College of Medicine and surrounding institutions. To address the growing demand for integrating quantitative spatial assessment of biomarkers with molecular pathway discovery, we request funding for the Leica Aperio GT-450 digital microscope scanner. This instrument will facilitate research that seeks to better understand molecular mechanisms of tumorigenesis in the context of its spatial environment and will be critical for the development of clinically correlative biomarkers for next generation precision medicine research initiatives at Baylor College of Medicine.",S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450,9940426,S10OD028671,"['Animals', 'Biological Models', 'Breast', 'Cancer Model', 'Cell Line', 'Development', 'Disease', 'Faculty', 'Funding', 'Grant', 'Heterogeneity', 'Image Analysis', 'Institution', 'Laboratories', 'Machine Learning', 'Medicine', 'Microscope', 'Modernization', 'Molecular', 'Morphology', 'Pathology', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Slide', 'Specimen', 'Systems Biology', 'Xenograft procedure', 'base', 'biobank', 'cancer cell', 'clinical investigation', 'cohort', 'college', 'cost effective', 'digital', 'digital pathology', 'improved', 'instrument', 'instrumentation', 'new therapeutic target', 'novel marker', 'open source', 'pre-clinical', 'precision medicine', 'programs', 'whole slide imaging']",OD,BAYLOR COLLEGE OF MEDICINE,S10,2020,477043,-0.01949887760218455
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,-0.008878809170391095
"Developing novel technologies that ensure privacy and security in biomedical data science research Data science holds the promise of enabling new pathways to discovery and can improve the understanding, prevention and treatment of complex disorders such as cancer, diabetes, substance abuse, etc., which are significantly on the rise. The promise of data science can be fully realized only when collected data can be collaboratively shared and analyzed. However, the widespread increases in healthcare data breaches due to inappropriate access as well as the increasing number of novel privacy attacks restrict institutions from sharing data. Indeed, in some cases, the results of the analysis can themselves lead to significant privacy harm. The success of the data commons depends on ensuring the maximal access to data, subject to all of the patient privacy requirements including those mandated by legislation, and all of the constraints of the organization collecting the data itself. While there are existing solutions that can solve parts of the problem, there are significant challenges in truly incorporating these into comprehensive working solutions that are usable by the biomedical research community, and new challenges brought on by modern techniques such as deep learning. The long-term goal of this research is to develop technologies that can holistically enable data sharing while respecting privacy and security considerations and to ensure that they are implemented in existing platforms that have widespread acceptance in the research community. Towards this, the objective of this project is to develop complementary solutions for risk inference, distributed learning, and access control that can enable different modalities of data sharing. The problems studied are general in nature and will evolve depending on research successes and new impediments that arise. The proposed program of research is significant since lack of access to biomedical data can lead to fragmentation of care, resulting in higher economic and social costs, and is a significant impediment to biomedical research. The project will result in open-source, freely available software tools that will be integrated into widely used data collection, cohort identification, and distributed analytics platforms. There are several ongoing collaborations that will serve as initial pilot customers to provide use cases, identify the requirements, evaluate results, and in general validate the developed solutions. Project Narrative Statement of Relevance to Public Health Being able to ensure privacy and security while enabling data sharing and analysis is critical to pave the way forward for public health research and improve our understanding of diseases. The proposed work will address the challenges that impede the use of data across all of the different modalities of data sharing. The integration into existing platforms will ensure that the developed models, tools, and solutions directly impact the research community and improve public health interventions.",Developing novel technologies that ensure privacy and security in biomedical data science research,9851602,R35GM134927,"['Address', 'Biomedical Research', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Science', 'Diabetes Mellitus', 'Disease', 'Economics', 'Ensure', 'Goals', 'Healthcare', 'Institution', 'Lead', 'Learning', 'Malignant Neoplasms', 'Modality', 'Modeling', 'Modernization', 'Nature', 'Pathway interactions', 'Prevention', 'Privacy', 'Public Health', 'Research', 'Risk', 'Security', 'Software Tools', 'Statutes and Laws', 'Substance abuse problem', 'Techniques', 'Technology', 'Work', 'biomedical data science', 'care fragmentation', 'cohort', 'cost', 'data sharing', 'deep learning', 'improved', 'new technology', 'novel', 'open source', 'patient privacy', 'programs', 'public health intervention', 'public health research', 'social', 'success', 'tool']",NIGMS,RUTGERS THE STATE UNIV OF NJ NEWARK,R35,2020,382108,0.0032429928267882374
"Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences PROJECT SUMMARY/ABSTRACT In the era of newly emerging computational tools for data science, biostatisticians need to play a fundamental role in health sciences research. There is an urgent need to encourage US Citizens and Permanent Residents to pursue graduate training in biostatistics. The design, conduct, and analysis of clinical trials and observational studies; the setting of regulatory policy; and the conception of laboratory experiments have been shaped by the fundamental contributions of biostatisticians for decades. Advances in genomics, medical imaging technologies, and computational biology; the increasing emphasis on precision and evidence-based medicine; and the widespread adoption of electronic health records; demand the skills of biostatisticians trained to collaborate effectively in a multidisciplinary environment and to develop statistical and machine learning methods to address the challenges presented by this data-rich revolutionary era of health sciences research. The proposed summer program which includes world-renowned clinical scientists and biostatisticians from two local universities, will provide an immense opportunity for student participants to learn basic yet modern statistical methods that are critical to uncovering new insights from such big and complex biomedical data and also illustrate the potential pitfalls of confounding and bias that may arise when analyzing biomedical data. A unique feature of the proposed training program is thus to expose the participants to not only basic statistical methods but also to the topics of computer science and bioinformatics which will be invaluable in creating the multidisciplinary teams required to tackle the complex research questions that often requires multipronged approaches. The proposed six-week training program will be structured around the NIH's Translation Science Spectrum and will introduce participants to opportunities in biostatistics through the lens of the science advanced by the contributions of biostatisticians. Following an initial set of weeks on basic training of biostatistical methods, the program will culminate in a data hack-a-thon style competition in which participants will employ the statistical and scientific knowledge gained during the program to produce the most innovative, statistically-sound, scientifically-relevant and effectively-communicated response to a set of research questions. The proposed research education program will enroll up to 20 such participants from across the nation and, through lectures, field trips, and opportunities to analyze data from real health sciences, inspire them to pursue graduate training. The program will draw upon considerable past collaborations and complementary resources of two local world-renowned universities to provide participants with an unparalleled view of the field, including award-winning instructors, internationally known methodological and clinical researchers, and a local area rich in opportunities to showcase careers in biostatistics. Special efforts will be made to enroll participants from underrepresented groups. Participants will be followed after completion, and the numbers attending graduate school in statistics and pursuing biostatistics careers will be documented. PROJECT NARRATIVE Biostatisticians are indispensible contributors to health sciences research. The demand for professionals with advanced training in biostatistics is high and will continue to increase, especially with the expanding challenges posed by big biomedical data. This six week summer research education program, a joint effort of North Carolina State University and Duke University, will enroll up to 20 US citizen/permanent resident participants from across the nation in the summers of 2020-2022 and expose them to the opportunities presented by careers in biostatistics and encourage them to seek graduate training in the field.",Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences,9888421,R25HL147228,"['Address', 'Adoption', 'Area', 'Attention', 'Award', 'Bioinformatics', 'Biomedical Research', 'Biometry', 'Biostatistical Methods', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Conceptions', 'Data', 'Data Science', 'Development', 'Discipline', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Exposure to', 'Faculty', 'Future', 'Genomics', 'Goals', 'Health Sciences', 'Health system', 'Imaging technology', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Medical Imaging', 'Medical center', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Names', 'National Heart, Lung, and Blood Institute', 'North Carolina', 'Observational Study', 'Participant', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Program Effectiveness', 'Request for Applications', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Role', 'Schools', 'Science', 'Scientist', 'Statistical Methods', 'Strategic Planning', 'Structure', 'Students', 'Talents', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'analytical method', 'big biomedical data', 'career', 'career development', 'clinical trial analysis', 'cohort', 'computer science', 'computerized tools', 'data resource', 'design', 'education research', 'experience', 'field trip', 'graduate student', 'health science research', 'innovation', 'insight', 'instructor', 'interest', 'investigator training', 'laboratory experiment', 'lectures', 'lens', 'machine learning method', 'multidisciplinary', 'next generation', 'programs', 'public health research', 'recruit', 'response', 'skills', 'sound', 'statistical and machine learning', 'statistics', 'summer institute', 'summer program', 'summer research', 'tool', 'undergraduate student']",NHLBI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2020,249789,-0.03387743123846133
"Center for Modeling Complex Interactions Biomedical problems are innately complex, and their solutions require input from many fields. Many centers focus on a single disease or organ system. By contrast, the Center for Modeling Complex Interactions focuses on an approach that can address many biomedical problems: team-based, interdisciplinary research centered around modeling. Our goal is to support and facilitate biomedical discovery by integrating modeling into interdisciplinary research. Modeling improves research at all stages—hypothesis formulation, experimental design, analysis, and interpretation. It provides a unifying language by which exchange of ideas can highlight commonalities and uncover unforeseen connections between problems. Formalization of ideas into this unifying language also improves rigor and reproducibility. We define modeling broadly to include everything from deterministic and stochastic mathematical approaches, to physical and computational models of three- dimensional objects, to agent-based and machine learning approaches where exact solutions are not possible. We seek to support modelers by increasing their numbers, and by giving them opportunities to play on interdisciplinary teams. We seek to support empiricists by giving them access to relevant modeling expertise, and by creating a community and a culture to facilitate interdisciplinary research. In Phase I, the Center for Modeling Complex Interactions created the intellectual, cultural, and physical environment to promote team- based, interdisciplinary research. In Phase II, we will build on that foundation by maintaining a strong interdisciplinary culture to foster collaboration among people who might otherwise never connect, and by adding additional faculty to expand our modeling expertise. We have four Aims: 1) Support faculty to carry out model-based, interdisciplinary biomedical research and increase their competitiveness for external funding. Research in the Center is carried out in the context of Working Groups—zero-barrier, interdisciplinary, goal- focused teams that meet regularly to get work done. Supported research includes three Research Projects, Pilot Projects, Modeling Access Grants, and ad hoc teams. Our comprehensive plan for proposal preparation improves grantsmanship, and our staff assists with submission and grant management. 2) Increase University of Idaho’s faculty participation in biomedical research. We will add six new faculty as a commitment to this Phase II COBRE and attract broader participation from across the University. 3) Extend the reach of the Modeling Core into new areas of modeling to capitalize on emerging opportunities. The Modeling Core accelerates interdisciplinary research by placing Core Fellows at the hub of the research community. We have added new Core Initiatives in machine learning and geospatial modeling to stimulate research in these areas with high potential for future growth. 4) Establish a path to long-term sustainability under the umbrella of the Institute for Modeling Collaboration & Innovation. The major hurdle for sustainability is to maintain a robust Modeling Core. We have developed a business plan that calls for us to diversify our funding sources to include institutional, state, and private support to supplement federal grants. Human health is determined by interactions of complex biological systems at multiple scales, from the ecological to the biophysical; these are layered with spatial and temporal variation. To decipher these systems requires predictive modeling, coupled with strong empirical work, to be guided by and to feed the models. The Center for Modeling Complex Interactions generates model-based biomedical research and connects people who might otherwise never interact, which enhances the strong interdisciplinary culture of the University of Idaho.",Center for Modeling Complex Interactions,10026000,P20GM104420,"['Address', 'Area', 'Biological', 'Biomedical Research', 'Biophysics', 'Businesses', 'Centers of Research Excellence', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Models', 'Coupled', 'Data', 'Development', 'Disease', 'Ensure', 'Experimental Designs', 'Faculty', 'Feedback', 'Formulation', 'Fostering', 'Foundations', 'Funding', 'Funding Agency', 'Future', 'Generations', 'Goals', 'Grant', 'Growth', 'Health', 'Holly', 'Home environment', 'Human', 'Human Resources', 'Idaho', 'Incubators', 'Individual', 'Infrastructure', 'Institutes', 'Interdisciplinary Study', 'Language', 'Lead', 'Machine Learning', 'Modeling', 'Outcome', 'Phase', 'Physical environment', 'Pilot Projects', 'Play', 'Population', 'Postdoctoral Fellow', 'Preparation', 'Privatization', 'Progress Reports', 'Property', 'Reproducibility', 'Research', 'Research Institute', 'Research Project Grants', 'Research Support', 'Schedule', 'Scientist', 'Structure', 'Students', 'System', 'Testing', 'Time', 'Training', 'Uncertainty', 'Universities', 'Ursidae Family', 'Work', 'base', 'biological systems', 'body system', 'complex biological systems', 'experience', 'faculty support', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'mathematical methods', 'member', 'next generation', 'novel strategies', 'physical model', 'predictive modeling', 'spatial temporal variation', 'success', 'three-dimensional modeling', 'undergraduate student', 'working group']",NIGMS,UNIVERSITY OF IDAHO,P20,2020,2172986,0.0072748447160401195
"N3C & All of Us Research Program Collaborative Project Project Summary/Abstract The COVID-19 pandemic presents unprecedented clinical and public health challenges. Though institutions collect large amounts of clinical data about COVID-19 cases, these datasets individually might not be diverse enough to draw population level conclusions. Also, statistical, machine learning, and causal analyses are most successful with large-scale data beyond what is available in any given organization. To tackle this problem, NCATS introduced the National COVID Cohort Collaborative (N3C), an open science, community-based initiative to share patient level data for analysis. The initiative requires participating institutions to share information about their COVID-19 patients in a standard-driven way, including demographics, vital signs, diagnoses, laboratory results, medications, and other treatments. The data from multiple institutions will be merged and consolidated, and access will be provided to investigators through a centralized analytical platform. The COVID-19 data sharing collaboration with the N3C initiative offers a mechanism to initiate collaborations with other NIH sponsored data sharing programs, such as the All of Us Research Program (AoURP). This administrative supplement will support efforts to clean and standardize data at VCU, and to transfer it to the N3C data repository. The supplement will also assist in introducing new services at the Wright Center to support our investigators to use the N3C resources. It will also enable collaboration with the AoURP by establishing a pipeline to collect and transmit consented patients' EHR data and by building on existing community outreach pathways to recruit additional participants for the AoURP. The project will be overseen by the PI/Executive Committee and supervised by the Director of Research Informatics. Procedures and services developed at our local CTSA hub will be shared and disseminated to the CTSA network. Project Narrative NIH/NCATS has been working on the National COVID Cohort Collaborative (N3C), which aims to build a centralized national data resource to be used by the research community to study the COVID-19 pandemic and identify potential treatments as the pandemic continues to evolve. The COVID-19 data sharing collaboration with the N3C initiative also offers a mechanism to initiate collaborations with the All of Us Research Program (AoURP). This administrative supplement will support the creation and management of a data extraction and transfer pipeline to the N3C and AoURP data repositories from VCU.",N3C & All of Us Research Program Collaborative Project,10217339,UL1TR002649,"['Administrative Supplement', 'All of Us Research Program', 'COVID-19', 'COVID-19 pandemic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Outreach', 'Consent', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Ecosystem', 'Effectiveness', 'Funding Opportunities', 'Goals', 'Health', 'Health Status', 'Individual', 'Informatics', 'Infrastructure', 'Institution', 'Laboratories', 'Outcomes Research', 'Participant', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Procedures', 'Public Health', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Services', 'Supervision', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'base', 'biomedical informatics', 'clinical center', 'cohort', 'coronavirus disease', 'data resource', 'data sharing', 'data standards', 'data warehouse', 'demographics', 'design', 'improved', 'informatics infrastructure', 'innovation', 'large scale data', 'multi-site trial', 'network informatics', 'open data', 'pandemic disease', 'parent grant', 'programs', 'recruit', 'response', 'statistical and machine learning', 'tool']",NCATS,VIRGINIA COMMONWEALTH UNIVERSITY,UL1,2020,346608,-0.006370023269552929
"Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center The “clinical high risk” (CHR) for psychosis syndrome is an antecedent period characterized by attenuated psychotic symptoms that are marked by subtle deviations from normal development in thinking, motivation, affect, behavior, and a decline in functioning. Early intervention in this CHR population is critical to prevent psychosis onset as well as other adverse outcomes. However, the presentation of symptoms and subsequent course is highly variable, and there is a paucity of biomarkers to guide treatment development. Thus, to improve predictive models that are clinically relevant, several issues need to be addressed: 1) focusing on outcomes beyond psychosis; 2) taking into account heterogeneity in samples and outcomes; and 3) integrating data sets with a broad array of variables using innovative algorithms to overcome variability across studies. To address these challenges, the proposed “Psychosis Risk Evaluation Data Integration and Computational Technologies: Data Processing, Analysis, and Coordination Center” (PREDICT-DPACC) brings together a multidisciplinary team of highly experienced researchers with proven capabilities in all aspects of large-scale studies, CHR studies, as well as computational expertise. The ultimate goal is to identify new CHR biomarkers, and CHR subtypes that will enhance future clinical trials. To do so, the PREDICT-DPACC will 1) aggregate extant CHR- related data sets from legacy datasets; 2) provide collaborative management, direction, data processing and coordination for new U01 multisite network(s); and 3) develop and apply advanced algorithms to identify biomarkers that predict outcomes, and to stratify CHR into subtypes based on outcome trajectories, first from the extant data and then refined and applied to the new data. The PREDICT-DPACC team has the broad, comprehensive, and robust infrastructure that is sufficiently flexible to accommodate the inclusion of multiple data types and to optimally address the needs of the CHR U01 network(s). Carefully selected extant data will be rapidly obtained, processed, and uploaded to the NIMH Data Archive (NDA). Proposed analysis methods are powerful and robust, leveraging the expertise and experience of computer scientist developers, and experienced clinical researchers. The U01 network(s) will be coordinated by a team that is experienced in managing large studies, familiar with the needs of such studies, flexible, and is knowledgeable in all aspects of CHR studies, including measures, outcomes, biomarkers, and cohorts. Upon meeting the goals of this U24, and the supported U01 network(s), the expected outcomes of the PREDICT-DPACC will be new predictive biomarkers for CHR outcomes, new definitions of CHR subtypes that are clinically useful, and new curated and comprehensive CHR datasets (extant and new) as well as processing tools and prediction algorithms that are shared with the research community through the NIMH Data Archive. NARRATIVE The “Clinical High Risk” (CHR) for psychosis syndrome in young people represents an opportune window for early intervention to prevent the onset of psychosis and other disorders, and to forestall disability; however, clinical heterogeneity and the paucity of biomarkers have hampered the development of effective intervention. To address these challenges, working with NIMH and key stakeholders, we will harmonize and aggregate existing “legacy” CHR data, and guide and coordinate the collection of new data across a network of sites, to develop biomarker algorithms that can predict individual trajectories for diverse outcomes. This proposal leverages a multidisciplinary team with broad and CHR-specific experience in large-scale multisite and multimodal studies (including clinical trials), along with expertise in data type-specific processing, coordination, analysis, and computational analyses (e.g., machine and deep learning tools from artificial intelligence, and advanced statistical approaches), ethics, community outreach, and data dissemination, all of which will ensure the success of this project.","Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center",10092398,U24MH124629,"['Address', 'Adolescent', 'Affect', 'Algorithms', 'Anxiety Disorders', 'Artificial Intelligence', 'Attenuated', 'Behavior', 'Big Data', 'Biological Markers', 'Child', 'Clinical', 'Clinical Trials', 'Collection', 'Common Data Element', 'Communities', 'Community Outreach', 'Computer Analysis', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease remission', 'Early Intervention', 'Early identification', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'FAIR principles', 'Follow-Up Studies', 'Funding', 'Future', 'Goals', 'Heterogeneity', 'Human Resources', 'Impaired cognition', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Intervention', 'Lead', 'Leadership', 'Longterm Follow-up', 'Machine Learning', 'Measures', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Monitor', 'Moods', 'Motivation', 'National Institute of Mental Health', 'Online Systems', 'Outcome', 'Output', 'Perception', 'Procedures', 'Process', 'Protocols documentation', 'Psychotic Disorders', 'Quality Control', 'Recovery', 'Research', 'Research Personnel', 'Risk', 'Risk stratification', 'Safety', 'Sampling', 'Scientist', 'Secure', 'Site', 'Social Functioning', 'Standardization', 'Substance Use Disorder', 'Suggestion', 'Symptoms', 'Technology', 'Thinking', 'Time', 'Training', 'Transact', 'United States', 'Validation', 'Visualization software', 'adverse outcome', 'analytical tool', 'attenuated psychosis syndrome', 'base', 'bioinformatics infrastructure', 'candidate marker', 'clinical heterogeneity', 'clinical risk', 'clinical subtypes', 'clinically relevant', 'cloud based', 'cohort', 'computerized data processing', 'data acquisition', 'data archive', 'data dictionary', 'data dissemination', 'data harmonization', 'data infrastructure', 'data integration', 'data tools', 'deep learning', 'demographics', 'design', 'disability', 'effective intervention', 'experience', 'flexibility', 'functional decline', 'functional disability', 'high risk', 'high risk population', 'improved', 'inclusion criteria', 'innovation', 'meetings', 'member', 'multidisciplinary', 'multimodal data', 'multimodality', 'multiple data types', 'outcome prediction', 'persistent symptom', 'prediction algorithm', 'predictive marker', 'predictive modeling', 'prevent', 'prospective', 'psychotic symptoms', 'quality assurance', 'recruit', 'research study', 'resilience', 'response', 'success', 'therapy development', 'tool', 'working group']",NIMH,BRIGHAM AND WOMEN'S HOSPITAL,U24,2020,3935239,-0.020157993505901136
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,10000112,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'ZIKA', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'data infrastructure', 'data modeling', 'data streams', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'heterogenous data', 'improved', 'innovation', 'mathematical methods', 'multiple data sources', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2020,365601,-0.003539252339474563
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9856493,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'data exchange', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual machine', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,339098,-0.040986075511068226
"Meta-analysis in human brain mapping This is the competing renewal of R01MH074457-13, which sustains the BrainMap Project (www.brainmap.org). The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data. To date, the BrainMap Project has designed and populated two coordinate-based databases: 1) a task-activation repository (TA DB); and, 2) a voxel-based morphometry repository (VBM DB). The TA DB contains >17,200 experiments, collectively representing > 78,000 subjects and > 110 task- activation paradigms. The VBM DB contains > 3,100 experiments, collectively representing > 81,000 subjects with > 80 psychiatric, neurologic and developmental disorders with ICD-10 coding. The BrainMap Project has created, optimized and validated an integrated pipeline of multi-platform (Javascript), open-access tools to curate (Scribe), filter and retrieve (Sleuth), analyze (GingerALE), visualize (Mango) and interpret analysis output (BrainMap meta-data plugins for Mango). Several network-modeling approaches have been applied to BrainMap data -- MACM, independent components analysis (ICA), graph theory modeling (GTM), author-topic modeling (ATM), structural equation modeling (SEM), and connectivity-based parcellation (CBP) – but none are yet pipeline components. Utilization of these CBMA resources is substantial: BrainMap software, data and meta-data have been used in > 825 peer-reviewed publications. Of these, > 350 were published within the current funding period (April 2015-March 2019; brainmap.org/pubs). In this competing renewal, four tool- development aims are proposed, each of which extends this high-impact research resource. Aim 1. Database Expansion. BrainMap data repositories will be expanded. Aim 2. Meta-analytic Network Modeling. Network modeling will be added to the BrainMap pipeline. Aim 3. Large-Scale Simulations, Comparisons and Validations. Data simulations, characterizations and validations will be performed. Aim 4. Meta-data Inferential tools. Tools for mining BrainMap’s location-linked meta-data will be expanded. Data Sharing Plan. BrainMap data, meta-data, pipeline tools, and templates created by whole-database modeling (e.g., ICA and ATM network masks) are shared at BrainMap.org. Of all new data entries, more than half are contributed by BrainMap users, i.e., community data sharing via BrainMap.org. For community-coded entries, the BrainMap team provides curation and quality control. Comprehensive database images (database dumps) are available to tool developers through Collaborative Use Agreements. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data  sets, metadata, computational tools, and related resources that enable coordinate-­based meta-­analyses  (CBMA), meta-­analytic connectivity modeling (MACM), meta-­data informed interpretation (“decoding”) of  imaging results, and meta-­analytic priors for mining (including machine learning) primary (per-­subject)  neuroimaging data.    ",Meta-analysis in human brain mapping,10056029,R56MH074457,"['Agreement', 'Area', 'Brain', 'Brain Mapping', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Set', 'Databases', 'Disease', 'Educational workshop', 'Equation', 'Functional disorder', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Institution', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Java', 'Link', 'Location', 'Machine Learning', 'Mango - dietary', 'Masks', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Output', 'Peer Review', 'Plug-in', 'Publications', 'Publishing', 'Quality Control', 'Research Domain Criteria', 'Resources', 'Rest', 'Site', 'Software Framework', 'Specificity', 'Structure', 'Training', 'Universities', 'Validation', 'base', 'candidate marker', 'computerized tools', 'data pipeline', 'data sharing', 'data warehouse', 'design', 'developmental disease', 'experimental study', 'graph theory', 'independent component analysis', 'interest', 'large scale simulation', 'morphometry', 'nervous system disorder', 'network architecture', 'network models', 'neuroimaging', 'neuropsychiatric disorder', 'repository', 'simulation', 'tool', 'tool development']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R56,2020,543396,-0.0053882592880938355
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,10002192,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'data standards', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2020,427122,-0.016500509255939714
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domainagnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10057127,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2020,609068,-0.02537464108786049
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Contact PD/PI: Panettieri, Reynold Alexander Project Summary/Abstract Overview Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Page 337 Project Summary/Abstract Contact PD/PI: Panettieri, Reynold Alexander New Jersey Alliance for Clinical and Translational Science (NJ ACTS) Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health.",New Jersey Alliance for Clinical Translational Science: NJ ACTS,9890029,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'Neighborhood Health Center', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical database', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'experience', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2020,4640627,-0.02486975898611428
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9979969,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Estrogen receptor positive', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'data standards', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'public repository', 'repository', 'structured data', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2020,511367,-0.02038992505473483
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Contact PD/PI: Panettieri, Reynold Alexander Project Summary/Abstract Overview Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Page 337 Project Summary/Abstract Contact PD/PI: Panettieri, Reynold Alexander New Jersey Alliance for Clinical and Translational Science (NJ ACTS)  Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health. Page 338 Project Narrative",New Jersey Alliance for Clinical Translational Science: NJ ACTS,10201004,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'Neighborhood Health Center', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical database', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2020,1482000,-0.024780088649813115
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,0.0024132737635838156
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9894759,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Information Retrieval', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Medicine', 'Methods', 'Mining', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'machine learning method', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'structured data', 'tool', 'unstructured data']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2020,269500,-0.012811118118361092
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",10023935,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'automated segmentation', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2020,256578,-0.012600909651988406
"ShapeWorks in the Cloud Project Summary This application is submitted in response to NOT-OD-20-073 as an administrative supplement to the parent award R01AR076120 titled: ""Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches."" The form (or shape) of anatomies is the clinical language that describes abnormal mor- phologies tied to pathologic functions. Quantifying such subtle morphological shape changes requires parsing the anatomy into a quantitative description that is consistent across the population in question. For more than 100 years, morphometrics has been an indispensable quantitative tool in medical and biological sciences to study anatomical forms. But its representation capacity is limited to linear distances, angles, and areas. Sta- tistical shape modeling (SSM) is the computational extension of classical morphometric techniques to analyze more detailed representations of complex anatomy and their variability within populations The parent award ad- dresses existing roadblocks for the widespread adoption of SSM computational tools in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM) and its associated suite of open-source software tools, ShapeWorks. ShapeWorks enables learning population-level shape representation via automatic dense placement of homologous landmarks on image segmentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread applicability and impact to medicine and biology are hindered by computational barriers that most existing shape modeling packages face. The goal of this supplement award is to provide supplemental support for Aim 3 of the parent award to leverage best practices in software development and advances in cloud computing to enable researchers with limited computational resources and/or large-scale cohorts to build and execute custom SSM workﬂows us- ing remote scalable computational resources. To achieve this goal, we have developed a plan to enhance the design, implementation, and cloud-readiness of ShapeWorks and augmented our scientiﬁc team to add senior, experienced software engineers/developers who have extensive experience in professional programming, code refactoring, and scientiﬁc computing. This award will provide our team with the support necessary to (Aim 1) de- sign ShapeWorks as a collection of modular and reusable services, (Aim 2) decouple ShapeWorks services from explicitly encoded data sources, and (Aim 3) refactor ShapeWorks to scale efﬁciently on the cloud. All software development will be performed in adherence to software engineering practices and design principles, including coding style, documentation, and version control. The proposed efforts will be released as open-source software in a manner consistent with the principles of reproducible research and the practices of open science. Our long- term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein in addition to the parent award will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. The impact and scientiﬁc value of ShapeWorks have been recognized in a range of applications, including psychology, biological phenotyping, car- diology, and orthopedics. If funded, this supplement will provide support to revise, refactor, and redeploy Shape- Works to take advantage of new cloud computing paradigms, to be robust, sustainable, scalable, and accessible to a broader community, and to address the growing need for shape modeling tools to handle large collections of clinical data and to obtain sufﬁcient statistical power for large shape studies.",ShapeWorks in the Cloud,10166337,R01AR076120,"['Address', 'Adherence', 'Administrative Supplement', 'Adoption', 'Anatomy', 'Applied Research', 'Architecture', 'Area', 'Award', 'Biological', 'Biological Sciences', 'Biology', 'Cardiology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cloud Computing', 'Cloud Service', 'Code', 'Collection', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computer Models', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Databases', 'Disabled Persons', 'Documentation', 'Environment', 'Face', 'Funding', 'Goals', 'Image', 'Imagery', 'Language', 'Learning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'Occupations', 'Online Systems', 'Orthopedics', 'Parents', 'Pathologic', 'Phenotype', 'Population', 'Privatization', 'Psychology', 'Readiness', 'Reproducibility', 'Research', 'Research Personnel', 'Running', 'Scientist', 'Services', 'Shapes', 'Software Design', 'Software Engineering', 'Software Tools', 'Source Code', 'Speed', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'base', 'cohort', 'computational platform', 'computerized tools', 'computing resources', 'data management', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'large datasets', 'model development', 'open data', 'open source', 'particle', 'response', 'scientific computing', 'shape analysis', 'software development', 'statistics', 'tool', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,210000,0.008679821097223166
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9920211,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'data quality', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,709525,-0.007795703589082292
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,10019388,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2020,381282,-0.00429932375351552
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9888378,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelial', 'Epithelium', 'Esophageal Adenocarcinoma', 'Esophageal Tissue', 'Esophagitis', 'Esophagus', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'intelligent algorithm', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'stem cells', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2020,171720,-0.017758058645418746
"The University of Iowa Clinical and Translational Science Award ABSTRACT The Institute for Clinical and Translational Science (ICTS) at the University of Iowa (UI) was established by the Board of Regents to realize three objectives – first, to lead the development of translational science at the UI; second, to advance translational science as a distinct academic discipline; and third, to disseminate capacities in translational science across the State of Iowa. This mandate enabled us to tackle large problems affecting translational science that required institutional solutions, such as transforming regulatory processes for human subjects research, developing an informatics infrastructure for integrating electronic medical record and other health care data, establishing bi-directional relationships with community organizations, and revitalizing the pipeline of well trained clinical and translational researchers. Iowa is a rural state, which brings special health care needs and challenges. We have used these rural considerations as a catalyst for driving our approach to clinical and translational research pushing our teams to develop strategies to engage rural populations of all ages and backgrounds and to create new approaches that overcome the geographic barriers in a rural state. We are capitalizing on our established community practice networks of family physicians, clinics, school nurses and pharmacists. We utilize e Health/ e Learning platforms in novel ways and will test the efficacy of these new methods of engagement. As we move research “Beyond Our Borders,” we have created methods to capture real-time, real-life data from the home and to correlate this environmentally specific, comprehensive data to human performance. The ICTS is engaging with other CTSA hubs and national CTR systems to empirically test different approaches and to develop the evidence base of proven strategies for accelerating translation that can be more broadly disseminated. Though distance and rurality drive our approaches, the strategies that we develop are simply new and potentially better ways to generate broad representation and improved participation by patients, healthcare teams and academicians. Through our local, state and national partnerships, UI and the ICTS are poised to move clinical and translational discovery rapidly into healthcare practice in a variety of clinical settings. PROJECT NARRATIVE The mission of the University of Iowa Institute for Clinical and Translational Science (ICTS) is to accelerate translational science through programs to develop the translational workforce, promote the engagement of community members and other stakeholders, to promote research integration across the lifespan, and to catalyze innovative clinical and translational research. The ongoing development of collaborative data-based infrastructure and services is a central tenant to achieving this mission. The goals of this proposal are: for Iowa to join the N3C partnership, contributing data in an effective method that evolves and improves over time, to enable Iowa researchers to leverage the N3C data resource in their research and to leverage the N3C framework for other areas of research in the future.",The University of Iowa Clinical and Translational Science Award,10201104,UL1TR002537,"['Affect', 'Age', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Clinics and Hospitals', 'Communities', 'Community Practice', 'Computerized Medical Record', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Development', 'Discipline', 'Disease', 'E-learning', 'Emergency Situation', 'Family Physicians', 'Future', 'Genomics', 'Geography', 'Goals', 'Health', 'Health system', 'Healthcare', 'Home environment', 'Human', 'Human Subject Research', 'Image', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Iowa', 'Knowledge', 'Lead', 'Life', 'Longevity', 'Machine Learning', 'Medical Care Team', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Patient Participation', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacists', 'Phenotype', 'Positioning Attribute', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rural', 'Rural Health', 'Rural Population', 'School Nursing', 'Services', 'System', 'Testing', 'Time', 'Training', 'Transfer Agreement', 'Translational Research', 'Translations', 'Universities', 'Work', 'base', 'biomedical informatics', 'catalyst', 'clinical decision support', 'cohort', 'collaborative approach', 'community organizations', 'coronavirus disease', 'data acquisition', 'data enclave', 'data exchange', 'data resource', 'data sharing', 'data warehouse', 'efficacy testing', 'evidence base', 'health management', 'improved', 'informatics infrastructure', 'innovation', 'interest', 'member', 'novel', 'novel strategies', 'pandemic disease', 'phenotypic data', 'programs', 'rurality', 'social determinants', 'support tools', 'translational scientist']",NCATS,UNIVERSITY OF IOWA,UL1,2020,98933,-0.004605750841920133
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9965942,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2020,989602,-0.01015259411262775
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,10258317,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2020,157500,-0.01015259411262775
"Carolina Population Center PROJECT SUMMARY/ABSTRACT The Carolina Population Center requests infrastructure support that will advance population dynamics research at CPC by increasing research impact, innovation, and productivity, supporting the development of junior scientists, and reducing the administrative burden on scientists. Infrastructure support will advance science in three primary research areas: Sexuality, Reproduction, Fertility, and Families; Population, Health, and the Environment; and Inequality, Mobility, Disparities, and Well-Being. Much of the research at CPC draws on large publicly available longitudinal data sets that our faculty have designed and collected, including the National Longitudinal Study of Adolescent to Adult Health, the China Health and Nutrition Survey, newer surveys associated with the Transfer Project, and the Study of the Tsunami Aftermath and Recovery, all of which will continue to be important in work related to our primary research areas over the next five years. These projects embody several themes that have guided research at CPC since the Center's inception. These themes, which will continue to shape our work, are the importance of life course processes and longitudinal data, multi-level processes and measurement of context, interventions and natural experiments as means of learning about causal processes, and the relevance of sociodemographic variables such as age, gender, race- ethnicity, and socioeconomic status for disparities in health and well-being. By embedding these themes, our projects provide data that enable us to address barriers that otherwise impede progress in the population sciences generally, and in our primary research areas in particular. We request support for three cores which in combination will provide an institutional infrastructure that will push populations dynamics research forward by empowering CPC faculty to tackle challenging questions using state of the art measurement techniques and methods. The Administrative Core plans activities that maintain a stimulating intellectual community, streamlines administrative processes so that scientists can focus on research, coordinates activities of the Cores so that services are offered efficiently, and communicates information about research and data more broadly. The Development Core supports early stage investigators and other faculty with exciting new ideas through multiple mechanisms: workshops, access to technical expertise in measurement, and seed grants. The Research Services Core enables scientists to address complex and important population research issues by providing access to state-of-the-art research tools and professional support for programming, survey development, and analysis. NARRATIVE This project will provide infrastructure support for a cutting edge program of research on population dynamics at the Carolina Population Center. Research at the Center will analyze state-of-the art data to address fundamental questions regarding fertility, adolescent health, and links between the environment and health. Special attention will be paid to factors creating health disparities.",Carolina Population Center,10005569,P2CHD050924,"['Address', 'Adolescent', 'Adopted', 'Adult', 'Age', 'Applications Grants', 'Area', 'Attention', 'Biological Markers', 'China', 'Cognitive', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Vision Systems', 'Creativeness', 'Data', 'Data Collection', 'Development', 'Diffuse', 'Educational workshop', 'Environment', 'Ethnic Origin', 'Extramural Activities', 'Faculty', 'Family', 'Fertility', 'Fostering', 'Funding', 'Gender', 'Genetic', 'Grant', 'Hand', 'Health', 'Health Surveys', 'Home environment', 'Inequality', 'Infrastructure', 'Intervention', 'Journals', 'Learning', 'Life Cycle Stages', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mainstreaming', 'Measurement', 'Mentors', 'Methods', 'Natural experiment', 'Nutrition Surveys', 'Personal Satisfaction', 'Phase', 'Policy Making', 'Population', 'Population Dynamics', 'Population Research', 'Population Sciences', 'Postdoctoral Fellow', 'Process', 'Production', 'Productivity', 'Publishing', 'Race', 'Recovery', 'Reproduction', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Schools', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Seeds', 'Services', 'Sexuality', 'Shapes', 'Socioeconomic Status', 'Structure', 'Students', 'Surveys', 'Talents', 'Teacher Professional Development', 'Technical Expertise', 'Techniques', 'Training Programs', 'Tsunami', 'Universities', 'Work', 'adolescent health', 'career', 'collaborative environment', 'cost', 'data access', 'design', 'empowered', 'experience', 'faculty support', 'health disparity', 'innovation', 'interdisciplinary collaboration', 'longitudinal dataset', 'novel strategies', 'population health', 'privacy protection', 'programs', 'research and development', 'response', 'sociodemographic variables', 'success', 'tool']",NICHD,UNIV OF NORTH CAROLINA CHAPEL HILL,P2C,2020,774402,-0.0030812052126260674
"FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control PROJECT SUMMARY In this proposal we plan to contribute addressing the above foundational and operational challenges by advancing the science of influenza modeling and contributing novel methods and data sources that will increase the accuracy and availability of seasonal and pandemic influenza models. To address these challenges, we plan to build on the unique mechanistic spatially structured modeling approaches developed by our consortium, that includes stochastic metapopulation models and fully developed agent-based models nested together in our global epidemic and mobility modeling (GLEAM) approach. The objective of this project is to generate novel and actionable scientific insights from dynamic transmission models of influenza transmission that effectively integrate key socio-demographic indicators of the focus population, as well as a wide spectrum of pharmaceutical and non-pharmaceutical interventions. Our proposed work in specific aim 1 (A1) will leverage our global modeling (from the global to local scale) framework that can be used to explore the multi-year impact of influenza vaccination, antiviral prophylaxis/treatment, and community mitigation during influenza seasons and pandemics. Our specific aim 2 (A2) will focus on using high quality data to model heterogeneous transmission drivers and novel contact pattern stratifications that will allow us to guide mitigation strategies and prioritization for interventions. In our Aim 3 (A3) we will use artificial intelligence approaches to identify interventions that are particularly synergistic and well-suited to particular epidemic scenarios, for seasonal and pandemic influenza. Our overarching goal is to provide a modeling portfolio with flexible and innovative mathematical and computational approaches. We aim to address several questions commonly asked about seasonal and pandemic influenza and match these with analytical methods and outbreak projections. The modeling and data developed in this project can help facilitate and justify transparent public health decisions, while contributing to the definition of standard methods for model selection and validation. Finally, our influenza modeling platform can also benefit the broader network of modeling teams and can be used to improve result sharing and harmonization of modeling approaches. The objective of this proposal is to advance the science of modeling and contribute novel methods and data analytics tools that will increase the understanding of seasonal and pandemic influenza in the context of the network of modeling teams coordinated by the CDC. To address these challenges, we plan to develop a novel global modeling framework, contribute new data and methods for improve the accuracy and validation of flu modeling approaches, and evolve successful methodologies to advance the analysis of layered intervention with artificial Intelligence.",FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control,10071782,U01IP001137,[' '],NCIRD,NORTHEASTERN UNIVERSITY,U01,2020,371721,-0.011922786415338964
"A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers PROJECT SUMMARY/ABSTRACT  This application, “A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers,” is in response to PAR-16-238, Dissemination and Implementation Research in Health (R01). Acute respiratory distress syndrome (ARDS) has high prevalence (10% of intensive care unit admissions) and mortality up to 46%. Low tidal volume ventilation (LTVV) is the most effective therapy for ARDS, lowering mortality by 20-25%, and is part of standard practice. However, use of LTVV is as low as 19% of ARDS patients. There is a poor understanding of the barriers to LTVV adoption: current approaches are deficient because they incorporate biases, lack consistency and comprehensiveness, ignore the influence of interpersonal network- or team- based factors, and do not address setting-specific variation. Our research team has previously identified some patient- and clinician-specific facilitators of and barriers to LTVV adoption. We have used two state-of-the-art data driven methods—data science and network analysis—to preliminarily quantify the impact of a diverse array of potential factors affecting LTVV adoption, including network- and team-based factors. The proposed research is guided by the Consolidated Framework for Implementation Research (CFIR) and Rogers' Diffusion of Innovations theory. The overall goals of the proposed research are to understand the differences in facilitators and barriers to LTVV adoption between academic and community settings through a definitive, systematic study in a large, diverse consortium of medical centers, and to advance implementation science by providing a model for how data science and network analysis can be applied to understand the adoption of a complex intervention. The overarching hypothesis is that there are different patient-, clinician-, network-, and team-based facilitators and barriers to LTVV adoption in academic and community settings. We will determine whether different patient- and clinician- (Aim 1 cohort study, clinician survey, and data science analysis), clinician interpersonal network- (Aim 2 network analysis), and team structure and dynamics-based (Aim 3 team construction and modeling) facilitators of and barriers to LTVV adoption exist between academic and community hospital settings. Successful completion of the proposed research will provide a comprehensive understanding of the differences in the facilitators of and barriers to LTVV adoption between academic and community settings, and will advance implementation science by serving as a model of how data science and network analysis can be applied to complex implementation problems. Implementation strategies that account for all these factors may be more likely to lead to significant practice change. PROJECT NARRATIVE  Acute respiratory distress syndrome (ARDS) has high prevalence and mortality among critically ill patients; low tidal volume ventilation is the most effective therapy for ARDS but is used infrequently. Successful completion of the proposed research will identify differences in the facilitators of and barriers to adoption of low tidal volume ventilation between academic and community hospital settings in a large and diverse consortium of medical centers. The proposed research will generate a model of how data science and network analysis can be used to understand the implementation of a complex evidence-based practice.",A novel data science and network analysis approach to quantifying facilitators and barriers of low tidal volume ventilation in an international consortium of medical centers,9963341,R01HL140362,"['Acute', 'Admission activity', 'Adoption', 'Adult Respiratory Distress Syndrome', 'Affect', 'Attitude', 'Caring', 'Characteristics', 'Cohort Studies', 'Community Hospitals', 'Complex', 'Critical Illness', 'Data', 'Data Science', 'Diffusion of Innovation', 'Environment', 'Evidence based practice', 'Goals', 'Health', 'Healthcare Systems', 'Height', 'High Prevalence', 'Hypoxemia', 'Individual', 'Inflammatory', 'Intensive Care Units', 'International', 'Intervention', 'Investigation', 'Knowledge', 'Lead', 'Machine Learning', 'Measurement', 'Medical center', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'Nurses', 'Pathway Analysis', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Physicians', 'Professional Organizations', 'Pulmonary Edema', 'Research', 'Severities', 'Speed', 'Structure', 'Surveys', 'Syndrome', 'System', 'Testing', 'Tidal Volume', 'Variant', 'base', 'community setting', 'complex data ', 'computer science', 'dissemination research', 'effective therapy', 'experimental study', 'health care settings', 'implementation research', 'implementation science', 'implementation strategy', 'lung injury', 'machine learning method', 'mortality', 'multidisciplinary', 'novel', 'respiratory', 'response', 'theories', 'ventilation']",NHLBI,NORTHSHORE UNIVERSITY HEALTHSYSTEM,R01,2020,745390,-0.014892793805822162
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9848600,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data quality', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'large datasets', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2020,559088,-0.01451183588858766
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9929423,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Visualization', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data tools', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2020,754121,-0.015035066211128307
ICEES+ Knowledge Provider: Leveraging Open Clinical and Environmental Data to Accelerate and Drive Innovation in Translational Research and Clinical Care. No abstract provided n/a,ICEES+ Knowledge Provider: Leveraging Open Clinical and Environmental Data to Accelerate and Drive Innovation in Translational Research and Clinical Care.,10056783,OT2TR003430,"['Address', 'Algorithms', 'Automobile Driving', 'Award', 'Bayesian neural network', 'Clinical', 'Computational algorithm', 'Data', 'Data Analyses', 'Ensure', 'Etiology', 'Event', 'Funding', 'Goals', 'Infrastructure', 'Institutional Review Boards', 'Knowledge', 'Methods', 'Modeling', 'Modification', 'Multivariate Analysis', 'Neural Network Simulation', 'Pathway interactions', 'Privacy', 'Protocols documentation', 'Provider', 'Rare Diseases', 'Research', 'Schools', 'Secure', 'Security', 'Services', 'Specificity', 'Statistical Algorithm', 'Statistical Methods', 'Time', 'Translational Research', 'Variant', 'Work', 'adverse outcome', 'application programming interface', 'autoencoder', 'clinical care', 'cohort', 'data sharing', 'design', 'innovation', 'insight', 'machine learning method', 'patient mobility', 'patient privacy', 'preservation', 'prototype', 'recurrent neural network', 'spatiotemporal', 'tool']",NCATS,UNIV OF NORTH CAROLINA CHAPEL HILL,OT2,2020,811792,-0.015412606694854773
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),10019333,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip Prosthesis', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee Prosthesis', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'clinical center', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2020,629384,0.007106394158509185
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9984446,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2020,253654,-0.003918137454347447
"Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes PROJECT SUMMARY Overview: We will extend and develop implementations of foundational methods for analyzing populations of attributed connectomes. Our toolbox will enable brain scientists to (1) infer latent structure from individual connectomes, (2) identify meaningful clusters among populations of connectomes, and (3) detect relationships between connectomes and multivariate phenotypes. The methods we develop and extend will naturally overcome the challenges inherent in connectomics: high-dimensional non-Euclidean data with multi-level nonlinear interactions. Our implementations will comply with the highest open-source standards by: providing extensive online documentation and extended tutorials, hosting workshops to demonstrate our tools on an annual basis, and merging our implementations into commonly used packages such as scikit-learn [1], scipy [2], and networkx [3]. All of the code we develop is open source. We strive to ensure that our code is shared in accordance with the strictest guiding principles. We chose to implement these algorithms in Python due to its wide adoption in the neuroscience and data science fields. In particular, many other neuroscience tools applicable to connectomics, including NetworkX DiPy, mindboggle, nilearn, and nipy, are also implemented in Python. This will enable researchers to chain our analysis tools onto pre-existing pipelines for data preprocessing and visualization. Nonetheless, we feel that sharing our code in our own public repositories is insufficient for global reach. We have also begun reaching out to developers of the leading data science packages in python, including scipy, sklearn, networkx, scikit-image, and DiPy. For each of those packages, we have informal approval to begin integrating algorithms that we have developed. Those packages are collectively used by >220,000 other packages, so merging our algorithms into those packages will significantly extend our global reach. All researchers investigating connectomics, including all the authors of the 24,000 papers that mention the word “connectome”, will be able to apply state-of-the-art statistical theory and methods to their data. Currently, we have about 150 open source software projects on our NeuroData GitHub organization. Collectively, these projects get about 2,000 downloads and >11,000 views per month. As we incorporate additional functionality as described in this proposal, we expect far more researchers across disciplines and sectors will utilize our software. 20 ​ ​​ ​ ​​ Project Narrative Connectomes are an increasingly important modality for characterizing the structure of the brain, to complement behavior, genetics, and physiology. We and others have developed foundational statistical theory and methods over the last decade for the analysis of networks, networks with edge, vertex, and other attributes, and populations thereof, with preliminary implementations of those tools that we leverage in our laboratory for various application papers. In this project, we will extend our package, called graspy, to be of professional quality, implementing key functionality to include (1) estimating latent structure from attributed connectomes, (2) identifying meaningful clusters among populations of connectomes, and (3) detecting relationships between connectomes and multivariate phenotypes, such as behavior, genetics, and physiology. 18",Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes,10012519,RF1MH123233,"['Adoption', 'Algorithms', 'Behavioral Genetics', 'Brain', 'Code', 'Coin', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Development', 'Discipline', 'Documentation', 'Educational workshop', 'Ensure', 'Foundations', 'Funding', 'Genes', 'Human', 'Image', 'Individual', 'Journals', 'Laboratories', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modality', 'Modernization', 'Motivation', 'Neurosciences', 'Paper', 'Pathway Analysis', 'Phenotype', 'Physiology', 'Population', 'Population Analysis', 'Population Study', 'Property', 'PubMed', 'Publishing', 'Pythons', 'Research Personnel', 'Scientist', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Study', 'Structure', 'Telecommunications', 'Testing', 'Visualization', 'Work', 'brain research', 'connectome', 'data pipeline', 'design', 'high dimensionality', 'high standard', 'open source', 'public repository', 'software development', 'theories', 'tool', 'user-friendly']",NIMH,JOHNS HOPKINS UNIVERSITY,RF1,2020,1246005,-0.011338616763325562
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,10021018,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2020,255238,-0.006763481356079873
"CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience The field of network neuroscience has developed powerful analysis tools for studying brain networks and holds promise for deepening our understanding of the role played by brain networks in health, disease, development, and cognition. Despite widespread interest, barriers exist that prevent these tools from having broader impact. These include (1) unstandardized practices for sharing and documenting software, (2) long delays from when a method is first introduced to when it becomes publicly available, and (3) gaps in theoretic knowledge and understanding leading to incorrect, delays due to mistakes, and errors in reported results. These barriers ultimately slow the rate of neuroscientific discovery and stall progress in applied domains. To overcome these challenges, we will use open science methods and cloud-computing, to increase the availability of network neuroscience tools. We will use the platform ""brainlife.io"" for sharing these tools, which will be packaged into self-contained, standardized, reproducible Apps, shared with and modified by a community of users, and integrated into existing brainlife.io analysis pipelines. Apps will also be accompanied by links to primary sources, in-depth tutorials, and documentation, and worked-through examples, highlighting their correct usage and offering solutions for mitigating possible pitfalls. In standardizing and packaging network neuroscience tools as Apps, this proposed research will engage a new generation of neuroscientists, providing them powerful new and leading to new discoveries. Second, the proposed research will contribute growing suite of modeling analysis that can be modified to suit specialized purposes. Finally, the Brainlife.io platform will serve as part of the infrastructure supporting neuroscience research. Altogether, these advances will lead to new opportunities in network neuroscience research and further stimulate its growth while increasing synergies with other domains in neuroscience. Structural and functional networks support cognitive processes. Miswiring networks lead to maladaptive behavior and neuropsychicatric disorders. Network neuroscience is a young field that provides a quantitative framework for modeling brain networks. This project will make network neuroscientific tools available to new users via open science and cloud-computing. New applications of these tools this will lead deeper insight into the role of networks in health as well as in clinical disorders.",CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience,10019389,R01EB029272,"['Address', 'Aging', 'Behavior', 'Biophysics', 'Brain', 'Clinical', 'Cloud Computing', 'Cognition', 'Communities', 'Complex Analysis', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ecosystem', 'Education', 'Elements', 'France', 'Funding', 'Generations', 'Graph', 'Growth', 'Health', 'Infrastructure', 'Instruction', 'Knowledge', 'Language', 'Lead', 'Libraries', 'Link', 'Literature', 'Mathematics', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Pathway Analysis', 'Play', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Science', 'Sociology', 'Source', 'Standardization', 'Structure', 'Study models', 'System', 'Techniques', 'Time', 'Training', 'Work', 'analysis pipeline', 'brain computer interface', 'cloud based', 'cognitive process', 'cyber infrastructure', 'data sharing', 'experience', 'innovation', 'insight', 'interest', 'machine learning method', 'network architecture', 'network models', 'neuroimaging', 'open data', 'prevent', 'relating to nervous system', 'statistics', 'support network', 'synergism', 'tool']",NIBIB,INDIANA UNIVERSITY BLOOMINGTON,R01,2020,218594,-0.021840064511976093
"Mental, measurement, and model complexity in neuroscience PROJECT SUMMARY Neuroscience is producing increasingly complex data sets, including measures and manipulations of sub- cellular, cellular, and multi-cellular mechanisms operating over multiple timescales and in the context of different behaviors and task conditions. These data sets pose several fundamental challenges. First, for a given data set, what are the relevant spatial, temporal, and computational scales in which the underlying information-processing dynamics are best understood? Second, what are the best ways to design and select models to account for these dynamics, given the inevitably limited, noisy, and uneven spatial and temporal sampling used to collect the data? Third, what can increasingly complex data sets, collected under increasingly complex conditions, tells us about how the brain itself processes complex information? The goal of this project is to develop and disseminate new, theoretically grounded methods to help researchers to overcome these challenges. Our primary hypothesis is that resolving, modeling, and interpreting relevant information- processing dynamics from complex data sets depends critically on approaches that are built upon understanding the notion of complexity itself. A key insight driving this proposal is that definitions of complexity that come from different fields, and often with different interpretations, in fact have a common mathematical foundation. This common foundation implies that different approaches, from direct analyses of empirical data to model fitting, can extract statistical features related to computational complexity that can be compared directly to each other and interpreted in the context of ideal-observer benchmarks. Starting with this idea, we will pursue three specific aims: 1) establish a common theoretical foundation for analyzing both data and model complexity; 2) develop practical, complexity-based tools for data analysis and model selection; and 3) establish the usefulness of complexity-based metrics for understanding how the brain processes complex information. Together, these Aims provide new theoretical and practical tools for understanding how the brain integrates information across large temporal and spatial scales, using formal, universal definitions of complexity to facilitate the analysis and interpretation of complex neural and behavioral data sets. PROJECT NARRATIVE The proposed work will establish new, theoretically grounded computational tools to help neuroscience researchers design and analyze studies of brain function. These tools, which will be made widely available to the neuroscience research community, will help support a broad range of studies of the brain, enhance scientific discovery, and promote rigor and reproducibility.","Mental, measurement, and model complexity in neuroscience",10002220,R01EB026945,"['Address', 'Algorithms', 'Automobile Driving', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Benchmarking', 'Brain', 'Characteristics', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Decision Making', 'Dimensions', 'Foundations', 'Goals', 'Guidelines', 'Human', 'Individual', 'Information Theory', 'Length', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Pattern', 'Physics', 'Process', 'Psyche structure', 'Reproducibility', 'Research Personnel', 'Rodent', 'Sampling', 'Series', 'Structure', 'System', 'Techniques', 'Time', 'Work', 'base', 'complex data ', 'computer science', 'computerized tools', 'data modeling', 'data streams', 'data tools', 'design', 'information processing', 'insight', 'nonhuman primate', 'relating to nervous system', 'statistics', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2020,20957,-0.02151146091545368
"University of Buffalo Clinical and Translational Science Institute The Buffalo Translational Consortium (BTC), which includes the University at Buffalo (UB) health sciences schools, the major healthcare institutions in our region, four key research institutes and five influential community partners, have embarked on a comprehensive strategic plan to build a strong foundation for clinical and translational research in response to our community needs. Buffalo is the second most populous city in New York State and has a rich cultural history. The proportion of underrepresented minorities in Buffalo in 2018 (50%) parallels that projected for the US in 2050, making Buffalo a microcosm of what the US will look like in 30 years. A similar proportion of our population experiences health disparities. The vision for our CTSA hub is to perform innovative research across the translational spectrum to improve the health of our community and the nation. We will develop, test and share novel approaches to engage difficult-to-engage populations and reduce health disparities in our community, which represents a “population of the future”. Guided by our vision, the CTSA has catalyzed a transformation of our environment since our CTSA was first funded in August 2015 with remarkable growth in clinical and translational research. Further, in just the past year, the UB medical school has moved into a spectacular new building and our clinical partner, Kaleida Health, the largest healthcare system in the region, opened the new Oishei Children’s Hospital, both on the Buffalo Niagara Medical Campus and connected to the Clinical and Translational Research Center devoted entirely to clinical and translational research that opened in 2012. This rapid and continuing trajectory of growth in healthcare and research in the region has resulted in a new 21st century Academic Health Center with healthcare, medical education and clinical and translational research on one campus in the heart of Buffalo, creating a foundation to enhance the impact of our CTSA even further. While launching our CTSA, we have prioritized participation in the national consortium through hosting and testing Innovation Labs as a team science tool, working with multiple hubs on initiatives to solve translational research barriers and sharing tools that we have developed with the CTSA consortium, including novel health informatics tools. Our CTSA has five ambitious but achievable aims, including: 1) Accelerate innovative translational research with teams that engage communities, regional stakeholders and the national consortium; 2) Train an excellent, diverse workforce to advance translation of discoveries; 3) Enhance inclusion of special populations across the lifespan and difficult-to-engage populations; 4) Streamline clinical research processes focusing on quality and efficiency with emphasis on multisite studies; 5) Develop, test and share biomedical informatics tools to integrate data from multiple sources to speed translation. Guided by our vision to perform research to improve the health of our community and the nation, we will continue our momentum to expand translational research, train our diverse workforce, streamline processes, engage our community, and actively contribute to the national consortium. The University at Buffalo Clinical and Translational Science Institute (CTSI) is the coordinating center of the Buffalo Translational Consortium, which includes the region's premier research, educational and clinical institutions with influential community partners. The vision of the CTSI is to perform innovative clinical and translational research to reduce health disparities and improve the health of our community and the nation. We engage our community as research partners to create a shared environment to bring discoveries in the laboratory, clinic and community to benefit individual and public health.",University of Buffalo Clinical and Translational Science Institute,10053435,UL1TR001412,"['Achievement', 'Address', 'Adopted', 'African American', 'Buffaloes', 'Center for Translational Science Activities', 'Cities', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Health', 'County', 'Coupled', 'Cultural Backgrounds', 'Data', 'Diverse Workforce', 'Ensure', 'Environment', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Healthcare', 'Healthcare Systems', 'Heart', 'Image', 'Imaging technology', 'Individual', 'Influentials', 'Informatics', 'Institutes', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Life Expectancy', 'Longevity', 'Medical', 'Medical Education', 'Medical center', 'Methods', 'Natural Language Processing', 'New York', 'Outcomes Research', 'Participant', 'Pediatric Hospitals', 'Phenotype', 'Population', 'Poverty', 'Process', 'Program Development', 'Prospective Studies', 'Public Health', 'Public Health Informatics', 'Recording of previous events', 'Recruitment Activity', 'Refugees', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Schools', 'Science', 'Sensitivity and Specificity', 'Site', 'Speed', 'Strategic Planning', 'System', 'Testing', 'Training', 'Translational Research', 'Translations', 'Underrepresented Minority', 'Universities', 'Vision', 'Work', 'Workforce Development', 'base', 'biomedical informatics', 'clinical center', 'clinical data warehouse', 'community partnership', 'data sharing', 'education research', 'experience', 'health care disparity', 'health disparity', 'imaging genetics', 'improved', 'informatics tool', 'innovation', 'interoperability', 'medical schools', 'multidisciplinary', 'multiple data sources', 'named group', 'novel', 'novel strategies', 'recruit', 'response', 'sharing platform', 'skills', 'social health determinants', 'structured data', 'tool', 'translational impact', 'translational pipeline', 'translational scientist', 'unstructured data']",NCATS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,UL1,2020,4118079,0.007547926144678857
"Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics Abstract Support is requested for a Keystone Symposia conference entitled Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics, organized by Drs. Jose M. Lora and Timothy K. Lu. The conference will be held in Breckenridge, Colorado from March 29- April 1, 2019. Synthetic Biology tools and principles have matured tremendously over the last decade and have reached extraordinary levels of sophistication, both in eukaryotic and prokaryotic systems. Synthetic biology as a therapeutic modality is starting to enter multiple clinical studies and has the potential to have a significant impact on medicine across a wide range of diseases (e.g., metabolic, immune-mediated, cancer, and neurologic diseases). This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine. While there are conferences that capture synthetic biology in only a few talks mixed in among other various topics, there is a paucity of conferences focused on synthetic biology as drugs to treat disease. However, due to the rapid pace of fundamental scientific advances along with an expanding number of biotechnology companies and emerging clinical studies with synthetic biology at their core, this conference will be highly relevant for a wide audience of scientists both from academia and industry. In addition, other meetings in this field have a highly technology-driven focus on synthetic biology techniques with relatively little attention given to biological and medical context. Ultimately, this Keystone Symposia conference should inspire researchers from diverse backgrounds to discuss synthetic biology via many new angles. PROJECT NARRATIVE Over the past two decades, tremendous advances have been made in the use of biological parts to engineer systems that can effectively direct living cells for a vast variety of purposes (a.k.a. synthetic biology). Synthetic biology is being used to construct more effective therapies in diseases such as cancer, but there are remaining obstacles to the clinical translation of these therapies. This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine.",Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics,9913772,R13EB029305,"['Academia', 'Address', 'Area', 'Attention', 'Biological', 'Biomedical Research', 'Biotechnology', 'Cells', 'Clinical Research', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Colorado', 'Computers', 'Disease', 'Educational workshop', 'Engineering', 'Future', 'Genetic Engineering', 'Genetic Screening', 'Human', 'Immune', 'Industrialization', 'Industry', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Medicine', 'Metabolic', 'Methodology', 'Modality', 'Neurologic', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Preventive', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Scientific Advances and Accomplishments', 'Scientist', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Work', 'clinical application', 'clinical practice', 'clinical translation', 'combinatorial', 'design', 'effective therapy', 'graduate student', 'meetings', 'nervous system disorder', 'next generation', 'novel diagnostics', 'posters', 'symposium', 'synthetic biology', 'targeted treatment', 'tool']",NIBIB,KEYSTONE SYMPOSIA,R13,2020,10000,-0.009865768318992427
"Toward a High Dimensional Computational Description of Variation in Human Decision-Making Across Psychiatric and Non-Psychiatric Populations TOWARD A HIGH DIMENSIONAL COMPUTATIONAL DESCRIPTION OF VARIATION IN HUMAN DECISION-MAKING ACROSS PSYCHIATRIC AND NON-PSYCHIATRIC POPULATIONS PI: Dr. John P. O'Doherty Institution: California Institute of Technology PROJECT SUMMARY  Computational psychiatry (CP) promises to gain deeper explanatory insight into psychiatric disorders through the application of formal computational models to task-related behavioral data and brain measures. However, research in CP to date has mostly involved a narrow unidimensional focus, utilizing either a relatively limited set of computational constructs such as simple model-free (MF) reinforcement learning (RL), and/or restricted to studying a specific task, a specific disease, or even a particular model parameter. For CP to reach its potential, we need to broaden the field's scope. To achieve this, it is necessary to develop an integrated theory and formal framework supported by a task battery that will enable the quantification of individual differences across a range of computational mechanisms pertinent to the diagnosis and treatment of clinical disorders. The objective of the current proposal is to implement the initial groundwork needed to build and test a computational framework and task battery that can facilitate a multi-dimensional computational description of individual variability in parameters relevant for characterizing psychiatric dysfunction.  We have constructed a computational assessment battery (CAB) consisting of four distinct yet inter- related tasks that move beyond simple RL to probe various aspects of learning, cognition, and decision-making. First, we assess learning about losses as well as rewards. Secondly, we measure model-based (MB) alongside simple MF learning and decision making, and the arbitration allocating control to either strategy. Thirdly, we examine strategies for solving the exploration/exploitation dilemma, in which individuals have to decide whether to exploit an option known to yield reward or explore an option whose outcomes are unknown. Finally, we assess social-learning, in which an individual can either infer the goals of another individual or simply imitate that agent's behavior.  We propose to build an integrated computational model that can capture relevant computations being implemented in each of the tasks in our battery, alongside a hierarchical model-fitting and parameter estimation framework to enable us to retrieve reliable parameter estimates for each computational variable of interest. We will leverage common computational mechanisms engaged across our task battery to improve estimability and generalizability. We will then acquire behavioral data in a large on-line (n=1000), and modest (n=100) in-lab sample on the CAB to establish the internal validity and test/re-test reliability of our computational model and parameter estimates. Finally, we will explore the relationship between model-estimated parameter estimates from behavior on our task battery and variability in self-reported traits and states relevant for psychiatric disease in our healthy samples, as well as in a pilot sample (n=60) of patients recruited from the UCLA psychiatry outpatient clinics. These efforts will form the basis of a Computational Psychiatry Toolbox for behavioral testing, model-fitting and parameter estimation that will eventually be released as a resource to the research community. PROJECT NARRATIVE Our goal is to develop a comprehensive approach to characterize an individual's ability to think, learn and make decisions in terms of a series of specific brain processes or `computations'. We then aim to determine whether variation in these computations can in turn be related to a variety of symptoms that can occur in psychiatric disorders. To achieve this, we will develop new mathematical and experimental tools to assess variation in behavior across individuals, and refine, test and validate these tools in a preliminary sample of healthy participants and psychiatric patients.",Toward a High Dimensional Computational Description of Variation in Human Decision-Making Across Psychiatric and Non-Psychiatric Populations,9989897,R21MH120805,"['Ambulatory Care Facilities', 'Anxiety', 'Arbitration', 'Behavior', 'Behavioral', 'Biological', 'Brain', 'California', 'Cell Nucleus', 'Classification', 'Clinical', 'Clinical Treatment', 'Cognition', 'Communities', 'Complex', 'Computational algorithm', 'Computer Models', 'Data', 'Decision Making', 'Development', 'Diagnosis', 'Dimensions', 'Discipline', 'Disease', 'Ensure', 'Functional disorder', 'General Population', 'Goals', 'Human', 'Impulsivity', 'Individual', 'Individual Differences', 'Institutes', 'Institution', 'Learning', 'Mathematics', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Moods', 'National Institute of Mental Health', 'Outcome', 'Participant', 'Patient Recruitments', 'Patient Self-Report', 'Patients', 'Performance', 'Plant Roots', 'Population', 'Procedures', 'Process', 'Psychiatric therapeutic procedure', 'Psychiatry', 'Psychological reinforcement', 'Research', 'Resources', 'Rewards', 'Sampling', 'Series', 'Supervision', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Treatment Protocols', 'Undifferentiated', 'Validation', 'Validity and Reliability', 'Variant', 'base', 'behavior test', 'cohort', 'computer framework', 'high dimensionality', 'improved', 'individual variation', 'insight', 'interest', 'large scale simulation', 'recruit', 'social learning', 'targeted treatment', 'theories', 'tool', 'trait', 'unsupervised learning']",NIMH,CALIFORNIA INSTITUTE OF TECHNOLOGY,R21,2020,204080,-0.021610720199301853
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,0.00035212529467485446
"South Carolina Clinical & Translational Research Institute (SCTR) PROJECT SUMMARY – SCTR INSTITUTE Parent Award UL1-TR001450 Since 2009, the South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina (SC) by creating a Learning Health System that supports high- quality clinical and translational research (CTR) and fosters collaboration and innovation. Headquartered at the Medical University of South Carolina (MUSC), SCTR has engaged stakeholders and created statewide partnerships to improve care and address social determinants of health across SC. However, greater than 75% of SC is rural, and all 46 counties contain areas designated as medically underserved, so health disparities remain an issue. Over the next five years, SCTR will strengthen its outreach to these medically underserved areas through collaboration with the Clemson University Health Extension Program and the MUSC Telehealth Center of Excellence. With a focus on implementation and dissemination as well as discovery, we will develop and demonstrate innovative technologies and outreach to improve the health of our stakeholders. We will build on prior successes and introduce innovative approaches to expand CTR across SC through the following aims: Aim 1. Extend and enhance high-quality, innovative, flexible curricula and training experiences for all levels of the CTR workforce, with particular emphasis on enhancing workforce heterogeneity and team science. Aim 2. Engage a diverse group of stakeholders as active partners in CTR to address health care priorities while enhancing the scientific knowledge base about collaboration and engagement. Aim 3. Promote greater inclusion across the full translational spectrum of research by engaging investigators from many disciplines and patient populations from diverse demographic backgrounds and geographic areas. Aim 4. Develop, demonstrate and disseminate innovative methods and processes to address barriers and accelerate the translation of research discoveries to improvements in human health that can be generalized to a variety of practice settings. Aim 5. Enhance the conduct of translational research through the development of secure and innovative informatics and digital health solutions, tools and methodologies that affect every aspect of CTR. SCTR’s vision is to be a major force in facilitating the translation of innovative science into practice to address the health priorities of the citizens of SC and beyond. To achieve this vision, SCTR’s mission is to catalyze the development of methods and technologies that lead to more efficient translation of biomedical discoveries into interventions that improve individual and public health. SCTR will serve as the statewide academic home for CTR, one that is well-integrated with SC’s healthcare systems and provides essential support for innovative, efficient, multidisciplinary research and research training. We will work within SCTR, with our partners across SC and with the CTSA Consortium to realize this vision. PROJECT NARRATIVE The South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina by creating a Learning Health System characterized by strong training and infrastructure resources that stimulate collaboration and innovation as a means to accelerate the translation of biomedical research discoveries into human health improvements. A major focus of this application is to strengthen SCTR’s statewide collaborations through innovative partnerships and initiatives with an emphasis on rural and medically underserved communities where significant health disparities exist. We will develop, demonstrate and disseminate innovative ways to train a diverse workforce and address barriers to translational research, and we will continue to collaborate across the CTSA Consortium to maximize impact and improve the health of the nation.",South Carolina Clinical & Translational Research Institute (SCTR),10241088,UL1TR001450,"['2019-nCoV', 'Address', 'Administrative Supplement', 'Affect', 'Area', 'Award', 'Biomedical Research', 'COVID-19', 'COVID-19 pandemic', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials Network', 'Clinical and Translational Science Awards', 'Clinical effectiveness', 'Collaborations', 'Communities', 'County', 'Databases', 'Development', 'Discipline', 'Diverse Workforce', 'Educational Curriculum', 'Electronic Health Record', 'Emergency Situation', 'Environment', 'Fast Healthcare Interoperability Resources', 'Fostering', 'Geographic Locations', 'Health', 'Health Priorities', 'Health Sciences', 'Health system', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Home environment', 'Human', 'Individual', 'Informatics', 'Information Retrieval', 'Interdisciplinary Study', 'Intervention', 'Lead', 'Learning', 'Link', 'Medical', 'Medically Underserved Area', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Natural Language Processing', 'Outcome', 'Parents', 'Patients', 'Phenotype', 'Population', 'Population Heterogeneity', 'Process', 'Public Health', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Rural', 'Science', 'Secure', 'South Carolina', 'Support System', 'Technology', 'Terminology', 'Text', 'Training', 'Training and Infrastructure', 'Translating', 'Translational Research', 'Translations', 'Universities', 'Vision', 'Work', 'base', 'biomedical informatics', 'cohort', 'coronavirus disease', 'cost effective', 'data access', 'data enclave', 'data interoperability', 'data modeling', 'data sharing', 'digital', 'expectation', 'experience', 'flexibility', 'health disparity', 'improved', 'innovation', 'innovative technologies', 'interest', 'knowledge base', 'medically underserved', 'member', 'method development', 'outreach', 'parent grant', 'patient population', 'practice setting', 'programs', 'response', 'rural underserved', 'social health determinants', 'success', 'synergism', 'telehealth', 'tool', 'translational pipeline', 'web services']",NCATS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,UL1,2020,100000,-0.0003178004731016059
"Tufts Clinical and Translational Science Institute (N3C Supplement) PROJECT SUMMARY Tufts Clinical and Translational Science Institute (Tufts CTSI) is based on the conviction that authentic involvement of the entire spectrum of clinical and translational research (CTR) is critical to fulfilling the promise of biomedical science for meeting the public's needs. This includes not only from translation from bench to bedside (T1 translation), but also, crucially for having health impact, translation into effective clinical practice (T2), care delivery and public health (T3), and health policy (T4). Advances on all of these fronts is increasingly dependent on making effective use of scientific data from multiple domains. The COVID-19 global emergency presents both an immediate challenge and an opportunity to progress on important data sharing aims emphasized by NIH. In response, NCATS and the Centers for Translational Science Award (CTSA) hubs, several HHS agencies, and other partnering organizations have committed to developing a next-generation repository for clinical data related to COVID-19, the National COVID Cohort Collaborative (N3C), as a means of accelerating global research into the disease and aiding the development of diagnostics, therapeutics, and effective vaccines. The N3C initiative's goal of improving the efficiency and accessibility of analyses with clinical data is consistent with the primary informatics objectives of Tufts CTSI, which am to reduce barriers to the integration of healthcare and research by providing innovative systems, data repositories, and analytical tools, and by enabling greater exchange and collaboration through interoperability, standardization, and resource sharing. In- line with shared objectives, in this supplement we seek to contribute to the N3C initiative as a data provider and thought partner through the following specific aims: (1) continue to play an important role providing tools and resources for N3C's analytics platform; and (2) ensure Tufts CTSI's Informatics Program has sufficient staff and technical resources to continue to provide COVID-specific patient data from our hub to the N3C repository. PROJECT NARRATIVE An integrated, continuously updated data repository and a platform of putting powerful analytics capabilities at the disposal of the scientific community can generate insights into COVID-19 and accelerate the development of effective treatments and vaccines to counter the disease. In this project, Tufts Clinical and Translational Science Institute plans to contribute to this goal by providing carefully structured clinical data and innovative informatics tools to the National COVID Cohort Collaborative (N3C), an initiative demonstrating a novel approach for collaborative pandemic data sharing.",Tufts Clinical and Translational Science Institute (N3C Supplement),10172199,UL1TR002544,"['Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communities', 'Data', 'Development', 'Diagnostic', 'Disease', 'Emergency Situation', 'Ensure', 'Environment', 'Funding', 'Goals', 'Health', 'Health Care Research', 'Health Policy', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Mission', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Play', 'Provider', 'Public Health', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Risk', 'Role', 'Science', 'Secure', 'Speed', 'Standardization', 'Structure', 'System', 'Time', 'Translational Research', 'Translations', 'Treatment Efficacy', 'United States National Institutes of Health', 'Update', 'Vaccines', 'analytical tool', 'base', 'bench to bedside', 'care delivery', 'clinical data warehouse', 'clinical decision support', 'clinical practice', 'cohort', 'collaborative approach', 'convict', 'coronavirus disease', 'data integration', 'data sharing', 'data warehouse', 'effective therapy', 'health management', 'improved', 'informatics tool', 'innovation', 'insight', 'interoperability', 'meetings', 'next generation', 'novel strategies', 'pandemic disease', 'programs', 'repository', 'response', 'social determinants', 'support tools', 'tool']",NCATS,TUFTS UNIVERSITY BOSTON,UL1,2020,100000,-2.1166538728958613e-05
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,-0.026189269610606462
"AUGS/DUKE UrogynCREST program PROJECT SUMMARY Health Services Research (HSR) and predictive analytics are rapidly growing fields and will have enormous implications for women’s health research in pelvic floor disorders (PFDs). The AUGS/DUKE Urogynecology Clinical Research Educational Scientist Training (UrogynCREST) program will prepare participants to recognize the critical role that data play in delivering high quality health care. It brings together expertise in health service and women’s health research, medical informatics and prediction modeling. This program will target Urogynecology Faculty at the Assistant Professor level who seek successful careers in health services research (HSR) and analytics. Participants will obtain skills through a combination of didactic and interactive coursework; hands-on manipulation of data through extraction, cleaning, and analysis; and project-based one on one mentoring. The UrogynCREST program will be an interactive, hands-on educational program with centralized activities organized and delivered by distance through a popular on-line learning platform called Sakai, with educational software designed to support teaching, research and collaboration. A diverse faculty with expertise in data sciences teaches courses and the advanced methodology required to perform HSR. Yearly in-person meetings at the annual American Urogynecologic Society meeting enhance networking and the development of partnerships between participants from various institutions, as well as, interactions with the mentors and other HSR in the field. The program’s strategy allows national leaders with particular skills in the field to provide their knowledge to the participants and help mentor them through development of a relevant research question and identification of an appropriate and existing database(s) to address the question. With the guidance of a dedicated statistician and analyst programmer, participants will learn and perform the necessary computer programming needed to extract, clean and analyze these data. Participants whose projects involve the development of prediction models in the form of scores, nomograms or other tools will learn how to build and validate such tools in the existing project. Each participant’s project will culminate in the completion of a submitted manuscript to a peer- reviewed journal or study proposal and publicly available tools when relevant. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and resources for invigorating data discovery and tools for investigations in HSR specifically addressing (PFDs). PROJECT NARRATIVE: The AUGS/DUKE UrogynCREST program will prepare participants to recognize the critical role that data play in delivering high quality health care for pelvic floor disorders. It will add structure to the health data science education for Assistant Professor Level Faculty in Urogynecology by bringing together expertise in health service and women’s health research, medical informatics, and prediction modeling. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and mentorship for invigorating data discovery and tools for investigations in health service research specifically addressing pelvic floor disorders.",AUGS/DUKE UrogynCREST program,9918946,R25HD094667,"['Address', 'Age', 'American', 'Area', 'Caring', 'Clinical Research', 'Collaborations', 'Communities', 'Connective Tissue', 'Data', 'Data Discovery', 'Data Science', 'Databases', 'Development', 'E-learning', 'Educational process of instructing', 'Faculty', 'Fecal Incontinence', 'Fostering', 'Future', 'Goals', 'Health Services', 'Health Services Research', 'Healthcare', 'Infrastructure', 'Institution', 'Instruction', 'Investigation', 'Journals', 'Knowledge', 'Lead', 'Learning', 'Manuscripts', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modernization', 'Muscle', 'Nomograms', 'Participant', 'Peer Review', 'Pelvic Floor Disorders', 'Pelvis', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Public Health', 'Research', 'Resources', 'Role', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Shapes', 'Societies', 'Software Design', 'Structure', 'Techniques', 'Testing', 'Training Programs', 'Urinary Incontinence', 'Woman', 'Women&apos', 's Health', 'base', 'career', 'clinical decision-making', 'clinical development', 'computer program', 'computer science', 'data tools', 'design', 'health care quality', 'health data', 'improved', 'injured', 'innovation', 'meetings', 'pelvic organ prolapse', 'predictive modeling', 'professor', 'programs', 'recruit', 'science education', 'skills', 'social', 'statistical and machine learning', 'tool']",NICHD,DUKE UNIVERSITY,R25,2020,153800,-0.005936854529088514
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,9924443,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Models', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2020,7143424,-0.02214869461433943
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),10007746,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Infrastructure', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data de-identification', 'data harmonization', 'data management', 'data portal', 'data privacy', 'data sharing', 'data tools', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'preservation', 'social', 'success', 'theories', 'therapy design']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2020,103799,-0.0010061296907460879
"Modeling the Incompleteness and Biases of Health Data Modeling the Incompleteness and Biases of Health Data Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. Existing efforts for missing health data imputation often focus on only cross-sectional correlation (e.g., correlation across subjects or across variables) but neglect autocorrelation (e.g., correlation across time points). Moreover, they often focus on modeling incompleteness but neglect the biases in health data. Modeling both the incompleteness and bias may contribute to better understanding of health data and better support clinical decision making. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets. Aim 1 introduces the MICA framework to jointly consider cross-sectional correlation and auto-correlation. In Aim 2, we will augment MICA to be bias-aware (hence BAMICA) to account for biases stemmed from multiple roots such as healthcare process and use them as features in imputing missing health data. This augmentation is achieved by a novel recurrent neural network architecture that keeps track of both evolution of health data variables and bias factors. In Aim 3, we will supplement unstructured clinical notes to structured health data for modeling incompleteness and biases using a novel architecture of graph neural network on top of memory network. We will apply graph neural networks to process clinical notes in order to learn proper representations as input to the memory networks for imputation and downstream predictive modeling tasks. Depending on the clinical problem and data availability, not all modules may be needed. Thus our proposed BAMICA framework is designed to be flexible and consists of selectable modules to meet some or all of the above needs. In summary, our proposal bridges a key knowledge gap in jointly modeling incompleteness and biases in health data and utilizes unstructured clinical notes to supplement and augment such modeling in order to better support predictive modeling and clinical decision making. We will demonstrate generalizability by experimenting on four large clinical and cohort study datasets, and by scaling up to the eMERGE network spanning 11 institutions nationwide. We will disseminate the open-source framework. The principled and flexible framework generated by this project will bring significant methodological advancement and have a direct impact on enhancing discovery from health data. Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets.",Modeling the Incompleteness and Biases of Health Data,9941499,R01LM013337,"['Adoption', 'Algorithms', 'Architecture', 'Awareness', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Collection', 'Communities', 'Computer software', 'Critical Care', 'Data', 'Data Collection', 'Data Set', 'Dependence', 'Derivation procedure', 'Development', 'Diagnostic', 'Diagnostic tests', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Evolution', 'Functional disorder', 'General Hospitals', 'Goals', 'Graph', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Hour', 'Individual', 'Inpatients', 'Institution', 'Intuition', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Learning', 'Measurement', 'Medical', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Procedures', 'Process', 'Protocols documentation', 'Regimen', 'Research', 'Research Personnel', 'Resources', 'Role', 'Schedule', 'Structure', 'Symptoms', 'System', 'Test Result', 'Testing', 'Time', 'Training', 'Validation', 'clinical decision support', 'clinical decision-making', 'data mining', 'data quality', 'design', 'experimental study', 'flexibility', 'health care service utilization', 'health data', 'improved', 'lifetime risk', 'machine learning algorithm', 'neglect', 'neural network', 'neural network architecture', 'novel', 'open source', 'patient population', 'personalized diagnostics', 'personalized therapeutic', 'predictive modeling', 'recurrent neural network', 'scale up', 'social health determinants', 'stem', 'structured data', 'text searching', 'tool', 'trait']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,348397,-0.008081077679124098
"WASHINGTON UNIVERSITY SCHOOL OF MEDICINE UNDIAGNOSED DISEASES NETWORK CLINICAL SITE 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",WASHINGTON UNIVERSITY SCHOOL OF MEDICINE UNDIAGNOSED DISEASES NETWORK CLINICAL SITE,10124937,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Models', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data dissemination', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2020,150000,-0.007560771926925439
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9977220,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Models', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data dissemination', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2020,550000,-0.007560771926925439
"Computational Methods for Enhancing Privacy in Biomedical Data Sharing Project Summary Data sharing is essential to modern biomedical data science. Access to a large amount of genomic and clinical data can help us better understand human genetics and its impact on health and disease. However, the sensitive nature of biomedical information presents a key bottleneck in data sharing and collection efforts, limiting the utility of these data for science. The goal of this project is to leverage cutting-edge advances in cryptography and information theory to develop innovative computational frameworks for privacy-preserving sharing and analysis of biomedical data. We will draw upon our recent success in developing secure pipelines for collaborative biomedical analyses to address the imminent need to share sensitive data securely and at scale.  Practical adoption of existing privacy-preserving techniques in biomedicine has thus far been largely limited due to two major pitfalls, which this project overcomes with novel technical advances. First, emerging cryptographic data sharing frameworks, which promise to enable collaborative analysis pipelines that securely combine data across multiple institutions with theoretical privacy guarantees, are too costly to support complex and large-scale computations required in biomedical analyses. In this project, we will build upon recent advances in cryptography (e.g., secure distributed computation, pseudorandom correlation, zero-knowledge proofs) to significantly enhance the scalability and security of cryptographic biomedical data sharing pipelines. Second, existing approaches that locally transform data to protect sensitive information before sharing (e.g. de-identification techniques) either offer insufficient levels of protection or require excessive perturbation in order to ensure privacy. We will draw upon recent tools from information theory to develop effective local privacy protection methods that achieve superior utility-privacy tradeoffs on a range of biomedical data including genomes, transcriptomes, and medical images by directly exploiting the latent correlation structure of the data.  To promote the use of our privacy techniques, we will create production-grade software of our tools and publicly release them. We will also actively participate in international standard-setting organizations in genomics, e.g. GA4GH and ICDA, to incorporate our insights into community guidelines for biomedical privacy. Successful completion of these aims will result in computational methods and software tools that open the door to secure sharing and analysis of massive sets of sensitive genomic and clinical data. Our long-term goal is to broadly enable data sharing and collaboration efforts in biomedicine, thus empowering researchers to better understand the molecular basis of human health and to drive translation of new biological insights to the clinic. Project Narrative Rapidly-growing volume of biomedical datasets around the world promises to enable unprecedented insights into human health and disease. However, increasing concerns for individual privacy severely limited the extent of data sharing in the field. This project draws upon cutting-edge tools from cryptography and information theory to develop effective privacy- preserving methods for collecting, sharing, and analyzing sensitive biomedical data to empower advances in genomics and medicine.",Computational Methods for Enhancing Privacy in Biomedical Data Sharing,10017554,DP5OD029574,"['Address', 'Adoption', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Clinic', 'Clinical Data', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Disease', 'Electronic Health Record', 'Engineering', 'Enhancement Technology', 'Ensure', 'Foundations', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Genetics', 'Image', 'Individual', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'International', 'Knowledge', 'Letters', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mainstreaming', 'Mathematics', 'Medical Genetics', 'Medical Imaging', 'Mentorship', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Pattern', 'Pharmacology', 'Policies', 'Polynomial Models', 'Preservation Technique', 'Privacy', 'Privatization', 'Production', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Science', 'Secure', 'Security', 'Software Tools', 'Structure', 'Techniques', 'Technology', 'Translations', 'Vision', 'Work', 'analysis pipeline', 'base', 'biomedical data science', 'computer framework', 'computing resources', 'cost', 'cryptography', 'data sharing', 'design', 'empowered', 'experience', 'experimental study', 'genetic analysis', 'genome wide association study', 'genomic data', 'infancy', 'innovation', 'insight', 'novel', 'privacy preservation', 'privacy protection', 'software development', 'statistics', 'structured data', 'success', 'task analysis', 'tool', 'transcriptome', 'transcriptomics', 'web server']",OD,"BROAD INSTITUTE, INC.",DP5,2020,392799,0.01075464428461077
"Alliance for Regenerative Rehabilitation Research & Training 2.0 (AR3T 2.0) OVERALL: ABSTRACT  The scope of regenerative medicine encompasses the repair, regeneration, and replacement of defective, injured, and diseased tissues and organs. The success of regenerative therapies is dependent, at least in part, on a favorable microenvironment in which the regenerative processes occur. Technological innovations and a deepened mechanistic understanding of how these microenvironmental signals influence tissue regeneration has drawn attention to the critical importance of the clinical field with foundations in the application of physical, thermal, and electrical stimuli to promote functional restoration—rehabilitation. We propose that the fields of regenerative medicine and rehabilitative science are inextricably intertwined, an intersection of disciplines that we and others have termed Regenerative Rehabilitation. To realize the full potential of Regenerative Rehabilitation, there is a need for formalized mechanisms that promote the interaction of basic scientists with rehabilitation specialists. During the initial funding cycle, the Alliance for Regenerative Rehabilitation Research & Training (AR3T) built a national network of investigators and programs that has helped to expand scientific knowledge, expertise and methodologies across the domains of regenerative medicine and rehabilitation. This proposal seeks funding for AR3T 2.0, in which we will build on successes achieved and lessons learned over the initial period of support with the goal of being even more responsive to the needs of the greater community. Six specific aims define a framework upon which we will achieve our goals. AR3T will provide education and drive the science underlying Regenerative Rehabilitation by: 1) Providing didactic programs that expose rehabilitation researchers to cutting-edge investigations and state-of-the-art technologies in the field of regenerative medicine (Didactic Aim); 2) Cultivating collaborative opportunities between renowned investigators in the fields of regenerative medicine and rehabilitation (Collaborations Aim); 3) Coordinating a pilot funding program to support novel lines of Regenerative Rehabilitation research (Pilot Funding Aim); 4) Developing and validating technologies to advance the measurement and use of the regenerative rehabilitation programs (Technology Aim); 5) Promoting our center’s expertise to a broad community of trainees, investigators, and clinicians (Promotion Aim); 6) Carefully monitoring and evaluating the effectiveness of our program will ensure that we are successful in achieving our goals (Quality Control Aim). Administrative note: In the preparation of this proposal, we made every effort to present a comprehensive and detailed plan for achieving our goals while minimizing redundancy. Therefore, in multiple places, we refer the reader to specific components of the application, rather than repeating text. We appreciate the time and effort the reviewers devote to the evaluation of the proposals.  Sincerely, Fabrisia, Tom and Mike PROJECT NARRATIVE  Regenerative Rehabilitation is the integration of principles and approaches across the fields of rehabilitation science and regenerative medicine. The integration of these two fields will increase the efficiency of interventions designed to optimize physical functioning to the benefit of a wide range of individuals with disabilities. The Alliance for Regenerative Rehabilitation Research & Training (AR3T) 2.0 will build on the momentum gained over the first cycle of funding with the goal of continuing to illuminate and seize opportunities to expand scientific knowledge, expertise and methodologies in the domain of Regenerative Rehabilitation.",Alliance for Regenerative Rehabilitation Research & Training 2.0 (AR3T 2.0),9967689,P2CHD086843,"['Accountability', 'Activities of Daily Living', 'Age', 'Attention', 'Awareness', 'Basic Science', 'Biocompatible Materials', 'Clinical', 'Collaborations', 'Communities', 'Congenital Abnormality', 'Country', 'Data Analyses', 'Development', 'Disabled Persons', 'Discipline', 'Disease', 'Documentation', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'In Vitro', 'Incubators', 'Individual', 'Injury', 'Intervention', 'Investigation', 'Journals', 'Knowledge', 'Laboratories', 'Machine Learning', 'Marketing', 'Measurement', 'Mechanics', 'Mentors', 'Methodology', 'Methods', 'Mission', 'Monitor', 'Natural regeneration', 'Organ', 'Performance', 'Physical Function', 'Pre-Clinical Model', 'Preparation', 'Process', 'Quality Control', 'Reader', 'Regenerative Medicine', 'Rehabilitation therapy', 'Research', 'Research Design', 'Research Personnel', 'Research Training', 'Resources', 'Science', 'Scientist', 'Series', 'Signal Transduction', 'Specialist', 'Stimulus', 'Structure', 'Systems Analysis', 'Technology', 'Text', 'Time', 'Tissues', 'Training', 'Trauma', 'Treatment Efficacy', 'Update', 'career', 'effectiveness evaluation', 'falls', 'functional restoration', 'gait examination', 'healing', 'injured', 'innovation', 'interest', 'investigator training', 'multidisciplinary', 'new technology', 'novel', 'novel strategies', 'pre-clinical', 'programs', 'regenerative', 'regenerative therapy', 'rehabilitation research', 'rehabilitation science', 'repaired', 'response', 'sabbatical', 'social media', 'success', 'symposium', 'technological innovation', 'therapy design', 'tissue regeneration', 'webinar']",NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P2C,2020,1059198,-0.010588452932301864
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10050843,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk stratification', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,759918,-0.0022645321724801453
"Patient Oriented Research and Mentorship and Training in Functional Neuroimaging of Obsessive-Compulsive Disorder ABSTRACT  This K24 Mid-Career Investigator Career Development Award seeks support for training and mentorship for Christopher Pittenger, MD, Ph.D., a well-established, tenured Associate Professor in Psychiatry at Yale University, Director of the Yale OCD Research Clinic, and Assistant Chair for Translational Research in the Department of Psychiatry. Dr. Pittenger leads a robust patient-oriented research (POR) research program, which is integrated with his basic/translational lab-based research and has produced important new insights into the neurobiological underpinnings and novel treatment avenues for obsessive-compulsive disorder (OCD) and Tourette syndrome (TS). Dr. Pittenger has a long-standing commitment to mentorship; most notably, he is Co-Director of the Neuroscience Research Training Program (NRTP), the research track within the Yale psychiatry residency.  The training plan supported by this grant will allow Dr. Pittenger to increase his skills in quantitative analysis, with a focus on advanced statistical methods and on the design and analysis of fMRI studies. These are areas in which he already has active research with expert collaborators; the aim of the proposed training plan is to enhance his own proficiency to make him a more effective collaborator and mentor in these important domains. Additional training is focused on his own abilities as a mentor and leader, with the goal of increasing his efficacy in the management of his own research groups and in effective and individualized mentorship. These training activities will take place in the context of two NIMH- funded research studies. The first, R01 MH116038, is a recently funded grant on which Dr. Pittenger is co-PI with his close collaborator Alan Anticevic and deploys cutting-edge imaging technology and data analysis approaches to examine brain network connectivity parallels and predictors of therapeutic response to pharmacotherapy in OCD. The second, R01 MH100068, is a grant with collaborator Michelle Hampson that is developing real-time fMRI neurofeedback as a probe and potential treatment for OCD, with promising early results. These two exciting projects provide a fruitful vehicle for the proposed training in statistics and neuroimaging.  Dr. Pittenger will devote substantial time to mentoring under this award. One major focus will be the NRTP; the plan is for him to take over as Director of this program over the next few years, and to take the lead in the next resubmission of our T32 grant in 2022. Support of this increased mentorship effort is a second major motivation for the current application. Dr. Pittenger will also provide mentorship to students, postdocs, and junior faculty in his own research program and throughout the Department of Psychiatry  Together, these integrated plans for training, research, and mentorship will support a well-established mid-career investigator whose robust research program is producing important insights into the neurobiology and treatment of OCD and TS, and whose dedicated mentorship efforts are helping to establish a new generation of translationally grounded patient-oriented researchers in psychiatric neuroscience. NARRATIVE This K24 Mid-Career Investigator Career Development Award in Patient-Oriented Research seeks support for Christopher Pittenger, a tenured Associate Professor of Psychiatry at Yale University and Director of the Yale OCD Research Clinic; training and mentorship activities supported by this award will take place in the context of two NIMH-funded projects, R01 MH100068 and R01 MH116038. The Award will support Dr. Pittenger in a robust training plan aimed at increasing his skills in statistics and in the design and analysis of fMRI experiments, with the goal of making him a more effective collaborator and mentor in these domains. The robust Mentorship Plan supports a range of trainees, from high school students through junior faculty, with a particular focus on the research track with the Yale Psychiatry Residency, of which Dr. Pittenger will become the Director in the coming years.",Patient Oriented Research and Mentorship and Training in Functional Neuroimaging of Obsessive-Compulsive Disorder,9871479,K24MH121571,"['Achievement', 'Adult', 'Area', 'Award', 'Brain', 'Clinic', 'Clinical', 'Code', 'Collaborations', 'Data', 'Data Analyses', 'Department chair', 'Disease', 'Dissociation', 'Doctor of Philosophy', 'Double-Blind Method', 'Drug Exposure', 'Faculty', 'Feedback', 'Fluoxetine', 'Fostering', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Funding', 'Future', 'Generations', 'Gilles de la Tourette syndrome', 'Goals', 'Grant', 'Health Sciences', 'High School Student', 'Human', 'Image', 'Imaging technology', 'Individual', 'Infrastructure', 'K-Series Research Career Programs', 'Lead', 'Leadership', 'Linux', 'Machine Learning', 'Mentors', 'Mentorship', 'Motivation', 'National Institute of Mental Health', 'Neurobiology', 'Neurosciences', 'Neurosciences Research', 'Obsessive-Compulsive Disorder', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Physicians', 'Positioning Attribute', 'Postdoctoral Fellow', 'Preceptorship', 'Prediction of Response to Therapy', 'Protocols documentation', 'Psychiatry', 'Publishing', 'Pythons', 'Research', 'Research Ethics', 'Research Personnel', 'Research Project Grants', 'Residencies', 'Rest', 'Scanning', 'Scientist', 'Services', 'Statistical Methods', 'Structure', 'Students', 'Time', 'Training', 'Training Activity', 'Training Programs', 'Training Support', 'Translational Research', 'Universities', 'Work', 'base', 'career', 'career development', 'connectome', 'design', 'disorder control', 'experimental study', 'graph theory', 'innovation', 'insight', 'mid-career faculty', 'neurofeedback', 'neuroimaging', 'next generation', 'novel', 'patient oriented', 'patient oriented research', 'predicting response', 'programs', 'research study', 'simulation', 'skills', 'statistics', 'success', 'symptomatic improvement', 'symptomatology', 'teacher mentor']",NIMH,YALE UNIVERSITY,K24,2020,181291,-0.0438075069936383
"Mental Health Research Network III Practice-based research has the potential to dramatically improve the speed, efficiency, relevance, and impact of mental health clinical and services research. Realizing those gains will require a practice-based research network capable of anticipating and adapting to changes in mental health care delivery, including: • Research fully embedded in real-world practice • Alignment of research goals with priorities of patient and health system stakeholders • Large-scale data infrastructure available for rapid analysis • A culture of trust and transparency to facilitate collaborative learning and improvement We propose to expand the existing Mental Health Research Network (MHRN) to include 14 research centers embedded in health systems serving a combined population of over 25 million patients in 16 states. MHRN infrastructure will be enhanced to support a next-generation practice-based network, including: • Increased engagement of patients, health system leaders, and other stakeholders in network governance • An expanded public, open-source library of software tools and other technical resources • More formal processes for conducting feasibility pilot projects and rapid response to stakeholder queries • Expanded outreach to external stakeholders and research partners This Overall application requests support for an Administrative Core, a Methods core, and four research projects. The Administrative Core will support all network activities, including ongoing affiliated research projects, new research projects proposed in this application, and new projects developed during the coming funding cycle. An Organizational Unit will include executive leadership, governance, strategic planning, and responsibility for financial management and regulatory compliance. An Outreach and External Collaboration Unit will be responsible for stakeholder engagement and developing collaborations with new research partners. An Emerging Issues Unit will support timely responses to emerging scientific and public health questions. The Methods Core will support optimal use of existing methods across all network research and develop new methods to address high-priority public health questions. An Informatics Unit will maintain and improve network data infrastructure and disseminate network-developed informatics resources. A Scientific Analysis Unit will focus on improving the methodologic rigor of network projects and development of innovative analytic methods driven by stakeholder needs. These infrastructure resources will support two Signature research projects (a pragmatic trial of eHealth interventions for perinatal depression and a real-time evaluation of new medications to address suicide risk) and two Pilot research projects (an investigation of stakeholder perspectives on implementation of suicide risk prediction models and a pilot trial of outreach to reduce disparities in depression treatment initiation). The Mental Health Research Network will conduct practice-based mental health research in large healthcare systems serving over 25 million patients in 16 states. Infrastructure will include an Administrative Core responsible for governance, communications, and fostering research collaborations as well as a Methods Core responsible for advancing methods in mental health informatics and analytic methods. Four research projects will leverage this infrastructure to address questions of high priority to patient, health system, and policy stakeholders.",Mental Health Research Network III,10021725,U19MH121738,"['Address', 'Antipsychotic Agents', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Complex', 'Development', 'Effectiveness', 'Evaluation', 'Feeling suicidal', 'Fostering', 'Funding', 'Future', 'Glutamate Receptor', 'Goals', 'Health Services Research', 'Health system', 'Healthcare Systems', 'Hybrids', 'Informatics', 'Infrastructure', 'Integrated Health Care Systems', 'Investigation', 'Leadership', 'Learning', 'Libraries', 'Machine Learning', 'Mental Depression', 'Mental Health', 'Methodology', 'Methods', 'National Institute of Mental Health', 'Network Infrastructure', 'Network-based', 'Observational Study', 'Observational epidemiology', 'Occupational activity of managing finances', 'Organization administrative structures', 'Patients', 'Pharmaceutical Preparations', 'Pilot Projects', 'Policies', 'Population', 'Practice based research', 'Primary Health Care', 'Process', 'Public Health', 'Public Health Informatics', 'Qualitative Research', 'Reaction Time', 'Recording of previous events', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resource Informatics', 'Resources', 'Site', 'Software Tools', 'Speed', 'Strategic Planning', 'Suicide prevention', 'Testing', 'Time', 'Trust', 'Youth', 'analytical method', 'base', 'data infrastructure', 'design', 'disparity reduction', 'eHealth', 'effectiveness evaluation', 'first episode psychosis', 'health care delivery', 'implementation science', 'improved', 'innovation', 'large scale data', 'member', 'next generation', 'open source', 'outreach', 'patient engagement', 'patient population', 'patient portal', 'perinatal intervention', 'peripartum depression', 'pilot trial', 'practice-based research network', 'pragmatic trial', 'programs', 'public health priorities', 'racial and ethnic disparities', 'response', 'risk prediction model', 'suicidal behavior', 'suicidal risk', 'treatment-resistant depression']",NIMH,KAISER FOUNDATION RESEARCH INSTITUTE,U19,2020,2052966,-0.03037469604711896
"Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt Founded in 1965 as one of the original Intellectual and Developmental Disorders Research Centers (IDDRC), the Vanderbilt Kennedy Center (VKC) IDDRC serves as the central nexus across Vanderbilt for interdisciplinary research, communication, and training in intellectual and developmental disabilities (IDD). The VKC IDDRC serves as a trans-institutional institute that brings together over 200 faculty from 38 departments in 10 schools at Vanderbilt. The VKC’s mission to facilitate discoveries that inform best practices to improve the lives of people with IDD and their families. This mission is met by leveraging our outstanding institutional resources and support, partnering with disability communities, and capitalizing on synergistic interactions across the VKC’s federally-designated centers: the VKC IDDRC, a University Center of Excellence in Developmental Disabilities and a Leadership Education in Neurodevelopmental Disabilities program. The IDDRC as the centerpiece of the VKC is the foundational organizing structure that creates a “Center culture” wherein research and discovery permeates the VKC’s broader training and service activities, thus enhancing the translational research goals of the IDDRC. Demonstrable IDDRC success includes 976 investigator- authored publications and robust NIH funding to Vanderbilt to support IDD-related research ($52.6M in FY20). Harnessing and leveraging this trans-institutional strength to focus on unique challenges in IDD, the overarching goal of the next phase of the IDDRC is to develop precision care for IDD by providing infrastructure and scientific leadership to enable rapid translation of basic discoveries into high- impact IDD interventions and treatments. Three global Aims guide the IDDRC’s work. Aim 1 provides core services to enable and disseminate impactful research on individualizing treatments based upon the causes, mechanisms, and contributing co-morbid sequelae of IDD; Aim 2 focuses on incorporating innovative methods and approaches to enhance multidisciplinary IDD research; and Aim 3 proposes to conduct a signature research project to improve the precision use of antipsychotic medication in people with autism. Across these Aims and five Cores supported by the IDDRC (Administrative, Clinical Translational, Translational Neuroscience, Behavioral Phenotyping, and Data Sciences), three themes permeate our work: (1) recruitment of highly-skilled researchers not currently conducting IDD research (non-traditional researchers); (2) inclusion of IDD participants into research studies that currently do not include IDD (non-traditional subjects); and (3) incorporation of novel scientific approaches and methods (non-traditional approaches). Our IDDRC is ideally posed to enable rapid discovery of precision care approaches by supporting 50 investigators leading 70 research projects (15 from NICHD) and, as highlighted by the Signature Research Project, to promote and implement generative, novel, and impactful research directions, thus meeting the NICHD’s vision of applying newly evolved technologies and approaches to rapidly accelerate the prevention and/or amelioration of IDDs. PUBLIC HEALTH RELEVANCE: As a group, intellectual and developmental disabilities, including Down syndrome and autism spectrum disorder, have dramatic effects on affected people’s and their caregiver’s lives. Unfortunately, there remains a lack of understanding about what causes these disabilities and, critically, how to treat them with targeted therapies. The Vanderbilt Kennedy Center’s Intellectual and Developmental Disabilities Research Center serves as the hub for Vanderbilt’s research efforts focusing on improving the lives of people with intellectual and developmental disabilities by understanding the causes of these disorders and developing and testing therapies tailored to each individual’s precise needs.",Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt,10085550,P50HD103537,"['Academic Medical Centers', 'Affect', 'Antipsychotic Agents', 'Basic Science', 'Behavioral', 'Biomedical Research', 'Caregivers', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communication', 'Communities', 'Computerized Medical Record', 'Data', 'Data Science', 'Development', 'Developmental Disabilities', 'Diagnosis', 'Disease', 'Disease model', 'Down Syndrome', 'Education', 'Evaluation', 'Faculty', 'Family', 'Foundations', 'Funding', 'Future', 'Gap Junctions', 'Genotype', 'Goals', 'Image', 'Individual', 'Infrastructure', 'Institutes', 'Intellectual and Developmental Disabilities Research Centers', 'Intellectual functioning disability', 'Interdisciplinary Study', 'Intervention', 'Leadership', 'Longevity', 'Machine Learning', 'Medical Records', 'Methods', 'Mission', 'Modeling', 'National Institute of Child Health and Human Development', 'Neurodevelopmental Disability', 'Obesity', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Phase', 'Pilot Projects', 'Policy Research', 'Prevention', 'Problem behavior', 'Publications', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Sampling', 'Schools', 'Series', 'Services', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Universities', 'Vision', 'Weight Gain', 'Work', 'autism spectrum disorder', 'base', 'behavioral phenotyping', 'clinical translation', 'comorbidity', 'cost effective', 'developmental disease', 'disability', 'drug-induced weight gain', 'experience', 'image processing', 'implementation science', 'improved', 'individualized medicine', 'innovation', 'large datasets', 'lectures', 'meetings', 'multidisciplinary', 'novel', 'personalized approach', 'personalized care', 'personalized medicine', 'population based', 'pragmatic trial', 'predictive modeling', 'programs', 'public health relevance', 'recruit', 'research study', 'success', 'targeted treatment', 'translational neuroscience', 'trial comparing']",NICHD,VANDERBILT UNIVERSITY MEDICAL CENTER,P50,2020,1387605,-0.015582196503487609
