text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"BRAIN Initiative: Assessing development of event-related cortical network dynamics Project Summary The Child Mind Institute’s Healthy Brain Network (HBN) is an ongoing initiative focused on creating and sharing a biobank of brain and behavioral data now being collected from 10,000 New York City area children and adolescents (ages 5-21). In response to RFA-MH-120-20 (‘Integration and analysis of BRAIN Initiative data’), we will collect their high-density EEG and continuous eye tracking data, recorded from now over 3,000 young participants while they watch movies and rest, plus participant MR head images, to create a developmental EEG and eye tracking data and analysis resource within the BRAIN Initiative-funded NEMAR portal (NEMAR.org) to the OpenNeuro human neuroimaging resource (OpenNeuro.org). We will store the data in accord with the latest advances in the Brain Imaging Data Standard (BIDS), including innovative co-registration of eye-tracking information and EEG, extraction of anonymized Leadfield matrices for advanced participant-based source localization, and Hierarchical Event Descriptor (HED) annotation of movie and eye tracking events in the data, to enable sophisticated exploratory and model-based analyses by any user of the Neuroscience Gateway (nsgportal.org), which offers processing of complex tasks on the U.S. XSEDE computer network (XSEDE.org) for users of standard neuroscience software. We propose to build on the large existing and readily extensible set of tools for analysis, modeling, and visualization of human EEG data in the world leading EEGLAB software environment for electrophysiological signal processing we develop and maintain (5R01-NS047293-15) (sccn.ucsd.edu/eeglab), to process the large corpus of innovative EEG and synchronous eye tracking data recorded during movie viewing by the HBN project. We will then apply source-resolved event-related analysis models to cortical EEG network dynamics associated with movie watching, will model their changes across development, and will compare the time course of these changes with changes in eye gaze patterns at the same age levels. We will use cortical source-resolved, event-related functional EEG connectivity measures implemented in the Source Information Flow Toolbox (SIFT) to model event-related EEG dynamics of individual participants, and will apply advanced hierarchical Bayes-based methods and statistical assessment to perform statistical testing on subject group models. We will study the development of event-related cortical information flow during movie watching and rest across the large HBN developmental (5-21 years) sample, and compare it to the developmental trajectory of changes in eye gaze patterns during movie viewing. Project narrative. The Child Mind Institute’s Healthy Brain Network is collecting and sharing a biobank of brain and behavioral data from 10,000 New York City area children and adolescents (ages 5-21). We will upload high-density EEG and continuous eye tracking data from this project recorded during movie watching and rest to an integrated data, tools, and computer analysis resource, the NEMAR portal to the BRAIN Initiative OpenNeuro human neuroimaging resource. We will then image cortical EEG network dynamics during movie watching, and will study its changes across development.",BRAIN Initiative: Assessing development of event-related cortical network dynamics,10190670,RF1MH125934,"['Adolescent', 'Affective', 'Age', 'Archives', 'Area', 'BRAIN initiative', 'Bayesian learning', 'Behavioral', 'Brain', 'Brain imaging', 'Child', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Store', 'Descriptor', 'Development', 'Developmental Course', 'Diet', 'Electroencephalography', 'Electrophysiology (science)', 'Environment', 'Event', 'Exhibits', 'Eye', 'Family', 'Film', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Goals', 'Head', 'High Performance Computing', 'Human', 'Image', 'Individual', 'Institutes', 'Life Style', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Mind', 'Modeling', 'Monoclonal Antibody R24', 'Neurosciences', 'New York City', 'Participant', 'Pathway Analysis', 'Pattern', 'Phenotype', 'Process', 'Psychopathology', 'Pythons', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Saccades', 'Sampling', 'Signal Transduction', 'Source', 'System', 'Testing', 'Time', 'Video Recording', 'Videotape', 'Visualization', 'Voice', 'actigraphy', 'animation', 'archive data', 'archived data', 'base', 'biobank', 'brain health', 'computer network', 'computing resources', 'data archive', 'data portal', 'data resource', 'data standards', 'data tools', 'density', 'digital', 'experimental study', 'fitness', 'gaze', 'independent component analysis', 'innovation', 'movie', 'multimodality', 'network models', 'neuroimaging', 'open source', 'psychiatric symptom', 'recruit', 'response', 'sample fixation', 'secondary analysis', 'signal processing', 'source localization', 'tool', 'visual tracking']",NIMH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",RF1,2021,1114819
"Center for Open Bioimage Analysis Project Summary  The Center for Open Bioimage Analysis will serve the cell biology community’s growing need for sophisticated software for light microscopy image analysis. Quantitative image analysis has become an indispensable tool for biologists using microscopy throughout basic biological and biomedical research.  Quantifying images is now a critical, widespread need as imaging experiments continue to grow in scale, size, dimensionality, scope, modality, and complexity. Many biologists are missing out on the quantitative bioimaging revolution due to lack of effective algorithms and/or usable software for their needs, or lack of access to training. The Center brings together the Carpenter laboratory at the Broad Institute and the Eliceiri laboratory at the University of Wisconsin­Madison, and in doing so brings together the two most popular open source bioimage analysis projects, ImageJ (including ImageJ2 and FIJI) and CellProfiler. Through the collaborative development and dissemination of open source image analysis software, as well as training events and resources, the Center will empower thousands of researchers to apply advanced analytics in innovative ways to address new experimental areas.  Building on the team’s expertise developing algorithms and user­friendly software for use in biology under real­world conditions, the Center will focus on two Technology Research and Development (TR&D) projects: deep learning­based image processing, and accessibility of image­processing algorithms for biologists. This work will not occur in isolation at the Center; rather, the Center will nucleate a larger community working on these two areas and serve as a catalyst and organizing force to create software and resources shared by all.  The Driving Biological Projects (DBPs) will serve a major role in driving the TR&D work: our teams are accustomed to working deeply and iteratively on problems side by side and with frequent feedback from biologists. This will ensure that important cell biological problems drive the work of the Center. The DBPs reflect tremendous variety in terms of biological questions, model systems, imaging modalities, and researcher expertise and will ensure robustness of our tools for the widest possible impact on the community. Continuing the teams’ track record with ImageJ and CellProfiler, two mature open source bioimage analysis software projects critical to the work of biologists worldwide, the Center will also assist and train biologists in applying the latest computational techniques to important biological problems involving images.  In short, the need for robust, accurate, and readily usable software is more urgent than ever. The Center for Open Bioimage Analysis will serve as a hub for pioneering new computational strategies for diverse biological problems, translating them into user­friendly software, further developing ImageJ and CellProfiler, and training the biological community to apply advanced software to important and diverse problems in cell biology. Project Narrative Biologists studying a huge variety of diseases and basic biological processes need software to measure cells, tissues, and organisms in microscopy images. We will create the Center for Open Bioimage Analysis which will catalyze the scientific community, creating resources, free software, and training that allow biologists to analyze images using deep learning and other new image processing algorithms, offering improved accuracy, convenience, and reproducibility.",Center for Open Bioimage Analysis,10061631,P41GM135019,"['Address', 'Algorithmic Software', 'Algorithms', 'Area', 'Automobile Driving', 'Benchmarking', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Cellular Structures', 'Cellular biology', 'Characteristics', 'Collaborations', 'Communities', 'Complex', 'Computational Technique', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Data Science', 'Development', 'Dimensions', 'Disease', 'Educational workshop', 'Ensure', 'Event', 'Feedback', 'Hand', 'Image', 'Image Analysis', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Measures', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Modernization', 'Organism', 'Organoids', 'Reproducibility', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Role', 'Savings', 'Scientist', 'Side', 'Software Engineering', 'System', 'Technology', 'Time', 'Tissues', 'Training', 'Translating', 'Universities', 'Wisconsin', 'Work', 'advanced analytics', 'algorithmic methodologies', 'base', 'bioimaging', 'biological research', 'biological systems', 'catalyst', 'deep learning', 'experimental study', 'hackathon', 'image processing', 'imaging modality', 'improved', 'innovation', 'light microscopy', 'microscopic imaging', 'next generation', 'novel', 'open source', 'quantitative imaging', 'research and development', 'skills', 'symposium', 'technology research and development', 'tool', 'user friendly software', 'user-friendly']",NIGMS,"BROAD INSTITUTE, INC.",P41,2021,1261742
"Scalable Software for Distributed Processing and Visualization of Multi-Site MEG/EEG Datasets Project Summary During the past three decades non-invasive functional brain imaging has developed immensely in terms of measurement technologies, analysis methods, and innovative paradigms to capture information about brain function both in healthy and diseased individuals. Although functional MRI (fMRI) has become very useful, it only provides indirect information about neuronal activity through the neurovascular coupling with a limited temporal resolution. Magnetoencephalography (MEG) and electroencephalography (EEG) remain the only available noninvasive techniques capable of directly measuring the electrophysiological activity with a millisecond resolution. During the past eight years we have developed, with NIH support, the MNE-Python software, which covers multiple methods of data preprocessing, source localization, statistical analysis, and estimation of functional connectivity between distributed brain regions. All algorithms and utility functions are implemented in a consistent manner with well-documented interfaces, enabling users to create M/EEG data analysis pipelines by writing Python scripts. To further extend our software to meet the needs of a growing user base and reflect recent developments in the MEG/EEG field we will pursue three specific Aims. In Aim 1 we will: (i) Create an all-embracing suite of noise cancellation tools incorporating and extending methods present in different MEG systems; (ii) Implement device independent methods for head-movement determination and compensation on the basis of head movement data recorded during a MEG session; (iii) Develop methods for automatic tagging of artifacts using machine learning approaches. In Aim 2 our focus is to extend the software to make modern distributed computing resources easily usable in processing and to allow for remote visualization without the need to move large amounts of data across the network. Finally, in Aim 3, we will continue to develop MNE-Python using best programming practices ensuring multiplatform compatibility, extensive web-based documentation, training and forums, and hands-on training workshops. As a result of these developments the MNE-Python will be able to effectively process large number of subjects and huge amounts data ensuing and from multi-site studies harmoniously across different MEG/EEG systems. Narrative MEG and EEG can be used to understand and diagnose abnormalities underlying a wide range neurological and psychiatric illnesses including epilepsy, schizophrenia, obsessive-compulsive disorder, autism spectrum disorders, and Alzheimer's disease, as well as cognitive deficits such as delayed acquisition of language. However, widespread use of these methods especially in large populations has been problematic because of the lack of well-established analysis approaches, which map the sensor data into the brain space for detailed temporal, spatial, and connectivity analysis. This research will provide well-documented and tested novel analysis software to promote both basic neuroscience and clinical research applications using MEG and EEG.",Scalable Software for Distributed Processing and Visualization of Multi-Site MEG/EEG Datasets,10175064,R01NS104585,"['Adult', 'Age', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Brain imaging', 'Brain region', 'Clinical Research', 'Cloud Computing', 'Code', 'Cognitive deficits', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Scientist', 'Data Set', 'Databases', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Documentation', 'Ecosystem', 'Educational workshop', 'Electroencephalography', 'Electrophysiology (science)', 'Ensure', 'Epilepsy', 'Experimental Designs', 'Financial compensation', 'Functional Magnetic Resonance Imaging', 'Guidelines', 'Head', 'Head Movements', 'Hour', 'Human', 'Individual', 'Laboratories', 'Language Development', 'Link', 'Machine Learning', 'Magnetoencephalography', 'Maintenance', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modernization', 'Morphologic artifacts', 'Neurologic', 'Neurons', 'Neurosciences Research', 'Noise', 'Obsessive-Compulsive Disorder', 'Online Systems', 'Population', 'Process', 'Pythons', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Schizophrenia', 'Science', 'Scientist', 'Site', 'Statistical Data Interpretation', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Writing', 'analysis pipeline', 'autism spectrum disorder', 'base', 'cloud based', 'cluster computing', 'computing resources', 'data acquisition', 'data analysis pipeline', 'data exchange', 'falls', 'human data', 'innovation', 'large datasets', 'millisecond', 'multithreading', 'neurovascular coupling', 'novel', 'open source', 'pedagogy', 'sensor', 'sensor technology', 'software development', 'source localization', 'symposium', 'temporal measurement', 'tool', 'verification and validation']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,R01,2021,543993
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10228757,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2021,349146
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",10173765,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'automated segmentation', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'efficacy evaluation', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2021,256578
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,10186744,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2021,377473
"Shape Analysis Toolbox: From medical images to quantitative insights of anatomy PROJECT SUMMARY Three-dimensional shape lies at the core of understanding the physical objects that surround us. The Shape AnaLysis Toolbox (SALT) was created to be a dissemination vehicle for advanced shape modeling and analysis methodology as an open-source, comprehensive and freely distributed software. Over the past four years, we have been successful in increasing the ease of use and effectiveness of state-of-the-art shape analysis methodology for biomedical researchers in need of such techniques. We now propose necessary and novel enhancements to our methods and our dissemination model in order to continue maximizing the success of SALT. We will also modify the architecture of SALT to better integrate biomedical imaging research workflows by improving the efficiency and scripting capabilities so SlicerSALT can be deployed in batch mode for large-scale sequential computations. We will also shift our focus from shape modeling into state-of-the-art statistical shape analysis methodologies, necessary to serve clinical applications and to increase the interpretability of shape biomarkers. We will continue to disseminate novel example applications that best demonstrate how to use our tools to perform impactful research and will provide fully digital documentation for user support. The ultimate goal of SlicerSALT is to maximize the potential benefits of the geometric information contained in medical data and to expand its use beyond simple visualization to support clinical research. PROJECT NARRATIVE Slicer Shape AnaLysis Toolbox (SALT) was developed as an open-source, free comprehensive software that allows biomedical scientists to precisely locate shape changes in their imaging studies. This proposal is designed to increase the continued success of SALT by recognizing that shape models and dynamic anatomical changes are challenging to interpret despite quantification of the geometry of physical objects. We will address this need by incorporating state-of-the-art and interpretable shape statistics methodology into SALT and new driving biological problems to illustrate their utility while continuing to provide effective user support.",Shape Analysis Toolbox: From medical images to quantitative insights of anatomy,10426508,R56EB021391,"['3-Dimensional', 'Accounting', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Automobile Driving', 'Biological', 'Biological Markers', 'Biomedical Research', 'Brain', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Consultations', 'Data', 'Development', 'Disease', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Fostering', 'Funding', 'Geometry', 'Goals', 'Image Analysis', 'Infrastructure', 'Longitudinal Studies', 'Measures', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Nature', 'Online Systems', 'Pediatric cardiology', 'Phase', 'Population', 'Process', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Shapes', 'Software Tools', 'Statistical Methods', 'Structure', 'Surveys', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Time', 'Training', 'Ultrasonography', 'Use Effectiveness', 'Variant', 'Visualization', 'base', 'bioimaging', 'biomedical scientist', 'clinical application', 'complex data', 'computer science', 'deep learning', 'design', 'digital', 'efficacy evaluation', 'fetal', 'geometric structure', 'imaging study', 'improved', 'innovation', 'insight', 'large scale data', 'longitudinal analysis', 'new technology', 'novel', 'open source', 'outreach', 'shape analysis', 'statistics', 'success', 'tool', 'usability', 'web site']",NIBIB,"KITWARE, INC.",R56,2021,436264
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10225620,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'risk stratification', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2021,707970
"BrainStorm: Highly Extensible Software for Advanced Electrophysiology and MEG/EEG Imaging Project Summary Electrophysiological recordings in humans and animals play an essential role in developing an understanding of the human brain. Signal recording technology spans the entire scale from invasive microelectrode single-unit recordings, through mesoscale macroelectrode measures of local field potentials, to whole-brain monitoring through measurement of scalp potentials (EEG) and extracranial magnetic fields (MEG). Analysis of these data presents a host of challenges, from low level noise removal and artifact rejection to sophisticated spatio-temporal modeling and statistical inference. The multidisciplinary neuroscience research community has an ongoing need for validated and documented open-source software to perform this analysis and to facilitate reproducible and large-scale research involving electrophysiological data. This proposal describes our plans to continue to develop and support Brainstorm, open-source software that meets this need. Brainstorm is a Matlab/Java multi-platform (Linux, MacOS, Windows) software package for analysis and visualization of electrophysiological data. The software is extensively documented through a series of detailed tutorials and actively supported through a user forum and a mailing list. Over the past 8 years we have registered 16,000 distinct users, provided hands on instruction to 1,200 trainees, and the software has been used and cited in ~600 journal papers. Brainstorm includes tools for importing MEG/EEG, intracranial EEG, animal electrophysiology, and near-infrared spectroscopy (NIRS) data from multiple vendors, extensive interactive features for data preprocessing, selection and visualization, coregistration to volume and surface MRIs and atlases, forward and inverse mapping of cortical current density, time-series and connectivity analysis, and a range of statistical tools. Data can be analyzed through a graphical interface or through scripted pipelines. The current proposal represents a plan to extend Brainstorm in a manner that leverages the unique features of our software and addresses important needs for large-scale data analysis. In this project we will continue to extend and support our software through the following three specific aims: (i) we will harness recent developments in distributed and shared data and high performance computing resources, together with standardization of data organization, to facilitate large-scale, reproducible analysis of electrophysiological data. (ii) We will also address the need for improved modeling resulting from the increasing use of both invasive recordings and direct brain stimulation through development of new modeling software for accurate computation of the intracranial electromagnetic fields produced by brain stimulation and neuronal activation. (iii) Finally, we will continue to add new functionality and to support the software through in-person training, online forums, documentation and other resources. Project Narrative Magnetoencephalography (MEG) and Electroencephalography (EEG) are absolutely non-invasive brain imaging tools, which provide information on the spatial distribution and precise temporal orchestration of human brain activity. In addition to basic neuroscience research, MEG and EEG can be also used to understand and diagnose abnormalities underlying a wide range neurological and psychiatric illnesses, including epilepsy, schizophrenia, obsessive-compulsive disorder, autism spectrum disorders, and Alzheimer's disease, as well as cognitive deficits such as delayed acquisition of language. The neuroscience research community has an ongoing need for validated and documented open-source software to perform these analyses and to facilitate reproducible and large-scale research involving electrophysiological data. This proposal describes our plans to continue to develop and support Brainstorm, open-source software that meets these needs with well-documented and tested novel analyses using MEG and EEG in combination with anatomical MRI and intracranial EEG data.",BrainStorm: Highly Extensible Software for Advanced Electrophysiology and MEG/EEG Imaging,10113609,R01EB026299,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Animals', 'Archives', 'Area', 'Atlases', 'Basic Science', 'Benchmarking', 'Brain', 'Brain imaging', 'Clinical Research', 'Cloud Computing', 'Code', 'Cognitive deficits', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Development', 'Diagnosis', 'Documentation', 'Educational workshop', 'Electrodes', 'Electroencephalography', 'Electromagnetic Fields', 'Electromagnetics', 'Electrophysiology (science)', 'Ensure', 'Environment', 'Epilepsy', 'Excision', 'Frequencies', 'Goals', 'Grant', 'High Performance Computing', 'Human', 'Image', 'Imaging Device', 'Institution', 'Java', 'Joints', 'Journals', 'Language Development', 'Lead', 'Linux', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Maintenance', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Microelectrodes', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Near-Infrared Spectroscopy', 'Neurologic', 'Neurons', 'Neurosciences Research', 'Noise', 'Obsessive-Compulsive Disorder', 'Online Systems', 'Paper', 'Pathway Analysis', 'Pattern', 'Persons', 'Play', 'Pythons', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scalp structure', 'Schizophrenia', 'Series', 'Signal Transduction', 'Source', 'Spatial Distribution', 'Surface', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Universities', 'Vendor', 'Visualization', 'Work', 'autism spectrum disorder', 'cloud storage', 'cognitive benefits', 'computerized tools', 'computing resources', 'cortex mapping', 'data archive', 'data curation', 'data repository', 'data resource', 'data sharing', 'data standards', 'data structure', 'density', 'design', 'electric field', 'graphical user interface', 'hands on instruction', 'improved', 'interoperability', 'large datasets', 'large scale data', 'magnetic field', 'multidisciplinary', 'neuroimaging', 'novel', 'open source', 'relating to nervous system', 'response', 'spatiotemporal', 'tool']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2021,616433
"Advancing algorithms for image-based profiling Project Summary    Most laboratories studying biological processes and human disease use microscopes to image samples.  Whether in small­ or large­scale microscopy experiments, biologists increasingly need software to identify and  measure cells and other biological entities in images, to improve speed, objectivity, and/or statistical power.   The principal investigator envisions bringing transformative image analysis and machine learning algorithms  and software to a wide swath of biomedical researchers. In a decade, researchers will tackle fundamentally  new problems with quantitative image analysis, using seamless imaging workflows that have dramatic new  capabilities going beyond the constraints of human vision.  To this end, the PI will collaborate with biologists on important quantitative imaging projects that also yield  major advancements to their open­source image analysis software, CellProfiler. This versatile, user­friendly  software is indispensable for biomedical research. Launched 125,000+ times/year worldwide, it is cited in  3,400+ papers from 1,000+ laboratories, impacting a huge variety of biomedical fields via assays from counting  cells to scoring complex phenotypes by machine learning. CellProfiler evolves in an intensely collaborative and  interdisciplinary research environment that has yielded dozens of discoveries and several potential drugs.  Still, many biologists are missing out on the quantitative bioimaging revolution due to lack of effective  algorithms and usable software for their needs. In addition to maintaining and supporting CellProfiler, the team  will implement biologist­requested features, algorithms, and interoperability to cope with the changing land­  scape of microscopy experiments. Challenges include increases in scale (sometimes millions of images), size  (20+ GB images), and dimensionality (time­lapse, three­dimensional, multi­spectral). Researchers also need to  accommodate a variety of modalities (super­resolution, single­molecule, and others) and integrate image  analysis into complex workflows with other software for microscope control, cloud computing, and data mining.   The PI will also pioneer novel algorithms and approaches changing the way images are used in biology,  including: (1) a fundamental redesign of the image processing workflow for biologists, leveraging revolutionary  advancements in deep learning, (2) image analysis for more physiologically relevant systems, such as model  organisms, human tissue samples, and patient­derived cultures, and (3) data visualization and interpretation  software for high­dimensional single­cell morphological profiling. In profiling, subtle patterns of morphological  changes in cells are detected to identify causes and treatments for various diseases. We will also (4) integrate  multiple profiling data types: morphology with gene expression, epigenetics, and proteomics. Ultimately, we  aim to make perturbations in cell morphology as computable as other large­scale functional genomics data.  Overall, the laboratory’s research will yield high­impact discoveries from microscopy images, and its  software will enable hundreds of other NIH­funded laboratories to do the same, across all biological disciplines.          Public Health Relevance/Narrative    Modern microscopy experiments are increasing in scale and scope; the research will result in pioneering  computational techniques and software that will change the way microscopy images are used in biology.  Biologists will use the resulting software to tackle fundamentally new problems using quantitative image  analysis, including detecting changes in the appearance of cells that are overlooked by human vision and  studying intact organisms and human tissue rather than isolated cells. The methods will be developed in the  context of dozens of projects addressing important fundamental biological questions and world health  problems, and the resulting new functionality will be added to the team’s popular, user­friendly, open­source  image analysis software, CellProfiler.          ",Advancing algorithms for image-based profiling,10150027,R35GM122547,"['3-Dimensional', 'Address', 'Algorithmic Software', 'Algorithms', 'Animal Model', 'Appearance', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Research', 'Cell Count', 'Cells', 'Cellular Morphology', 'Cloud Computing', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Data Analyses', 'Dimensions', 'Discipline', 'Disease', 'Environment', 'Epigenetic Process', 'Funding', 'Gene Expression', 'Human', 'Image', 'Image Analysis', 'Interdisciplinary Study', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Machine Learning', 'Measures', 'Methods', 'Microscope', 'Microscopy', 'Modality', 'Modernization', 'Morphology', 'Organism', 'Paper', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Principal Investigator', 'Proteomics', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Speed', 'System', 'Time', 'Tissue Sample', 'United States National Institutes of Health', 'Vision', 'World Health', 'base', 'bioimaging', 'data mining', 'data visualization', 'deep learning', 'experimental study', 'functional genomics', 'genomic data', 'high dimensionality', 'human disease', 'human tissue', 'image processing', 'improved', 'interoperability', 'machine learning algorithm', 'microscopic imaging', 'novel', 'open source', 'public health relevance', 'quantitative imaging', 'single molecule', 'user friendly software', 'user-friendly']",NIGMS,"BROAD INSTITUTE, INC.",R35,2021,695400
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10269008,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data repository', 'data reuse', 'data sharing', 'data visualization', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2021,617911
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,10175029,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data curation', 'data format', 'data repository', 'data resource', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'large scale data', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'trustworthiness', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2021,330299
"mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment - Phase II 1 Project Summary NIH is increasing its investment in large, multi-center brain MRI studies via projects such as the recently announced BRAIN initiative. The success of these studies depends on the quality of MRIs and the resulting image measurements, regardless of sample size. Even though quality control of MRIs and corresponding measurements could be outsourced, most neuroscience studies rely on in-house procedures that combine automatically generated scores with manually guided checks, such as visual inspection. Implementing these procedures typically requires combining several software systems. For example, the NIH NIAAA- and BD2K- funded Data Analysis Resource (DAR) of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) uses XNAT to consolidate the structural, diffusion, and functional MRIs acquired across five sites, and has also developed their own custom software package to comply with study requirements for a multi-tier, quality control (QC) workflow. However, these custom, one-off tools lack support for the multi-site QC workflows that will come with the unified platform that MIQA represents: a design that supports collaboration and sharing, and strong cohesion between technologies. To improve the effectiveness of QC efforts specific to multi-center neuroimaging studies, we will develop a widely accessible and broadly compatible software platform that simplifies the creation of custom QC workflows in compliance with study requirements, provides core functionality for performing QC of medical images, and automatically generates documentation compliant with the FAIR principle, i.e., making scientific results findable, accessible, interoperable, and reusable.  Specifically, our multi-site, web-based software platform for Medical Image Quality Assurance (MIQA) will enable efficient and accurate QC processing by leveraging open-source, state-of-the-art web interface technologies, such as a web-based dataset caching system and machine learning to aid in QC processes. Users will be able to configure workflows that not only reflect the specific requirements of medical imaging studies but also minimize the time spent on labor-intensive operations, such as visually reviewing scans. Issue tracking technology will enhance communication between geographically-distributed team members, as they can easily share image annotations and receive automated notifications of outstanding QC issues. The system will be easy to deploy as it will be able to interface with various imaging storage backends, such as local file systems and XNAT. While parts of this functionality have been developed elsewhere, MIQA is unique as it provides a unified, standard interface for efficient QC setup, maintenance, and review for projects analyzing multiple, independently managed data sources.  The usefulness of this unique QC system will be demonstrated on increasing the efficiency of the diverse QC team of the multi-center NCANDA study. Narrative The goal of this proposal is to develop a web-based, multi-site, open-source platform for Medical Image Quality Assurance (MIQA) to address the QC needs of geographically diverse teams using small and large medical image-based studies alike. MIQA will enable efficient and accurate QC processing by levering state-of-the-art machine learning, data management, and web interface technologies. Our effort will minimize the time spent on labor-intensive reviews and analysis operations by supporting team-oriented reviewing that is guided by highly customizable workflows seamlessly interacting with existing data management systems.",mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment - Phase II,10183329,R44MH119022,"['3-Dimensional', 'Active Learning', 'Address', 'Adolescence', 'Alcohols', 'Archives', 'Area', 'BRAIN initiative', 'Big Data to Knowledge', 'Brain', 'Brain imaging', 'Classification', 'Collaborations', 'Communication', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Management Resources', 'Data Provenance', 'Data Set', 'Data Sources', 'Detection', 'Diffusion', 'Documentation', 'Effectiveness', 'Ensure', 'Evaluation', 'Evaluation Studies', 'FAIR principles', 'Four-dimensional', 'Funding', 'Generations', 'Geography', 'Goals', 'Human', 'Image', 'Intelligence', 'Internet', 'Investments', 'Iowa', 'Label', 'Learning', 'Licensing', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Manuals', 'Measurement', 'Medical Imaging', 'Modeling', 'Monitor', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences', 'Notification', 'Online Systems', 'Peer Review', 'Phase', 'Procedures', 'Process', 'Publications', 'Quality Control', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Scanning', 'Site', 'Structure', 'System', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'Update', 'Visual', 'Visualization', 'Work', 'Writing', 'annotation  system', 'base', 'cohesion', 'computing resources', 'cost', 'data management', 'deep learning', 'design', 'dexterity', 'image archival system', 'imaging study', 'improved', 'innovation', 'learning algorithm', 'learning strategy', 'member', 'nervous system disorder', 'neurodevelopment', 'neuroimaging', 'open source', 'operation', 'quality assurance', 'research study', 'software systems', 'success', 'three-dimensional visualization', 'tool', 'web interface']",NIMH,"KITWARE, INC.",R44,2021,797476
"Accelerating Community-Driven Medical Innovation with VTK Abstract Thousands of medical researchers around the world use VTK —the Visualization Toolkit— an open-source, freely available software development toolkit providing advanced 3D interactive visualization, image processing and data analysis algorithms. They either use VTK directly in their in-house research applications or indirectly via one of the multitude of medical image analysis and bioinformatics applications that is built using VTK: Osirix, 3D Slicer, BioImageXD, MedINRIA, SCIRun, ParaView, and others. Furthermore, VTK also provides 3D visualizations for clinical applications such as BrainLAB’s VectorVision surgical guidance system and Zimmer’s prosthesis design and evaluation platform. VTK has been downloaded many hundreds of thousands of times since its initial release in 1993. Considering its broad distribution and prevalent use, it can be argued that VTK has had a greater impact on medical research, and patient care, than any other open-source visualization package.  This proposal is in response to the multitude of requests we have been receiving from the VTK medical community. The aims are as follows:  1. Aim 1: Adaptive visualization framework: Produce an integrated framework that supports  visualization applications that balance server-side and client-side processing depending on data size,  analysis requirements, and the user platform (e.g., phone, tablet, or GPU-enabled desktop).  2. Aim 2: Integrated, interactive applications: Extend VTK to support a diversity of programming  paradigms ranging from C++ to JavaScript to Python and associated tools such as Jupyter Notebooks,  integrating with emerging technologies such as deep learning technologies.  3. Aim 3: Advanced rendering, including AR/VR: Target shader-based rendering systems and AR/VR  libraries that achieve high frame rates with minimal latency for ubiquitous applications that combine  low-cost, portable devices such as phones, ultrasound transducers, and other biometric sensors for  visually monitoring, guiding, and delivering advanced healthcare.  4. Aim 4: Infrastructure, Outreach, and Validation: Engage the VTK community and the proposed  External Advisory Board during the creation and assessment of the proposed work and corresponding  modern, digital documentation in the form of videos and interactive web-based content. Project Narrative The Visualization Toolkit (VTK) is an open source, freely available software library for the interactive display and processing of medical images. It is being used in most major medical imaging research applications, e.g., 3D Slicer and Osirix, and in several commercial medical applications, e.g., BrainLAB’s VectorVision surgical guidance system. VTK development began in 1993 and since then an extensive community of users and developers has grown around it. However, the rapid advancement of cloud computing, GPU hardware, deep learning algorithms, and VR/AR systems require corresponding advances in VTK so that the research and products that depend on VTK continue to deliver leading edge healthcare technologies. With the proposed updates, not only will existing applications continue to provide advanced healthcare, but new, innovative medical applications will also be inspired.",Accelerating Community-Driven Medical Innovation with VTK,10091434,R01EB014955,"['3-Dimensional', 'Adopted', 'Algorithmic Analysis', 'Algorithms', 'Bioinformatics', 'Biomechanics', 'Biomedical Technology', 'Biometry', 'Client', 'Cloud Computing', 'Cloud Service', 'Code', 'Communities', 'Computational Geometry', 'Computer software', 'Data', 'Data Analyses', 'Development', 'Devices', 'Documentation', 'Emerging Technologies', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Explosion', 'Foundations', 'Funding', 'Grant', 'Health Technology', 'Healthcare', 'Hybrids', 'Image Analysis', 'Industry', 'Infrastructure', 'Internet', 'Language', 'Letters', 'Libraries', 'Licensing', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modernization', 'Monitor', 'Online Systems', 'Operative Surgical Procedures', 'Patient Care', 'Prevalence', 'Process', 'Prosthesis Design', 'Publications', 'Pythons', 'Research', 'Research Personnel', 'Resources', 'Side', 'Surveys', 'System', 'Tablets', 'Techniques', 'Technology', 'Telephone', 'TensorFlow', 'Testing', 'Time', 'Training', 'Ultrasonic Transducer', 'Update', 'Validation', 'Virtual and Augmented reality', 'Visual', 'Visualization', 'Work', 'base', 'clinical application', 'cloud based', 'computerized data processing', 'cost', 'deep learning', 'deep learning algorithm', 'design', 'digital', 'health care delivery', 'image processing', 'innovation', 'interest', 'learning strategy', 'meetings', 'new technology', 'open source', 'outreach', 'point of care', 'portability', 'processing speed', 'real world application', 'response', 'sensor', 'software development', 'statistics', 'success', 'supercomputer', 'synergism', 'three-dimensional visualization', 'tool', 'trend', 'web services']",NIBIB,"KITWARE, INC.",R01,2021,498914
